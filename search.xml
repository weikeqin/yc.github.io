<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[redis高性能分析]]></title>
    <url>%2F2020%2F05%2F28%2Fredis-high-performance-analysis%2F</url>
    <content type="text"><![CDATA[redis高性能分析 References[1] redis.io/documentation[1] redis.io/topics/data-types[2] redis.io/topics/data-types-intr[3] redis-doc[4] 10万+QPS 真的只是因为单线程和基于内存？]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>cache</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis高性能实践]]></title>
    <url>%2F2020%2F05%2F26%2Fredis-high-performance-practice%2F</url>
    <content type="text"><![CDATA[在压测的时候看了一下缓存监控，看到监控上的数字震惊了，单分片每秒的出流量172MB。从来没想到过性能可以这么高。(优化前的) 分享一些高性能的知识点吧。 (1) 大多数情况存储的时候推荐使用byte redis-data-types redis-value-as-byte-vs-plain-string jvm-serializers string类型 redis底层存储的都是二进制，所以redis是二进制安全的(binary safe) 看代码的话你会发现redis存储的时候用的都是 byte[] Redis Strings are binary safe, this means that a Redis string can contain any kind of data, for instance a JPEG image or a serialized Ruby object. 对于存取对象，大多数情况建议使用 set(final byte[] key, final byte[] value) get(final byte[] key) 对于直接存储字符串的，建议直接使用 set(final String key, final String value) get(final String key) (2) 使用mset mget mset mget (3) 批量处理时优化成一次请求 批处理时可以使用 Pipeline 一次处理，减少网络开销]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>cache</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis数据类型]]></title>
    <url>%2F2020%2F05%2F25%2Fredis-data-types%2F</url>
    <content type="text"><![CDATA[Redis is not a plain key-value store, it is actually a data structures server, supporting different kinds of values. What this means is that, while in traditional key-value stores you associate string keys to string values, in Redis the value is not limited to a simple string, but can also hold more complex data structures. The following is the list of all the data structures supported by Redis, which will be covered separately in this tutorial: redis data types Binary-safe strings. Lists: collections of string elements sorted according to the order of insertion. They are basically linked lists. Sets: collections of unique, unsorted string elements. Sorted sets, similar to Sets but where every string element is associated to a floating number value, called score. The elements are always taken sorted by their score, so unlike Sets it is possible to retrieve a range of elements (for example you may ask: give me the top 10, or the bottom 10). Hashes, which are maps composed of fields associated with values. Both the field and the value are strings. This is very similar to Ruby or Python hashes. Bit arrays (or simply bitmaps): it is possible, using special commands, to handle String values like an array of bits: you can set and clear individual bits, count all the bits set to 1, find the first set or unset bit, and so forth. HyperLogLogs: this is a probabilistic data structure which is used in order to estimate the cardinality of a set. Don’t be scared, it is simpler than it seems… See later in the HyperLogLog section of this tutorial. Streams: append-only collections of map-like entries that provide an abstract log data type. They are covered in depth in the Introduction to Redis Streams. Redis keys Redis的key可以是任意类型，可以是字符串，还可以是图片，还可以是空字符串 Redis的key是二进制安全的，底层存储用的byte[] 建议key不要太大 建议key不要太小，太小会比较省内存，但是可读性、可维护性较差。 尝试坚持一个模式。 建议使用 object-type:id，比如 user:1000 redis允许最大的key大小为 512MB 官方原文如下: Redis keys are binary safe, this means that you can use any binary sequence as a key, from a string like “foo” to the content of a JPEG file. The empty string is also a valid key. A few other rules about keys: Very long keys are not a good idea. For instance a key of 1024 bytes is a bad idea not only memory-wise, but also because the lookup of the key in the dataset may require several costly key-comparisons. Even when the task at hand is to match the existence of a large value, hashing it (for example with SHA1) is a better idea, especially from the perspective of memory and bandwidth. Very short keys are often not a good idea. There is little point in writing “u1000flw” as a key if you can instead write “user:1000:followers”. The latter is more readable and the added space is minor compared to the space used by the key object itself and the value object. While short keys will obviously consume a bit less memory, your job is to find the right balance. Try to stick with a schema. For instance “object-type:id” is a good idea, as in “user:1000”. Dots or dashes are often used for multi-word fields, as in “comment:1234:reply.to” or “comment:1234:reply-to”. The maximum allowed key size is 512 MB. Redis Strings Values can be strings (including binary data) of every kind, for instance you can store a jpeg image inside a value. A value can’t be bigger than 512 MB. string 可以用 set get 命令 set k v get k set counter 100 incr counter incrby counter 50 decr counter 1 decrby counter 10 incr 是原子性的 What does it mean that INCR is atomic? That even multiple clients issuing INCR against the same key will never enter into a race condition. getset mycount “0” mset a 10 b 20 c 30 mget a b c set myKey wkq exit myKey type myKey del myKey set key wkq expire key 5 Redis Lists Redis lists are implemented via Linked Lists. Redis Lists are implemented with linked lists because for a database system it is crucial to be able to add elements to a very long list in a very fast way. Another strong advantage, as you’ll see in a moment, is that Redis Lists can be taken at constant length in constant time. Redis expires EXPIRE key seconds Set a timeout on key. After the timeout has expired, the key will automatically be deleted. A key with an associated timeout is often said to be volatile in Redis terminology. The timeout will only be cleared by commands that delete or overwrite the contents of the key, including DEL, SET, GETSET and all the *STORE commands. This means that all the operations that conceptually alter the value stored at the key without replacing it with a new one will leave the timeout untouched. For instance, incrementing the value of a key with INCR, pushing a new value into a list with LPUSH, or altering the field value of a hash with HSET are all operations that will leave the timeout untouched. The timeout can also be cleared, turning the key back into a persistent key, using the PERSIST command. If a key is renamed with RENAME, the associated time to live is transferred to the new key name. Expire accuracyIn Redis 2.4 the expire might not be pin-point accurate, and it could be between zero to one seconds out.Since Redis 2.6 the expire error is from 0 to 1 milliseconds. Expires and persistenceKeys expiring information is stored as absolute Unix timestamps (in milliseconds in case of Redis version 2.6 or greater). This means that the time is flowing even when the Redis instance is not active.For expires to work well, the computer time must be taken stable. If you move an RDB file from two computers with a big desync in their clocks, funny things may happen (like all the keys loaded to be expired at loading time).Even running instances will always check the computer clock, so for instance if you set a key with a time to live of 1000 seconds, and then set your computer time 2000 seconds in the future, the key will be expired immediately, instead of lasting for 1000 seconds. How Redis expires keysRedis keys are expired in two ways: a passive way, and an active way. A key is passively expired simply when some client tries to access it, and the key is found to be timed out. Of course this is not enough as there are expired keys that will never be accessed again. These keys should be expired anyway, so periodically Redis tests a few keys at random among keys with an expire set. All the keys that are already expired are deleted from the keyspace. Specifically this is what Redis does 10 times per second: Test 20 random keys from the set of keys with an associated expire.Delete all the keys found expired.If more than 25% of keys were expired, start again from step 1.This is a trivial probabilistic algorithm, basically the assumption is that our sample is representative of the whole key space, and we continue to expire until the percentage of keys that are likely to be expired is under 25% This means that at any given moment the maximum amount of keys already expired that are using memory is at max equal to max amount of write operations per second divided by 4. References[1] redis.io/documentation[2] redis.io/topics/data-types-intro[3] redis.io/topics/data-types[4] commands/#string[5] commands#list[6] commands#set]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch配置]]></title>
    <url>%2F2020%2F05%2F16%2Felasticsearch-setup%2F</url>
    <content type="text"><![CDATA[elasticsearch 配置笔记 Elasticsearch has three configuration files:123elasticsearch.yml for configuring Elasticsearchjvm.options for configuring Elasticsearch JVM settingslog4j2.properties for configuring Elasticsearch logging ES Server 配置ES 配置12#自动创建索引action.auto_create_index: .monitoring*,.watches,.triggered_watches,.watcher-history*,.ml* 可重新加载配置 Just like the settings values in elasticsearch.yml, changes to the keystore contents are not automatically applied to the running Elasticsearch node. Re-reading settings requires a node restart. However, certain secure settings are marked as reloadable. 12POST _nodes/reload_secure_settingsPOST _nodes/&lt;node_id&gt;/reload_secure_settings 12345curl -X POST &quot;localhost:9200/_nodes/reload_secure_settings?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;secure_settings_password&quot;: &quot;s3cr3t&quot; &#125;&apos; jvm配置 可以通过配置 jvm.options文件 或ES_JAVA_OPTS`环境变量配置es jvm 12-Xmx16g8-9:-Xmx20g ES日志配置12345678910111213141516171819202122######## Server JSON ############################appender.rolling.type = RollingFile appender.rolling.name = rollingappender.rolling.fileName = $&#123;sys:es.logs.base_path&#125;$&#123;sys:file.separator&#125;$&#123;sys:es.logs.cluster_name&#125;_server.json appender.rolling.layout.type = ESJsonLayout appender.rolling.layout.type_name = server appender.rolling.filePattern = $&#123;sys:es.logs.base_path&#125;$&#123;sys:file.separator&#125;$&#123;sys:es.logs.cluster_name&#125;-%d&#123;yyyy-MM-dd&#125;-%i.json.gz appender.rolling.policies.type = Policiesappender.rolling.policies.time.type = TimeBasedTriggeringPolicy appender.rolling.policies.time.interval = 1 appender.rolling.policies.time.modulate = true appender.rolling.policies.size.type = SizeBasedTriggeringPolicy appender.rolling.policies.size.size = 256MB appender.rolling.strategy.type = DefaultRolloverStrategyappender.rolling.strategy.fileIndex = nomaxappender.rolling.strategy.action.type = Delete appender.rolling.strategy.action.basepath = $&#123;sys:es.logs.base_path&#125;appender.rolling.strategy.action.condition.type = IfFileName appender.rolling.strategy.action.condition.glob = $&#123;sys:es.logs.cluster_name&#125;-* appender.rolling.strategy.action.condition.nested_condition.type = IfAccumulatedFileSize appender.rolling.strategy.action.condition.nested_condition.exceeds = 2GB ################################################ 设置ES log级别 -E logger.org.elasticsearch.transport=trace debug in single node 修改 elasticsearch.yml logger.org.elasticsearch.transport: info es未启动时配置 动态修改日志级别 dynamically need to adjust a logging level on an actively-running cluster. 123456PUT /_cluster/settings&#123; &quot;transient&quot;: &#123; &quot;logger.org.elasticsearch.transport&quot;: &quot;trace&quot; &#125;&#125; 修改 log4j2.properties12logger.transport.name = org.elasticsearch.transportlogger.transport.level = trace References https://www.elastic.co/guide/en/elasticsearch/reference/current/setup.html https://www.elastic.co/guide/en/elasticsearch/reference/current/targz.html https://www.elastic.co/guide/en/elasticsearch/reference/current/settings.html https://www.elastic.co/guide/en/elasticsearch/reference/current/jvm-options.html https://www.elastic.co/guide/en/elasticsearch/reference/current/secure-settings.html https://www.elastic.co/guide/en/elasticsearch/reference/current/logging.html https://www.elastic.co/guide/en/elasticsearch/reference/current/auditing-settings.html https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-settings.html https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-settings.html]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch Logstash Kibana 笔记]]></title>
    <url>%2F2020%2F05%2F16%2Felk-notes%2F</url>
    <content type="text"><![CDATA[Elastic Stack 是适用于数据采集、充实、存储、分析和可视化的一组开源工具。人们通常将 Elastic Stack 称为 ELK Stack（代指 Elasticsearch、Logstash 和 Kibana） Elasticsearch 是一个分布式的开源搜索和分析引擎，适用于所有类型的数据，包括文本、数字、地理空间、结构化和非结构化数据。 类似于MySQL，用来存储和分析数据。 Logstash可用来对数据进行聚合和处理，并将数据发送到 Elasticsearch。Logstash 是一个开源的服务器端数据处理管道，允许您在将数据索引到 Elasticsearch 之前同时从多个来源采集数据，并对数据进行充实和转换。 类似于程序，抽取解析数据并储存到ES Kibana 是一款适用于 Elasticsearch 的数据可视化和管理工具。类似于使用MySQL时用的Navicat 简单的说，LogStash用来收取解析日志并发消息写入ES，Elasticsearch用来存储和分析查询，Kibana用来查看 es下载地址 https://www.elastic.co/cn/downloads/elasticsearch logstash下载地址 https://www.elastic.co/cn/downloads/logstash kibana下载地址 https://www.elastic.co/cn/downloads/kibana filebeats elasticsearch-7.x 要求JDK版本大于等于1112345ZBMAC-C02PGMT0F:~ weikeqin1$ java -versionjava version &quot;14&quot; 2020-03-17Java(TM) SE Runtime Environment (build 14+36-1461)Java HotSpot(TM) 64-Bit Server VM (build 14+36-1461, mixed mode, sharing)ZBMAC-C02PGMT0F:~ weikeqin1$ elasticsearch安装配置启动下载123wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.7.0-linux-x86_64.tar.gztar -xzf elasticsearch-7.7.0-linux-x86_64.tar.gzcd elasticsearch-7.7.0/ 配置 修改 elasticsearch-7.7.0/config/elasticsearch.yml 12action.destructive_requires_name: truexpack.ml.enabled: true 启动12cd elasticsearch-7.7.0/./bin/elasticsearch 后台启动可以用 ./bin/elasticsearch -d 验证是否启动成功 在浏览器里输入 http://localhost:9200/?pretty 或者用 curl &#39;http://localhost:9200/?pretty&#39; 1234567891011121314151617&#123; &quot;name&quot;: &quot;192.168.0.110&quot;, &quot;cluster_name&quot;: &quot;elasticsearch&quot;, &quot;cluster_uuid&quot;: &quot;aHsnglvYQvSrJeoFR70mCQ&quot;, &quot;version&quot;: &#123; &quot;number&quot;: &quot;7.7.0&quot;, &quot;build_flavor&quot;: &quot;default&quot;, &quot;build_type&quot;: &quot;tar&quot;, &quot;build_hash&quot;: &quot;81a1e9eda8e6183f5237786246f6dced26a10eaf&quot;, &quot;build_date&quot;: &quot;2020-05-16T02:01:37.602180Z&quot;, &quot;build_snapshot&quot;: false, &quot;lucene_version&quot;: &quot;8.5.1&quot;, &quot;minimum_wire_compatibility_version&quot;: &quot;6.8.0&quot;, &quot;minimum_index_compatibility_version&quot;: &quot;6.0.0-beta1&quot; &#125;, &quot;tagline&quot;: &quot;You Know, for Search&quot;&#125; 看到类似以上结果说明es启动成功。 kibana安装配置启动下载123wget https://artifacts.elastic.co/downloads/kibana/kibana-7.7.0-linux-x86_64.tar.gz tar -zxf kibana-7.7.0-linux-x86_64.tar.gz cd kibana-7.7.0-linux-x86_64 配置 修改 kibana-7.7.0-darwin-x86_64/config/kibana.yml 12345678910# Kibana is served by a back end server. This setting specifies the port to use.server.port: 5601# Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.# The default is &apos;localhost&apos;, which usually means remote machines will not be able to connect.# To allow connections from remote users, set this parameter to a non-loopback address.server.host: &quot;localhost&quot;# The URLs of the Elasticsearch instances to use for all your queries.elasticsearch.hosts: [&quot;http://localhost:9200&quot;] 启动12cd kibana-7.7.0-linux-x86_64/./bin/kinana 后台运行可以用 ./bin/kinana &amp; 验证是否启动成功 浏览器里输入 http://127.0.0.1:5601/ 如果可以打开页面说明启动成功 logstash安装配置启动下载123wget https://artifacts.elastic.co/downloads/logstash/logstash-7.7.0.tar.gztar -zxf logstash-7.7.0.tar.gzcd logstash-7.7.0 配置 在 config 目录下新建配置 logstash-wkq-test.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# Sample Logstash configuration for creating a simple# file -&gt; Logstash -&gt; Elasticsearch pipeline.input &#123; file &#123; path =&gt; [ &quot;/Users/weikeqin1/WorkSpaces/java/springboot-test/logs/entrance.log&quot;, &quot;/Users/weikeqin1/WorkSpaces/java/springboot-test/logs/*.log&quot; ] type =&gt; &quot;entrance&quot; start_position =&gt; &quot;beginning&quot; &#125; file &#123; path =&gt; [ &quot;/Users/weikeqin1/WorkSpaces/java/springboot-test/logs/error*.log&quot; ] type =&gt; &quot;error&quot; start_position =&gt; &quot;beginning&quot; &#125; file &#123; path =&gt; &quot;/var/log/apache/access.log&quot; type =&gt; &quot;apache&quot; start_position =&gt; &quot;beginning&quot; &#125;&#125;filter &#123; if [path] =~ &quot;entrance&quot; &#123; mutate &#123; replace =&gt; &#123; &quot;type&quot; =&gt; &quot;entrance_log&quot; &#125; &#125; grok &#123; match =&gt; &#123; &quot;msg&quot; =&gt; &quot;%&#123;COMBINEDAPACHELOG&#125;&quot; &#125; &#125; &#125; else if [path] =~ &quot;error&quot; &#123; mutate &#123; replace =&gt; &#123; type =&gt; &quot;error_log&quot; &#125; &#125; &#125; date &#123; match =&gt; [ &quot;timestamp&quot; , &quot;yyyy-MM-dd HH:mm:ss&quot; ] &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;http://localhost:9200&quot;] index =&gt; &quot;log-%&#123;+YYYY.MM.dd&#125;&quot; #user =&gt; &quot;elastic&quot; #password =&gt; &quot;changeme&quot; &#125;&#125; 配置文件中的 grok 对应解析日志的字段 日志样例12020-05-17 11:14:22.887 INFO [XNIO-1 task-9] cn.wkq.controller.TestController.test|&#123;&quot;_traceId&quot;:&quot;1589685262886CY74&quot;,&quot;_method&quot;:&quot;test&quot;,&quot;param&quot;:&#123;&quot;appKey&quot;:&quot;wkq&quot;&#125;&#125; 解析规则123%&#123;DATA:logDate&#125; %&#123;DATA:logTime&#125; %&#123;DATA:logLevel&#125; \[%&#123;DATA:thread&#125;\]%&#123;DATA:classnameMethod&#125;\|\&#123;%&#123;DATA:msg&#125;\&#125; %&#123;DATA:logDate&#125; %&#123;DATA:logTime&#125; %&#123;DATA:logLevel&#125; \[%&#123;DATA:thread&#125;\] %&#123;DATA:classnameMethod&#125;\|\&#123;\&quot;_traceId\&quot;:\&quot;%&#123;DATA:traceId&#125;\&quot;\,\&quot;_method\&quot;:\&quot;%&#123;DATA:method&#125;\&quot;,\&quot;param\&quot;\:\&#123;%&#123;DATA:param&#125;\&#125; 可以在kibana http://127.0.0.1:5601/app/kibana#/dev_tools/grokdebugger 调试 启动1./bin/logstash -f ./config/logstash-wkq-test.conf 测试 用压测工具压测程序，会生成很多日志，压测的时候看监控，程序才占用75M内存，logstash就占1000m内存，实际使用500m左右，elasticsearch用的默认配置，也占1000m内存，实际使用750m左右 logstash太占内存了 filebeat下载安装12curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.7.0-linux-x86_64.tar.gztar xzvf filebeat-7.7.0-linux-x86_64.tar.gz 配置123456789101112131415161718192021222324252627282930313233343536373839404142434445#=========================== Filebeat inputs =============================filebeat.inputs:# Each - is an input. Most options can be set at the input level, so# you can use different inputs for various configurations.# Below are the input specific configurations.- type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /Users/weikeqin1/WorkSpaces/java/springboot-test/logs/*.log #- c:\programdata\elasticsearch\logs\*#-------------------------- Elasticsearch output ------------------------------#output.elasticsearch: # Array of hosts to connect to. #hosts: [&quot;localhost:9200&quot;] # Protocol - either `http` (default) or `https`. #protocol: &quot;https&quot; # Authentication credentials - either API key or username/password. #api_key: &quot;id:api_key&quot; #username: &quot;elastic&quot; #password: &quot;changeme&quot;#----------------------------- Logstash output --------------------------------output.logstash: # The Logstash hosts hosts: [&quot;localhost:5044&quot;] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #ssl.certificate_authorities: [&quot;/etc/pki/root/ca.pem&quot;] # Certificate for SSL client authentication #ssl.certificate: &quot;/etc/pki/client/cert.pem&quot; # Client Certificate Key #ssl.key: &quot;/etc/pki/client/cert.key&quot; 启动12cd filebeat-7.7.0-linux-x86_64sudo ./filebeat -e 在kibana查看日志12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&#123; &quot;_index&quot; : &quot;filebeat-7.7.0-2020.05.17&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;y42gIHIBU4Je81Q8Jaun&quot;, &quot;_score&quot; : null, &quot;_source&quot; : &#123; &quot;log&quot; : &#123; &quot;file&quot; : &#123; &quot;path&quot; : &quot;/Users/weikeqin1/WorkSpaces/java/springboot-test/logs/springboot-test.2020-05-17_0.log&quot; &#125;, &quot;offset&quot; : 371432 &#125;, &quot;tags&quot; : [ &quot;beats_input_codec_plain_applied&quot; ], &quot;host&quot; : &#123; &quot;architecture&quot; : &quot;x86_64&quot;, &quot;id&quot; : &quot;DAA105AB-C93E-5E37-AFBF-A747032048AF&quot;, &quot;ip&quot; : [ &quot;xx::xx:xx:xx:xx&quot;, &quot;192.168.0.110&quot;, &quot;xx::xx:xx:xx:xx&quot;, &quot;xx::xx:xx:xx:xx&quot; ], &quot;name&quot; : &quot;192.168.0.110&quot;, &quot;mac&quot; : [ &quot;xx:xx:xx:xx:xx:xx&quot;, &quot;xx:xx:xx:xx:xx:xx&quot;, &quot;xx:xx:xx:xx:xx:xx&quot;, &quot;xx:xx:xx:xx:xx:xx&quot;, &quot;xx:xx:xx:xx:xx:xx&quot;, &quot;xx:xx:xx:xx:xx:xx&quot; ], &quot;os&quot; : &#123; &quot;version&quot; : &quot;10.13.6&quot;, &quot;build&quot; : &quot;17G8030&quot;, &quot;kernel&quot; : &quot;17.7.0&quot;, &quot;name&quot; : &quot;Mac OS X&quot;, &quot;family&quot; : &quot;darwin&quot;, &quot;platform&quot; : &quot;darwin&quot; &#125;, &quot;hostname&quot; : &quot;192.168.0.110&quot; &#125;, &quot;ecs&quot; : &#123; &quot;version&quot; : &quot;1.5.0&quot; &#125;, &quot;agent&quot; : &#123; &quot;id&quot; : &quot;1dcf526f-dc97-4a59-bccc-4751954962e6&quot;, &quot;type&quot; : &quot;filebeat&quot;, &quot;version&quot; : &quot;7.7.0&quot;, &quot;ephemeral_id&quot; : &quot;810f6da6-b38a-4071-8d2f-9c45dee14440&quot;, &quot;hostname&quot; : &quot;192.168.0.110&quot; &#125;, &quot;@version&quot; : &quot;1&quot;, &quot;@timestamp&quot; : &quot;2020-05-17T03:14:23.865Z&quot;, &quot;input&quot; : &#123; &quot;type&quot; : &quot;log&quot; &#125;, &quot;message&quot; : &quot;&quot;&quot;2020-05-17 11:14:22.879 [http-nio-11011-exec-20] INFO [cn.wkq.controller.TestController] [32] - &#123;&quot;_traceId&quot;:&quot;15896852628794XZ3&quot;,&quot;_method&quot;:&quot;TestController#test#1589685262879#&quot;&#125;&quot;&quot;&quot; &#125;, &quot;sort&quot; : [ 1589685263865 ]&#125; References[1] elastic官网[2] elasticsearch官网中文简介[3] elasticsearch官网logstash中文简介[4] elasticsearch官网kibana中文简介[5] elasticsearch下载安装启动及配置[6] logstash-configuration[7] kibana-getting-started[8] beats-getting-started[9] logstash-patterns]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jdk javac 使用笔记]]></title>
    <url>%2F2020%2F05%2F14%2Fjavac-notes%2F</url>
    <content type="text"><![CDATA[最近有一个需求，需要测试一下新数据库(mysql优化版)某个表的写入速度和查询速度，需要从我们的测试环境数据库查数据，插入到新的测试环境的新数据库。 本地连不上测试的数据库 有很多应用服务器，但是所有的服务器不能连外网，只能访问公司内网。 测试环境的应用可以连测试的数据库 想写个python脚本，但是服务器上python是2.7的，而且很多包没有。 想了想，应用服务器部署了java程序，java的包都有，想写一个java脚本来测试。(没错是java脚本) 测试环境数据库 db_test_old 192.168.1.10 新测试环境新数据库 db_test_new 192.168.2.11 测试环境应用 test-java 192.168.3.12 在idea里写完脚本，运行没问题，然后在 terminal 运行，被啪啪的打脸 (1) 想象中运行java文件的样子(1.1) 查看jdk版本12345[wkq@VM_77_25_centos ~]$ java -versionjava version &quot;1.8.0_172&quot;Java(TM) SE Runtime Environment (build 1.8.0_172-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.172-b11, mixed mode)[wkq@VM_77_25_centos ~]$ (1.2) 新建一个HelloWord.java文件测试123[wkq@VM_77_25_centos java_test]$ pwd/home/wkq/workspaces/java_testvi HelloWorld.java 12345public class HelloWorld&#123; public static void main(String[] args)&#123; System.out.println(&quot;Hello, World!&quot;); &#125;&#125; (1.3) 编译HelloWorld.java123[wkq@VM_77_25_centos java_test]$ javac HelloWorld.java[wkq@VM_77_25_centos java_test]$ (1.4) 运行HelloWorld12345678910[wkq@VM_77_25_centos java_test]$ pwd/home/wkq/workspaces/java_test[wkq@VM_77_25_centos java_test]$ lltotal 8-rw-rw-r-- 1 wkq wkq 427 May 17 18:48 HelloWorld.class-rw-rw-r-- 1 wkq wkq 112 May 17 18:47 HelloWorld.java[wkq@VM_77_25_centos java_test]$[wkq@VM_77_25_centos java_test]$ java HelloWorldHello, World![wkq@VM_77_25_centos java_test]$ 没问题，很完美 (1.5) 被打脸的一幕123[wkq@VM_77_25_centos java_test]$ java HelloWorld.classError: Could not find or load main class HelloWorld.class[wkq@VM_77_25_centos java_test]$ 竟然不能运行 class文件 被打脸后才明白，java的参数，传入的是main函数所在的类的名字，而不是class文件；java会根据类名自动去找class文件。 (2) 复盘(2.1) 查看jdk版本12345[wkq@VM_77_25_centos ~]$ java -versionjava version &quot;1.8.0_172&quot;Java(TM) SE Runtime Environment (build 1.8.0_172-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.172-b11, mixed mode)[wkq@VM_77_25_centos ~]$ (2.2) 新建MysqlLoadDataTest.java1vi MySqlLoadDataTest.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344package cn.wkq.java.test;import java.sql.*;import java.util.*;/** * MySqlLoadDataTest * * @author: weikeqin.cn@gmail.com * @date: 2020-05-13 20:06 **/public class MySqlLoadDataTest &#123; /** * @param args */ public static void main(String[] args) &#123; MySqlLoadDataTest t = new MySqlLoadDataTest(); t.loadData(); System.out.println(&quot;执行完了。&quot;); &#125; /** * */ public void loadData() &#123; int batchSize = 100; // String driverClassName = &quot;com.mysql.jdbc.Driver&quot;; try &#123; Class.forName(driverClassName); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; // 省略其它逻辑 &#125;&#125; (2.3) 编译1234[wkq@VM_77_25_centos java_test]$ pwd/home/wkq/workspaces/java_test[wkq@VM_77_25_centos java_test]$ javac -encoding &quot;utf-8&quot; MySqlLoadDataTest.java[wkq@VM_77_25_centos java_test]$ (2.4) 运行123456[wkq@VM_77_25_centos java_test]$ java MySqlLoadDataTestError: Could not find or load main class MySqlLoadDataTest[wkq@VM_77_25_centos java_test]$[wkq@VM_77_25_centos java_test]$ java cn.wkq.java.test.MySqlLoadDataTestError: Could not find or load main class cn.wkq.java.test.MySqlLoadDataTest[wkq@VM_77_25_centos java_test]$ (2.5) 注释掉第一行包名重新运行 如果把 文件的第一行注释掉，重新编译运行会出现 123456789101112131415[wkq@VM_77_25_centos java_test]$ vi MySqlLoadDataTest.java[wkq@VM_77_25_centos java_test]$ [wkq@VM_77_25_centos java_test]$ javac -encoding &quot;utf-8&quot; MySqlLoadDataTest.java[wkq@VM_77_25_centos java_test]$[wkq@VM_77_25_centos java_test]$ java MySqlLoadDataTestjava.lang.ClassNotFoundException: com.mysql.jdbc.Driver at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:264) at MySqlLoadDataTest.loadData(MySqlLoadDataTest.java:35) at MySqlLoadDataTest.main(MySqlLoadDataTest.java:20)????? 123456789101112131415161718192021222324[wkq@VM_77_25_centos java_test]$ pwd/home/wkq/workspaces/java_test[wkq@VM_77_25_centos java_test]$ cd /home/wkq/lib/[wkq@VM_77_25_centos lib]$ wget &quot;https://repo1.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jar&quot;--2020-05-17 21:59:08-- https://repo1.maven.org/maven2/mysql/mysql-connector-java/5.1.47/mysql-connector-java-5.1.47.jarResolving repo1.maven.org (repo1.maven.org)... 151.101.24.209Connecting to repo1.maven.org (repo1.maven.org)|151.101.24.209|:443... connected.HTTP request sent, awaiting response... 200 OKLength: 1007502 (984K) [application/java-archive]Saving to: &apos;mysql-connector-java-5.1.47.jar&apos;100%[=====================================================================================&gt;] 1,007,502 10.1KB/s in 85s2020-05-17 22:00:34 (11.6 KB/s) - &apos;mysql-connector-java-5.1.47.jar&apos; saved [1007502/1007502][wkq@VM_77_25_centos lib]$[wkq@VM_77_25_centos lib]$ lltotal 988-rw-rw-r-- 1 wkq wkq 1007502 Aug 7 2018 mysql-connector-java-5.1.47.jar[wkq@VM_77_25_centos lib]$ pwd/home/wkq/lib[wkq@VM_77_25_centos java_test]$ java -Dfile.encoding=UTF-8 -classpath /home/wkq/lib/mysql-connector-java-5.1.47.jar:/home/wkq/workspaces/java_test MySqlLoadDataTest执行完了。[wkq@VM_77_25_centos java_test]$ 在注释了第一行包路径后，重新编译 指定了classpath后就可以运行成功。 但是现在还有一个问题，测试的时候可以写一个java文件，但是实际开发的时候肯定不能不带包名 (2.6) 不注释第一行包名，把MySqlLoadDataTest.class换个目录1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[wkq@VM_77_25_centos java_test]$ lltotal 16-rw-rw-r-- 1 wkq wkq 427 May 17 18:48 HelloWorld.class-rw-rw-r-- 1 wkq wkq 112 May 17 18:47 HelloWorld.java-rw-rw-r-- 1 wkq wkq 752 May 17 22:17 MySqlLoadDataTest.javadrwxrwxr-x 3 wkq wkq 4096 May 17 22:17 cn[wkq@VM_77_25_centos java_test]$ pwd/home/wkq/workspaces/java_test[wkq@VM_77_25_centos java_test]$ java cn.wkq.java.test.MySqlLoadDataTestjava.lang.ClassNotFoundException: com.mysql.jdbc.Driver at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:264) at cn.wkq.java.test.MySqlLoadDataTest.loadData(MySqlLoadDataTest.java:35) at cn.wkq.java.test.MySqlLoadDataTest.main(MySqlLoadDataTest.java:20)?????[wkq@VM_77_25_centos java_test]$[wkq@VM_77_25_centos java_test]$ java -Dfile.encoding=UTF-8 -classpath /home/wkq/lib/mysql-connector-java-5.1.47.jar cn.wkq.java.test.MySqlLoadDataTestError: Could not find or load main class cn.wkq.java.test.MySqlLoadDataTest[wkq@VM_77_25_centos java_test]$[wkq@VM_77_25_centos java_test]$ java -Dfile.encoding=UTF-8 -classpath /home/wkq/lib/mysql-connector-java-5.1.47.jar:/home/wkq/workspaces/java_test cn.wkq.java.test.MySqlLoadDataTest执行完了。[wkq@VM_77_25_centos java_test]$``` &gt; 可以看到，运行成功了，果然是加了包名以后，编译的main方法放到了对应目录下。&gt; 终于用教训理解了全限定类名&gt; 用教训理解了 Java 会根据包名对应出目录结构，并从class path搜索该目录去找class文件。 (默认是当前目录，如果运行时不在当前目录需要指定 classpath)## (2.7) 不注释第一行包名，不换目录 &gt; 正常情况下，我们写完java代码，直接编译后就可以运行，不可能再对每个文件创建目录。所以得换个办法。```log[wkq@VM_77_25_centos java_test]$ rm -rf cn/[wkq@VM_77_25_centos java_test]$[wkq@VM_77_25_centos java_test]$ javac -Dfile.encoding=UTF-8 -d MySqlLoadDataTest.javajavac: invalid flag: -Dfile.encoding=UTF-8Usage: javac &lt;options&gt; &lt;source files&gt;use -help for a list of possible options[wkq@VM_77_25_centos java_test]$ javac -Dfile.encoding=UTF-8 -d . MySqlLoadDataTest.javajavac: invalid flag: -Dfile.encoding=UTF-8Usage: javac &lt;options&gt; &lt;source files&gt;use -help for a list of possible options[wkq@VM_77_25_centos java_test]$ javac -encoding &quot;utf-8&quot; -d . MySqlLoadDataTest.java[wkq@VM_77_25_centos java_test]$ lltotal 16-rw-rw-r-- 1 wkq wkq 427 May 17 18:48 HelloWorld.class-rw-rw-r-- 1 wkq wkq 112 May 17 18:47 HelloWorld.java-rw-rw-r-- 1 wkq wkq 752 May 17 22:17 MySqlLoadDataTest.javadrwxrwxr-x 3 wkq wkq 4096 May 17 22:29 cn[wkq@VM_77_25_centos java_test]$[wkq@VM_77_25_centos java_test]$ java -Dfile.encoding=UTF-8 -classpath /home/wkq/lib/mysql-connector-java-5.1.47.jar:/home/wkq/workspaces/java_test cn.wkq.java.test.MySqlLoadDataTest执行完了。[wkq@VM_77_25_centos java_test]$ 果然可以运行 编译java文件到指定目录12345678910111213141516[wkq@VM_77_25_centos java_test]$ javac -encoding &quot;utf-8&quot; -d /home/wkq/workspaces/test_target/ MySqlLoadDataTest.java[wkq@VM_77_25_centos java_test]$[wkq@VM_77_25_centos java_test]$ lltotal 16-rw-rw-r-- 1 wkq wkq 427 May 17 18:48 HelloWorld.class-rw-rw-r-- 1 wkq wkq 112 May 17 18:47 HelloWorld.java-rw-rw-r-- 1 wkq wkq 752 May 17 22:17 MySqlLoadDataTest.javadrwxrwxr-x 3 wkq wkq 4096 May 17 22:29 cn[wkq@VM_77_25_centos java_test]$ cd /home/wkq/workspaces/test_target/[wkq@VM_77_25_centos test_target]$ lltotal 4drwxrwxr-x 3 wkq wkq 4096 May 17 22:32 cn[wkq@VM_77_25_centos test_target]$[wkq@VM_77_25_centos test_target]$ java -Dfile.encoding=UTF-8 -classpath /home/wkq/lib/mysql-connector-java-5.1.47.jar:/home/wkq/workspaces/java_test cn.wkq.java.test.MySqlLoadDataTest执行完了。[wkq@VM_77_25_centos test_target]$ 开发工具idea是怎么编译代码的1javac -d target src/cn/wkq/java/test/*.java 1/home/wkq/software/jdk1.8.0_172/bin/javac -encoding &quot;utf-8&quot; -d /home/wkq/workspaces/java_test/target/ /home/wkq/workspaces/java_test/src/*.java jdk目录 /home/wkq/software/jdk1.8.0_172/bin/java代码目录 /home/wkq/workspaces/java_test/src/编译后存放目录 /home/wkq/workspaces/java_test/target/ 开发工具idea是怎么运行代码的1/home/wkq/software/jdk1.8.0_172/bin/java -Dfile.encoding=UTF-8 -classpath /home/wkq/lib/mysql-connector-java-5.1.47.jar:/home/wkq/workspaces/java_test cn.wkq.java.test.MySqlLoadDataTest jdk目录 /home/wkq/software/jdk1.8.0_172/bin/指定 classpath目录 -classpath /home/wkq/lib/mysql-connector-java-5.1.47.jar:/home/wkq/workspaces/java_test运行main方法 cn.wkq.java.test.MySqlLoadDataTest javac命令123456789101112131415161718192021222324252627282930313233343536[wkq@VM_77_25_centos ~]$ /home/wkq/software/jdk1.8.0_172/bin/javacUsage: javac &lt;options&gt; &lt;source files&gt;where possible options include: -g Generate all debugging info -g:none Generate no debugging info -g:&#123;lines,vars,source&#125; Generate only some debugging info -nowarn Generate no warnings -verbose Output messages about what the compiler is doing -deprecation Output source locations where deprecated APIs are used -classpath &lt;path&gt; Specify where to find user class files and annotation processors -cp &lt;path&gt; Specify where to find user class files and annotation processors -sourcepath &lt;path&gt; Specify where to find input source files -bootclasspath &lt;path&gt; Override location of bootstrap class files -extdirs &lt;dirs&gt; Override location of installed extensions -endorseddirs &lt;dirs&gt; Override location of endorsed standards path -proc:&#123;none,only&#125; Control whether annotation processing and/or compilation is done. -processor &lt;class1&gt;[,&lt;class2&gt;,&lt;class3&gt;...] Names of the annotation processors to run; bypasses default discovery process -processorpath &lt;path&gt; Specify where to find annotation processors -parameters Generate metadata for reflection on method parameters -d &lt;directory&gt; Specify where to place generated class files -s &lt;directory&gt; Specify where to place generated source files -h &lt;directory&gt; Specify where to place generated native header files -implicit:&#123;none,class&#125; Specify whether or not to generate class files for implicitly referenced files -encoding &lt;encoding&gt; Specify character encoding used by source files -source &lt;release&gt; Provide source compatibility with specified release -target &lt;release&gt; Generate class files for specific VM version -profile &lt;profile&gt; Check that API used is available in the specified profile -version Version information -help Print a synopsis of standard options -Akey[=value] Options to pass to annotation processors -X Print a synopsis of nonstandard options -J&lt;flag&gt; Pass &lt;flag&gt; directly to the runtime system -Werror Terminate compilation if warnings occur @&lt;filename&gt; Read options and filenames from file[wkq@VM_77_25_centos ~]$ java命令12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[wkq@VM_77_25_centos ~]$ /home/wkq/software/jdk1.8.0_172/bin/javaUsage: java [-options] class [args...] (to execute a class) or java [-options] -jar jarfile [args...] (to execute a jar file)where options include: -d32 use a 32-bit data model if available -d64 use a 64-bit data model if available -server to select the &quot;server&quot; VM The default VM is server. -cp &lt;class search path of directories and zip/jar files&gt; -classpath &lt;class search path of directories and zip/jar files&gt; A : separated list of directories, JAR archives, and ZIP archives to search for class files. -D&lt;name&gt;=&lt;value&gt; set a system property -verbose:[class|gc|jni] enable verbose output -version print product version and exit -version:&lt;value&gt; Warning: this feature is deprecated and will be removed in a future release. require the specified version to run -showversion print product version and continue -jre-restrict-search | -no-jre-restrict-search Warning: this feature is deprecated and will be removed in a future release. include/exclude user private JREs in the version search -? -help print this help message -X print help on non-standard options -ea[:&lt;packagename&gt;...|:&lt;classname&gt;] -enableassertions[:&lt;packagename&gt;...|:&lt;classname&gt;] enable assertions with specified granularity -da[:&lt;packagename&gt;...|:&lt;classname&gt;] -disableassertions[:&lt;packagename&gt;...|:&lt;classname&gt;] disable assertions with specified granularity -esa | -enablesystemassertions enable system assertions -dsa | -disablesystemassertions disable system assertions -agentlib:&lt;libname&gt;[=&lt;options&gt;] load native agent library &lt;libname&gt;, e.g. -agentlib:hprof see also, -agentlib:jdwp=help and -agentlib:hprof=help -agentpath:&lt;pathname&gt;[=&lt;options&gt;] load native agent library by full pathname -javaagent:&lt;jarpath&gt;[=&lt;options&gt;] load Java programming language agent, see java.lang.instrument -splash:&lt;imagepath&gt; show splash screen with specified imageSee http://www.oracle.com/technetwork/java/javase/documentation/index.html for more details.[wkq@VM_77_25_centos ~]$ References[1] javac[2] 第1期：抛开IDE，了解一下javac如何编译]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[guava缓存]]></title>
    <url>%2F2020%2F05%2F13%2Fguava-cache%2F</url>
    <content type="text"><![CDATA[google guava-cache redis做缓存很快，但是还有网络IO，在并发量大要求耗时低的情况下还是不满足需求，所以想试试本地缓存。 发现guava缓存不错，想试试 在本地demo测试的时候发现更新缓存时有毛刺出现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119package cn.wkq.cache;import com.google.common.cache.CacheBuilder;import com.google.common.cache.CacheLoader;import com.google.common.cache.LoadingCache;import lombok.extern.slf4j.Slf4j;import org.junit.Test;import java.util.concurrent.*;/** * LocalCache * * @author: weikeqin.cn@gmail.com * @date: 2020-05-10 17:08 **/@Slf4jpublic class LocalCache &#123; private static LoadingCache&lt;String, Object&gt; cache = null; static &#123; StringBuilder sb = new StringBuilder(1 &lt;&lt; 20); for (int i = 0; i &lt; 102400; i++) &#123; sb.append(i); &#125; String testString = sb.toString(); log.info("&#123;&#125;", testString.length()); cache = CacheBuilder.newBuilder() //设置并发级别为8，并发级别是指可以同时写缓存的线程数 .concurrencyLevel(8) //设置缓存容器的初始容量为10 .initialCapacity(10) //设置缓存最大容量为100，超过100之后就会按照LRU最近虽少使用算法来移除缓存项 .maximumSize(100) //是否需要统计缓存情况,该操作消耗一定的性能,生产环境应该去除 .recordStats() //设置写缓存后n秒钟过期 .expireAfterWrite(1, TimeUnit.SECONDS) //设置读写缓存后n秒钟过期,实际很少用到,类似于expireAfterWrite //.expireAfterAccess(17, TimeUnit.SECONDS) //只阻塞当前数据加载线程，其他线程返回旧值 .refreshAfterWrite(1, TimeUnit.SECONDS) //设置缓存的移除通知 .removalListener(notification -&gt; &#123; log.info(" 本地缓存key:&#123;&#125; 被移除，原因:&#123;&#125; ", notification.getKey(), notification.getCause()); //log.info(" 本地缓存key:&#123;&#125; 被移除，原因:&#123;&#125; 对应值:&#123;&#125;", notification.getKey(), notification.getCause(), notification.getValue()); &#125;) //build方法中可以指定CacheLoader，在缓存不存在时通过CacheLoader的实现自动加载缓存 .build(new CacheLoader&lt;String, Object&gt;() &#123; @Override public Object load(String key) throws Exception &#123; log.info("重新查库"); return "获取的缓存的值" + testString; &#125; &#125;); &#125; /** * */ @Test public void cacheTest() &#123; ThreadPoolExecutor executor = new ThreadPoolExecutor(4, 8, 60, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); String key = "1"; int count = 100000; for (int i = 0; i &lt; count; i++) &#123; int finalI = i; executor.execute(() -&gt; &#123; long t1 = System.currentTimeMillis(); Object value = null; try &#123; value = cache.get(key); &#125; catch (ExecutionException e) &#123; log.error("", e); &#125; long t2 = System.currentTimeMillis(); log.info(" &#123;&#125; cost &#123;&#125; ms ", finalI, t2 - t1); //log.info("&#123;&#125; 根据 &#123;&#125; 获取到的值 &#123;&#125; ", Thread.currentThread().getName(), key, ((String) value).length()); &#125;);// try &#123;// Thread.sleep(1);// &#125; catch (InterruptedException e) &#123;// log.error("", e);// &#125; &#125; executor.shutdown(); log.info("线程池关闭。"); while (!executor.isTerminated()) &#123; try &#123; TimeUnit.SECONDS.sleep(1); log.info("线程池中线程数目：&#123;&#125;，队列中等待执行的任务数目：&#123;&#125;，已执行玩别的任务数目：&#123;&#125;", executor.getPoolSize(), executor.getQueue().size(), executor.getCompletedTaskCount()); &#125; catch (InterruptedException e) &#123; log.error("", e); &#125; log.info("还有线程未执行完成"); &#125; //缓存状态查看 log.info(cache.stats().toString()); &#125;&#125; 少了网络IO，整体 TP99 比redis好了不少 但是在压测时发现毛刺更严重 References[1] google/guava/wiki/CachesExplained[2] [Google Guava] 3-缓存 并发编程网 - ifeve.com[3] 缓存篇 : Guava cache 之全面剖析]]></content>
      <categories>
        <category>cache</category>
      </categories>
      <tags>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim笔记]]></title>
    <url>%2F2020%2F04%2F18%2Fvim-notes%2F</url>
    <content type="text"><![CDATA[vim有三种模式，一般模式、编辑模式、命令模式。 一般模式下只能 查看文件。 (输入 ESC 可以进入一般模式。) 编辑模式下可以对文件进行 新增、修改、删除。 (输入 i 可以进入编辑模式。进入后左下角可以看到 INSERT 标识) 命令模式下可以对文件进行 保存、退出 等操作。 (输入 ESC + : 可以进入命令模式。) 输入ESC+:进入命令模式后，输入q!可以退出文件并且不保存修改。 输入ESC+:进入命令模式后，输入qw可以退出文件并且保存修改。 (1) 一般模式 一般模式下只能查看。 (1.1) 移动光标 命令 对应结果 ← 或 h 光标向左移动一个字符 ↓ 或 j 光标向下移动一个字符 ↑ 或 k 光标向上移动一个字符 → 或 l 光标向右移动一个字符 Ctrl + f 屏幕 向下 移动一页，相当于 Page Down按键 Ctrl + b 屏幕 向上 移动一页，相当于 Page Up 按键 Ctrl + d 屏幕 向下 移动半页 (常用) Ctrl + u 屏幕 向上 移动半页 (常用) + 光标移动到非空格符的下一行 - 光标移动到非空格符的上一行 n 那个 n 表示数字，例如 20 。按下数字后再按空格键，光标会向右移动这一行的 n 个字符。 例如 20n 则光标会向后面移动 20 个字符距离。 0 或 功能键Home 这是数字 0 ：移动到这一行的最前面字符处 (常用) $ 或 功能键End 移动到这一行的最后面字符处(常用) H 光标移动到这个屏幕的最上方那一行的第一个字符 M 光标移动到这个屏幕的中央那一行的第一个字符 L 光标移动到这个屏幕的最下方那一行的第一个字符 G 移动到这个档案的最后一行(常用) nG n 为数字。移动到这个档案的第 n 行。例如 20G 则会移动到这个档案的第 20 行(可配合 :set nu) gg 移动到这个档案的第一行，相当于 1G 啊！ (常用) n n 为数字。光标向下移动 n 行(常用) (1.2) 搜索替换 命令 对应结果 /word 向光标之下寻找一个名称为 word 的字符串。例如要在档案内搜寻 vbird 这个字符串，就输入 /vbird 即可！ (常用) ?word 向光标之上寻找一个字符串名称为 word 的字符串。 n 这个 n 是英文按键。代表重复前一个搜寻的动作。举例来说， 如果刚刚我们执行 /vbird 去向下搜寻 vbird 这个字符串，则按下 n 后，会向下继续搜寻下一个名称为 vbird 的字符串。如果是执行 ?vbird 的话，那么按下 n 则会向上继续搜寻名称为 vbird 的字符串！ N 这个 N 是英文按键。与 n 刚好相反，为反向进行前一个搜寻动作。 例如 /vbird 后，按下 N 则表示向上搜寻 vbird 。 使用 /word 配合 n 及 N 是非常有帮助的！可以让你重复的找到一些你搜寻的关键词！ :n1,n2s/word1/word2/g n1 与 n2 为数字。在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 ！(常用) 例如，在 100 到 200 行之间搜寻 vbird 并取代为 VBIRD ： :100,200s/vbird/VBIRD/g :1,$s/word1/word2/g 或 :%s/word1/word2/g 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！(常用) :1,$s/word1/word2/gc 或 :%s/word1/word2/gc 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 ！且在取代前显示提示字符给用户确认 (confirm) 是否需要取代！(常用) (1.3) 复制、粘贴、删除 命令 对应结果 yy 复制游标所在的那一行(常用) nyy n 为数字。复制光标所在的向下 n 行，例如 20yy 则是复制 20 行(常用) y1G 复制游标所在行到第一行的所有数据 yG 复制游标所在行到最后一行的所有数据 y0 复制光标所在的那个字符到该行行首的所有数据 y$ 复制光标所在的那个字符到该行行尾的所有数据 p, P p 为将已复制的数据在光标下一行贴上，P 则为贴在游标上一行！ 举例来说，我目前光标在第 20 行，且已经复制了 10 行数据。则按下 p 后， 那 10 行数据会贴在原本的 20 行之后，亦即由 21 行开始贴。但如果是按下 P 呢？ 那么原本的第 20 行会被推到变成 30 行。 (常用) J 将光标所在行与下一行的数据结合成同一行 x, X 在一行字当中，x 为向后删除一个字符 (相当于 [del] 按键)， X 为向前删除一个字符(相当于 [backspace] 亦即是退格键) (常用) nx n 为数字，连续向后删除 n 个字符。举例来说，我要连续删除 10 个字符， 10x。 dd 删除游标所在的那一整行(常用) ndd n 为数字。删除光标所在的向下 n 行，例如 20dd 则是删除 20 行 (常用) d1G 删除光标所在到第一行的所有数据 dG 删除光标所在到最后一行的所有数据 d$ 删除游标所在处，到该行的最后一个字符 d0 那个是数字的 0 ，删除游标所在处，到该行的最前面一个字符 c 重复删除多个数据，例如向下删除 10 行，[ 10cj ] u 复原前一个动作。(常用) [Ctrl]+r 重做上一个动作。(常用) . 不要怀疑！这就是小数点！意思是重复前一个动作的意思。 如果你想要重复删除、重复贴上等等动作，按下小数点.就好了！ (常用) (2) 编辑模式 从一般模式进入编辑模式的方式。 命令 对应结果 i, I 进入输入模式(Insert mode)： i 为从目前光标所在处输入， I 为在目前所在行的第一个非空格符处开始输入。 (常用) a, A 进入输入模式(Insert mode)： a 为从目前光标所在的下一个字符处开始输入， A 为从光标所在行的最后一个字符处开始输入。(常用) o, O 进入输入模式(Insert mode)： o 为在目前光标所在的下一行处输入新的一行； O 为在目前光标所在处的上一行输入新的一行！(常用) r, R 进入取代模式(Replace mode)： r 只会取代光标所在的那一个字符一次；R会一直取代光标所在的文字，直到按下 ESC 为止；(常用) [Esc] 退出编辑模式，回到一般模式中(常用) (3) 命令模式 命令 对应结果 :w 将编辑的数据写入硬盘档案中(常用) :w! 若文件属性为只读时，强制写入该档案。不过，到底能不能写入， 还是跟你对该档案的档案权限有关啊！ :q 离开 vi (常用) :q! 若曾修改过档案，又不想储存，使用 ! 为强制离开不储存档案。 注意一下啊，那个惊叹号 (!) 在 vi 当中，常常具有强制的意思 :wq 储存后离开，若为 :wq! 则为强制储存后离开 (常用) ZZ 这是大写的 Z 喔！若档案没有更动，则不储存离开，若档案已经被更动过，则储存后离开！ :w [filename] 将编辑的数据储存成另一个档案（类似另存新档） :r [filename] 在编辑的数据中，读入另一个档案的数据。亦即将 filename 这个档案内容加到游标所在行后面 :n1,n2 w [filename] 将 n1 到 n2 的内容储存成 filename 这个档案。 :! command 暂时离开 vi 到指令行模式下执行 command 的显示结果！ 例如 :! ls /home即可在 vi 当中察看 /home 底下以 ls 输出的档案信息！ References[1] Linux vi/vim[2] vim 命令详解[3] vim.org]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vs-code-notes]]></title>
    <url>%2F2020%2F04%2F18%2Fvs-code-notes%2F</url>
    <content type="text"><![CDATA[(1) vs-code 设置语言 打开命令面板 ，输入 configure display language，设置语言为 zh-cn 备注：打开命令面板快捷键 mac Shift + Command + P Windows Ctrl + Shift + p (2) vs-code 设置背景豆沙绿 修改配置文件 ~/Library/Application Support/Code/User/settings.json 把 &quot;editor.background&quot; 设置成 &quot;#C7EDCC&quot;，具体如下： 1234567891011121314151617181920212223&#123; "workbench.colorTheme": "Default Light+", "workbench.colorCustomizations": &#123; "editor.background": "#C7EDCC" //设置用户选中代码段的颜色 //"editor.selectionBackground": "#2f00ff", //搜索匹配的背景色 //"editor.findMatchBackground": "#ff0000", //"editor.findMatchHighlightBackground": "#ff00ff", //"editor.findRangeHighlightBackground": "#ff9900" &#125;, "editor.fontSize": 16, "window.openFoldersInNewWindow": "off", "window.openWithoutArgumentsInNewWindow": "on", "editor.largeFileOptimizations": false, "editor.suggestSelection": "first", "vsintellicode.modify.editor.suggestSelection": "automaticallyOverrodeDefaultValue", "java.configuration.checkProjectSettingsExclusions": false, "extensions.autoUpdate": false, "window.zoomLevel": 0, "update.mode": "none"&#125; References[1] vscode-docs]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cache笔记]]></title>
    <url>%2F2020%2F04%2F06%2Fcache-notes%2F</url>
    <content type="text"><![CDATA[References[1] 缓存穿透，缓存击穿，缓存雪崩解决方案分析[2] 缓存雪崩、缓存穿透、缓存预热、缓存更新、缓存降级等问题]]></content>
      <categories>
        <category>cache</category>
      </categories>
      <tags>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM性能调优监控工具 jps jstat jinfo jmap jhat jstack]]></title>
    <url>%2F2020%2F03%2F28%2Fjvm-performance-tuning-monitoring-tool%2F</url>
    <content type="text"><![CDATA[最近测试环境总是莫名的出问题，然后部署服务的容器里除了服务打印的日志，没有其它信息，想看看是什么原因导致服务很卡，是不是有哪块代码占用内存高。 代码执行缓慢、OutOfMemoryError，内存不足、内存泄露、线程死锁、锁争用（Lock Contention）、Java进程消耗CPU过高 都可以使用JDK的命令行工具排查。 (1) JDK的命令行工具 jps、jstat、jinfo、jmap、jhat、jstack、hprofjps : 虚拟机进程状况工具jstat : 虚拟机统计信息监视工具jinfo : Java 配置信息工具jmap : Java 内存映像工具jhat : 虚拟机堆转储快照分析工具jstack : Java 堆栈跟踪工具 jps将打印所有正在运行的 Java 进程。 jstat允许用户查看目标 Java 进程的类加载、即时编译以及垃圾回收相关的信息。它常用于检测垃圾回收问题以及内存泄漏问题。 jmap允许用户统计目标 Java 进程的堆中存放的 Java 对象，并将它们导出成二进制文件。 jinfo将打印目标 Java 进程的配置参数，并能够改动其中 manageabe 的参数。 jstack将打印目标 Java 进程中各个线程的栈轨迹、线程状态、锁状况等信息。它还将自动检测死锁。 jcmd则是一把瑞士军刀，可以用来实现前面除了jstat之外所有命令的功能。 Java虚拟机的监控及诊断工具-GUI JConsole : Java 监视与管理控制台VisualVM : 多合一故障处理工具eclipse MATJMCJITWatch (2) 虚拟机进程状况工具 jps (JVM Process Status Tool) jps主要用来输出JVM中运行的进程状态信息。 jps命令使用Java启动器来查找传递给main方法的类名和参数。 1234567[wkq@VM_77_25_centos ~]$ jps -helpusage: jps [-help] jps [-q] [-mlvV] [&lt;hostid&gt;]Definitions: &lt;hostid&gt;: &lt;hostname&gt;[:&lt;port&gt;][wkq@VM_77_25_centos ~]$ (2.1) jps命令 jps命令用于输出JVM中运行的进程状态信息。可以获取到进程的pid、全限定名、传入main方法的参数、传入JVM的参数 等。 语法格式1jps [ options ] [ hostid ] 如果不指定hostid就默认为当前主机或服务器。 详细信息见 man jps (2.1.1) jps 不加参数 在本地主机上搜索检测到的JVM。 12345[wkq@VM_77_25_centos ~]$ jps14916 Jps13050 Elasticsearch[wkq@VM_77_25_centos ~]$[wkq@VM_77_25_centos ~]$ (2.1.2) jps -q 不输出类名、Jar名和传入main方法的参数 1234[wkq@VM_77_25_centos ~]$ jps -q1505913050[wkq@VM_77_25_centos ~]$ (2.1.3) jps -m 显示传递给main方法的参数。对于嵌入式JVM，输出可能为null。 1234[wkq@VM_77_25_centos ~]$ jps -m15092 Jps -m13050 Elasticsearch -d[wkq@VM_77_25_centos ~]$ (2.1.4) jps -l 输出main类或Jar的全限定名 1234[wkq@VM_77_25_centos ~]$ jps -l15139 sun.tools.jps.Jps13050 org.elasticsearch.bootstrap.Elasticsearch[wkq@VM_77_25_centos ~]$ (2.1.5) jps -v 显示传递给JVM的参数。 1234[wkq@VM_77_25_centos ~]$ jps -v15157 Jps -Denv.class.path=.:/home/wkq/software/jdk1.8.0_172/lib/dt.jar:/home/wkq/software/jdk1.8.0_172/lib/tools.jar -Dapplication.home=/home/wkq/software/jdk1.8.0_172 -Xms8m13050 Elasticsearch -Xms1g -Xmx1g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.io.tmpdir=/tmp/elasticsearch-5035355569386013893 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data -XX:ErrorFile=logs/hs_err_pid%p.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -Xloggc:logs/gc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=32 -XX:GCLogFileSize=64m -Des.path.home=/home/wkq/software/elasticsearch-6.6.2 -Des.path.conf=/home/wkq/software/elasticsearch-6.6.2/config -Des.distribution.flavor=default -Des.distribution.type=tar[wkq@VM_77_25_centos ~]$ (2.1.6) jps -V 禁止输出类名，JAR全限定名和传递给main方法的参数的输出，从而产生仅本地JVM标识符的列表。 1234[wkq@VM_77_25_centos ~]$ jps -V15204 Jps13050 Elasticsearch[wkq@VM_77_25_centos ~]$ (2.2) 常用jps命令(2.2.1) jps -lm 显示 pid、全限定名、传递给main方法的参数 12345[wkq@VM_77_25_centos ~]$ jps -lm17539 sun.tools.jps.Jps -lm13050 org.elasticsearch.bootstrap.Elasticsearch -d[wkq@VM_77_25_centos ~]$[wkq@VM_77_25_centos ~]$ (2.2.2) jps -lvm 显示 pid、全限定名、传递给main方法的参数、传递给JVM的参数 1234[wkq@VM_77_25_centos ~]$ jps -lvm13050 org.elasticsearch.bootstrap.Elasticsearch -d -Xms1g -Xmx1g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.io.tmpdir=/tmp/elasticsearch-5035355569386013893 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data -XX:ErrorFile=logs/hs_err_pid%p.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -Xloggc:logs/gc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=32 -XX:GCLogFileSize=64m -Des.path.home=/home/wkq/software/elasticsearch-6.6.2 -Des.path.conf=/home/wkq/software/elasticsearch-6.6.2/config -Des.distribution.flavor=default -Des.distribution.type=tar17567 sun.tools.jps.Jps -lvm -Denv.class.path=.:/home/wkq/software/jdk1.8.0_172/lib/dt.jar:/home/wkq/software/jdk1.8.0_172/lib/tools.jar -Dapplication.home=/home/wkq/software/jdk1.8.0_172 -Xms8m[wkq@VM_77_25_centos ~]$ (3) 虚拟机统计信息监视工具 jstat (JVM Statics Monitoring Tool) jstat是用于监视虚拟机各种运行状态信息的命令行工具。 (3.1) jstat命令 语法格式1jstat [ generalOption | outputOptions vmid [ interval[s|ms] [ count ] ] 详细信息见 man jstat (3.1.1) jstat命令12345678910111213141516171819202122[wkq@VM_77_25_centos ~]$ jstat -helpUsage: jstat -help|-options jstat -&lt;option&gt; [-t] [-h&lt;lines&gt;] &lt;vmid&gt; [&lt;interval&gt; [&lt;count&gt;]]Definitions: &lt;option&gt; An option reported by the -options option &lt;vmid&gt; Virtual Machine Identifier. A vmid takes the following form: &lt;lvmid&gt;[@&lt;hostname&gt;[:&lt;port&gt;]] Where &lt;lvmid&gt; is the local vm identifier for the target Java virtual machine, typically a process id; &lt;hostname&gt; is the name of the host running the target Java virtual machine; and &lt;port&gt; is the port number for the rmiregistry on the target host. See the jvmstat documentation for a more complete description of the Virtual Machine Identifier. &lt;lines&gt; Number of samples between header lines. &lt;interval&gt; Sampling interval. The following forms are allowed: &lt;n&gt;[&quot;ms&quot;|&quot;s&quot;] Where &lt;n&gt; is an integer and the suffix specifies the units as milliseconds(&quot;ms&quot;) or seconds(&quot;s&quot;). The default units are &quot;ms&quot;. &lt;count&gt; Number of samples to take before terminating. -J&lt;flag&gt; Pass &lt;flag&gt; directly to the runtime system.[wkq@VM_77_25_centos ~]$ 1234567891011121314[wkq@VM_77_25_centos ~]$ jstat -options-class-compiler-gc-gccapacity-gccause-gcmetacapacity-gcnew-gcnewcapacity-gcold-gcoldcapacity-gcutil-printcompilation[wkq@VM_77_25_centos ~]$ (3.1.1) jstat -class 130501234[wkq@VM_77_25_centos ~]$ jstat -class 13050Loaded Bytes Unloaded Bytes Time 15177 27005.8 41 49.7 24.44[wkq@VM_77_25_centos ~]$ (3.1.2) jstat -compiler 130501234[wkq@VM_77_25_centos ~]$ jstat -compiler 13050Compiled Failed Invalid Time FailedType FailedMethod 7469 0 0 43.18 0[wkq@VM_77_25_centos ~]$ (3.1.3) jstat -gc 130501234[wkq@VM_77_25_centos ~]$ jstat -gc 13050 S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT8512.0 8512.0 0.0 24.5 68160.0 6380.0 963392.0 61694.4 77732.0 71385.7 11984.0 9862.8 69 2.112 6 0.286 2.397[wkq@VM_77_25_centos ~]$ 参数 描述 原文 S0C 年轻代中第一个survivor 的容量 (kB) Current survivor space 0 capacity (kB). S1C 年轻代中第二个survivor 的容量 (kB) Current survivor space 1 capacity (kB). S0U 年轻代中第一个survivor 已使用空间 (kB) Survivor space 0 utilization (kB). S1U 年轻代中第二个survivor已使用空间 (kB) Survivor space 1 utilization (kB). EC 年轻代中 Eden space 的容量 (kB) Current eden space capacity (kB). EU 年轻代中Eden space 目前已使用空间 (kB) Eden space utilization (kB). OC Old space的容量 (kB) Current old space capacity (kB). OU Old space 已使用空间 (kB) Old space utilization (kB). MC Metaspace 的容量 (kB) Metaspace capacity (kB). MU Metaspace 已使用空间 (kB) Metacspace utilization (kB). CCSC Compressed class spcage 容量 (kB) Compressed class space capacity (kB). CCSU Compressed class spcage 已使用空间 (kB) Compressed class space used (kB). YGC 从应用程序启动到采样时年轻代中gc次数 Number of young generation garbage collection events. YGCT 从应用程序启动到采样时年轻代中gc所用时间 (s) Young generation garbage collection time. FGC 从应用程序启动到采样时old代 (全gc) gc次数 Number of full GC events. FGCT 从应用程序启动到采样时old代 (全gc) gc所用时间 (s) Full garbage collection time. GCT 从应用程序启动到采样时gc用的总时间 (s) Total garbage collection time. (3.1.4) jstat -gccapacity 130501234[wkq@VM_77_25_centos ~]$ jstat -gccapacity 13050 NGCMN NGCMX NGC S0C S1C EC OGCMN OGCMX OGC OC MCMN MCMX MC CCSMN CCSMX CCSC YGC FGC 85184.0 85184.0 85184.0 8512.0 8512.0 68160.0 963392.0 963392.0 963392.0 963392.0 0.0 1116160.0 77732.0 0.0 1048576.0 11984.0 69 6[wkq@VM_77_25_centos ~]$ (3.1.5) jstat -gccause 130501234[wkq@VM_77_25_centos ~]$ jstat -gccause 13050 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT LGCC GCC 0.00 0.29 11.90 6.40 91.84 82.30 69 2.112 6 0.286 2.397 Allocation Failure No GC[wkq@VM_77_25_centos ~]$ (3.1.6) jstat -gcmetacapacity 130501234[wkq@VM_77_25_centos ~]$ jstat -gcmetacapacity 13050 MCMN MCMX MC CCSMN CCSMX CCSC YGC FGC FGCT GCT 0.0 1116160.0 77732.0 0.0 1048576.0 11984.0 69 6 0.286 2.397[wkq@VM_77_25_centos ~]$ (3.1.7) jstat -gcnew 130501234[wkq@VM_77_25_centos ~]$ jstat -gcnew 13050 S0C S1C S0U S1U TT MTT DSS EC EU YGC YGCT8512.0 8512.0 0.0 24.5 6 6 4256.0 68160.0 13229.3 69 2.112[wkq@VM_77_25_centos ~]$ (3.1.8) jstat -gcnewcapacity 130501234[wkq@VM_77_25_centos ~]$ jstat -gcnewcapacity 13050 NGCMN NGCMX NGC S0CMX S0C S1CMX S1C ECMX EC YGC FGC 85184.0 85184.0 85184.0 8512.0 8512.0 8512.0 8512.0 68160.0 68160.0 69 6[wkq@VM_77_25_centos ~]$ (3.1.9) jstat -gcold 13050123[wkq@VM_77_25_centos ~]$ jstat -gcold 13050 MC MU CCSC CCSU OC OU YGC FGC FGCT GCT 77732.0 71385.7 11984.0 9862.8 963392.0 61694.4 69 6 0.286 2.397 (3.1.10) jstat -gcoldcapacity 130501234[wkq@VM_77_25_centos ~]$ jstat -gcoldcapacity 13050 OGCMN OGCMX OGC OC YGC FGC FGCT GCT 963392.0 963392.0 963392.0 963392.0 69 6 0.286 2.397[wkq@VM_77_25_centos ~]$ (3.1.11) jstat -gcutil 13050 垃圾回收状态摘要 1234[wkq@VM_77_25_centos ~]$ jstat -gcutil 13050 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 0.00 0.29 24.37 6.40 91.84 82.30 69 2.112 6 0.286 2.397[wkq@VM_77_25_centos ~]$ 参数 描述 原文 S0 年轻代中第一个survivor 已使用的占当前容量百分比 Survivor space 0 utilization as a percentage of the space’s current capacity. S1 年轻代中第二个survivor 已使用的占当前容量百分比 Survivor space 1 utilization as a percentage of the space’s current capacity. E Eden space 中Eden 已使用的占当前容量百分比 Eden space utilization as a percentage of the space’s current capacity. O Old space 已使用的占当前容量百分比 Old space utilization as a percentage of the space’s current capacity. M Metaspace 已使用的占当前容量百分比 Metaspace utilization as a percentage of the space’s current capacity. CCS Compressed class 空间利用率 Compressed class space utilization as a percentage. YGC 从应用程序启动到采样时 young generation gc次数 Number of young generation GC events. YGCT 从应用程序启动到采样时Young gc所用时间(s) Young generation garbage collection time. FGC 从应用程序启动到采样时Full gc次数 Number of full GC events. FGCT 从应用程序启动到采样时Full gc所用时间(s) Full garbage collection time. GCT 从应用程序启动到采样时gc用的总时间(s) Total garbage collection time. (3.1.12) jstat -printcompilation 130501234[wkq@VM_77_25_centos ~]$ jstat -printcompilation 13050Compiled Size Type Method 7469 63 1 org/apache/logging/log4j/spi/AbstractLogger trace[wkq@VM_77_25_centos ~]$ (3.2) 常用jstat命令(3.2.1) jstat -gc -h5 -t 13050 1000 10 分析进程id为13050的gc情况，每隔1000ms打印一次记录，打印10次停止，每5行后打印指标头部-gc 查看gc情况-h5 每5行后打印指标头部-t 进程启动时间13050 进程id 也就是linux的pid1000 每隔1000ms打印一次10 共打印10行 1234567891011121314[wkq@VM_77_25_centos ~]$ jstat -gc -h5 -t 13050 1000 10Timestamp S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT 217492.5 8512.0 8512.0 0.0 77.3 68160.0 13154.8 963392.0 60728.2 77732.0 71386.5 11984.0 9862.9 133 2.921 8 0.947 3.868 217493.7 8512.0 8512.0 0.0 77.3 68160.0 13154.8 963392.0 60728.2 77732.0 71386.5 11984.0 9862.9 133 2.921 8 0.947 3.868 217494.7 8512.0 8512.0 0.0 77.3 68160.0 14055.9 963392.0 60728.2 77732.0 71386.5 11984.0 9862.9 133 2.921 8 0.947 3.868 217495.7 8512.0 8512.0 0.0 77.3 68160.0 14055.9 963392.0 60728.2 77732.0 71386.5 11984.0 9862.9 133 2.921 8 0.947 3.868 217496.7 8512.0 8512.0 0.0 77.3 68160.0 14055.9 963392.0 60728.2 77732.0 71386.5 11984.0 9862.9 133 2.921 8 0.947 3.868Timestamp S0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT 217497.7 8512.0 8512.0 0.0 77.3 68160.0 14055.9 963392.0 60728.2 77732.0 71386.5 11984.0 9862.9 133 2.921 8 0.947 3.868 217498.7 8512.0 8512.0 0.0 77.3 68160.0 14055.9 963392.0 60728.2 77732.0 71386.5 11984.0 9862.9 133 2.921 8 0.947 3.868 217499.7 8512.0 8512.0 0.0 77.3 68160.0 14055.9 963392.0 60728.2 77732.0 71386.5 11984.0 9862.9 133 2.921 8 0.947 3.868 217500.7 8512.0 8512.0 0.0 77.3 68160.0 14055.9 963392.0 60728.2 77732.0 71386.5 11984.0 9862.9 133 2.921 8 0.947 3.868 217501.7 8512.0 8512.0 0.0 77.3 68160.0 14068.0 963392.0 60728.2 77732.0 71386.5 11984.0 9862.9 133 2.921 8 0.947 3.868[wkq@VM_77_25_centos ~]$ 我们可以比较 Java 进程的启动时间以及总 GC 时间（GCT 列），或者两次测量的间隔时间以及总 GC 时间的增量，来得出 GC 时间占运行时间的比例。如果该比例超过 20%，则说明目前堆的压力较大；如果该比例超过 90%，则说明堆里几乎没有可用空间，随时都可能抛出 OOM 异常。 (3.2.2) jstat -gcutil -h5 -t 13050 1000 10 分析进程id为13050的gcutil情况，每隔1000ms打印一次记录，打印15次停止，每5行后打印指标头部-gcutil 查看gcutil情况-h5 每5行后打印指标头部-t 进程启动时间13050 进程id 也就是linux的pid1000 每隔1000ms打印一次10 共打印10行 1234567891011121314[wkq@VM_77_25_centos ~]$ jstat -gcutil -h5 -t 13050 1000 10Timestamp S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 217569.9 0.00 0.91 23.55 6.30 91.84 82.30 133 2.921 8 0.947 3.868 217570.9 0.00 0.91 23.55 6.30 91.84 82.30 133 2.921 8 0.947 3.868 217571.9 0.00 0.91 23.55 6.30 91.84 82.30 133 2.921 8 0.947 3.868 217572.9 0.00 0.91 23.55 6.30 91.84 82.30 133 2.921 8 0.947 3.868 217573.9 0.00 0.91 23.55 6.30 91.84 82.30 133 2.921 8 0.947 3.868Timestamp S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 217574.9 0.00 0.91 23.55 6.30 91.84 82.30 133 2.921 8 0.947 3.868 217575.9 0.00 0.91 23.55 6.30 91.84 82.30 133 2.921 8 0.947 3.868 217576.9 0.00 0.91 23.55 6.30 91.84 82.30 133 2.921 8 0.947 3.868 217577.9 0.00 0.91 23.55 6.30 91.84 82.30 133 2.921 8 0.947 3.868 217578.9 0.00 0.91 23.55 6.30 91.84 82.30 133 2.921 8 0.947 3.868[wkq@VM_77_25_centos ~]$ (4) Java配置信息工具 jinfo (Java Configuration Info ) jinfo是实时查看和调整虚拟机各项参数的工具 (4.1) jinfo命令语法格式1jinfo [ option ] pid 123456789101112131415161718[wkq@VM_77_25_centos ~]$ jinfo -helpUsage: jinfo [option] &lt;pid&gt; (to connect to running process) jinfo [option] &lt;executable &lt;core&gt; (to connect to a core file) jinfo [option] [server_id@]&lt;remote server IP or hostname&gt; (to connect to remote debug server)where &lt;option&gt; is one of: -flag &lt;name&gt; to print the value of the named VM flag -flag [+|-]&lt;name&gt; to enable or disable the named VM flag -flag &lt;name&gt;=&lt;value&gt; to set the named VM flag to the given value -flags to print VM flags -sysprops to print Java system properties &lt;no option&gt; to print both of the above -h | -help to print this help message[wkq@VM_77_25_centos ~]$ 123456789101112131415161718[wkq@VM_77_25_centos ~]$ jinfo -optionUsage: jinfo [option] &lt;pid&gt; (to connect to running process) jinfo [option] &lt;executable &lt;core&gt; (to connect to a core file) jinfo [option] [server_id@]&lt;remote server IP or hostname&gt; (to connect to remote debug server)where &lt;option&gt; is one of: -flag &lt;name&gt; to print the value of the named VM flag -flag [+|-]&lt;name&gt; to enable or disable the named VM flag -flag &lt;name&gt;=&lt;value&gt; to set the named VM flag to the given value -flags to print VM flags -sysprops to print Java system properties &lt;no option&gt; to print both of the above -h | -help to print this help message[wkq@VM_77_25_centos ~]$ 如果遇到 Error attaching to process: sun.jvm.hotspot.debugger.DebuggerException: Can&#39;t attach to the process: ptrace(PTRACE_ATTACH, ..) failed for : Operation not permitted，执行 echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope命令即可 (4.1.1) jinfo pid12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485[wkq@VM_77_25_centos ~]$ jinfo 13050Attaching to process ID 13050, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.172-b11Java System Properties:jna.platform.library.path = /usr/lib64:/lib64:/usr/lib:/lib:/usr/lib64/dyninst:/usr/lib64/mysqljava.runtime.name = Java(TM) SE Runtime Environmentsun.boot.library.path = /home/wkq/software/jdk1.8.0_172/jre/lib/amd64java.vm.version = 25.172-b11es.path.home = /home/wkq/software/elasticsearch-6.6.2log4j.shutdownHookEnabled = falsejava.vendor.url = http://java.oracle.com/java.vm.vendor = Oracle Corporationpath.separator = :file.encoding.pkg = sun.iojava.vm.name = Java HotSpot(TM) 64-Bit Server VMjna.loaded = truesun.os.patch.level = unknownuser.country = USsun.java.launcher = SUN_STANDARDes.networkaddress.cache.negative.ttl = 10jna.nosys = truejava.vm.specification.name = Java Virtual Machine Specificationuser.dir = /home/wkq/software/elasticsearch-6.6.2java.runtime.version = 1.8.0_172-b11java.awt.graphicsenv = sun.awt.X11GraphicsEnvironmentjava.endorsed.dirs = /home/wkq/software/jdk1.8.0_172/jre/lib/endorsedos.arch = amd64java.io.tmpdir = /tmp/elasticsearch-5035355569386013893line.separator =es.networkaddress.cache.ttl = 60es.logs.node_name = elasticsearch_001_datajava.vm.specification.vendor = Oracle Corporationos.name = Linuxio.netty.noKeySetOptimization = truesun.jnu.encoding = ANSI_X3.4-1968jnidispatch.path = /tmp/elasticsearch-5035355569386013893/jna-117789/jna8829728915470247279.tmpjava.library.path = /usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/libsun.nio.ch.bugLevel =es.logs.cluster_name = elasticsearch_testjava.specification.name = Java Platform API Specificationjava.class.version = 52.0sun.management.compiler = HotSpot 64-Bit Tiered Compilersos.version = 3.10.0-957.21.3.el7.x86_64user.home = /home/wkquser.timezone = Asia/Shanghaijava.awt.printerjob = sun.print.PSPrinterJobfile.encoding = UTF-8java.specification.version = 1.8es.distribution.type = tario.netty.recycler.maxCapacityPerThread = 0user.name = wkqes.logs.base_path = /home/wkq/software/elasticsearch-6.6.2/logsjava.class.path = es.path.conf = /home/wkq/software/elasticsearch-6.6.2/configjava.vm.specification.version = 1.8java.home = /home/wkq/software/jdk1.8.0_172/jresun.java.command = org.elasticsearch.bootstrap.Elasticsearch -dsun.arch.data.model = 64io.netty.noUnsafe = trueuser.language = enjava.specification.vendor = Oracle Corporationawt.toolkit = sun.awt.X11.XToolkitjava.vm.info = mixed modejava.version = 1.8.0_172java.ext.dirs = /home/wkq/software/jdk1.8.0_172/jre/lib/ext:/usr/java/packages/lib/extsun.boot.class.path = java.awt.headless = truejava.vendor = Oracle Corporationfile.separator = /java.vendor.url.bug = http://bugreport.sun.com/bugreport/es.distribution.flavor = defaultsun.io.unicode.encoding = UnicodeLittlesun.cpu.endian = littlelog4j2.disable.jmx = truesun.cpu.isalist =VM Flags:Non-default VM flags: -XX:+AlwaysPreTouch -XX:CICompilerCount=2 -XX:CMSInitiatingOccupancyFraction=75 -XX:ErrorFile=null -XX:GCLogFileSize=67108864 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=null -XX:InitialHeapSize=1073741824 -XX:MaxHeapSize=1073741824 -XX:MaxNewSize=87228416 -XX:MaxTenuringThreshold=6 -XX:MinHeapDeltaBytes=196608 -XX:NewSize=87228416 -XX:NumberOfGCLogFiles=32 -XX:OldPLABSize=16 -XX:OldSize=986513408 -XX:-OmitStackTraceInFastThrow -XX:+PrintGC -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintTenuringDistribution -XX:ThreadStackSize=1024 -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseGCLogFileRotation -XX:+UseParNewGCCommand line: -Xms1g -Xmx1g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.io.tmpdir=/tmp/elasticsearch-5035355569386013893 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data -XX:ErrorFile=logs/hs_err_pid%p.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -Xloggc:logs/gc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=32 -XX:GCLogFileSize=64m -Des.path.home=/home/wkq/software/elasticsearch-6.6.2 -Des.path.conf=/home/wkq/software/elasticsearch-6.6.2/config -Des.distribution.flavor=default -Des.distribution.type=tar[wkq@VM_77_25_centos ~]$ (4.1.2) jinfo -flag MaxMetaspaceSize pid123[wkq@VM_77_25_centos ~]$ jinfo -flag MaxMetaspaceSize 13050-XX:MaxMetaspaceSize=18446744073709547520[wkq@VM_77_25_centos ~]$ (4.1.3) jinfo -flag ThreadStackSize pid123[wkq@VM_77_25_centos ~]$ jinfo -flag ThreadStackSize 13050-XX:ThreadStackSize=1024[wkq@VM_77_25_centos ~]$ (4.1.4) jinfo -flag MaxNewSize pid123[wkq@VM_77_25_centos ~]$ jinfo -flag MaxNewSize 13050-XX:MaxNewSize=87228416[wkq@VM_77_25_centos ~]$ (4.1.5) jinfo -flag CMSInitiatingOccupancyFraction pid123[wkq@VM_77_25_centos ~]$ jinfo -flag CMSInitiatingOccupancyFraction 13050-XX:CMSInitiatingOccupancyFraction=75[wkq@VM_77_25_centos ~]$ (4.1.6) 查看所有JVM参数 java -XX:+PrintFlagsInitial123456789101112131415[wkq@VM_77_25_centos ~]$ java -XX:+PrintFlagsInitial[Global flags] uintx AdaptiveSizeDecrementScaleFactor = 4 &#123;product&#125; uintx AdaptiveSizeMajorGCDecayTimeScale = 10 &#123;product&#125;... intx WorkAroundNPTLTimedWaitHang = 1 &#123;product&#125; uintx YoungGenerationSizeIncrement = 20 &#123;product&#125; uintx YoungGenerationSizeSupplement = 80 &#123;product&#125; uintx YoungGenerationSizeSupplementDecay = 8 &#123;product&#125; uintx YoungPLABSize = 4096 &#123;product&#125; bool ZeroTLAB = false &#123;product&#125; intx hashCode = 5 &#123;product&#125;[wkq@VM_77_25_centos ~]$ (4.1.7) 查看所有支持动态修改的JVM参数 java -XX:+PrintFlagsInitial | grep manageable1234567891011121314151617181920[wkq@VM_77_25_centos ~]$ java -XX:+PrintFlagsInitial | grep manageable intx CMSAbortablePrecleanWaitMillis = 100 &#123;manageable&#125; intx CMSTriggerInterval = -1 &#123;manageable&#125; intx CMSWaitDuration = 2000 &#123;manageable&#125; bool HeapDumpAfterFullGC = false &#123;manageable&#125; bool HeapDumpBeforeFullGC = false &#123;manageable&#125; bool HeapDumpOnOutOfMemoryError = false &#123;manageable&#125; ccstr HeapDumpPath = &#123;manageable&#125; uintx MaxHeapFreeRatio = 70 &#123;manageable&#125; uintx MinHeapFreeRatio = 40 &#123;manageable&#125; bool PrintClassHistogram = false &#123;manageable&#125; bool PrintClassHistogramAfterFullGC = false &#123;manageable&#125; bool PrintClassHistogramBeforeFullGC = false &#123;manageable&#125; bool PrintConcurrentLocks = false &#123;manageable&#125; bool PrintGC = false &#123;manageable&#125; bool PrintGCDateStamps = false &#123;manageable&#125; bool PrintGCDetails = false &#123;manageable&#125; bool PrintGCID = false &#123;manageable&#125; bool PrintGCTimeStamps = false &#123;manageable&#125;[wkq@VM_77_25_centos ~]$ (4.1.7) 调整JVM参数-布尔类型1jinfo -flag [+|-]&lt;name&gt; PID 1234[wkq@VM_77_25_centos ~]$ jinfo -flag +PrintGC 13050[wkq@VM_77_25_centos ~]$[wkq@VM_77_25_centos ~]$ jinfo -flag +PrintGCDetails 13050[wkq@VM_77_25_centos ~]$ 如果没报错则代表生效，加完以后可以通过 jinfo -flags 13050 验证 (4.1.8) 调整JVM参数-数字/字符串类型1jinfo -flag &lt;name&gt;=&lt;value&gt; PID 12[wkq@VM_77_25_centos ~]$ jinfo -flag MaxHeapFreeRatio=65 13050[wkq@VM_77_25_centos ~]$ 没报错相当于修改成功，但是怎么验证是否生效，可以通过 jinfo -flags 13050 验证 12345678[wkq@VM_77_25_centos ~]$ jinfo -flags 13050Attaching to process ID 13050, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.172-b11Non-default VM flags: -XX:+AlwaysPreTouch -XX:CICompilerCount=2 -XX:CMSInitiatingOccupancyFraction=75 -XX:ErrorFile=null -XX:GCLogFileSize=67108864 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=null -XX:InitialHeapSize=1073741824 -XX:MaxHeapFreeRatio=65 -XX:MaxHeapSize=1073741824 -XX:MaxNewSize=87228416 -XX:MaxTenuringThreshold=6 -XX:MinHeapDeltaBytes=196608 -XX:NewSize=87228416 -XX:NumberOfGCLogFiles=32 -XX:OldPLABSize=16 -XX:OldSize=986513408 -XX:-OmitStackTraceInFastThrow -XX:+PrintGC -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintTenuringDistribution -XX:ThreadStackSize=1024 -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseGCLogFileRotation -XX:+UseParNewGCCommand line: -Xms1g -Xmx1g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.io.tmpdir=/tmp/elasticsearch-5035355569386013893 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data -XX:ErrorFile=logs/hs_err_pid%p.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -Xloggc:logs/gc.log -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=32 -XX:GCLogFileSize=64m -Des.path.home=/home/wkq/software/elasticsearch-6.6.2 -Des.path.conf=/home/wkq/software/elasticsearch-6.6.2/config -Des.distribution.flavor=default -Des.distribution.type=tar[wkq@VM_77_25_centos ~]$ 通过 jinfo -flags 13050 获取的结果可以看到 出现 Command failed in target VM 则表示这个flag参数不支持123456789[wkq@VM_77_25_centos ~]$ jinfo -flag ErrorFile=/home/wkq/es_error_file 13050Exception in thread &quot;main&quot; com.sun.tools.attach.AttachOperationFailedException: flag &apos;ErrorFile&apos; cannot be changed at sun.tools.attach.LinuxVirtualMachine.execute(LinuxVirtualMachine.java:229) at sun.tools.attach.HotSpotVirtualMachine.executeCommand(HotSpotVirtualMachine.java:261) at sun.tools.attach.HotSpotVirtualMachine.setFlag(HotSpotVirtualMachine.java:234) at sun.tools.jinfo.JInfo.flag(JInfo.java:134) at sun.tools.jinfo.JInfo.main(JInfo.java:81)[wkq@VM_77_25_centos ~]$ (5) Java内存映像工具 jmap Jmap是一个可以输出所有内存中对象的工具，甚至可以将VM 中的heap，以二进制输出成文本。打印出某个java进程（使用pid）内存内的，所有‘对象’的情况（如：产生那些对象，及其数量）。 (5.1) jmap命令123456789101112131415161718192021222324252627282930[wkq@VM_77_25_centos ~]$ jmap -helpUsage: jmap [option] &lt;pid&gt; (to connect to running process) jmap [option] &lt;executable &lt;core&gt; (to connect to a core file) jmap [option] [server_id@]&lt;remote server IP or hostname&gt; (to connect to remote debug server)where &lt;option&gt; is one of: &lt;none&gt; to print same info as Solaris pmap -heap to print java heap summary -histo[:live] to print histogram of java object heap; if the &quot;live&quot; suboption is specified, only count live objects -clstats to print class loader statistics -finalizerinfo to print information on objects awaiting finalization -dump:&lt;dump-options&gt; to dump java heap in hprof binary format dump-options: live dump only live objects; if not specified, all objects in the heap are dumped. format=b binary format file=&lt;file&gt; dump heap to &lt;file&gt; Example: jmap -dump:live,format=b,file=heap.bin &lt;pid&gt; -F force. Use with -dump:&lt;dump-options&gt; &lt;pid&gt; or -histo to force a heap dump or histogram when &lt;pid&gt; does not respond. The &quot;live&quot; suboption is not supported in this mode. -h | -help to print this help message -J&lt;flag&gt; to pass &lt;flag&gt; directly to the runtime system[wkq@VM_77_25_centos ~]$ 123456789101112131415161718192021222324252627282930[wkq@VM_77_25_centos ~]$ jmap -optionsUsage: jmap [option] &lt;pid&gt; (to connect to running process) jmap [option] &lt;executable &lt;core&gt; (to connect to a core file) jmap [option] [server_id@]&lt;remote server IP or hostname&gt; (to connect to remote debug server)where &lt;option&gt; is one of: &lt;none&gt; to print same info as Solaris pmap -heap to print java heap summary -histo[:live] to print histogram of java object heap; if the &quot;live&quot; suboption is specified, only count live objects -clstats to print class loader statistics -finalizerinfo to print information on objects awaiting finalization -dump:&lt;dump-options&gt; to dump java heap in hprof binary format dump-options: live dump only live objects; if not specified, all objects in the heap are dumped. format=b binary format file=&lt;file&gt; dump heap to &lt;file&gt; Example: jmap -dump:live,format=b,file=heap.bin &lt;pid&gt; -F force. Use with -dump:&lt;dump-options&gt; &lt;pid&gt; or -histo to force a heap dump or histogram when &lt;pid&gt; does not respond. The &quot;live&quot; suboption is not supported in this mode. -h | -help to print this help message -J&lt;flag&gt; to pass &lt;flag&gt; directly to the runtime system[wkq@VM_77_25_centos ~]$ 更多详细信息参考 man jmap (5.1.1) jmap pid123456789101112131415161718192021222324[wkq@VM_77_25_centos ~]$ jmap 13050Attaching to process ID 13050, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.172-b110x0000000000400000 7K /home/wkq/software/jdk1.8.0_172/bin/java0x00007fb911271000 49K /home/wkq/software/jdk1.8.0_172/jre/lib/amd64/libmanagement.so0x00007fb91167a000 86K /usr/lib64/libgcc_s-4.8.5-20150702.so.10x00007fb911890000 251K /home/wkq/software/jdk1.8.0_172/jre/lib/amd64/libsunec.so0x00007fb9242c2000 112K /home/wkq/software/jdk1.8.0_172/jre/lib/amd64/libnet.so0x00007fb9244d9000 91K /home/wkq/software/jdk1.8.0_172/jre/lib/amd64/libnio.so0x00007fb94065d000 125K /home/wkq/software/jdk1.8.0_172/jre/lib/amd64/libzip.so0x00007fb940879000 60K /usr/lib64/libnss_files-2.17.so0x00007fb940a8c000 221K /home/wkq/software/jdk1.8.0_172/jre/lib/amd64/libjava.so0x00007fb940cb8000 64K /home/wkq/software/jdk1.8.0_172/jre/lib/amd64/libverify.so0x00007fb940ec7000 43K /usr/lib64/librt-2.17.so0x00007fb9410cf000 1115K /usr/lib64/libm-2.17.so0x00007fb9413d1000 16667K /home/wkq/software/jdk1.8.0_172/jre/lib/amd64/server/libjvm.so0x00007fb9423d2000 2068K /usr/lib64/libc-2.17.so0x00007fb942793000 19K /usr/lib64/libdl-2.17.so0x00007fb942997000 101K /home/wkq/software/jdk1.8.0_172/lib/amd64/jli/libjli.so0x00007fb942bad000 140K /usr/lib64/libpthread-2.17.so0x00007fb942dc9000 155K /usr/lib64/ld-2.17.so[wkq@VM_77_25_centos ~]$ (5.1.2) jmap -dump:[live,] format=b, file=file_path1234[wkq@VM_77_25_centos ~]$ jmap -dump:live,format=b,file=13050.log 13050Dumping heap to /home/wkq/13050.log ...Heap dump file created[wkq@VM_77_25_centos ~]$ (5.1.3) jmap -finalizerinfo pid1234567[wkq@VM_77_25_centos ~]$ jmap -finalizerinfo 13050Attaching to process ID 13050, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.172-b11Number of objects pending for finalization: 0[wkq@VM_77_25_centos ~]$ (5.1.4) jmap -heap pid jmap -heap pid jmap-J-d64 -heap pid 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[wkq@VM_77_25_centos ~]$ jmap -heap 13050Attaching to process ID 13050, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.172-b11using parallel threads in the new generation.using thread-local object allocation.Concurrent Mark-Sweep GCHeap Configuration: MinHeapFreeRatio = 40 MaxHeapFreeRatio = 70 MaxHeapSize = 1073741824 (1024.0MB) NewSize = 87228416 (83.1875MB) MaxNewSize = 87228416 (83.1875MB) OldSize = 986513408 (940.8125MB) NewRatio = 2 SurvivorRatio = 8 MetaspaceSize = 21807104 (20.796875MB) CompressedClassSpaceSize = 1073741824 (1024.0MB) MaxMetaspaceSize = 17592186044415 MB G1HeapRegionSize = 0 (0.0MB)Heap Usage:New Generation (Eden + 1 Survivor Space): capacity = 78512128 (74.875MB) used = 7022576 (6.6972503662109375MB) free = 71489552 (68.17774963378906MB) 8.94457477958055% usedEden Space: capacity = 69795840 (66.5625MB) used = 7022576 (6.6972503662109375MB) free = 62773264 (59.86524963378906MB) 10.061596794307512% usedFrom Space: capacity = 8716288 (8.3125MB) used = 0 (0.0MB) free = 8716288 (8.3125MB) 0.0% usedTo Space: capacity = 8716288 (8.3125MB) used = 0 (0.0MB) free = 8716288 (8.3125MB) 0.0% usedconcurrent mark-sweep generation: capacity = 986513408 (940.8125MB) used = 62168384 (59.28839111328125MB) free = 924345024 (881.5241088867188MB) 6.301828591061582% used22982 interned Strings occupying 3212304 bytes.[wkq@VM_77_25_centos ~]$ (5.1.5) jmap -histo pid1234567891011121314151617181920212223242526[wkq@VM_77_25_centos ~]$ jmap -histo 13050 | more num #instances #bytes class name---------------------------------------------- 1: 258021 20550728 [C 2: 432469 13839008 java.util.HashMap$Node 3: 228108 5474592 java.lang.String 4: 38596 4839800 [Ljava.util.HashMap$Node; 5: 86202 4137696 java.util.HashMap 6: 21464 2574920 [B 7: 74456 2382592 java.util.Collections$UnmodifiableMap 8: 12306 2118456 [I 9: 30839 1957320 [Ljava.lang.Object; 10: 15798 1730136 java.lang.Class 11: 52294 1673408 java.util.concurrent.ConcurrentHashMap$Node 12: 24594 787008 java.util.AbstractList$Itr 13: 49038 784608 org.elasticsearch.common.lucene.LoggerInfoStream$$Lambda$3048/931367447 14: 24519 784608 org.elasticsearch.index.engine.Engine$$Lambda$3100/1060530393 15: 23576 754432 java.lang.ref.WeakReference 16: 30639 735336 java.util.Arrays$ArrayList 17: 24519 588456 [Lorg.elasticsearch.common.lease.Releasable; 18: 24519 588456 org.elasticsearch.index.engine.Engine$Searcher 19: 8132 585504 java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask 20: 9564 535584 java.lang.invoke.MemberName 21: 6052 532576 java.lang.reflect.Method 22: 33099 529584 java.lang.Object (5.1.6) jmap -clstats pid12345678910111213141516171819202122[wkq@VM_77_25_centos ~]$ jmap -clstats 13050Attaching to process ID 13050, please wait...Debugger attached successfully.Server compiler detected.JVM version is 25.172-b11finding class loader instances ..done.computing per loader stat ..done.please wait.. computing liveness.liveness analysis may be inaccurate ...class_loader classes bytes parent_loader alive? type&lt;bootstrap&gt; 2353 4122324 null live &lt;internal&gt;0x00000000c5fa9378 1 714 null dead org/elasticsearch/painless/lookup/PainlessLookupBuilder$BridgeLoader@0x00000001003f45d00x00000000c5fa9878 1 714 null dead org/elasticsearch/painless/lookup/PainlessLookupBuilder$BridgeLoader@0x00000001003f45d00x00000000c58b8708 0 0 0x00000000c544a3d0 dead org/elasticsearch/plugins/ExtendedPluginsClassLoader@0x0000000100263e900x00000000c5ab86a8 1 1471 0x00000000c544a3d0 dead sun/reflect/DelegatingClassLoader@0x000000010000a0280x00000000c5ac4b28 11 20182 0x00000000c5c12600 dead java/net/FactoryURLClassLoader@0x0000000100259738...total = 86 11582 19740221 N/A alive=1, dead=85 N/A[wkq@VM_77_25_centos ~]$ 省略部分详细信息 (6) 虚拟机堆转储快照分析工具 jhat (7) Java堆栈跟踪工具 jstack (Java Stack Trace) jstack主要用来查看某个Java进程内的线程堆栈信息。在实际运行中，往往一次 dump的信息，还不足以确认问题。建议产生三次 dump信息，如果每次 dump都指向同一个问题，我们才确定问题的典型性。 (7.1) jstack命令~]$ jstack -help12345678910111213141516Usage: jstack [-l] &lt;pid&gt; (to connect to running process) jstack -F [-m] [-l] &lt;pid&gt; (to connect to a hung process) jstack [-m] [-l] &lt;executable&gt; &lt;core&gt; (to connect to a core file) jstack [-m] [-l] [server_id@]&lt;remote server IP or hostname&gt; (to connect to a remote debug server)Options: -F to force a thread dump. Use when jstack &lt;pid&gt; does not respond (process is hung) -m to print both java and native frames (mixed mode) -l long listing. Prints additional information about locks -h or -help to print this help message[wkq@VM_77_25_centos ~]$ 1234567891011121314151617[wkq@VM_77_25_centos ~]$ jstack -optionsUsage: jstack [-l] &lt;pid&gt; (to connect to running process) jstack -F [-m] [-l] &lt;pid&gt; (to connect to a hung process) jstack [-m] [-l] &lt;executable&gt; &lt;core&gt; (to connect to a core file) jstack [-m] [-l] [server_id@]&lt;remote server IP or hostname&gt; (to connect to a remote debug server)Options: -F to force a thread dump. Use when jstack &lt;pid&gt; does not respond (process is hung) -m to print both java and native frames (mixed mode) -l long listing. Prints additional information about locks -h or -help to print this help message[wkq@VM_77_25_centos ~]$ (7.1.1) jstack pid12345678910111213[wkq@VM_77_25_centos ~]$ jstack 130502020-03-29 08:55:30Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.172-b11 mixed mode):&quot;Attach Listener&quot; #48 daemon prio=9 os_prio=0 tid=0x00007fb90805b800 nid=0x1c9f waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE&quot;elasticsearch[elasticsearch_001_data][flush][T#1]&quot; #47 daemon prio=5 os_prio=0 tid=0x00007fb93c693800 nid=0x376c waiting on condition [0x00007fb8fd520000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for &lt;0x00000000c600d388&gt; (a org.elasticsearch.common.util.concurrent.EsExecutors$ExecutorScalingQueue)...[wkq@VM_77_25_centos ~]$ (7.1.2) jstack -l pid1234567891011121314151617181920212223242526272829303132[wkq@VM_77_25_centos ~]$ jstack -l 130502020-03-29 23:09:50Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.172-b11 mixed mode):&quot;elasticsearch[elasticsearch_001_data][generic][T#8]&quot; #51 daemon prio=5 os_prio=0 tid=0x00007fb93df68000 nid=0x77cf waiting on condition [0x00007fb8ff934000] java.lang.Thread.State: TIMED_WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for &lt;0x00000000c5ed5f70&gt; (a org.elasticsearch.common.util.concurrent.EsExecutors$ExecutorScalingQueue) at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215) Locked ownable synchronizers: - None...&quot;Attach Listener&quot; #48 daemon prio=9 os_prio=0 tid=0x00007fb90805b800 nid=0x1c9f waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE...&quot;VM Thread&quot; os_prio=0 tid=0x00007fb93c0b5000 nid=0x32ff runnable&quot;Gang worker#0 (Parallel GC Threads)&quot; os_prio=0 tid=0x00007fb93c01d000 nid=0x32fd runnable&quot;Concurrent Mark-Sweep GC Thread&quot; os_prio=0 tid=0x00007fb93c040000 nid=0x32fe runnable&quot;VM Periodic Task Thread&quot; os_prio=0 tid=0x00007fb93c103800 nid=0x3307 waiting on conditionJNI global references: 8827[wkq@VM_77_25_centos ~]$ (8) Java监视与管理控制台 JConsole (Java Monitoring and Managerment Console) (9) 多合一故障处理工具 VisuaIVM (All-in-One Java Troubleshooting Tool) VisuaIVM (All-in-One Java Troubleshooting Tool) 是到目前为止随JDK发布的功能最强大的运行监视和故障处理程序。 References[1] 《深入理解JAVA虚拟机: JVM高级特性与最佳实践》 周志明[2] JVM性能调优监控工具jps、jstack、jmap、jhat、jstat、hprof使用详解[3] JAVA JPS 命令详解[4] jstat使用详解（分析JVM的使用情况）[5] 使用Java监控工具出现 Can’t attach to the process[6] JVM系列：jinfo命令详解[7] jmap命令详解[8] jstack命令详解[9] 深入拆解Java虚拟机 - 30 | Java虚拟机的监控及诊断工具（命令行篇）[10] oracle monitoring-tools-and-commands[11] 深入拆解Java虚拟机 - 31 | Java虚拟机的监控及诊断工具（GUI篇）[12] visualvm[13] jitwatch]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[chrome-notes]]></title>
    <url>%2F2020%2F03%2F20%2Fchrome-notes%2F</url>
    <content type="text"><![CDATA[References[1] chrome-devtools[2] chrome extensions[3] Chrome插件[4] crx4chrome]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式]]></title>
    <url>%2F2020%2F03%2F14%2Fdesign-pattern%2F</url>
    <content type="text"><![CDATA[其实，很多东西都是经历过，遇到问题，解决了，然后才知道。大学时候学过设计模式，但是没有实践过，很快就忘了。后来开发的时候遇到一些问题，解决了，然后感觉像哪个设计模式，回过头去看设计模式，感觉挺有用的。 学完设计模式以后，建议用简单、高效、易维护的态度去设计开发。 (1) 什么是设计模式 软件设计模式（Design pattern），又称设计模式，是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结。 使用设计模式是为了可重用代码、让代码更容易被他人理解、保证代码可靠性、程序的重用性。 (2) 设计模式的分类 创建型模式：对象实例化的模式，创建型模式用于解耦对象的实例化过程。 结构型模式：把类或对象结合在一起形成一个更大的结构。 行为型模式：类和对象如何交互，及划分责任和算法。 (3) 各个模式的关键点 单例模式：某个类只能有一个实例，提供一个全局的访问点。 工厂模式：定义一个创建对象的接口，让子类决定实例化那个类。 抽象工厂模式：创建相关或依赖对象的家族，而无需明确指定具体类。 建造者模式：封装一个复杂对象的构建过程，并可以按步骤构造。 原型模式：通过复制现有的实例来创建新的实例。 适配器模式：将一个类的方法接口转换成客户希望的另外一个接口。 组合模式：将对象组合成树形结构以表示“”部分-整体“”的层次结构。 装饰模式：动态的给对象添加新的功能。 代理模式：为其他对象提供一个代理以便控制这个对象的访问。 亨元模式：通过共享技术来有效的支持大量细粒度的对象。 外观模式：对外提供一个统一的方法，来访问子系统中的一群接口。 桥接模式：将抽象部分和它的实现部分分离，使它们都可以独立的变化。 模板模式：定义一个算法结构，而将一些步骤延迟到子类实现。 解释器模式：给定一个语言，定义它的文法的一种表示，并定义一个解释器。 策略模式：定义一系列算法，把他们封装起来，并且使它们可以相互替换。 状态模式：允许一个对象在其对象内部状态改变时改变它的行为。 观察者模式：对象间的一对多的依赖关系。 备忘录模式：在不破坏封装的前提下，保持对象的内部状态。 中介者模式：用一个中介对象来封装一系列的对象交互。 命令模式：将命令请求封装为一个对象，使得可以用不同的请求来进行参数化。 访问者模式：在不改变数据结构的前提下，增加作用于一组对象元素的新功能。 责任链模式：将请求的发送者和接收者解耦，使的多个对象都有处理这个请求的机会。 迭代器模式：一种遍历访问聚合对象中各个元素的方法，不暴露该对象的内部结构。 设计模式的六大原则 开闭原则（Open Close Principle） 里氏代换原则（Liskov Substitution Principle） 依赖倒转原则（Dependence Inversion Principle） 接口隔离原则（Interface Segregation Principle） 迪米特法则，又称最少知道原则（Demeter Principle） 合成复用原则（Composite Reuse Principle） (4) 概说23种设计模式(4.1) 单例模式 (Singleton Pattern)(4.1.1) 什么是单例模式 单例模式是为确保一个类只有一个实例，并为整个系统提供一个全局访问点的一种模式方法。 (4.1.2) 单例模式的特点 优点： 1、在内存里只有一个实例，减少了内存的开销，尤其是频繁的创建和销毁实例。 2、避免对资源的多重占用（比如写文件操作）。 缺点： 没有接口，不能继承，与单一职责原则冲突，一个类应该只关心内部逻辑，而不关心外面怎么样来实例化。 (4.1.3) 单例模式的应用 1、要求生产唯一序列号。 2、WEB 中的计数器，不用每次刷新都在数据库里加一次，用单例先缓存起来。 3、创建的一个对象需要消耗的资源过多，比如 I/O 与数据库的连接等。 例如: 在计算机系统中，线程池、缓存、日志对象、对话框、打印机、显卡的驱动程序对象常被设计成单例。 (4.1.4) 单例模式的实现 单例模式的实现有很多种。以下是比较常用的几种。 (4.1.4.1) 饿汉式单例 饿汉式单例是指在方法调用前，实例就已经创建好了。 (4.1.4.2) 懒汉式单例(4.1.4.3) 双检锁/双重校验锁（DCL，即 double-checked locking）(4.2) 工厂模式 (Factory Pattern)(4.2.1) 什么是工厂模式 工厂模式 工厂模式就好比房屋中介，你想租房，通过中介去租不同的房子。 (4.2.2) 工厂模式的特点 优点： 1、一个调用者想创建一个对象，只要知道其名称就可以了。 2、扩展性高，如果想增加一个产品，只要扩展一个工厂类就可以。 3、屏蔽产品的具体实现，调用者只关心产品的接口。 缺点： 每次增加一个产品时，都需要增加一个具体类和对象实现工厂，使得系统中类的个数成倍增加，在一定程度上增加了系统的复杂度，同时也增加了系统具体类的依赖。这并不是什么好事。 (4.2.3) 工厂模式的应用 1、日志记录器：记录可能记录到本地硬盘、系统事件、远程服务器等，用户可以选择记录日志到什么地方。 2、数据库访问，当用户不知道最后系统采用哪一类数据库，以及数据库可能有变化时。 3、设计一个连接服务器的框架，需要三个协议，”POP3”、”IMAP”、”HTTP”，可以把这三个作为产品类，共同实现一个接口。 (4.2.4) 实现(4.3) 抽象工厂模式(4.3.1) 什么是抽象工厂模式 抽象工厂模式（Abstract Factory Pattern）是围绕一个超级工厂创建其他工厂。该超级工厂又称为其他工厂的工厂。 抽象工厂模式好比所有中介的中介，就行房屋中介、婚姻中介、买菜中介的中介管家一样，租房找中介管家，中介管家找房屋中介，然后租房。 (4.3.2) 抽象工厂模式的特点 特点 (4.3.3) 抽象工厂模式的应用 QQ 换皮肤，一整套一起换。 生成不同操作系统的程序。 (4.3.4) 抽象工厂模式的实现(4.4) 建造者模式 (Builder Pattern) 注意事项：与工厂模式的区别是：建造者模式更加关注与零件装配的顺序。 (4.4.1) 什么是建造者模式 指将一个复杂对象的构造与它的表示分离，使同样的构建过程可以创建不同的表示，这样的设计模式被称为建造者模式。 计算机是由 CPU、主板、内存、硬盘、显卡、机箱、显示器、键盘、鼠标等部件组装而成的，采购员不可能自己去组装计算机，而是将计算机的配置要求告诉计算机销售公司，计算机销售公司安排技术人员去组装计算机，然后再交给要买计算机的采购员。 游戏中的不同角色，其性别、个性、能力、脸型、体型、服装、发型等特性都有所差异。 汽车中的方向盘、发动机、车架、轮胎等部件也多种多样。 每封电子邮件的发件人、收件人、主题、内容、附件等内容也各不相同。 (4.4.2) 建造者模式的特点 优点： 1、建造者独立，易扩展。 2、便于控制细节风险。 缺点： 1、产品必须有共同点，范围有限制。 2、如内部变化复杂，会有很多的建造类。 (4.4.3) 建造者模式的应用 一些基本部件不会变，而其组合经常变化的时候。 1、计算机是由 CPU、主板、内存、硬盘、显卡、机箱、显示器、键盘、鼠标等部件组装而成的，购买电脑时选取自己喜欢的CPU、主板、内存条、硬盘、显卡、机箱、显示器、键盘、鼠标等部件。 2、JAVA 中的 StringBuilder，可以拼接不同的对象。 (4.4.4) 建造者模式的实现References[1] 23种设计模式[2] 23种设计模式全面解析[3] 24种设计模式及案例]]></content>
      <categories>
        <category>pattern</category>
      </categories>
      <tags>
        <tag>pattern</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[域名重定向 hexo博客更换域名]]></title>
    <url>%2F2020%2F03%2F02%2Fdomain-redirection%2F</url>
    <content type="text"><![CDATA[在很长的一段时间里，使用的是多域名，主要有 weikeqin.cn weikeqin.com，但是两个域名解析到的是同一个博客，想把 weikeqin.cn 的流量全部转到 weikeqin.com，因为有些时候 .cn 不如 .com 域名，在某个特殊的时候感觉特别明显。然后就有了这篇文章。 在具体更换域名前的测试中，发现github.com还是比coding.net好用。 (1) 博客域名更换 域名的重定向的方法有 1.域名转发 2.301重定向 3.JS跳转 方法一，域名注册商支持域名转发功能才行！国内的绝大部分域名注册商不支持。 方法二，是Web 服务器给访问老域名的请求返回一个 301 或者 302，然后跳转到新域名上。 如果转发前的域名和转发后的域名访问的都是中国大陆的服务器而且已经备案，可以直接通过域名解析配置 显性URL 301重定向。 由于使用的是 github.com coding.net 的page服务，解析到的不是中国大陆的服务器，这个方法也不可行。 方法三，使用js跳转。具体方法如下。 (1.1) index.html123456789101112131415&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;新域名 https://weikeqin.com&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;p&gt; 跳转中，访问新域名站点 &lt;a href="https://weikeqin.com" target="_blank"&gt; https://weikeqin.com &lt;/a&gt; &lt;/p&gt;&lt;/body&gt;&lt;script type="text/javascript"&gt;console.log("index.html")&lt;/script&gt;&lt;script type="text/javascript"&gt;window.location.href = "https://weikeqin.com";&lt;/script&gt;&lt;/html&gt; (1.2) 404.html1234567891011121314151617181920212223242526272829303132333435363738&lt;script&gt; /*! purl v2.3.1 | MIT */ /** http://cdn.bootcss.com/purl/2.3.1/purl.min.js */ (function (factory) &#123; if (typeof define === "function" &amp;&amp; define.amd) &#123; define(factory) &#125; else &#123; window.purl = factory() &#125; &#125;)(function () &#123; var tag2attr = &#123; a: "href", img: "src", form: "action", base: "href", script: "src", iframe: "src", link: "href" &#125;, key = ["source", "protocol", "authority", "userInfo", "user", "password", "host", "port", "relative", "path", "directory", "file", "query", "fragment"], aliases = &#123; anchor: "fragment" &#125;, parser = &#123; strict: /^(?:([^:\/?#]+):)?(?:\/\/((?:(([^:@]*):?([^:@]*))?@)?([^:\/?#]*)(?::(\d*))?))?((((?:[^?#\/]*\/)*)([^?#]*))(?:\?([^#]*))?(?:#(.*))?)/, loose: /^(?:(?![^:@]+:[^:@\/]*@)([^:\/?#.]+):)?(?:\/\/)?((?:(([^:@]*):?([^:@]*))?@)?([^:\/?#]*)(?::(\d*))?)(((\/(?:[^?#](?![^?#\/]*\.[^?#\/.]+(?:[?#]|$)))*\/?)?([^?#\/]*))(?:\?([^#]*))?(?:#(.*))?)/ &#125;, isint = /^[0-9]+$/; function parseUri(url, strictMode) &#123; var str = decodeURI(url), res = parser[strictMode || false ? "strict" : "loose"].exec(str), uri = &#123; attr: &#123;&#125;, param: &#123;&#125;, seg: &#123;&#125; &#125;, i = 14; while (i--) &#123; uri.attr[key[i]] = res[i] || "" &#125; uri.param["query"] = parseString(uri.attr["query"]); uri.param["fragment"] = parseString(uri.attr["fragment"]); uri.seg["path"] = uri.attr.path.replace(/^\/+|\/+$/g, "").split("/"); uri.seg["fragment"] = uri.attr.fragment.replace(/^\/+|\/+$/g, "").split("/"); uri.attr["base"] = uri.attr.host ? (uri.attr.protocol ? uri.attr.protocol + "://" + uri.attr.host : uri.attr.host) + (uri.attr.port ? ":" + uri.attr.port : "") : ""; return uri &#125; function getAttrName(elm) &#123; var tn = elm.tagName; if (typeof tn !== "undefined") return tag2attr[tn.toLowerCase()]; return tn &#125; function promote(parent, key) &#123; if (parent[key].length === 0) return parent[key] = &#123;&#125;; var t = &#123;&#125;; for (var i in parent[key]) t[i] = parent[key][i]; parent[key] = t; return t &#125; function parse(parts, parent, key, val) &#123; var part = parts.shift(); if (!part) &#123; if (isArray(parent[key])) &#123; parent[key].push(val) &#125; else if ("object" == typeof parent[key]) &#123; parent[key] = val &#125; else if ("undefined" == typeof parent[key]) &#123; parent[key] = val &#125; else &#123; parent[key] = [parent[key], val] &#125; &#125; else &#123; var obj = parent[key] = parent[key] || []; if ("]" == part) &#123; if (isArray(obj)) &#123; if ("" !== val) obj.push(val) &#125; else if ("object" == typeof obj) &#123; obj[keys(obj).length] = val &#125; else &#123; obj = parent[key] = [parent[key], val] &#125; &#125; else if (~part.indexOf("]")) &#123; part = part.substr(0, part.length - 1); if (!isint.test(part) &amp;&amp; isArray(obj)) obj = promote(parent, key); parse(parts, obj, part, val) &#125; else &#123; if (!isint.test(part) &amp;&amp; isArray(obj)) obj = promote(parent, key); parse(parts, obj, part, val) &#125; &#125; &#125; function merge(parent, key, val) &#123; if (~key.indexOf("]")) &#123; var parts = key.split("["); parse(parts, parent, "base", val) &#125; else &#123; if (!isint.test(key) &amp;&amp; isArray(parent.base)) &#123; var t = &#123;&#125;; for (var k in parent.base) t[k] = parent.base[k]; parent.base = t &#125; if (key !== "") &#123; set(parent.base, key, val) &#125; &#125; return parent &#125; function parseString(str) &#123; return reduce(String(str).split(/&amp;|;/), function (ret, pair) &#123; try &#123; pair = decodeURIComponent(pair.replace(/\+/g, " ")) &#125; catch (e) &#123; &#125; var eql = pair.indexOf("="), brace = lastBraceInKey(pair), key = pair.substr(0, brace || eql), val = pair.substr(brace || eql, pair.length); val = val.substr(val.indexOf("=") + 1, val.length); if (key === "") &#123; key = pair; val = "" &#125; return merge(ret, key, val) &#125;, &#123; base: &#123;&#125; &#125;).base &#125; function set(obj, key, val) &#123; var v = obj[key]; if (typeof v === "undefined") &#123; obj[key] = val &#125; else if (isArray(v)) &#123; v.push(val) &#125; else &#123; obj[key] = [v, val] &#125; &#125; function lastBraceInKey(str) &#123; var len = str.length, brace, c; for (var i = 0; i &lt; len; ++i) &#123; c = str[i]; if ("]" == c) brace = false; if ("[" == c) brace = true; if ("=" == c &amp;&amp; !brace) return i &#125; &#125; function reduce(obj, accumulator) &#123; var i = 0, l = obj.length &gt;&gt; 0, curr = arguments[2]; while (i &lt; l) &#123; if (i in obj) curr = accumulator.call(undefined, curr, obj[i], i, obj); ++i &#125; return curr &#125; function isArray(vArg) &#123; return Object.prototype.toString.call(vArg) === "[object Array]" &#125; function keys(obj) &#123; var key_array = []; for (var prop in obj) &#123; if (obj.hasOwnProperty(prop)) key_array.push(prop) &#125; return key_array &#125; function purl(url, strictMode) &#123; if (arguments.length === 1 &amp;&amp; url === true) &#123; strictMode = true; url = undefined &#125; strictMode = strictMode || false; url = url || window.location.toString(); return &#123; data: parseUri(url, strictMode), attr: function (attr) &#123; attr = aliases[attr] || attr; return typeof attr !== "undefined" ? this.data.attr[attr] : this.data.attr &#125;, param: function (param) &#123; return typeof param !== "undefined" ? this.data.param.query[param] : this.data.param.query &#125;, fparam: function (param) &#123; return typeof param !== "undefined" ? this.data.param.fragment[param] : this.data.param.fragment &#125;, segment: function (seg) &#123; if (typeof seg === "undefined") &#123; return this.data.seg.path &#125; else &#123; seg = seg &lt; 0 ? this.data.seg.path.length + seg : seg - 1; return this.data.seg.path[seg] &#125; &#125;, fsegment: function (seg) &#123; if (typeof seg === "undefined") &#123; return this.data.seg.fragment &#125; else &#123; seg = seg &lt; 0 ? this.data.seg.fragment.length + seg : seg - 1; return this.data.seg.fragment[seg] &#125; &#125; &#125; &#125; purl.jQuery = function ($) &#123; if ($ != null) &#123; $.fn.url = function (strictMode) &#123; var url = ""; if (this.length) &#123; url = $(this).attr(getAttrName(this[0])) || "" &#125; return purl(url, strictMode) &#125;; $.url = purl &#125; &#125;; purl.jQuery(window.jQuery); return purl &#125;);&lt;/script&gt;&lt;script&gt; console.log("404.html") var url = purl(); if (url.attr('host') == 'weikeqin.cn') &#123; var old_url = url.attr('source'); var new_url = old_url.replace('weikeqin.cn', "weikeqin.com"); console.log("old_url = " + old_url); console.log("new_url = " + new_url); window.location.replace(new_url); &#125; else if (url.attr('host') == 'www.weikeqin.cn') &#123; var old_url = url.attr('source'); var new_url = old_url.replace('www.weikeqin.cn', "weikeqin.com"); console.log("old_url = " + old_url); console.log("new_url = " + new_url); window.location.replace(new_url); &#125; else if (url.attr('host') == 'weikeqin.github.io') &#123; var old_url = url.attr('source'); var new_url = old_url.replace('weikeqin.github.io/weikeqin.cn.github.io', "weikeqin.com"); console.log("old_url = " + old_url); console.log("new_url = " + new_url); window.location.replace(new_url); &#125; else if (url.attr('host') == 'www.weikeqin.github.io') &#123; var old_url = url.attr('source'); var new_url = old_url.replace('www.weikeqin.github.io/weikeqin.cn.github.io', "weikeqin.com"); console.log("old_url = " + old_url); console.log("new_url = " + new_url); window.location.replace(new_url); &#125; else &#123; window.location.href = "http://weikeqin.com"; &#125;&lt;/script&gt; (1.3) 一些思考 为什么要分成两个文件 index.html 和 404.html，放一个文件不是更好吗？ 其实可以放一个文件里。 但是当脚本有问题的时候或者非正常流程时，分成两个文件会比一个文件更好。 如果脚本有问题，index.html出错的时候(测试的时候遇到了)，有404.html兜底。 (这种情况在开发的时候，或者部署前就可以发现。) 遇到一些域外的情况，404.html文件可以兜底。 分成两个文件是从架构和模块化的角度考虑。 js跳转脚本里为什么要加console.log日志 为了排查问题方便 (一般情况下用不到) (2) 具体实施 用的是万网的域名，weikeqin.cn和weikeqin.com用的都是coding.net的page服务。(实际更复杂) 修改后，weikeqin.cn会用github.com的pages服务，weikeqin.com会用coding.net的pages服务。 准备重定向脚本，在github pages服务准备好。 修改域名解析，在 域名控制台-&gt;解析设置 里 把weikeqin.cn 解析到coding.net改成github.com 回滚方案，如果有问题，修改域名解析，把 weikeqin.cn域名解析重新修改成coding.net对应的配置即可。 其实在实施前准备了很多测试，具体步骤忽略。 (3) 测试 http://weikeqin.cn/ https://weikeqin.cn/ 123Navigated to http://weikeqin.cn/(index):12 index.htmlNavigated to https://weikeqin.com/ http://weikeqin.cn/2017/03/16/git-notes/ https://weikeqin.cn/2017/03/16/git-notes/ 12345678GET http://weikeqin.cn/2017/03/16/git-notes/ 404 (Not Found)/2017/03/16/git-notes/:1 A cookie associated with a cross-site resource at http://hm.baidu.com/ was set without the `SameSite` attribute. A future release of Chrome will only deliver cookies with cross-site requests if they are set with `SameSite=None` and `Secure`. You can review cookies in developer tools under Application&gt;Storage&gt;Cookies and see more details at https://www.chromestatus.com/feature/5088147346030592 and https://www.chromestatus.com/feature/5633521622188032./2017/03/16/git-notes/:1 A cookie associated with a cross-site resource at http://baidu.com/ was set without the `SameSite` attribute. A future release of Chrome will only deliver cookies with cross-site requests if they are set with `SameSite=None` and `Secure`. You can review cookies in developer tools under Application&gt;Storage&gt;Cookies and see more details at https://www.chromestatus.com/feature/5088147346030592 and https://www.chromestatus.com/feature/5633521622188032.Navigated to http://weikeqin.cn/2017/03/16/git-notes/VM62:7 404.htmlVM62:12 old_url = http://weikeqin.cn/2017/03/16/git-notes/VM62:13 new_url = http://weikeqin.com/2017/03/16/git-notes/Navigated to http://weikeqin.com/2017/03/16/git-notes/ 可以看到 weikeqin.cn 重定向到 weikeqin.com 了 References[1] Github Pages页面重定向到新网址，实现域名跳转[2] HeTianCong.github.io[3] 在更换 hexo 博客的域名后需要做的配置工作[4] weikeqin.cn.github.io[5] 301-redirect-for-site-hosted-at-github[6] Permanent redirect from Github gh-pages[7] 网站域名301重定向的五大别样方法[8] 云解析 DNS &gt; 操作指南 &gt; 解析记录管理 &gt; 添加解析记录 &gt; 添加解析记录 [9] 云解析 DNS &gt; 操作指南 &gt; 解析生效测试方法]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github搜索]]></title>
    <url>%2F2020%2F02%2F23%2Fgithub-search%2F</url>
    <content type="text"><![CDATA[GitHub上搜索代码时，是怎么样操作的呢？是不是也是像普通人一样，直接在搜索框里输入要检索的内容，然后不断在列表里翻页找自己需要的内容？或者是简单筛选下，过滤一下。再或者改变一下列表的排序方式。这就是「全部」了吗？ 一般的系统检索功能，都会有一个「高级搜索」的功能。需要在另外的界面里展开，进行二次搜索之类的。 GitHub 有没有类似的呢？ 答案是「肯定的」。做为一个为万千工程师提供服务的网站，不仅要有，而且还要技术范儿。 (1) github高级搜索 高级搜索时，特殊字符会进行转码 %20 是 空格 %3A 是 : %3E 是 &gt; (1.1) 明确搜索仓库标题、仓库描述、README GitHub 提供了便捷的搜索方式，可以限定只搜索仓库的标题、或者描述、README等。 以Spring Cloud 为例，一般一个仓库，大概是这样的 红色箭头指的两个地方，分别是仓库的名称和描述。咱们可以直接限定关键字只查特定的地方。 比如只想查找仓库名称包含 spring cloud 的仓库，可以使用 in:name 关键词 https://github.com/search?utf8=%E2%9C%93&amp;q=in%3Aname+spring+cloud&amp;type= 如果想查找描述的内容，可以使用 in:descripton 关键词 https://github.com/search?q=in%3Adescripton+spring+cloud https://github.com/search?utf8=%E2%9C%93&amp;q=in%3Adescripton+spring+cloud&amp;type= (1.2) 明确搜索 star、fork 数大于多少的 一个项目 star 数的多少，一般代表该项目有受欢迎程度。 要找 star 数大于 3000 的Spring Cloud 仓库: stars:&gt; 数字 关键字 stars:&gt;3000 spring cloud stars: &gt; 3000 spring bloud 会搜不到，不要加空格 https://github.com/search?q=stars%3A%3E3000++spring+cloud 找star数在10到20之间的 spring cloud 仓库: stars:10..20 关键词 https://github.com/search?q=stars%3A10..20+spring+cloud 找fork数在10到20之间的 spring cloud 仓库: https://github.com/search?utf8=%E2%9C%93&amp;q=fork%3A+10..20+spring+cloud&amp;type= (1.3) 明确搜索仓库大小的 比如你只想看个简单的 Demo，不想找特别复杂的且占用磁盘空间较多的，可以在搜索的时候直接限定仓库的 size 。 这个数字代表K, 5000代表着5M。 (2) GitHub 可能提高日常效率的10个常用技巧References[1] GitHub竟然还可以这样玩？涨知识了！[2] 你必须收藏的Github技巧[3] GitHub 可能提高日常效率的10个常用技巧[4] 浏览Github必备的5款神器级别的Chrome插件 [5] 下载Chrome扩展插件Crx离线安装包]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql explain 详解]]></title>
    <url>%2F2020%2F02%2F05%2Fmysql-explain%2F</url>
    <content type="text"><![CDATA[在使用MySQL时，如果发现查询语句耗时，会进行排查及调优，其中常用的一个方法是用explain查看sql执行计划。 (1) explain示例1234567mysql&gt; explain select * from user ;+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+| 1 | SIMPLE | user | NULL | ALL | NULL | NULL | NULL | NULL | 5 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+-------+1 row in set, 1 warning (0.03 sec) 1234567891011121314151617mysql&gt; explain -&gt; select count(*) -&gt; from table_c -&gt; where l_id &lt;&gt; '' -&gt; and l_id is not null -&gt; and l_id not in ( -&gt; select l_id from table_l -&gt; ) ;+----+--------------------+---------------+------------+----------------+---------------+--------------+---------+------+-------+----------+------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+--------------------+---------------+------------+----------------+---------------+--------------+---------+------+-------+----------+------------------------------------+| 1 | PRIMARY | table_c | NULL | range | idx_l_id | idx_l_id | 51 | NULL | 98519 | 100.00 | Using where; Using index || 2 | DEPENDENT SUBQUERY | table_l | NULL | index_subquery | idx_l_id | idx_l_id | 387 | func | 2 | 100.00 | Using index; Full scan on NULL key |+----+--------------------+---------------+------------+----------------+---------------+--------------+---------+------+-------+----------+------------------------------------+2 rows in set, 1 warning (0.01 sec)mysql&gt; 123456789101112131415mysql&gt; explain -&gt; select count(*) -&gt; from table_c -&gt; where l_id not in ( -&gt; select l_id from table_l -&gt; ) ;+----+--------------------+---------------+------------+----------------+---------------+--------------+---------+------+--------+----------+------------------------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+--------------------+---------------+------------+----------------+---------------+--------------+---------+------+--------+----------+------------------------------------+| 1 | PRIMARY | table_c | NULL | index | NULL | idx_l_id | 51 | NULL | 197037 | 100.00 | Using where; Using index || 2 | DEPENDENT SUBQUERY | table_l | NULL | index_subquery | idx_l_id | idx_l_id | 387 | func | 2 | 100.00 | Using index; Full scan on NULL key |+----+--------------------+---------------+------------+----------------+---------------+--------------+---------+------+--------+----------+------------------------------------+2 rows in set, 1 warning (0.00 sec)mysql&gt; (2) explain参数详解 id Columns JSON Name Meaning 1 id select_id 每个select子句的标识id 2 select_type None select语句的类型 3 table table_name 当前表名 4 partitions partitions 匹配的分区 5 type access_type 当前表内访问方式 join type 6 possible_keys possible_keys 可能使用到的索引 7 key key 经过优化器评估最终使用的索引 8 key_len key_length 使用到的索引长度 9 ref ref 引用到的上一个表的列 10 rows rows rows_examined，要得到最终记录索要扫描经过的记录数 11 filtered filtered 按表条件过滤行的百分比 12 Extra None 额外的信息说明 (2.1) id SELECT识别符。这是SELECT查询序列号。这个不重要,查询序号即为sql语句执行的顺序。 (2.2) select_type select_type语句类型有 select类型，它有以下几种: id select_type value JSON name Meaning 1 SIMPLE None 简单的SELECT语句（不包括UNION操作或子查询操作） 2 PRIMARY None PRIMARY：查询中最外层的SELECT（如两表做UNION或者存在子查询的外层的表操作为PRIMARY，内层的操作为UNION） 3 UNION None UNION：UNION操作中，查询中处于内层的SELECT（内层的SELECT语句与外层的SELECT语句没有依赖关系） 4 DEPENDENT UNION dependent(true) DEPENDENT UNION：UNION操作中，查询中处于内层的SELECT（内层的SELECT语句与外层的SELECT语句有依赖关系） 5 UNIOIN RESULT union_result UNION RESULT：UNION操作的结果，id值通常为NULL 6 SUBQUERY None SUBQUERY：子查询中首个SELECT（如果有多个子查询存在） 7 DEPENDENT SUBQUERY dependent(true) DEPENDENT SUBQUERY：子查询中首个SELECT，但依赖于外层的表（如果有多个子查询存在） 8 DERIVED None DERIVED：被驱动的SELECT子查询（子查询位于FROM子句） 9 MATERIALIZED materialized_form_subquery MATERIALIZED：被物化的子查询 10 UNCACHEABLE SUBQUERY cacheable(false) UNCACHEABLE SUBQUERY：对于外层的主表，子查询不可被物化，每次都需要计算（耗时操作） 11 UNCACHEABLE UNION cacheable(false) UNCACHEABLE UNION：UNION操作中，内层的不可被物化的子查询（类似于UNCACHEABLE SUBQUERY） (2.3) table 当前表名 (2.4) partitions 匹配的分区 (2.5) type 当前表内访问方式 性能由好到坏排序： id type value Meaning 1 system 表中只有一行 2 const 单表中最多有一个匹配行，primary key 或者 unique index的检索 3 eq_ref 多表连接中被驱动表的连接列上有primary key或者unique index的检索 4 ref 与eq_ref类似，但不是使用primary key或者unique index，而是普通索引。也可以是单表上non-unique索引检索 5 fulltext 使用FULLTEXT索引执行连接 6 ref_or_null 与ref类似，区别在于条件中包含对NULL的查询 7 index_merge 索引合并优化，利用一个表里的N个索引查询,key_len表示这些索引键的和最长长度。 8 unique_subquery in的后面是一个查询primary key\unique字段的子查询 9 index_subquery in的后面是一个查询普通index字段的子查询 10 range 单表索引中的范围查询,使用索引查询出单个表中的一些行数据。ref列会变为null 11 index 等于ALL。它有两种情况：(1)覆盖索引 (2)用索引的顺序做一个全表扫描。 12 all 全表扫描 (2.6) possible_keys 提示使用哪个索引会在该表中找到行 (2.7) key MYSQL使用的索引 (2.8) key_lenkey_len 说明 key_len: 4 // INT NOT NULL key_len: 5 // INT DEFAULT NULL key_len: 30 // CHAR(30) NOT NULL key_len: 32 // VARCHAR(30) NOT NULL key_len: 92 // VARCHAR(30) NOT NULL CHARSET=utf8 key_len大小的计算规则: a、一般地，key_len 等于索引列类型字节长度，例如int类型为4-bytes，bigint为8-bytes； b、如果是字符串类型，还需要同时考虑字符集因素，例如：CHAR(30) UTF8则key_len至少是90-bytes； c、若该列类型定义时允许NULL，其key_len还需要再加 1-bytes； d、若该列类型为变长类型，例如 VARCHAR（TEXT\BLOB不允许整列创建索引，如果创建部分索引，也被视为动态列类型），其key_len还需要再加 2-bytes; (2.9) ref ref列显示使用哪个列或常数与key一起从表中选择行。 (2.10) rows rows_examined，要得到最终记录索要扫描经过的记录数，这个数越小越好。 (2.11) filterrd 按表条件过滤行的百分比 (2.12) Extra Extra是对执行计划的额外说明，包含重要信息。 例如： id type value Meaning 1 const row not found 所要查询的表为空 2 Distinct mysql正在查询distinct值，因此当它每查到一个distinct值之后就会停止当前组的搜索，去查询下一个值 3 Impossible WHERE where条件总为false，表里没有满足条件的记录 4 Impossible WHERE noticed after reading const tables 在优化器评估了const表之后，发现where条件均不满足 5 no matching row in const table 当前join的表为const表，不能匹配 6 Not exists 优化器发现内表记录不可能满足where条件 7 Select tables optimized away 在没有group by子句时，对于MyISAM的select count(*)操作，或者当对于min(),max()的操作可以利用索引优化，优化器发现只会返回一行。 8 Using filesort 使用filesort来进行order by操作 9 Using index 覆盖索引 10 Using index for group-by 对于group by列或者distinct列，可以利用索引检索出数据，而不需要去表里查数据、分组、排序、去重等等 11 Using join buffer 之前的表连接在nested loop之后放进join buffer，再来和本表进行join。适用于本表的访问type为range，index或all 12 Using sort_union,using union,using intersect index_merge的三种情况 13 Using temporary 使用了临时表来存储中间结果集，适用于group by，distinct，或order by列为不同表的列。 14 Using where 在存储引擎层检索出记录后，在server利用where条件进行过滤，并返回给客户端 (3) Explain妙用 查询表的大概数据总数。 EXPLAIN SELECT * FROM t1 ; 查询表t1的大概总数。 References[1] EXPLAIN Statement[2] explain-output[3] MYSQL explain详解]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java内存模型]]></title>
    <url>%2F2020%2F02%2F03%2Fjava-mermory-model%2F</url>
    <content type="text"><![CDATA[(1) 内存模型 在多核系统中，处理器一般有一层或者多层的缓存，这些的缓存通过加速数据访问（因为数据距离处理器更近）和降低共享内存在总线上的通讯（因为本地缓存能够满足许多内存操作）来提高CPU性能。缓存能够大大提升性能，但是它们也带来了许多挑战。例如，当两个CPU同时检查相同的内存地址时会发生什么？在什么样的条件下它们会看到相同的值？ 在处理器层面上，内存模型定义了一个充要条件，“让当前的处理器可以看到其他处理器写入到内存的数据”以及“其他处理器可以看到当前处理器写入到内存的数据”。有些处理器有很强的内存模型(strong memory model)，能够让所有的处理器在任何时候任何指定的内存地址上都可以看到完全相同的值。而另外一些处理器则有较弱的内存模型（weaker memory model），在这种处理器中，必须使用内存屏障（一种特殊的指令）来刷新本地处理器缓存并使本地处理器缓存无效，目的是为了让当前处理器能够看到其他处理器的写操作或者让其他处理器能看到当前处理器的写操作。这些内存屏障通常在lock和unlock操作的时候完成。内存屏障在高级语言中对程序员是不可见的。 (2) Java内存模型(2.1) 什么是Java内存模型 本质上可以理解为 Java 内存模型规范了 JVM 如何提供按需禁用缓存和编译优化的方法。具体来说，这些方法包括 volatile、synchronized 和 final 三个关键字，以及六项 Happens-Before 规则。 (2.2) Java内存模型解决了什么问题 Java内存模型解决了Java并发编程里的可见性和有序性问题 (2.3) Java内存模型的具体规则volatile 告诉编译器，对这个变量的读写，不能使用 CPU 缓存，必须从内存中读取或者写入。 解决可见性问题 synchronized 同步 解决原子性问题 final 不可变 Happens-Before 规则(1)程序的顺序性规则 在一个线程中，按照程序顺序，前面的操作 Happens-Before 于后续的任意操作。 在一个线程内，按照程序代码顺序，书写在前面的操作先行发生于书写在后面的操作。准确地说，应该是控制流顺序而不是程序代码顺序，因为要考虑分支、循环等结构。 (2)volatile 变量规则 对一个 volatile 变量的写操作， Happens-Before 于后续对这个 volatile 变量的读操作。 对一个volatile变量的写操作先行发生于后面对这个变量的读操作，这里的”后面”同样是指时间上的先后顺序。 12345678910111213class VolatileExample &#123; int x = 0; volatile boolean v = false; public void writer() &#123; x = 42; v = true; &#125; public void reader() &#123; if (v == true) &#123; // 这里x会是多少呢？ &#125; &#125;&#125; (3)传递性 如果 A Happens-Before B，且 B Happens-Before C，那么 A Happens-Before C。 (4)管程中锁的规则 对一个锁的解锁 Happens-Before 于后续对这个锁的加锁。 一个unlock操作先行发生于后面对同一个锁的lock操作。这里必须强调的是同一个锁，而”后面”是指时间上的先后顺序。 管程是一种通用的同步原语，在 Java 中指的就是 synchronized，synchronized 是 Java 里对管程的实现。 1234567synchronized (this) &#123; //此处自动加锁 // x是共享变量,初始值=10 if (this.x &lt; 12) &#123; this.x = 12; &#125; &#125; //此处自动解锁 (5)线程 start() 关于线程启动的 主线程 A 启动子线程 B 后，子线程 B 能够看到主线程在启动子线程 B 前的操作。 Thread对象的start()方法先行发生于此线程的每一个动作。 123456789Thread B = new Thread(()-&gt;&#123; // 主线程调用B.start()之前 // 所有对共享变量的修改，此处皆可见 // 此例中，var==77&#125;);// 此处对共享变量var修改var = 77;// 主线程启动子线程B.start(); (6)线程 join() 规则 关于线程等待的 主线程 A 等待子线程 B 完成（主线程 A 通过调用子线程 B 的 join() 方法实现），当子线程 B 完成后（主线程 A 中 join() 方法返回），主线程能够看到子线程的操作。 线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过Thread.join（）方法结束、Thread.isAlive（）的返回值等手段检测到线程已经终止执行。 123456789101112Thread B = new Thread(()-&gt;&#123; // 此处对共享变量var修改 var = 66;&#125;);// 例如此处对共享变量修改，// 则这个修改结果对线程B可见// 主线程启动子线程B.start();B.join()// 子线程所有对共享变量的修改// 在主线程调用B.join()之后皆可见// 此例中，var==66 再送一个 对象终结规则：一个对象的初始化完成(构造函数执行结束)先行发生于它的finalize()方法的开始。 References[1] 《深入理解Java虚拟机》 周志明[2] 02 | Java内存模型：看Java如何解决可见性和有序性问题[3] Java内存模型FAQ[4] jsr-133-faq.html[5] jsr133.pdf]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程]]></title>
    <url>%2F2020%2F02%2F02%2Fjava-concurrent-programming%2F</url>
    <content type="text"><![CDATA[并发编程最早的应用领域就是操作系统的实现。 随着硬件的发展，互联网系统并发量轻松过百万，传统的中间件和数据库已经不能满足需求，成为瓶颈所在。 Java 里 synchronized、wait()/notify() 相关的知识很琐碎，看懂难，会用更难。但实际上 synchronized、wait()、notify() 不过是操作系统领域里管程模型的一种实现而已，Java SDK 并发包里的条件变量 Condition 也是管程里的概念，synchronized、wait()/notify()、条件变量这些知识如果单独理解，自然是管中窥豹。但是如果站在管程这个理论模型的高度，你就会发现这些知识原来这么简单，同时用起来也就得心应手了。管程作为一种解决并发问题的模型，是继信号量模型之后的一项重大创新，它与信号量在逻辑上是等价的（可以用管程实现信号量，也可以用信号量实现管程），但是相比之下管程更易用。而且，很多编程语言都支持管程，搞懂管程，对学习其他很多语言的并发编程有很大帮助。然而，很多人急于学习 Java 并发编程技术，却忽略了技术背后的理论和模型，而理论和模型却往往比具体的技术更为重要。 Java SDK 并发包乃是并发大师 Doug Lea 出品，堪称经典，它内部一定是有章可循的。 并发编程可以总结为三个核心问题：分工、同步、互斥。 分工指的是如何高效地拆解任务并分配给线程，而同步指的是线程之间如何协作，互斥则是保证同一时刻只允许一个线程访问共享资源。Java SDK 并发包很大部分内容都是按照这三个维度组织的，例如 Fork/Join 框架就是一种分工模式，CountDownLatch 就是一种典型的同步方式，而可重入锁则是一种互斥手段。 当把并发编程核心的问题搞清楚，再回过头来看 Java SDK 并发包，你会感觉豁然开朗，它不过是针对并发问题开发出来的工具而已，此时的 SDK 并发包可以任你“盘”了。 并发编程涉及操作系统、CPU、内存等等多方面的知识。 问题背后的本质、问题的起源，同时站在理论、模型的角度讲解 Java 并发，让你的知识更成体系，融会贯通。 要理解可见性，就需要了解一些 CPU 和缓存的知识；要理解原子性，就需要理解一些操作系统的知识；很多无锁算法的实现往往也需要理解 CPU 缓存。 可见性：一个线程对共享变量的修改，另外一个线程能够立刻看到。 CPU缓存、内存 数据不一致导致。原子性：一个或者多个操作在 CPU 执行的过程中不被中断的特性。 线程切换导致。有序性：程序按照代码的先后顺序执行。 重排序导致。 Referenceshttps://wiki.sei.cmu.edu/confluence/display/java/2+Rules [0] 为什么需要学习并发编程[0] 学习攻略 | 如何才能学好并发编程？ [1] 01 | 可见性、原子性和有序性问题：并发编程Bug的源头[2] 02 | Java内存模型：看Java如何解决可见性和有序性问题[3] 03 | 互斥锁（上）：解决原子性问题[4] 04 | 互斥锁（下）：如何用一把锁保护多个资源？[5] 05 | 一不小心就死锁了，怎么办？[6] 06 | 用“等待-通知”机制优化循环等待[7] 07 | 安全性、活跃性以及性能问题[8] 08 | 管程：并发编程的万能钥匙[9] 09 | Java线程（上）：Java线程的生命周期[10] 10 | Java线程（中）：创建多少线程才是合适的？[11] 11 | Java线程（下）：为什么局部变量是线程安全的？]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>concurrent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新冠形病毒疫情传染数据模拟]]></title>
    <url>%2F2020%2F02%2F02%2Fnovel-coronavirus-pneumonia-infection-data-simulation%2F</url>
    <content type="text"><![CDATA[新冠形病毒(Novel Conronavirus Pneumonia)疫情数据模拟。 该模拟参考网上公开数据，模拟(数据可能不准确)只是为了让大家心里有个数，不要出门或尽量少出门。 由于建模使用数据误差较大，结果仅供参考。 需要数据 全国人口统计数据 行政区划分数据 人口流动数据 全国各小区数据 高考各大高校录取数据 疫情数据 ( 卫健委公布的各省市确诊人数、 同行数据 ) 使模型更加准确所需数据 通信运营商数据 (从2019.12.01~2020.02.16 用于通过手机定位)中国联通已经开始行动了，发送 cxmyd#身份证后四位 到 10010 可查询近14日内到访的省市信息(驻留超过4小时)，公益服务。 确诊人数经纬度坐标数据 (用于预防) 使模型更加准确需要优化的点： 数据层面人口数据更详细，具体到 城市具体到街道、小区，城镇具体到 村 甚至 小组。人口流动数据更详细， @通信运营商@中国电信@中国移动@中国联通@今日头条@美团@阿里巴巴@京东 建模层面数据统计需要耗费大量人力物力，不可能做到100%在当前数据的情况下，需要部分数据缺失，部分数据延迟根据 概率论和数理统计 以及 离散数学 去估计、模拟可能的情况。 (1) 数据准备 全国人口统计数据 行政区划分数据 人口流动数据 全国各小区数据高考各大高校录取数据 卫健委每天确诊数据 确诊数据 + 疑似数据 使模型更加准确所需数据 通信运营商数据 (从2019.12.01~2020.02.16 用于通过手机定位) 确诊人数经纬度坐标数据 (用于预防) (1.1) 国家统计局-人口普查数据 中华人民共和国中央人民政府-数据-数据详情-总人口 总人口数据下载 国家统计局-普查数据 2010年第6次人口普查数据 第6次人口普查数据 索引 对应下载文件 http://www.stats.gov.cn/tjsj/pcsj/rkpc/6rp/excel/A0101a.xls 各县市人口通过百度百科获取，结果基本准确。数据会有一些偏差。 村镇人口通过百度百科获取，结果不太准确 (1.1.1) 各年全国总人口 序号 统计时间 年末人口（万人） 17 2018 139538 16 2017 139008 15 2016 138271 14 2015 137462 13 2014 136782 12 2013 136072 11 2012 135404 10 2011 134735 9 2010 134091 8 2009 133450 7 2008 132802 6 2007 132129 5 2006 131448 4 2005 130756 3 2004 129988 2 2003 129227 1 2002 128453 (1.1.2) 全国总人口详细 地区 户数 合计 户数 家庭户 户数 集体户 合计 (合计)男 (合计)女 性别比(女=100) (家庭户)小计 (家庭户) 男 (家庭户)女 (家庭户) 性别比 (女=100) (集体户)小计 (集体户)男 (集体户)女 (集体户) 性别比 (女=100) 平均家庭 户规模 （人/户） 全 国 417722698 401934196 15788502 1332810869 682329104 650481765 104.90 1239981250 627410399 612570851 102.42 92829619 54918705 37910914 144.86 3.09 北 京 7355291 6680552 674739 19612368 10126430 9485938 106.75 16389723 8173161 8216562 99.47 3222645 1953269 1269376 153.88 2.45 天 津 3963604 3661992 301612 12938693 6907091 6031602 114.52 10262186 5129604 5132582 99.94 2676507 1777487 899020 197.71 2.80 河 北 20813492 20395116 418376 71854210 36430286 35423924 102.84 68538709 34552649 33986060 101.67 3315501 1877637 1437864 130.59 3.36 山 西 10654162 10330207 323955 35712101 18338760 17373341 105.56 33484131 16988087 16496044 102.98 2227970 1350673 877297 153.96 3.24 内 蒙 古 8470472 8205498 264974 24706291 12838243 11868048 108.17 23071690 11725291 11346399 103.34 1634601 1112952 521649 213.35 2.81 辽 宁 15334912 14994046 340866 43746323 22147745 21598578 102.54 41755874 20956756 20799118 100.76 1990449 1190989 799460 148.97 2.78 吉 林 9162183 8998492 163691 27452815 13907218 13545597 102.67 26457769 13358390 13099379 101.98 995046 548828 446218 123.00 2.94 黑 龙 江 13192935 13000088 192847 38313991 19426106 18887885 102.85 36884039 18603181 18280858 101.76 1429952 822925 607027 135.57 2.84 上 海 8893483 8253257 640226 23019196 11854916 11164280 106.19 20593430 10318168 10275262 100.42 2425766 1536748 889018 172.86 2.50 江 苏 25635291 24381782 1253509 78660941 39626707 39034234 101.52 71685839 35542124 36143715 98.34 6975102 4084583 2890519 141.31 2.94 浙 江 20060115 18854021 1206094 54426891 27965641 26461250 105.69 49425543 25037320 24388223 102.66 5001348 2928321 2073027 141.26 2.62 安 徽 19322432 18861956 460476 59500468 30245513 29254955 103.39 56493891 28462853 28031038 101.54 3006577 1782660 1223917 145.65 3.00 福 建 11971873 11206317 765556 36894217 18981054 17913163 105.96 33397663 16901083 16496580 102.45 3496554 2079971 1416583 146.83 2.98 江 西 11847841 11542527 305314 44567797 23003521 21564276 106.67 42181417 21600070 20581347 104.95 2386380 1403451 982929 142.78 3.65 山 东 30794664 30105454 689210 95792719 48446944 47345775 102.33 89855501 45023357 44832144 100.43 5937218 3423587 2513631 136.20 2.98 河 南 26404973 25928729 476244 94029939 47493063 46536876 102.05 90028072 45262137 44765935 101.11 4001867 2230926 1770941 125.97 3.47 湖 北 17253385 16695121 558264 57237727 29391247 27846480 105.55 52745625 26826301 25919324 103.50 4492102 2564946 1927156 133.09 3.16 湖 南 19029894 18625710 404184 65700762 33776459 31924303 105.80 61911446 31611459 30299987 104.33 3789316 2165000 1624316 133.29 3.32 广 东 32222752 28630609 3592143 104320459 54400538 49919921 108.98 88979305 45465958 43513347 104.49 15341154 8934580 6406574 139.46 3.11 广 西 13467663 13151404 316259 46023761 23924704 22099057 108.26 43970320 22733969 21236351 107.05 2053441 1190735 862706 138.02 3.34 海 南 2451819 2331149 120670 8671485 4592283 4079202 112.58 8060519 4231490 3829029 110.51 610966 360793 250173 144.22 3.46 重 庆 10272559 10000965 271594 28846170 14608870 14237300 102.61 26994017 13542424 13451593 100.68 1852153 1066446 785707 135.73 2.70 四 川 26383458 25794161 589297 80417528 40827834 39589694 103.13 76207174 38380622 37826552 101.46 4210354 2447212 1763142 138.80 2.95 贵 州 10745630 10558461 187169 34748556 17905471 16843085 106.31 33571308 17153547 16417761 104.48 1177248 751924 425324 176.79 3.18 云 南 12695396 12339961 355435 45966766 23856696 22110070 107.90 43626674 22391253 21235421 105.44 2340092 1465443 874649 167.55 3.54 西 藏 689521 670838 18683 3002165 1542652 1459513 105.70 2837769 1429541 1408228 101.51 164396 113111 51285 220.55 4.23 陕 西 11084516 10718563 365953 37327379 19287575 18039804 106.92 34462115 17556257 16905858 103.85 2865264 1731318 1133946 152.68 3.22 甘 肃 7113833 6900369 213464 25575263 13064193 12511070 104.42 24052594 12141360 11911234 101.93 1522669 922833 599836 153.85 3.49 青 海 1586635 1529039 57596 5626723 2913793 2712930 107.40 5284525 2675766 2608759 102.57 342198 238027 104171 228.50 3.46 宁 夏 1945064 1882205 62859 6301350 3227404 3073946 104.99 5970133 3015722 2954411 102.08 331217 211682 119535 177.09 3.17 新 疆 6902850 6705607 197243 21815815 11270147 10545668 106.87 20802249 10620499 10181750 104.31 1013566 649648 363918 178.51 3.10 (1.2) 民政局-行政区划数据民政部、国家统计局： 国家统计局-2018年统计用区划代码和城乡划分代码(截止2018年10月31日) 中华人民共和国民政部-中华人民共和国行政区划代码 中华人民共和国国家统计局-统计用区划和城乡划分代码 中华人民共和国国家统计局-统计用区划代码和城乡划分代码编制规则 2018年12月中华人民共和国县以上行政区划代码 2019年11月中华人民共和国县以上行政区划代码 (1.3) 人口流动数据 假设流动人口 ≈ 春运期间 飞机 + 火车(高铁) + 汽车 + 摩托车 总和 机票-航旅纵横 机票-携程 火车票 汽车票 (1.4) 全国各小区数据 小区 + 村(屯) 国家统计局-2018年统计用区划代码和城乡划分代码(截止2018年10月31日) 房地产中介 网站获取小区数据 北京-东城-小区 链家-所有城市 北京-小区 我爱我家-所有城市 北京小区 安居客-所有城市 (1.4.1) 高考各大高校录取数据 这个是为了统计武汉到各个省市的学生数据 (1.5) 公布的疫情数据 国家卫生健康委员会-&gt;疫情通报 新型冠状病毒感染的肺炎-确诊患者同行程查询工具-人民日报 https://2019ncov.nosugartech.com/data.json?439373 新型冠状病毒肺炎-疫情实时动态-丁香园 肺炎疫情实时动态播报-网易新闻 https://c.m.163.com/ug/api/wuhan/app/data/list-total?t=1581731399558 新型冠状病毒肺炎-疫情实时追踪 https://view.inews.qq.com/g2/getOnsInfo?name=disease_h5&amp;callback=jQuery341008224555239086584_1581732532940&amp;_=1581732532941 References[1] 中华人民共和国中央人民政府-数据-数据详情-总人口[2] 总人口数据下载ß[3] 国家统计局-2018年统计用区划代码和城乡划分代码(截止2018年10月31日)[4] 机票-航旅纵横[5] 机票-携程[6] 火车票[7] 汽车票[8] 北京-东城-小区[9] 北京-小区 我爱我家-所有城市[10] 北京小区 安居客-所有城市[11] 国家卫生健康委员会-&gt;疫情通报[12] 新型冠状病毒感染的肺炎-确诊患者同行程查询工具[13] 新型冠状病毒肺炎-疫情实时动态-丁香园[14] 肺炎疫情实时动态播报-网易新闻]]></content>
      <categories>
        <category>data</category>
      </categories>
      <tags>
        <tag>data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch 映射和分析]]></title>
    <url>%2F2020%2F01%2F21%2Felasticsearch-mapping-analysis%2F</url>
    <content type="text"><![CDATA[当摆弄索引里面的数据时，我们发现一些奇怪的事情。一些事情看起来被打乱了：在我们的索引中有12条推文，其中只有一条包含日期 2014-09-15 ，但是看一看下面查询命中的 总数 （total）： 1234GET /_search?q=2014 # 12 resultsGET /_search?q=2014-09-15 # 12 results !GET /_search?q=date:2014-09-15 # 1 resultGET /_search?q=date:2014 # 0 results ! 为什么在 _all 字段查询日期返回所有推文，而在 date 字段只查询年份却没有返回结果？为什么我们在 _all 字段和 date 字段的查询结果有差别？ 因为数据在 _all 字段与 date 字段的索引方式不同。 (1) 精确值 VS 全文 Elasticsearch 中的数据可以概括的分为两类：精确值和全文。 精确值 如它们听起来那样精确。例如日期或者用户 ID，但字符串也可以表示精确值，例如用户名或邮箱地址。 另一方面，全文 是指文本数据（通常以人类容易识别的语言书写），例如一个推文的内容或一封邮件的内容。 全文通常是指非结构化的数据，但这里有一个误解：自然语言是高度结构化的。问题在于自然语言的规则是复杂的，导致计算机难以正确解析。 精确值很容易查询。结果是二进制的：要么匹配查询，要么不匹配。这种查询很容易用 SQL 表示：123WHERE name = "John Smith"AND user_id = 2AND date &gt; "2014-09-15" 查询全文数据要微妙的多。我们问的不只是“这个文档匹配查询吗”，而是“该文档匹配查询的程度有多大？”换句话说，该文档与给定查询的相关性如何？ 我们很少对全文类型的域做精确匹配。相反，我们希望在文本类型的域中搜索。不仅如此，我们还希望搜索能够理解我们的 意图 ： 搜索 UK ，会返回包含 United Kindom 的文档。 搜索 jump ，会匹配 jumped ， jumps ， jumping ，甚至是 leap 。 搜索 johnny walker 会匹配 Johnnie Walker ， johnnie depp 应该匹配 Johnny Depp 。 fox news hunting 应该返回福克斯新闻（ Foxs News ）中关于狩猎的故事，同时， fox hunting news 应该返回关于猎狐的故事。 为了促进这类在全文域中的查询，Elasticsearch 首先 分析 文档，之后根据结果创建 倒排索引 。 (2) 倒排索引Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 例如，假设我们有两个文档，每个文档的 content 域包含如下内容： The quick brown fox jumped over the lazy dog Quick brown foxes leap over lazy dogs in summer 为了创建倒排索引，我们首先将每个文档的 content 域拆分成单独的 词（我们称它为 词条 或 tokens ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。结果如下所示：123456789101112131415161718Trem Doc_1 Doc_2-------------------------------Quick | | X The | X | brown | X | X dog | X | dogs | | X fox | X | foxes | | X in | | X jumped | X | lazy | X | X leap | | X over | X | X quick | X | summer | | X the | X | ------------------------------- 现在，如果我们想搜索 quick brown，我们只需要查找包含每个词条的文档：123456Trem Doc_1 Doc_2-------------------------------brown | X | X quick | X | -------------------------------Total | 2 | 1 两个文档都匹配，但是第一个文档比第二个匹配度更高。如果我们使用仅计算匹配词条数量的简单 相似性算法 ，那么，我们可以说，对于我们查询的相关性来讲，第一个文档比第二个文档更佳。 但是，我们目前的倒排索引有一些问题： Quick 和 quick 以独立的词条出现，然而用户可能认为它们是相同的词。 fox 和 foxes 非常相似, 就像 dog 和 dogs ；他们有相同的词根。 jumped 和 leap, 尽管没有相同的词根，但他们的意思很相近。他们是同义词。 使用前面的索引搜索 +Quick +fox 不会得到任何匹配文档。（记住，+ 前缀表明这个词必须存在。）只有同时出现 Quick 和 fox 的文档才满足这个查询条件，但是第一个文档包含 quick fox ，第二个文档包含 Quick foxes 。 我们的用户可以合理的期望两个文档与查询匹配。我们可以做的更好。 如果我们将词条规范为标准模式，那么我们可以找到与用户搜索的词条不完全一致，但具有足够相关性的文档。例如： Quick 可以小写化为 quick 。 foxes 可以 词干提取 --变为词根的格式-- 为 fox 。类似的， dogs 可以为提取为 dog 。 jumped 和 leap 是同义词，可以索引为相同的单词 jump 。 现在索引看上去像这样：12345678910111213Trem Doc_1 Doc_2------------------------------- brown | X | X dog | X | X fox | X | X in | | X jumped | X | X lazy | X | X over | X | X quick | X | X summer | | X the | X | X------------------------------- 这还远远不够。我们搜索 +Quick +fox 仍然 会失败，因为在我们的索引中，已经没有 Quick 了。但是，如果我们对搜索的字符串使用与 content 域相同的标准化规则，会变成查询 +quick +fox ，这样两个文档都会匹配！ (2) 分析与分析器分析 包含下面的过程： 首先，将一块文本分成适合于倒排索引的独立的 词条 ， 之后，将这些词条统一化为标准格式以提高它们的“可搜索性”，或者 recall 分析器执行上面的工作。 分析器 实际上是将三个功能封装到了一个包里： 字符过滤器 首先，字符串按顺序通过每个 字符过滤器 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将 &amp; 转化成 and。分词器 其次，字符串被 分词器 分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。Token 过滤器 最后，词条按顺序通过每个 token 过滤器 。这个过程可能会改变词条（例如，小写化 Quick ），删除词条（例如， 像 a， and， the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词）。 Elasticsearch提供了开箱即用的字符过滤器、分词器和token 过滤器。 这些可以组合起来形成自定义的分析器以用于不同的目的。 (2.1) 内置分析器 Elasticsearch还附带了可以直接使用的预包装的分析器。接下来我们会列出最重要的分析器。为了证明它们的差异，我们看看每个分析器会从下面的字符串得到哪些词条： “Set the shape to semi-transparent by calling set_trans(5)” 标准分析器 标准分析器是Elasticsearch默认使用的分析器。它是分析各种语言文本最常用的选择。它根据 Unicode 联盟 定义的 单词边界 划分文本。删除绝大部分标点。最后，将词条小写。它会产生 set, the, shape, to, semi, transparent, by, calling, set_trans, 5 简单分析器 简单分析器在任何不是字母的地方分隔文本，将词条小写。它会产生 set, the, shape, to, semi, transparent, by, calling, set, trans 空格分析器 空格分析器在空格的地方划分文本。它会产生 Set, the, shape, to, semi-transparent, by, calling, set_trans(5) 语言分析器 特定语言分析器可用于 很多语言。它们可以考虑指定语言的特点。例如， 英语 分析器附带了一组英语无用词（常用单词，例如 and 或者 the ，它们对相关性没有多少影响），它们会被删除。 由于理解英语语法的规则，这个分词器可以提取英语单词的 词干 。 英语 分词器会产生下面的词条： set, shape, semi, transpar, call, set_tran, 5 注意看 transparent、 calling 和 set_trans 已经变为词根格式。 References[0] 映射和分析[1] 精确值 VS 全文[2] 倒排索引[3] 分析与分析器[] [] []]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch-search]]></title>
    <url>%2F2020%2F01%2F20%2Felasticsearch-search%2F</url>
    <content type="text"><![CDATA[search 最基本的工具 Elasticsearch 不只会存储（stores） 文档，为了能被搜索到也会为文档添加索引（indexes） ，这也是为什么我们使用结构化的 JSON 文档，而不是无结构的二进制数据。 文档中的每个字段都将被索引并且可以被查询 。不仅如此，在简单查询时，Elasticsearch 可以使用 所有（all） 这些索引字段，以惊人的速度返回结果。 搜索（search） 可以做到： 在类似于 gender 或者 age 这样的字段上使用结构化查询，join_date 这样的字段上使用排序，就像SQL的结构化查询一样。 全文检索，找出所有匹配关键字的文档并按照相关性（relevance） 排序后返回结果。 以上二者兼而有之。 很多搜索都是开箱即用的，为了充分挖掘 Elasticsearch 的潜力，你需要理解以下三个概念： 映射（Mapping） 描述数据在每个字段内如何存储分析（Analysis） 全文是如何处理使之可以被搜索的领域特定查询语言（Query DSL） Elasticsearch 中强大灵活的查询语言 123456789101112131415161718192021222324252627282930curl -XPOST 'http://localhost:9200/_bulk?pretty' -H 'Content-Type: application/json' -d '&#123; "create": &#123; "_index": "user", "_type": "doc", "_id": "1" &#125;&#125;&#123; "email" : "john@smith.com", "name" : "John Smith", "username" : "@john" &#125;&#123; "create": &#123; "_index": "user", "_type": "doc", "_id": "2" &#125;&#125;&#123; "email" : "mary@jones.com", "name" : "Mary Jones", "username" : "@mary" &#125;&#123; "create": &#123; "_index": "tweet", "_type": "doc", "_id": "3" &#125;&#125;&#123; "date" : "2014-09-13", "name" : "Mary Jones", "tweet" : "Elasticsearch means full text search has never been so easy", "user_id" : 2 &#125;&#123; "create": &#123; "_index": "tweet", "_type": "doc", "_id": "4" &#125;&#125;&#123; "date" : "2014-09-14", "name" : "John Smith", "tweet" : "@mary it is not just text, it does everything", "user_id" : 1 &#125;&#123; "create": &#123; "_index": "tweet", "_type": "doc", "_id": "5" &#125;&#125;&#123; "date" : "2014-09-15", "name" : "Mary Jones", "tweet" : "However did I manage before Elasticsearch?", "user_id" : 2 &#125;&#123; "create": &#123; "_index": "tweet", "_type": "doc", "_id": "6" &#125;&#125;&#123; "date" : "2014-09-16", "name" : "John Smith", "tweet" : "The Elasticsearch API is really easy to use", "user_id" : 1 &#125;&#123; "create": &#123; "_index": "tweet", "_type": "doc", "_id": "7" &#125;&#125;&#123; "date" : "2014-09-17", "name" : "Mary Jones", "tweet" : "The Query DSL is really powerful and flexible", "user_id" : 2 &#125;&#123; "create": &#123; "_index": "tweet", "_type": "doc", "_id": "8" &#125;&#125;&#123; "date" : "2014-09-18", "name" : "John Smith", "user_id" : 1 &#125;&#123; "create": &#123; "_index": "tweet", "_type": "doc", "_id": "9" &#125;&#125;&#123; "date" : "2014-09-19", "name" : "Mary Jones", "tweet" : "Geo-location aggregations are really cool", "user_id" : 2 &#125;&#123; "create": &#123; "_index": "tweet", "_type": "doc", "_id": "10" &#125;&#125;&#123; "date" : "2014-09-20", "name" : "John Smith", "tweet" : "Elasticsearch surely is one of the hottest new NoSQL products", "user_id" : 1 &#125;&#123; "create": &#123; "_index": "tweet", "_type": "doc", "_id": "11" &#125;&#125;&#123; "date" : "2014-09-21", "name" : "Mary Jones", "tweet" : "Elasticsearch is built for the cloud, easy to scale", "user_id" : 2 &#125;&#123; "create": &#123; "_index": "tweet", "_type": "doc", "_id": "12" &#125;&#125;&#123; "date" : "2014-09-22", "name" : "John Smith", "tweet" : "Elasticsearch and I have left the honeymoon stage, and I still love her.", "user_id" : 1 &#125;&#123; "create": &#123; "_index": "tweet", "_type": "doc", "_id": "13" &#125;&#125;&#123; "date" : "2014-09-23", "name" : "Mary Jones", "tweet" : "So yes, I am an Elasticsearch fanboy", "user_id" : 2 &#125;&#123; "create": &#123; "_index": "tweet", "_type": "doc", "_id": "14" &#125;&#125;&#123; "date" : "2014-09-24", "name" : "John Smith", "tweet" : "How many more cheesy tweets do I have to write?", "user_id" : 1 &#125;' (1) 空搜索1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253$ curl -X GET "localhost:9200/_search?pretty"&#123; "took" : 16, "timed_out" : false, "_shards" : &#123; "total" : 13, "successful" : 13, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 8, "max_score" : 1.0, "hits" : [ &#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_score" : 1.0, "_source" : &#123; "title" : "My updated blog post" &#125; &#125;, &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "2", "_score" : 1.0, "_source" : &#123; "first_name" : "Jane", "last_name" : "Smith", "age" : 32, "about" : "I like to collect rock albums", "interests" : [ "music" ] &#125; &#125;, ... &#123; "_index" : "website", "_type" : "blog", "_id" : "1qMXwm8BXjhxAah64Ysk", "_score" : 1.0, "_source" : &#123; "title" : "My second blog post" &#125; &#125; ] &#125;&#125; (1.1) hits 返回结果中最重要的部分是 hits ，它包含 total 字段来表示匹配到的文档总数，并且一个 hits 数组包含所查询结果的前十个文档。 在 hits 数组中每个结果包含文档的 _index 、 _type 、 _id ，加上 _source 字段。这意味着我们可以直接从返回的搜索结果中使用整个文档。这不像其他的搜索引擎，仅仅返回文档的ID，需要你单独去获取文档。 每个结果还有一个 _score ，它衡量了文档与查询的匹配程度。默认情况下，首先返回最相关的文档结果，就是说，返回的文档是按照 _score 降序排列的。在这个例子中，我们没有指定任何查询，故所有的文档具有相同的相关性，因此对所有的结果而言 1 是中性的 _score 。 max_score 值是与查询所匹配文档的 _score 的最大值。 (1.2) took took 值告诉我们执行整个搜索请求耗费了多少毫秒。 (1.3) shards _shards 部分告诉我们在查询中参与分片的总数，以及这些分片成功了多少个失败了多少个。正常情况下我们不希望分片失败，但是分片失败是可能发生的。如果我们遭遇到一种灾难级别的故障，在这个故障中丢失了相同分片的原始数据和副本，那么对这个分片将没有可用副本来对搜索请求作出响应。假若这样，Elasticsearch 将报告这个分片是失败的，但是会继续返回剩余分片的结果。 (1.4) timeout timed_out 值告诉我们查询是否超时。默认情况下，搜索请求不会超时。如果低响应时间比完成结果更重要，你可以指定 timeout 为 10 或者 10ms（10毫秒），或者 1s（1秒）: $ curl -X GET &quot;localhost:9200/_search?timeout=10ms&amp;pretty&quot; 在请求超时之前，Elasticsearch 将会返回已经成功从每个分片获取的结果。 应当注意的是 timeout 不是停止执行查询，它仅仅是告知正在协调的节点返回到目前为止收集的结果并且关闭连接。在后台，其他的分片可能仍在执行查询即使是结果已经被发送了。 使用超时是因为 SLA(服务等级协议)对你是很重要的，而不是因为想去中止长时间运行的查询。 (2) 多索引，多类型如果不对某一特殊的索引或者类型做限制，就会搜索集群中的所有文档。Elasticsearch 转发搜索请求到每一个主分片或者副本分片，汇集查询出的前10个结果，并且返回给我们。 然而，经常的情况下，你想在一个或多个特殊的索引并且在一个或者多个特殊的类型中进行搜索。我们可以通过在URL中指定特殊的索引和类型达到这种效果，如下所示： /_search 在所有的索引中搜索所有的类型/gb/_search 在 gb 索引中搜索所有的类型/gb,us/_search 在 gb 和 us 索引中搜索所有的文档/g,u/_search 在任何以 g 或者 u 开头的索引中搜索所有的类型/gb/user/_search 在 gb 索引中搜索 user 类型/gb,us/user,tweet/_search 在 gb 和 us 索引中搜索 user 和 tweet 类型/_all/user,tweet/_search 在所有的索引中搜索 user 和 tweet 类型 当在单一的索引下进行搜索的时候，Elasticsearch 转发请求到索引的每个分片中，可以是主分片也可以是副本分片，然后从每个分片中收集结果。多索引搜索恰好也是用相同的方式工作的—​只是会涉及到更多的分片。 搜索一个索引有五个主分片和搜索五个索引各有一个分片准确来所说是等价的。 (3) 分页 和 SQL 使用 LIMIT 关键字返回单个 page 结果的方法相同，Elasticsearch 接受 from 和 size 参数： size 显示应该返回的结果数量，默认是 10from 显示应该跳过的初始结果数量，默认是 0 123curl -X GET &quot;localhost:9200/_search?size=5&amp;pretty&quot;curl -X GET &quot;localhost:9200/_search?size=5&amp;from=5&amp;pretty&quot;curl -X GET &quot;localhost:9200/_search?size=5&amp;from=10&amp;pretty&quot; 考虑到分页过深以及一次请求太多结果的情况，结果集在返回之前先进行排序。 但请记住一个请求经常跨越多个分片，每个分片都产生自己的排序结果，这些结果需要进行集中排序以保证整体顺序是正确的。 (3.1) 在分布式系统中深度分页 理解为什么深度分页是有问题的，我们可以假设在一个有 5 个主分片的索引中搜索。 当我们请求结果的第一页（结果从 1 到 10 ），每一个分片产生前 10 的结果，并且返回给 协调节点 ，协调节点对 50 个结果排序得到全部结果的前 10 个。 现在假设我们请求第 1000 页—​结果从 10001 到 10010 。所有都以相同的方式工作除了每个分片不得不产生前10010个结果以外。 然后协调节点对全部 50050 个结果排序最后丢弃掉这些结果中的 50040 个结果。 可以看到，在分布式系统中，对结果排序的成本随分页的深度成指数上升。这就是 web 搜索引擎对任何查询都不要返回超过 1000 个结果的原因。 (4) 轻量搜索 有两种形式的 搜索 API：一种是 “轻量的” 查询字符串 版本，要求在查询字符串中传递所有的参数，另一种是更完整的 请求体 版本，要求使用 JSON 格式和更丰富的查询表达式作为搜索语言。 (4.1) 查询字符串搜索 查询字符串搜索非常适用于通过命令行做即席查询。 (4.1.1) 条件查询 例如，查询在 tweet 类型中 tweet 字段包含 elasticsearch 单词的所有文档： 12345678910111213141516curl -X GET "localhost:9200/_all/tweet/_search?q=tweet:elasticsearch&amp;pretty"&#123; "took" : 156, "timed_out" : false, "_shards" : &#123; "total" : 13, "successful" : 13, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 0, "max_score" : null, "hits" : [ ] &#125;&#125; (4.1.2) 多条件查询下一个查询在 name 字段中包含 john 并且在 tweet 字段中包含 mary 的文档。实际的查询就是这样 +name:john +tweet:mary 但是查询字符串参数所需要的 百分比编码 （译者注：URL编码）实际上更加难懂：12345678910111213141516curl -X GET "localhost:9200/_search?q=%2Bname%3Ajohn+%2Btweet%3Amary&amp;pretty"&#123; "took" : 21, "timed_out" : false, "_shards" : &#123; "total" : 13, "successful" : 13, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 0, "max_score" : null, "hits" : [ ] &#125;&#125; 前缀表示必须与查询条件匹配。类似地， - 前缀表示一定不与查询条件匹配。没有 + 或者 - 的所有其他条件都是可选的——匹配的越多，文档就越相关。 (4.1.3) 全字段查询 这个简单搜索返回包含 mary 的所有文档： 12345678910111213141516$ curl -X GET "localhost:9200/_search?q=mary&amp;pretty"&#123; "took" : 38, "timed_out" : false, "_shards" : &#123; "total" : 13, "successful" : 13, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 0, "max_score" : null, "hits" : [ ] &#125;&#125; Elasticsearch 是如何在三个不同的字段中查找到结果的呢？ 当索引一个文档的时候，Elasticsearch 取出所有字段的值拼接成一个大的字符串，作为 _all 字段进行索引。例如，当索引这个文档时： 123456&#123; "tweet": "However did I manage before Elasticsearch?", "date": "2014-09-14", "name": "Mary Jones", "user_id": 1&#125; 这就好似增加了一个名叫 _all 的额外字段：1&quot;However did I manage before Elasticsearch? 2014-09-14 Mary Jones 1&quot; 除非设置特定字段，否则查询字符串就使用 _all 字段进行搜索。 在刚开始开发一个应用时，_all 字段是一个很实用的特性。之后，你会发现如果搜索时用指定字段来代替 _all 字段，将会更好控制搜索结果。当 _all 字段不再有用的时候，可以将它置为失效，正如在 元数据: _all 字段 中所解释的。 (4.2) 更复杂的查询下面的查询针对tweents类型，并使用以下的条件： name 字段中包含 mary 或者 john date 值大于 2014-09-10 _all 字段包含 aggregations 或者 geo 查询字符串搜索允许任何用户在索引的任意字段上执行可能较慢且重量级的查询，这可能会暴露隐私信息，甚至将集群拖垮。 因为这些原因，不推荐直接向用户暴露查询字符串搜索功能，除非对于集群和数据来说非常信任他们。 References[0] 最基本的工具[1] 空搜索[2] 多索引，多类型[3] 分页[4] 轻量搜索]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch 分布式文档存储]]></title>
    <url>%2F2020%2F01%2F20%2Felasticsearch-distributed-docs%2F</url>
    <content type="text"><![CDATA[文件是如何分布到集群的，又是如何从集群中获取的。 (1) 路由一个文档到一个分片中 当索引一个文档的时候，文档会被存储到一个主分片中。 这个过程是根据下面这个公式决定的： shard = hash(routing) % number_of_primary_shards routing 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。 routing 通过 hash 函数生成一个数字，然后这个数字再除以 number_of_primary_shards （主分片的数量）后得到 余数 。这个分布在 0 到 number_of_primary_shards-1 之间的余数，就是我们所寻求的文档所在分片的位置。 这就解释了为什么我们要在创建索引的时候就确定好主分片的数量 并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。 所有的文档 API（ get 、 index 、 delete 、 bulk 、 update 以及 mget ）都接受一个叫做 routing 的路由参数 ，通过这个参数我们可以自定义文档到分片的映射。一个自定义的路由参数可以用来确保所有相关的文档——例如所有属于同一个用户的文档——都被存储到同一个分片中。 (2) 主分片和副本分片如何交互 假设有一个集群由三个节点组成。 它包含一个叫 blogs 的索引，有两个主分片，每个主分片有两个副本分片。相同分片的副本不会放在同一节点。 我们可以发送请求到集群中的任一节点。 每个节点都有能力处理任意请求。 每个节点都知道集群中任一文档位置，所以可以直接将请求转发到需要的节点上。 在下面的例子中，将所有的请求发送到 Node 1 ，我们将其称为 协调节点(coordinating node) 。 当发送请求的时候， 为了扩展负载，更好的做法是轮询集群中所有的节点。 (3) 新建、索引和删除文档 新建、索引和删除 请求都是 写 操作， 必须在主分片上面完成之后才能被复制到相关的副本分片。 以下是在主副分片和任何副本分片上面 成功新建，索引和删除文档所需要的步骤顺序： 1. 客户端向 Node 1 发送新建、索引或者删除请求。 2. 节点使用文档的 _id 确定文档属于分片 0 。请求会被转发到 Node 3，因为分片 0 的主分片目前被分配在 Node 3 上。 3. Node 3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node 1 和 Node 2 的副本分片上。一旦所有的副本分片都报告成功, Node 3 将向协调节点报告成功，协调节点向客户端报告成功。 在客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成，变更是安全的。 有一些可选的请求参数允许影响这个过程，可能以数据安全为代价提升性能。这些选项很少使用，因为Elasticsearch已经很快，但是为了完整起见，在这里阐述如下： consistency consistency，即一致性。在默认设置下，即使仅仅是在试图执行一个_写操作之前，主分片都会要求 必须要有 规定数量(quorum)（或者换种说法，也即必须要有大多数）的分片副本处于活跃可用状态，才会去执行写操作(其中分片副本可以是主分片或者副本分片)。这是为了避免在发生网络分区故障（network partition）的时候进行写操作，进而导致数据不一致。规定数量_即： int( (primary + number_of_replicas) / 2 ) + 1 consistency 参数的值可以设为 one （只要主分片状态 ok 就允许执行_写操作）,all（必须要主分片和所有副本分片的状态没问题才允许执行写操作）, 或 quorum 。默认值为 quorum , 即大多数的分片副本状态没问题就允许执行写_操作。 注意，规定数量 的计算公式中 number_of_replicas 指的是在索引设置中的设定副本分片数，而不是指当前处理活动状态的副本分片数。如果你的索引设置中指定了当前索引拥有三个副本分片，那规定数量的计算结果即： int( (primary + 3 replicas) / 2 ) + 1 = 3 如果此时你只启动两个节点，那么处于活跃状态的分片副本数量就达不到规定数量，也因此您将无法索引和删除任何文档。 timeout 如果没有足够的副本分片会发生什么？ Elasticsearch会等待，希望更多的分片出现。默认情况下，它最多等待1分钟。 如果你需要，你可以使用 timeout 参数 使它更早终止： 100 100毫秒，30s 是30秒。 新索引默认有 1 个副本分片，这意味着为满足 规定数量 应该 需要两个活动的分片副本。 但是，这些默认的设置会阻止我们在单一节点上做任何事情。为了避免这个问题，要求只有当 number_of_replicas 大于1的时候，规定数量才会执行。 (4) 取回一个文档 可以从主分片或者从其它任意副本分片检索文档。 以下是从主分片或者副本分片检索文档的步骤顺序： 1、客户端向 Node 1 发送获取请求。 2、节点使用文档的 _id 来确定文档属于分片 0 。分片 0 的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到 Node 2 。 3、Node 2 将文档返回给 Node 1 ，然后将文档返回给客户端。 在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。 在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。 在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。 一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的。 (5) 局部更新文档 以下是部分更新一个文档的步骤： 客户端向 Node 1 发送更新请求。 它将请求转发到主分片所在的 Node 3 。 Node 3 从主分片检索文档，修改 _source 字段中的 JSON ，并且尝试重新索引主分片的文档。 如果文档已经被另一个进程修改，它会重试步骤 3 ，超过 retry_on_conflict 次后放弃。 如果 Node 3 成功地更新文档，它将新版本的文档并行转发到 Node 1 和 Node 2 上的副本分片，重新建立索引。 一旦所有副本分片都返回成功， Node 3 向协调节点也返回成功，协调节点向客户端返回成功。 基于文档的复制 当主分片把更改转发到副本分片时， 它不会转发更新请求。 相反，它转发完整文档的新版本。请记住，这些更改将会异步转发到副本分片，并且不能保证它们以发送它们相同的顺序到达。 如果Elasticsearch仅转发更改请求，则可能以错误的顺序应用更改，导致得到损坏的文档。 (6) 多文档模式 mget 和 bulk API 的模式类似于单文档模式。区别在于协调节点知道每个文档存在于哪个分片中。 它将整个多文档请求分解成 每个分片 的多文档请求，并且将这些请求并行转发到每个参与节点。 协调节点一旦收到来自每个节点的应答，就将每个节点的响应收集整理成单个响应，返回给客户端。 以下是使用单个 mget 请求取回多个文档所需的步骤顺序： 客户端向 Node 1 发送 mget 请求。 Node 1 为每个分片构建多文档获取请求，然后并行转发这些请求到托管在每个所需的主分片或者副本分片的节点上。一旦收到所有答复， Node 1 构建响应并将其返回给客户端。可以对 docs 数组中每个文档设置 routing 参数。 bulk API 按如下步骤顺序执行： 客户端向 Node 1 发送 bulk 请求。 Node 1 为每个节点创建一个批量请求，并将这些请求并行转发到每个包含主分片的节点主机。 主分片一个接一个按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。 一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端。 bulk API 还可以在整个批量请求的最顶层使用 consistency 参数，以及在每个请求中的元数据中使用 routing 参数。 在批量请求中引用的每个文档可能属于不同的主分片， 每个文档可能被分配给集群中的任何节点。这意味着批量请求 bulk 中的每个 操作 都需要被转发到正确节点上的正确分片。 Elasticsearch可以直接读取被网络缓冲区接收的原始数据。 它使用换行符字符来识别和解析小的 action/metadata 行来决定哪个分片应该处理每个请求。 这些原始请求会被直接转发到正确的分片。没有冗余的数据复制，没有浪费的数据结构。整个请求尽可能在最小的内存中处理。 References[0] 分布式文档存储[1] 路由一个文档到一个分片中[2] 主分片和副本分片如何交互[3] 新建、索引和删除文档[4] 取回一个文档[5] 局部更新文档[6] 多文档模式]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch 数据输入和输出]]></title>
    <url>%2F2020%2F01%2F20%2Felasticsearch-data-in-data-out%2F</url>
    <content type="text"><![CDATA[Elastcisearch 是分布式的 文档 存储。它能存储和检索复杂的数据结构—​序列化成为JSON文档—​以 实时 的方式。 换句话说，一旦一个文档被存储在 Elasticsearch 中，它就是可以被集群中的任意节点检索到。 在 Elasticsearch 中， 每个字段的所有数据 都是 默认被索引的 。 即每个字段都有为了快速检索设置的专用倒排索引。而且，不像其他多数的数据库，它能在 同一个查询中 使用所有这些倒排索引，并以惊人的速度返回结果。 ElasticSearch 和 MySQL 结构对照 ES MySQLNode/Cluster ClusterIndex DatabaseType tableDocument row (一行)field field (一列) (1) 什么是文档?1234567891011121314151617181920&#123; "name": "John Smith", "age": 42, "confirmed": true, "join_date": "2014-06-01", "home": &#123; "lat": 51.5, "lon": 0.1 &#125;, "accounts": [ &#123; "type": "facebook", "id": "johnsmith" &#125;, &#123; "type": "twitter", "id": "johnsmith" &#125; ]&#125; 在 Elasticsearch 中，术语 文档 有着特定的含义。它是指最顶层或者根对象, 这个根对象被序列化成 JSON 并存储到 Elasticsearch 中，指定了唯一 ID。 字段的名字可以是任何合法的字符串，但 不可以 包含英文句号(.)。 (2) 文档元数据 一个文档不仅仅包含它的数据 ，也包含 元数据 —— 有关 文档的信息。 三个必须的元数据元素如下： _index 文档在哪存放 _type 文档表示的对象类别 _id 文档唯一标识 (2.1) _index 一个 索引 应该是因共同的特性被分组到一起的文档集合。 例如，你可能存储所有的产品在索引 products 中，而存储所有销售的交易到索引 sales 中。 虽然也允许存储不相关的数据到一个索引中，但这通常看作是一个反模式的做法。 实际上，在 Elasticsearch 中，我们的数据是被存储和索引在 分片 中，而一个索引仅仅是逻辑上的命名空间， 这个命名空间由一个或者多个分片组合在一起。 然而，这是一个内部细节，我们的应用程序根本不应该关心分片，对于应用程序而言，只需知道文档位于一个 索引 内。 Elasticsearch 会处理所有的细节。 (2.2) _type 数据可能在索引中只是松散的组合在一起，但是通常明确定义一些数据中的子分区是很有用的。 例如，所有的产品都放在一个索引中，但是你有许多不同的产品类别，比如 “electronics” 、 “kitchen” 和 “lawn-care”。 这些文档共享一种相同的（或非常相似）的模式：他们有一个标题、描述、产品代码和价格。他们只是正好属于“产品”下的一些子类。 Elasticsearch 公开了一个称为 types （类型）的特性，它允许您在索引中对数据进行逻辑分区。不同 types 的文档可能有不同的字段，但最好能够非常相似。 我们将在 类型和映射 中更多的讨论关于 types 的一些应用和限制。 一个 _type 命名可以是大写或者小写，但是不能以下划线或者句号开头，不应该包含逗号， 并且长度限制为256个字符. 我们使用 blog 作为类型名举例。 (2.3) _id ID 是一个字符串，当它和 _index 以及 _type 组合就可以唯一确定 Elasticsearch 中的一个文档。 当你创建一个新的文档，要么提供自己的 _id ，要么让 Elasticsearch 帮你生成。 (3) 索引文档 通过使用 index API ，文档可以被 索引 —— 存储和使文档可被搜索。 但是首先，我们要确定文档的位置。正如我们刚刚讨论的，一个文档的 _index 、 _type 和 _id 唯一标识一个文档。 我们可以提供自定义的 _id 值，或者让 index API 自动生成。 (3.1) 使用自定义id12345PUT ip:port/&#123;index&#125;/&#123;type&#125;/&#123;id&#125;&#123; "field": "value", ...&#125; 如果我们的索引称为 website ，类型称为 blog ，并且选择 123 作为 ID ，那么索引请求应该是下面这样： 123456789101112131415161718192021$ curl -X PUT &quot;localhost:9200/website/blog/123?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&gt; &#123;&gt; &quot;title&quot;: &quot;My first blog entry&quot;,&gt; &quot;text&quot;: &quot;Just trying this out...&quot;,&gt; &quot;date&quot;: &quot;2014/01/01&quot;&gt; &#125;&gt; &apos;&#123; &quot;_index&quot; : &quot;website&quot;, &quot;_type&quot; : &quot;blog&quot;, &quot;_id&quot; : &quot;123&quot;, &quot;_version&quot; : 1, &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 1, &quot;failed&quot; : 0 &#125;, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1&#125; 该响应表明文档已经成功创建，该索引包括 _index 、 _type 和 _id 元数据， 以及一个新元素： _version 。 在 Elasticsearch 中每个文档都有一个版本号。当每次对文档进行修改时（包括删除）， _version 的值会递增。 在 处理冲突 中，我们讨论了怎样使用 _version 号码确保你的应用程序中的一部分修改不会覆盖另一部分所做的修改。 (3.2) Autogenerating IDs 如果你的数据没有自然的 ID， Elasticsearch 可以帮我们自动生成 ID 。 请求的结构调整为： 不再使用 PUT 谓词(“使用这个 URL 存储这个文档”)， 而是使用 POST 谓词(“存储文档在这个 URL 命名空间下”)。 现在该 URL 只需包含 _index 和 _type :123456789101112131415161718192021$ curl -X POST &quot;localhost:9200/website/blog/?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&gt; &#123;&gt; &quot;title&quot;: &quot;My second blog entry&quot;,&gt; &quot;text&quot;: &quot;Still trying this out...&quot;,&gt; &quot;date&quot;: &quot;2014/01/01&quot;&gt; &#125;&gt; &apos;&#123; &quot;_index&quot; : &quot;website&quot;, &quot;_type&quot; : &quot;blog&quot;, &quot;_id&quot; : &quot;1aMWwW8BXjhxAah6xIse&quot;, &quot;_version&quot; : 1, &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 1, &quot;failed&quot; : 0 &#125;, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1&#125; 自动生成的 ID 是 URL-safe、 基于 Base64 编码且长度为20个字符的 GUID 字符串。 这些 GUID 字符串由可修改的 FlakeID 模式生成，这种模式允许多个节点并行生成唯一 ID ，且互相之间的冲突概率几乎为零。 (4) 取回一个文档 为了从 Elasticsearch 中检索出文档，我们仍然使用相同的 _index , _type , 和 _id ，但是 HTTP 谓词更改为 GET : 123456789101112131415$ curl -X GET "localhost:9200/website/blog/123?pretty&amp;pretty"&#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 1, "_seq_no" : 0, "_primary_term" : 1, "found" : true, "_source" : &#123; "title" : "My first blog entry", "text" : "Just trying this out...", "date" : "2014/01/01" &#125;&#125; 在请求的查询串参数中加上 pretty 参数，正如前面的例子中看到的，这将会调用 Elasticsearch 的 pretty-print 功能，该功能 使得 JSON 响应体更加可读。但是， _source 字段不能被格式化打印出来。相反，我们得到的 _source 字段中的 JSON 串，刚好是和我们传给它的一样。 1234567891011$ curl -i -XGET http://localhost:9200/website/blog/124?prettyHTTP/1.1 404 Not Foundcontent-type: application/json; charset=UTF-8content-length: 83&#123; "_index" : "website", "_type" : "blog", "_id" : "124", "found" : false&#125; 返回文档的一部分 1234567891011121314$ curl -X GET "localhost:9200/website/blog/123?_source=title,text&amp;pretty"&#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 1, "_seq_no" : 0, "_primary_term" : 1, "found" : true, "_source" : &#123; "text" : "Just trying this out...", "title" : "My first blog entry" &#125;&#125; 只想得到 _source 字段，不需要任何元数据，你能使用 _source 端点 123456$ curl -X GET &quot;localhost:9200/website/blog/123/_source?pretty&quot;&#123; &quot;title&quot; : &quot;My first blog entry&quot;, &quot;text&quot; : &quot;Just trying this out...&quot;, &quot;date&quot; : &quot;2014/01/01&quot;&#125; (5) 检查文档是否存在如果只想检查一个文档是否存在–根本不想关心内容—​那么用 HEAD 方法来代替 GET 方法。 HEAD 请求没有返回体，只返回一个 HTTP 请求报头 1234$ curl -i -XHEAD http://localhost:9200/website/blog/123HTTP/1.1 200 OKcontent-type: application/json; charset=UTF-8content-length: 215 果文档存在， Elasticsearch 将返回一个 200 ok 的状态码 1234$ curl -i -XHEAD http://localhost:9200/website/blog/124HTTP/1.1 404 Not Foundcontent-type: application/json; charset=UTF-8content-length: 61 文档不存在， Elasticsearch 将返回一个 404 Not Found 的状态码 (6) 更新整个文档123456789101112131415161718192021$ curl -X PUT "localhost:9200/website/blog/123?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "title": "My first blog entry",&gt; "text": "I am starting to get the hang of this...",&gt; "date": "2014/01/02"&gt; &#125;&gt; '&#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 2, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 1, "_primary_term" : 1&#125; created 标志设置成 false ，是因为相同的索引、类型和 ID 的文档已经存在。 在内部，Elasticsearch 已将旧文档标记为已删除，并增加一个全新的文档。 尽管你不能再对旧版本的文档进行访问，但它并不会立即消失。当继续索引更多的数据，Elasticsearch 会在后台清理这些已删除文档。 (7) 创建新文档 当我们索引一个文档，怎么确认我们正在创建一个完全新的文档，而不是覆盖现有的呢？ 请记住， _index 、 _type 和 _id 的组合可以唯一标识一个文档。所以，确保创建一个新文档的最简单办法是，使用索引请求的 POST 形式让 Elasticsearch 自动生成唯一 _id : 12345$ curl POST "localhost:9200//website/blog/" -H 'Content-Type: application/json' -d'&#123; ... &#125;' 如果已经有自己的 _id ，那么我们必须告诉 Elasticsearch ，只有在相同的 _index 、 _type 和 _id 不存在时才接受我们的索引请求。 第一种方法使用 op_type 查询-字符串参数：12PUT /website/blog/123?op_type=create&#123; ... &#125; 第二种方法是在 URL 末端使用 /_create :12PUT /website/blog/123/_create&#123; ... &#125; 如果创建新文档的请求成功执行，Elasticsearch 会返回元数据和一个 201 Created 的 HTTP 响应码。 另一方面，如果具有相同的 _index 、 _type 和 _id 的文档已经存在，Elasticsearch 将会返回 409 Conflict 响应码，以及如下的错误信息：1234567891011121314151617&#123; "error": &#123; "root_cause": [ &#123; "type": "document_already_exists_exception", "reason": "[blog][123]: document already exists", "shard": "0", "index": "website" &#125; ], "type": "document_already_exists_exception", "reason": "[blog][123]: document already exists", "shard": "0", "index": "website" &#125;, "status": 409&#125; (8) 删除文档123456789101112131415$ curl -X DELETE "localhost:9200/website/blog/123?pretty"&#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 3, "result" : "deleted", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 2, "_primary_term" : 1&#125; 删除文档不会立即将文档从磁盘中删除，只是将文档标记为已删除状态。随着你不断的索引更多的数据，Elasticsearch 将会在后台清理标记为已删除的文档。 如果文档没有找到，我们将得到 404 Not Found 的响应码和类似这样的响应体：1234567&#123; "found" : false, "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 4&#125; 即使文档不存在（ Found 是 false ）， _version 值仍然会增加。 (9) 处理冲突 丢失了一个变更就是 非常严重的 。 变更越频繁，读数据和更新数据的间隙越长，也就越可能丢失变更。 (9.1) 冲突解决 在数据库领域中，有两种方法通常被用来确保并发更新时变更不会丢失： (9.1.1) 悲观并发控制这种方法被关系型数据库广泛使用，它假定有变更冲突可能发生，因此阻塞访问资源以防止冲突。 一个典型的例子是读取一行数据之前先将其锁住，确保只有放置锁的线程能够对这行数据进行修改。 (9.1.2) 乐观并发控制Elasticsearch 中使用的这种方法假定冲突是不可能发生的，并且不会阻塞正在尝试的操作。 然而，如果源数据在读写当中被修改，更新将会失败。应用程序接下来将决定该如何解决冲突。 例如，可以重试更新、使用新的数据、或者将相关情况报告给用户。 (10) 乐观并发控制 Elasticsearch 是分布式的。当文档创建、更新或删除时， 新版本的文档必须复制到集群中的其他节点。Elasticsearch 也是异步和并发的，这意味着这些复制请求被并行发送，并且到达目的地时也许 顺序是乱的 。 Elasticsearch 需要一种方法确保文档的旧版本不会覆盖新的版本。 当我们之前讨论 index ， GET 和 delete 请求时，我们指出每个文档都有一个 _version （版本）号，当文档被修改时版本号递增。 Elasticsearch 使用这个 _version 号来确保变更以正确顺序得到执行。如果旧版本的文档在新版本之后到达，它可以被简单的忽略。 1234567891011121314151617181920$ curl -X PUT "localhost:9200/website/blog/1/_create?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "title": "My first blog entry",&gt; "text": "Just trying this out..."&gt; &#125;&gt; '&#123; "_index" : "website", "_type" : "blog", "_id" : "1", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 0, "_primary_term" : 1&#125; 响应体告诉我们，这个新创建的文档 _version 版本号是 1 。现在假设我们想编辑这个文档：我们加载其数据到 web 表单中， 做一些修改，然后保存新的版本。 1234567891011121314$ curl -X GET "localhost:9200/website/blog/1?pretty"&#123; "_index" : "website", "_type" : "blog", "_id" : "1", "_version" : 1, "_seq_no" : 0, "_primary_term" : 1, "found" : true, "_source" : &#123; "title" : "My first blog entry", "text" : "Just trying this out..." &#125;&#125; 当我们尝试通过重建文档的索引来保存修改，我们指定 version 为我们的修改会被应用的版本：1234567891011121314151617181920$ curl -X PUT "localhost:9200/website/blog/1?version=1&amp;pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "title": "My first blog entry",&gt; "text": "Starting to get the hang of this..."&gt; &#125;&gt; '&#123; "_index" : "website", "_type" : "blog", "_id" : "1", "_version" : 2, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 1, "_primary_term" : 1&#125; 我们想这个在我们索引中的文档只有现在的 _version 为 1 时，本次更新才能成功。 重新运行相同的索引请求，仍然指定 version=1 ， Elasticsearch 返回 409 Conflict HTTP 响应码，和一个如下所示的响应体：12345678910111213141516171819202122232425$ curl -X PUT "localhost:9200/website/blog/1?version=1&amp;pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "title": "My first blog entry",&gt; "text": "Starting to get the hang of this..."&gt; &#125;&gt; '&#123; "error" : &#123; "root_cause" : [ &#123; "type" : "version_conflict_engine_exception", "reason" : "[blog][1]: version conflict, current version [2] is different than the one provided [1]", "index_uuid" : "1Ptx5N-iTR2nDNtVgVMEpw", "shard" : "3", "index" : "website" &#125; ], "type" : "version_conflict_engine_exception", "reason" : "[blog][1]: version conflict, current version [2] is different than the one provided [1]", "index_uuid" : "1Ptx5N-iTR2nDNtVgVMEpw", "shard" : "3", "index" : "website" &#125;, "status" : 409&#125; 所有文档的更新或删除 API，都可以接受 version 参数，这允许你在代码中使用乐观的并发控制，这是一种明智的做法。 (10.2) 通过外部系统使用版本控制 一个常见的设置是使用其它数据库作为主要的数据存储，使用 Elasticsearch 做数据检索， 这意味着主数据库的所有更改发生时都需要被复制到 Elasticsearch ，如果多个进程负责这一数据同步，你可能遇到类似于之前描述的并发问题。 如果你的主数据库已经有了版本号 — 或一个能作为版本号的字段值比如 timestamp — 那么你就可以在 Elasticsearch 中通过增加 version_type=external 到查询字符串的方式重用这些相同的版本号， 版本号必须是大于零的整数， 且小于 9.2E+18 — 一个 Java 中 long 类型的正值。 外部版本号的处理方式和我们之前讨论的内部版本号的处理方式有些不同， Elasticsearch 不是检查当前 _version 和请求中指定的版本号是否相同， 而是检查当前 _version 是否 小于 指定的版本号。 如果请求成功，外部的版本号作为文档的新 _version 进行存储。 外部版本号不仅在索引和删除请求是可以指定，而且在 创建 新文档时也可以指定。 1234567891011121314151617181920$ curl -X PUT "localhost:9200/website/blog/2?version=5&amp;version_type=external&amp;pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "title": "My first external blog entry",&gt; "text": "Starting to get the hang of this..."&gt; &#125;&gt; '&#123; "_index" : "website", "_type" : "blog", "_id" : "2", "_version" : 5, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 0, "_primary_term" : 1&#125; 在响应中，我们能看到当前的 _version 版本号是 5 现在我们更新这个文档，指定一个新的 version 号是 101234567891011121314151617181920$ curl -X PUT "localhost:9200/website/blog/2?version=10&amp;version_type=external&amp;pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "title": "My first external blog entry",&gt; "text": "This is a piece of cake..."&gt; &#125;&gt; '&#123; "_index" : "website", "_type" : "blog", "_id" : "2", "_version" : 10, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 1, "_primary_term" : 1&#125; 请求成功并将当前 _version 设为 10 (11) 文档的部分更新 我们也介绍过文档是不可变的：他们不能被修改，只能被替换。 update API 必须遵循同样的规则。 从外部来看，我们在一个文档的某个位置进行部分更新。然而在内部， update API 简单使用与之前描述相同的 检索-修改-重建索引 的处理过程。 区别在于这个过程发生在分片内部，这样就避免了多次请求的网络开销。通过减少检索和重建索引步骤之间的时间，我们也减少了其他进程的变更带来冲突的可能性。 update 请求最简单的一种形式是接收文档的一部分作为 doc 的参数， 它只是与现有的文档进行合并。对象被合并到一起，覆盖现有的字段，增加新的字段。 例如，我们增加字段 tags 和 views 到我们的博客文章，如下所示：12345678910111213141516171819202122$ curl -X POST "localhost:9200/website/blog/1/_update?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "doc" : &#123;&gt; "tags" : [ "testing" ],&gt; "views": 0&gt; &#125;&gt; &#125;&gt; '&#123; "_index" : "website", "_type" : "blog", "_id" : "1", "_version" : 3, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 2, "_primary_term" : 1&#125; 查询更新后的文档123456789101112131415161718$ curl -X GET "localhost:9200/website/blog/1?pretty"&#123; "_index" : "website", "_type" : "blog", "_id" : "1", "_version" : 3, "_seq_no" : 2, "_primary_term" : 1, "found" : true, "_source" : &#123; "title" : "My first blog entry", "text" : "Starting to get the hang of this...", "views" : 0, "tags" : [ "testing" ] &#125;&#125; (11.1) 使用脚本部分更新文档12345678910111213141516171819$ curl -X POST "localhost:9200/website/blog/1/_update?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "script" : "ctx._source.views+=1"&gt; &#125;&gt; '&#123; "_index" : "website", "_type" : "blog", "_id" : "1", "_version" : 4, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 3, "_primary_term" : 1&#125; (11.2) 使用脚本部分更新文档对于那些 API 不能满足需求的情况，Elasticsearch 允许你使用脚本编写自定义的逻辑。 许多API都支持脚本的使用，包括搜索、排序、聚合和文档更新。 脚本可以作为请求的一部分被传递，从特殊的 .scripts 索引中检索，或者从磁盘加载脚本。 默认的脚本语言 是 Groovy，一种快速表达的脚本语言，在语法上与 JavaScript 类似。 它在 Elasticsearch V1.3.0 版本首次引入并运行在 沙盒 中，然而 Groovy 脚本引擎存在漏洞， 允许攻击者通过构建 Groovy 脚本，在 Elasticsearch Java VM 运行时脱离沙盒并执行 shell 命令。 因此，在版本 v1.3.8 、 1.4.3 和 V1.5.0 及更高的版本中，它已经被默认禁用。 此外，您可以通过设置集群中的所有节点的 config/elasticsearch.yml 文件来禁用动态 Groovy 脚本： script.groovy.sandbox.enabled: false 这将关闭 Groovy 沙盒，从而防止动态 Groovy 脚本作为请求的一部分被接受， 或者从特殊的 .scripts 索引中被检索。当然，你仍然可以使用存储在每个节点的 config/scripts/ 目录下的 Groovy 脚本。 如果你的架构和安全性不需要担心漏洞攻击，例如你的 Elasticsearch 终端仅暴露和提供给可信赖的应用， 当它是你的应用需要的特性时，你可以选择重新启用动态脚本。 你可以在 scripting reference documentation 获取更多关于脚本的资料。 通过设置参数 retry_on_conflict 来自动完成， 这个参数规定了失败之前 update 应该重试的次数，它的默认值为 0 12345678910111213141516171819202122$ curl -X POST "localhost:9200/website/blog/1/_update?retry_on_conflict=5&amp;pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "script" : "ctx._source.views+=1",&gt; "upsert": &#123;&gt; "views": 0&gt; &#125;&gt; &#125;&gt; '&#123; "_index" : "website", "_type" : "blog", "_id" : "1", "_version" : 6, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 5, "_primary_term" : 1&#125; (12) 取回多个文档 Elasticsearch 的速度已经很快了，但甚至能更快。 将多个请求合并成一个，避免单独处理每个请求花费的网络延时和开销。 如果你需要从 Elasticsearch 检索很多文档，那么使用 multi-get 或者 mget API 来将这些检索请求放在一个请求中，将比逐个文档请求更快地检索到全部文档。 mget API 要求有一个 docs 数组作为参数，每个元素包含需要检索文档的元数据， 包括 _index 、 _type 和 _id 。如果你想检索一个或者多个特定的字段，那么你可以通过 _source 参数来指定这些字段的名字： 12345678910111213141516171819202122232425262728293031323334353637383940$ curl -X GET "localhost:9200/_mget?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "docs" : [&gt; &#123;&gt; "_index" : "website",&gt; "_type" : "blog",&gt; "_id" : 2&gt; &#125;,&gt; &#123;&gt; "_index" : "website",&gt; "_type" : "pageviews",&gt; "_id" : 1,&gt; "_source": "views"&gt; &#125;&gt; ]&gt; &#125;&gt; '&#123; "docs" : [ &#123; "_index" : "website", "_type" : "blog", "_id" : "2", "_version" : 10, "_seq_no" : 1, "_primary_term" : 1, "found" : true, "_source" : &#123; "title" : "My first external blog entry", "text" : "This is a piece of cake..." &#125; &#125;, &#123; "_index" : "website", "_type" : "pageviews", "_id" : "1", "found" : false &#125; ]&#125; 该响应体也包含一个 docs 数组， 对于每一个在请求中指定的文档，这个数组中都包含有一个对应的响应，且顺序与请求中的顺序相同。 其中的每一个响应都和使用单个 get request 请求所得到的响应体相同 如果想检索的数据都在相同的 _index 中（甚至相同的 _type 中），则可以在 URL 中指定默认的 /_index 或者默认的 /_index/_type 。 12345678910111213141516171819202122232425262728293031$ curl -X GET "localhost:9200/website/blog/_mget?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "docs" : [&gt; &#123; "_id" : 2 &#125;,&gt; &#123; "_type" : "pageviews", "_id" : 1 &#125;&gt; ]&gt; &#125;&gt; '&#123; "docs" : [ &#123; "_index" : "website", "_type" : "blog", "_id" : "2", "_version" : 10, "_seq_no" : 1, "_primary_term" : 1, "found" : true, "_source" : &#123; "title" : "My first external blog entry", "text" : "This is a piece of cake..." &#125; &#125;, &#123; "_index" : "website", "_type" : "pageviews", "_id" : "1", "found" : false &#125; ]&#125; 123456789101112131415161718192021222324252627282930313233343536373839$ curl -X GET "localhost:9200/website/blog/_mget?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "ids" : [ "2", "1" ]&gt; &#125;&gt; '&#123; "docs" : [ &#123; "_index" : "website", "_type" : "blog", "_id" : "2", "_version" : 10, "_seq_no" : 1, "_primary_term" : 1, "found" : true, "_source" : &#123; "title" : "My first external blog entry", "text" : "This is a piece of cake..." &#125; &#125;, &#123; "_index" : "website", "_type" : "blog", "_id" : "1", "_version" : 6, "_seq_no" : 5, "_primary_term" : 1, "found" : true, "_source" : &#123; "title" : "My first blog entry", "text" : "Starting to get the hang of this...", "views" : 3, "tags" : [ "testing" ] &#125; &#125; ]&#125; (13) 代价较小的批量操作 与 mget 可以使我们一次取回多个文档同样的方式， bulk API 允许在单个步骤中进行多次 create 、 index 、 update 或 delete 请求。 如果你需要索引一个数据流比如日志事件，它可以排队和索引数百或数千批次。12345&#123; action: &#123; metadata &#125;&#125;\n&#123; request body &#125;\n&#123; action: &#123; metadata &#125;&#125;\n&#123; request body &#125;\n... 这种格式类似一个有效的单行 JSON 文档 流 ，它通过换行符(\n)连接到一起。注意两个要点： 每行一定要以换行符(\n)结尾， 包括最后一行 。这些换行符被用作一个标记，可以有效分隔行。 这些行不能包含未转义的换行符，因为他们将会对解析造成干扰。这意味着这个 JSON 不 能使用 pretty 参数打印。 action/metadata 行指定 哪一个文档 做 什么操作 。 action 必须是以下选项之一: create 如果文档不存在，那么就创建它。详情请见 创建新文档。 index 创建一个新文档或者替换一个现有的文档。详情请见 索引文档 和 更新整个文档。 update 部分更新一个文档。详情请见 文档的部分更新。 delete 删除一个文档。详情请见 删除文档。 metadata 应该指定被索引、创建、更新或者删除的文档的 _index 、 _type 和 _id 。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283$ curl -X POST "localhost:9200/_bulk?pretty" -H 'Content-Type: application/json' -d'&gt; &#123; "delete": &#123; "_index": "website", "_type": "blog", "_id": "123" &#125;&#125;&gt; &#123; "create": &#123; "_index": "website", "_type": "blog", "_id": "123" &#125;&#125;&gt; &#123; "title": "My first blog post" &#125;&gt; &#123; "index": &#123; "_index": "website", "_type": "blog" &#125;&#125;&gt; &#123; "title": "My second blog post" &#125;&gt; &#123; "update": &#123; "_index": "website", "_type": "blog", "_id": "123", "_retry_on_conflict" : 3&#125; &#125;&gt; &#123; "doc" : &#123;"title" : "My updated blog post"&#125; &#125;&gt; '&#123; "took" : 69, "errors" : false, "items" : [ &#123; "delete" : &#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 1, "result" : "not_found", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 3, "_primary_term" : 1, "status" : 404 &#125; &#125;, &#123; "create" : &#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 2, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 4, "_primary_term" : 1, "status" : 201 &#125; &#125;, &#123; "index" : &#123; "_index" : "website", "_type" : "blog", "_id" : "1qMXwm8BXjhxAah64Ysk", "_version" : 1, "result" : "created", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 1, "_primary_term" : 1, "status" : 201 &#125; &#125;, &#123; "update" : &#123; "_index" : "website", "_type" : "blog", "_id" : "123", "_version" : 3, "result" : "updated", "_shards" : &#123; "total" : 2, "successful" : 1, "failed" : 0 &#125;, "_seq_no" : 5, "_primary_term" : 1, "status" : 200 &#125; &#125; ]&#125; 每个子请求都是独立执行，因此某个子请求的失败不会对其他子请求的成功与否造成影响。 如果其中任何子请求失败，最顶层的 error 标志被设置为 true ，并且在相应的请求报告出错误明细 (13.1) 批量执行时多大效率最好整个批量请求都需要由接收到请求的节点加载到内存中，因此该请求越大，其他请求所能获得的内存就越少。 批量请求的大小有一个最佳值，大于这个值，性能将不再提升，甚至会下降。 但是最佳值不是一个固定的值。它完全取决于硬件、文档的大小和复杂度、索引和搜索的负载的整体情况。 幸运的是，很容易找到这个 最佳点 ：通过批量索引典型文档，并不断增加批量大小进行尝试。 当性能开始下降，那么你的批量大小就太大了。一个好的办法是开始时将 1,000 到 5,000 个文档作为一个批次, 如果你的文档非常大，那么就减少批量的文档个数。 密切关注你的批量请求的物理大小往往非常有用，一千个 1KB 的文档是完全不同于一千个 1MB 文档所占的物理大小。 一个好的批量大小在开始处理后所占用的物理大小约为 5-15 MB。 References[0] 数据输入和输出[1] 什么是文档?[2] 文档元数据[3] 索引文档[4] 取回一个文档[5] 检查文档是否存在[6] 更新整个文档[7] 创建新文档[8] 删除文档[9] 处理冲突[10] 乐观并发控制[11] 文档的部分更新[12] 取回多个文档[13] 代价较小的批量操作]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch-restful-api]]></title>
    <url>%2F2020%2F01%2F19%2Felasticsearch-restful-api%2F</url>
    <content type="text"><![CDATA[elasticsearch 的 resuful api 笔记 (1) 查看ES及索引信息(1.1) 查看ES基本信息123456789101112131415161718[wkq@VM_77_25_centos ~]$ curl 'http://localhost:9200?pretty'&#123; "name" : "elasticsearch_001_data", "cluster_name" : "elasticsearch_test", "cluster_uuid" : "NsxYKhI1Qw63MzaPKl34dA", "version" : &#123; "number" : "6.6.2", "build_flavor" : "default", "build_type" : "tar", "build_hash" : "3bd3e59", "build_date" : "2019-03-06T15:16:26.864148Z", "build_snapshot" : false, "lucene_version" : "7.6.0", "minimum_wire_compatibility_version" : "5.6.0", "minimum_index_compatibility_version" : "5.0.0" &#125;, "tagline" : "You Know, for Search"&#125; (1.2) 计算集群中文档的数量12345678910$ curl 'http://localhost:9200/_count?pretty' &#123; "count" : 3, "_shards" : &#123; "total" : 5, "successful" : 5, "skipped" : 0, "failed" : 0 &#125;&#125; 可以看到返回结果是3 (1.3) 查看ES里所有索引12345678910[wkq@VM_77_25_centos ~]$ curl &apos;localhost:9200/_cat/indices?v&apos;health status index uuid pri rep docs.count docs.deleted store.size pri.store.sizeyellow open megacorp EUu3nzyRQN2Mp7tRw2u_nQ 5 1 3 0 17.5kb 17.5kbyellow open gb kAXZAJZTRg2X0Wm72-n5qQ 5 1 1 0 5.7kb 5.7kbyellow open us Q6ubyCClQvyMB5iX8f9BZA 5 1 1 0 5.7kb 5.7kbyellow open tweet hNRmZ9RETWWPoUMIK3BivA 5 1 12 0 26kb 26kbyellow open website 1Ptx5N-iTR2nDNtVgVMEpw 5 1 5 0 21.2kb 21.2kbyellow open blogs bW_JTJkfS2GVN8FE_gX-Hg 3 2 0 0 783b 783byellow open user rOprq90rQsuyP0mad7I6iQ 5 1 2 0 10.2kb 10.2kb[wkq@VM_77_25_centos ~]$ (1.3) 插入数据12345678910$ curl -X PUT "localhost:9200/megacorp/employee/1" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "first_name" : "John",&gt; "last_name" : "Smith",&gt; "age" : 25,&gt; "about" : "I love to go rock climbing",&gt; "interests": [ "sports", "music" ]&gt; &#125;&gt; '&#123;"_index":"megacorp","_type":"employee","_id":"1","_version":2,"result":"updated","_shards":&#123;"total":2,"successful":1,"failed":0&#125;,"_seq_no":1,"_primary_term":1&#125; (1.4) 根据id查询数据1234567891011121314151617181920$ curl -X GET "localhost:9200/megacorp/employee/1?pretty"&#123; "_index" : "megacorp", "_type" : "employee", "_id" : "1", "_version" : 2, "_seq_no" : 1, "_primary_term" : 1, "found" : true, "_source" : &#123; "first_name" : "John", "last_name" : "Smith", "age" : 25, "about" : "I love to go rock climbing", "interests" : [ "sports", "music" ] &#125;&#125; (1.5) 查询某个索引里的所有数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263$ curl -X GET "localhost:9200/megacorp/employee/_search?pretty"&#123; "took" : 17, "timed_out" : false, "_shards" : &#123; "total" : 5, "successful" : 5, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 3, "max_score" : 1.0, "hits" : [ &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "2", "_score" : 1.0, "_source" : &#123; "first_name" : "Jane", "last_name" : "Smith", "age" : 32, "about" : "I like to collect rock albums", "interests" : [ "music" ] &#125; &#125;, &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "1", "_score" : 1.0, "_source" : &#123; "first_name" : "John", "last_name" : "Smith", "age" : 25, "about" : "I love to go rock climbing", "interests" : [ "sports", "music" ] &#125; &#125;, &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "3", "_score" : 1.0, "_source" : &#123; "first_name" : "Douglas", "last_name" : "Fir", "age" : 35, "about" : "I like to build cabinets", "interests" : [ "forestry" ] &#125; &#125; ] &#125;&#125; (1.5) 根据某个字段搜索123456789101112131415161718192021222324252627282930313233343536373839404142434445464748$ curl -X GET "localhost:9200/megacorp/employee/_search?q=last_name:Smith&amp;pretty"&#123; "took" : 112, "timed_out" : false, "_shards" : &#123; "total" : 5, "successful" : 5, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 2, "max_score" : 0.2876821, "hits" : [ &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "2", "_score" : 0.2876821, "_source" : &#123; "first_name" : "Jane", "last_name" : "Smith", "age" : 32, "about" : "I like to collect rock albums", "interests" : [ "music" ] &#125; &#125;, &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "1", "_score" : 0.2876821, "_source" : &#123; "first_name" : "John", "last_name" : "Smith", "age" : 25, "about" : "I love to go rock climbing", "interests" : [ "sports", "music" ] &#125; &#125; ] &#125;&#125; (1.6) 查看索引mapping123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051$ curl 'http://localhost:9200/megacorp/_mapping?pretty=true'&#123; "megacorp" : &#123; "mappings" : &#123; "employee" : &#123; "properties" : &#123; "about" : &#123; "type" : "text", "fields" : &#123; "keyword" : &#123; "type" : "keyword", "ignore_above" : 256 &#125; &#125; &#125;, "age" : &#123; "type" : "long" &#125;, "first_name" : &#123; "type" : "text", "fields" : &#123; "keyword" : &#123; "type" : "keyword", "ignore_above" : 256 &#125; &#125; &#125;, "interests" : &#123; "type" : "text", "fields" : &#123; "keyword" : &#123; "type" : "keyword", "ignore_above" : 256 &#125; &#125;, "fielddata" : true &#125;, "last_name" : &#123; "type" : "text", "fields" : &#123; "keyword" : &#123; "type" : "keyword", "ignore_above" : 256 &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125; (1.x)12345678910GET /_search&#123; "_source": &#123; "includes": [ "obj1.*", "obj2.*" ], "excludes": [ "*.description" ] &#125;, "query" : &#123; "term" : &#123; "user" : "kimchy" &#125; &#125;&#125; （2) 领域特定语言(DSL)搜索 领域特定语言(DSL)，使用 JSON 构造了一个请求。 (2.1) 计算集群中文档的数量12345678910111213141516$ curl -XGET 'http://localhost:9200/_count?pretty' -H "Content-Type: application/json" -d '&gt; &#123;&gt; "query": &#123;&gt; "match_all": &#123;&#125;&gt; &#125;&gt; &#125;&gt; '&#123; "count" : 3, "_shards" : &#123; "total" : 5, "successful" : 5, "skipped" : 0, "failed" : 0 &#125;&#125; 可以看到结果是3 (2.2) 使用查询表达式搜索123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657$ curl -X GET "localhost:9200/megacorp/employee/_search?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "query" : &#123;&gt; "match" : &#123;&gt; "last_name" : "Smith"&gt; &#125;&gt; &#125;&gt; &#125;&gt; '&#123; "took" : 7, "timed_out" : false, "_shards" : &#123; "total" : 5, "successful" : 5, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 2, "max_score" : 0.2876821, "hits" : [ &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "2", "_score" : 0.2876821, "_source" : &#123; "first_name" : "Jane", "last_name" : "Smith", "age" : 32, "about" : "I like to collect rock albums", "interests" : [ "music" ] &#125; &#125;, &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "1", "_score" : 0.2876821, "_source" : &#123; "first_name" : "John", "last_name" : "Smith", "age" : 25, "about" : "I love to go rock climbing", "interests" : [ "sports", "music" ] &#125; &#125; ] &#125;&#125;$ (2.3) 多条件查询 搜索姓氏为 Smith 的员工，但这次我们只需要年龄大于 30 的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950$ curl -X GET "localhost:9200/megacorp/employee/_search?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "query" : &#123;&gt; "bool": &#123;&gt; "must": &#123;&gt; "match" : &#123;&gt; "last_name" : "smith"&gt; &#125;&gt; &#125;,&gt; "filter": &#123;&gt; "range" : &#123;&gt; "age" : &#123; "gt" : 30 &#125;&gt; &#125;&gt; &#125;&gt; &#125;&gt; &#125;&gt; &#125;&gt; '&#123; "took" : 32, "timed_out" : false, "_shards" : &#123; "total" : 5, "successful" : 5, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 1, "max_score" : 0.2876821, "hits" : [ &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "2", "_score" : 0.2876821, "_source" : &#123; "first_name" : "Jane", "last_name" : "Smith", "age" : 32, "about" : "I like to collect rock albums", "interests" : [ "music" ] &#125; &#125; ] &#125;&#125;$ range 过滤器，它能找到年龄大于 30 的文档，其中 gt 表示_大于_(great than) (2.4) 全文搜索 搜索下所有喜欢攀岩（rock climbing）的员工 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556$ curl -X GET "localhost:9200/megacorp/employee/_search?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "query" : &#123;&gt; "match" : &#123;&gt; "about" : "rock climbing"&gt; &#125;&gt; &#125;&gt; &#125;&gt; '&#123; "took" : 13, "timed_out" : false, "_shards" : &#123; "total" : 5, "successful" : 5, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 2, "max_score" : 0.5753642, "hits" : [ &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "1", "_score" : 0.5753642, "_source" : &#123; "first_name" : "John", "last_name" : "Smith", "age" : 25, "about" : "I love to go rock climbing", "interests" : [ "sports", "music" ] &#125; &#125;, &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "2", "_score" : 0.2876821, "_source" : &#123; "first_name" : "Jane", "last_name" : "Smith", "age" : 32, "about" : "I like to collect rock albums", "interests" : [ "music" ] &#125; &#125; ] &#125;&#125; (2.5) 短语搜索 精确匹配一系列单词或者_短语_ 1234567891011121314151617181920212223242526272829303132333435363738394041$ curl -X GET "localhost:9200/megacorp/employee/_search?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "query" : &#123;&gt; "match_phrase" : &#123;&gt; "about" : "rock climbing"&gt; &#125;&gt; &#125;&gt; &#125;&gt; '&#123; "took" : 16, "timed_out" : false, "_shards" : &#123; "total" : 5, "successful" : 5, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 1, "max_score" : 0.5753642, "hits" : [ &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "1", "_score" : 0.5753642, "_source" : &#123; "first_name" : "John", "last_name" : "Smith", "age" : 25, "about" : "I love to go rock climbing", "interests" : [ "sports", "music" ] &#125; &#125; ] &#125;&#125; (2.6) 高亮搜索123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051$ curl -X GET "localhost:9200/megacorp/employee/_search?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "query" : &#123;&gt; "match_phrase" : &#123;&gt; "about" : "rock climbing"&gt; &#125;&gt; &#125;,&gt; "highlight": &#123;&gt; "fields" : &#123;&gt; "about" : &#123;&#125;&gt; &#125;&gt; &#125;&gt; &#125;&gt; '&#123; "took" : 305, "timed_out" : false, "_shards" : &#123; "total" : 5, "successful" : 5, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 1, "max_score" : 0.5753642, "hits" : [ &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "1", "_score" : 0.5753642, "_source" : &#123; "first_name" : "John", "last_name" : "Smith", "age" : 25, "about" : "I love to go rock climbing", "interests" : [ "sports", "music" ] &#125;, "highlight" : &#123; "about" : [ "I love to go &lt;em&gt;rock&lt;/em&gt; &lt;em&gt;climbing&lt;/em&gt;" ] &#125; &#125; ] &#125;&#125; (2.7) 聚合 聚合与 SQL 中的 GROUP BY 类似但更强大。 挖掘出员工中最受欢迎的兴趣爱好 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091$ curl -X GET "localhost:9200/megacorp/employee/_search?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "aggs": &#123;&gt; "all_interests": &#123;&gt; "terms": &#123; "field": "interests" &#125;&gt; &#125;&gt; &#125;&gt; &#125;&gt; '&#123; "took" : 139, "timed_out" : false, "_shards" : &#123; "total" : 5, "successful" : 5, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 3, "max_score" : 1.0, "hits" : [ &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "2", "_score" : 1.0, "_source" : &#123; "first_name" : "Jane", "last_name" : "Smith", "age" : 32, "about" : "I like to collect rock albums", "interests" : [ "music" ] &#125; &#125;, &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "1", "_score" : 1.0, "_source" : &#123; "first_name" : "John", "last_name" : "Smith", "age" : 25, "about" : "I love to go rock climbing", "interests" : [ "sports", "music" ] &#125; &#125;, &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "3", "_score" : 1.0, "_source" : &#123; "first_name" : "Douglas", "last_name" : "Fir", "age" : 35, "about" : "I like to build cabinets", "interests" : [ "forestry" ] &#125; &#125; ] &#125;, "aggregations" : &#123; "all_interests" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ &#123; "key" : "music", "doc_count" : 2 &#125;, &#123; "key" : "forestry", "doc_count" : 1 &#125;, &#123; "key" : "sports", "doc_count" : 1 &#125; ] &#125; &#125;&#125; 可以看到，两位员工对音乐感兴趣，一位对林业感兴趣，一位对运动感兴趣。这些聚合的结果数据并非预先统计，而是根据匹配当前查询的文档即时生成的。 (2.8) 条件聚合 想知道叫 Smith 的员工中最受欢迎的兴趣爱好 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879$ curl -X GET "localhost:9200/megacorp/employee/_search?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "query": &#123;&gt; "match": &#123;&gt; "last_name": "smith"&gt; &#125;&gt; &#125;,&gt; "aggs": &#123;&gt; "all_interests": &#123;&gt; "terms": &#123;&gt; "field": "interests"&gt; &#125;&gt; &#125;&gt; &#125;&gt; &#125;&gt; '&#123; "took" : 11, "timed_out" : false, "_shards" : &#123; "total" : 5, "successful" : 5, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 2, "max_score" : 0.2876821, "hits" : [ &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "2", "_score" : 0.2876821, "_source" : &#123; "first_name" : "Jane", "last_name" : "Smith", "age" : 32, "about" : "I like to collect rock albums", "interests" : [ "music" ] &#125; &#125;, &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "1", "_score" : 0.2876821, "_source" : &#123; "first_name" : "John", "last_name" : "Smith", "age" : 25, "about" : "I love to go rock climbing", "interests" : [ "sports", "music" ] &#125; &#125; ] &#125;, "aggregations" : &#123; "all_interests" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ &#123; "key" : "music", "doc_count" : 2 &#125;, &#123; "key" : "sports", "doc_count" : 1 &#125; ] &#125; &#125;&#125; (2.9) 聚合条件汇总 查询特定兴趣爱好员工的平均年龄 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105$ curl -X GET "localhost:9200/megacorp/employee/_search?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "aggs" : &#123;&gt; "all_interests" : &#123;&gt; "terms" : &#123; "field" : "interests" &#125;,&gt; "aggs" : &#123;&gt; "avg_age" : &#123;&gt; "avg" : &#123; "field" : "age" &#125;&gt; &#125;&gt; &#125;&gt; &#125;&gt; &#125;&gt; &#125;&gt; '&#123; "took" : 36, "timed_out" : false, "_shards" : &#123; "total" : 5, "successful" : 5, "skipped" : 0, "failed" : 0 &#125;, "hits" : &#123; "total" : 3, "max_score" : 1.0, "hits" : [ &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "2", "_score" : 1.0, "_source" : &#123; "first_name" : "Jane", "last_name" : "Smith", "age" : 32, "about" : "I like to collect rock albums", "interests" : [ "music" ] &#125; &#125;, &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "1", "_score" : 1.0, "_source" : &#123; "first_name" : "John", "last_name" : "Smith", "age" : 25, "about" : "I love to go rock climbing", "interests" : [ "sports", "music" ] &#125; &#125;, &#123; "_index" : "megacorp", "_type" : "employee", "_id" : "3", "_score" : 1.0, "_source" : &#123; "first_name" : "Douglas", "last_name" : "Fir", "age" : 35, "about" : "I like to build cabinets", "interests" : [ "forestry" ] &#125; &#125; ] &#125;, "aggregations" : &#123; "all_interests" : &#123; "doc_count_error_upper_bound" : 0, "sum_other_doc_count" : 0, "buckets" : [ &#123; "key" : "music", "doc_count" : 2, "avg_age" : &#123; "value" : 28.5 &#125; &#125;, &#123; "key" : "forestry", "doc_count" : 1, "avg_age" : &#123; "value" : 35.0 &#125; &#125;, &#123; "key" : "sports", "doc_count" : 1, "avg_age" : &#123; "value" : 25.0 &#125; &#125; ] &#125; &#125;&#125; (3) ES修改配置语句(3.1) es 5.x 开启全文检索语句 5.x后对排序，聚合这些操作用单独的数据结构(fielddata)缓存到内存里了，需要单独开启 1234567891011$ curl -X PUT "localhost:9200/megacorp/_mapping/employee/" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "properties": &#123;&gt; "interests": &#123;&gt; "type": "text",&gt; "fielddata": true&gt; &#125;&gt; &#125;&gt; &#125;&gt; '&#123;"acknowledged":true&#125; 推荐使用keyword聚合 (3.2) 创建索引并设置分片数和副本数12345678910111213$ curl -X PUT &quot;localhost:9200/blogs?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&gt; &#123;&gt; &quot;settings&quot; : &#123;&gt; &quot;number_of_shards&quot; : 3,&gt; &quot;number_of_replicas&quot; : 1&gt; &#125;&gt; &#125;&gt; &apos;&#123; &quot;acknowledged&quot; : true, &quot;shards_acknowledged&quot; : true, &quot;index&quot; : &quot;blogs&quot;&#125; (3.3) 设置副本数12345678$ curl -X PUT "localhost:9200/blogs/_settings?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "number_of_replicas" : 2&gt; &#125;&gt; '&#123; "acknowledged" : true&#125; (3.4) get set up123456789101112131415161718192021222324252627282930313233343536373839$ curl -X GET "localhost:9200/_nodes/transport?error_trace=true&amp;pretty=true"&#123; "_nodes" : &#123; "total" : 1, "successful" : 1, "failed" : 0 &#125;, "cluster_name" : "elasticsearch_test", "nodes" : &#123; "urmXtplyRmyt_LKCTC6_3w" : &#123; "name" : "elasticsearch_001_data", "transport_address" : "127.0.0.1:9300", "host" : "127.0.0.1", "ip" : "127.0.0.1", "version" : "6.6.2", "build_flavor" : "default", "build_type" : "tar", "build_hash" : "3bd3e59", "roles" : [ "master", "data", "ingest" ], "attributes" : &#123; "ml.machine_memory" : "1927528448", "xpack.installed" : "true", "ml.max_open_jobs" : "20", "ml.enabled" : "true" &#125;, "transport" : &#123; "bound_address" : [ "127.0.0.1:9300" ], "publish_address" : "127.0.0.1:9300", "profiles" : &#123; &#125; &#125; &#125; &#125;&#125; References[1] 索引员工文档[2] 检索文档[3] 轻量搜索[4] 使用查询表达式搜索[5] 更复杂的搜索[6] 全文搜索[7] 短语搜索[8] 高亮搜索[9] 分析]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式锁]]></title>
    <url>%2F2019%2F12%2F29%2Fdistributed-lock%2F</url>
    <content type="text"><![CDATA[分享下一个基于redis来实现的分布式锁。 加锁过程分析 我第一次读代码的时候，有这么几个疑惑： Q1：为什么不使用 SET key value [expiration EX seconds|PX milliseconds] [NX|XX] 这个指令来实现key的自动过期呢，反而放到应用代码判断key是否过期？ A1：我们的分布式锁开发的时候SET命令还不支持NX、PX，所以才想出这种办法来实现key过期，NX、PX在2.6.12以后开始支持； Q2：已经判断了当前key对应的时间戳已经过期了，为什么还要使用getset再获取一次呢，直接使用set指令覆盖不可以吗？ A2：这里其实牵扯到并发的一些事情，如果直接使用set，那有可能多个客户端会同时获取到锁，如果使用getset然后判断旧值是否过期就不会有这个问题，设想一下如下场景： C1加锁成功，不巧的是，这时C1意外的奔溃了，自然就不会释放锁；C2，C3尝试加锁，这时key已存在，所以C2，C3去判断key是否已过期，这里假设key已经过期了，所以C2，C3使用set指令去设置值，那两个都会加锁成功，这就闯大祸了；如果使用getset指令，然后判断下返回值是否过期就可以避免这种问题，假如C2跑的快，那C3判断返回的时间戳已经过期，自然就加锁失败； Q1：为什么释放锁时还需要判断key是否过期呢，直接del不是性能更高吗？ A1：考虑这样一种场景： C1获取锁成功，开始执行自己的操作，不幸的是C1这时被阻塞了；C2这时来获取锁，由于C1被阻塞了很长时间，所以key对应的value已经过期了，这时C2通过getset加锁成功；C1尘封了太久终于被再次唤醒，对于释放锁这件事它可是认真的，伴随着一波del操作，悲剧即将发生；C3来获取锁，好家伙，居然一下就成功了，接着就是一波操作猛如虎，接着就是一堆的客诉过来了；为什么会这样呢？回想C1被唤醒以后的事情，居然敢直接del，C2活都没干完呢，锁就被C1给释放了，这时C3来直接就加锁成功，所以为了安全起见C3释放锁时得分成两步：1.判断value是否已经过期 2.如果已过期直接忽略，如果没过期就执行del。这样就真的安全了吗？安全了吗？安全了吗？假如第一步和第二步之间相隔了很久是不是也会出现锁被其他人释放的问题呢？是吧？是的！有没有别的解决办法呢？听说借助lua就可以解决这个问题了，感兴趣的直接给你传送过去可好。 释放锁过程分析 Q1：Redis锁的过期时间小于业务的执行时间该如何续期？ A1：这个暂时没有实现，据说有一个叫Redisson的家伙解决了这个问题，我们也有部分业务在使用，未来有可能会切换到Redisson。 Q2：怎么实现的高可用？ A2：我们采用Failover机制，初始化redis锁的时候会维护一个redis连接池，加锁或者释放锁的时候采用多写的方式来保障一致性，如果某个节点不可用的时候会自动切换到其他节点，但是这种机制可能会导致多个客户端同时获取到锁的情况，考虑这种情况： C1去redis1加锁，加锁成功后会写到redis2，redis3；C2也去redis1加锁，但是此时C2到redis1的网络出现问题，这时C2切换到redis2去加锁，由于第一步中的redis多写并不是原子的，所有就有可能导致C2也获取锁成功；针对这种情况，目前有些业务方是通过数据库唯一索引的方式来规避的，未来会修复这个bug，具体方案目前还没有。 java实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133package cn.wkq.java.utils;import cn.wkq.util.JedisPoolUtil;import org.apache.commons.lang3.StringUtils;import redis.clients.jedis.Jedis;/** * 简单的基于Redis分布式锁 * * &lt;pre&gt; * 参考 https://www.cnblogs.com/chopper-poet/p/10802242.html * https://github.com/wyzssw/DistributedLock/blob/master/src/main/java/com/wyzssw/distributedLock/DistributedLock.java * &lt;/pre&gt; * * @author: weikeqin.cn@gmail.com * @date: 2019-12-29 21:27 **/public class RedisLock &#123; /** * redis 无用户名密码 默认配置 测试使用 */ public Jedis jedis = JedisPoolUtil.getJedis(); /** * 加锁 * * &lt;pre&gt; * 1. 尝试加锁 setNx，key是自己设置，value设置成 当前时间+过期时间 System.currentTimeMillis() + lockTimeOut * 2. 判断key是否存在，不存在，获取锁成功，返回 过期时间 ( expireTime = System.currentTimeMillis() + lockTimeOut )，结束。 * 3. key已存在，获取锁失败 * 4. 获取key对应的过期时间。 此时获取到的值是另一个设置的时间戳。 * 5. 拿当前时间戳 和 上一步取到的时间戳对比，判断key是否过期。 * 6. 过期，使用 getset 再次获取锁。 redis.getset(key, System.currentTimeMillis() + lockTimeOut) * 7. 用当前时间戳 对比 判断 返回的value是否过期。 * 8. 如果过期，获取锁成功。 * 9. 如果不过期，获取锁失败。 * 10. 拿当前时间戳判断key是否过期，不过期，获取锁失败。 * &lt;/pre&gt; * * @param k * @param lockTimeOut * @return */ public long tryLock(String k, long lockTimeOut) &#123; long expireTime = System.currentTimeMillis() + lockTimeOut; // SETNX KEY_NAME VALUE /** 1. 尝试加锁 setNx */ Long resNum = jedis.setnx(k, String.valueOf(expireTime)); /** 2. 判断key是否存在，不存在，获取锁成功 */ if (resNum == 1) &#123; // 获取锁成功 return expireTime; &#125; /** 3. key已存在，获取锁失败 */ /** 4. 获取key对应的过期时间。 */ String curLockTimeStr = jedis.get(k); /** 5. 拿当前时间戳 和 上一步取到的时间戳对比，判断key是否过期。 */ if (StringUtils.isBlank(curLockTimeStr) || System.currentTimeMillis() &gt; Long.valueOf(curLockTimeStr)) &#123; expireTime = System.currentTimeMillis() + lockTimeOut; /** 6. 过期，使用 getset 再次获取锁。 */ curLockTimeStr = jedis.getSet(k, String.valueOf(expireTime)); /** 7. 用当前时间戳 对比 判断 返回的value是否过期 */ if (StringUtils.isBlank(curLockTimeStr) || System.currentTimeMillis() &gt; Long.valueOf(curLockTimeStr)) &#123; /** 8. 如果过期，获取锁成功。 */ return expireTime; &#125; else &#123; /** 9. 如果不过期，获取锁失败。 */ return 0; &#125; &#125; /** 10. 拿当前时间戳判断key是否过期，不过期，获取锁失败。(其实是第5步判断的结果) */ return 0; &#125; /** * 得到锁返回设置的超时时间，得不到锁等待重试 * * @param key * @return * @throws InterruptedException */ public long lock(String key, int lockTimeOut, int perSleep) throws InterruptedException &#123; long sleep = (perSleep == 0 ? lockTimeOut / 10 : perSleep); long starttime = System.currentTimeMillis(); //得到锁后设置的过期时间，未得到锁返回0 long expireTime = 0; for (; ; ) &#123; long getexpireTime = tryLock(key, lockTimeOut); if (getexpireTime &gt; System.currentTimeMillis()) &#123; return getexpireTime; &#125; // 获取锁失败，休眠一会 Thread.sleep(sleep); if (lockTimeOut &gt; 0 &amp;&amp; ((System.currentTimeMillis() - starttime) &gt;= lockTimeOut)) &#123; expireTime = 0; return expireTime; &#125; &#125; &#125; /** * 先判断自己运行时间是否超过了锁设置时间，是则不用解锁 * * @param key * @param expireTime */ public void unlock(String key, long expireTime) &#123; if (System.currentTimeMillis() - expireTime &gt; 0) &#123; return; &#125; String curLockTimeStr = jedis.get(key); if (StringUtils.isNotBlank(curLockTimeStr) &amp;&amp; Long.valueOf(curLockTimeStr) &gt; System.currentTimeMillis()) &#123; jedis.del(key); &#125; &#125;&#125; References[1] 我司使用了六年的分布式锁[2] redis分布式锁-github/wyzssw/DistributedLock[3] Redis教程[4] 再有人问你分布式锁，这篇文章扔给他[5] 分布式锁看这篇就够了 [6] 分布式锁的实现与应用场景对比[7] 分布式锁的场景与实现]]></content>
      <categories>
        <category>distributed</category>
      </categories>
      <tags>
        <tag>lock</tag>
        <tag>distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库开发规范]]></title>
    <url>%2F2019%2F11%2F20%2Fmysql-norm%2F</url>
    <content type="text"><![CDATA[一、基础规范 （1）必须使用InnoDB存储引擎 解读：支持事务、行级锁、并发性能更好、CPU及内存缓存页优化使得资源利用率更高 （2）必须使用UTF8MB4字符集 解读：万国码，无需转码，无乱码风险，节省空间 （3）数据表、数据字段必须加入中文注释 解读：N年后谁知道这个r1,r2,r3字段是干嘛的 （4）禁止使用存储过程、视图、触发器、Event 解读：高并发大数据的互联网业务，架构设计思路是“解放数据库CPU，将计算转移到服务层”，并发量大的情况下，这些功能很可能将数据库拖死，业务逻辑放到服务层具备更好的扩展性，能够轻易实现“增机器就加性能”。数据库擅长存储与索引，CPU计算还是上移吧 （5）禁止存储大文件或者大照片 解读：为何要让数据库做它不擅长的事情？大文件和照片存储在文件系统，数据库里存URI多好 二、命名规范 （6）只允许使用内网域名，而不是ip连接数据库 （7）线上环境、开发环境、测试环境数据库内网域名遵循命名规范 业务名称：xxx 线上环境：pro 开发环境：dev 测试环境：test 从库在名称后加-s标识，备库在名称后加-ss标识 线上从库： （8）库名、表名、字段名：小写，下划线风格，不超过32个字符，必须见名知意，禁止拼音英文混用 （9）表名t_xxx，非唯一索引名idx_xxx，唯一索引名uniq_xxx 三、表设计规范 （10）单实例表数目必须小于500 （11）单表列数目必须小于30 （12）表必须有主键，例如自增主键 解读： a）主键递增，数据行写入可以提高插入性能，可以避免page分裂，减少表碎片提升空间和内存的使用 b）主键要选择较短的数据类型， Innodb引擎普通索引都会保存主键的值，较短的数据类型可以有效的减少索引的磁盘空间，提高索引的缓存效率 c） 无主键的表删除，在row模式的主从架构，会导致备库夯住 （13）禁止使用外键，如果有外键完整性约束，需要应用程序控制 解读：外键会导致表与表之间耦合，update与delete操作都会涉及相关联的表，十分影响sql 的性能，甚至会造成死锁。高并发情况下容易造成数据库性能，大数据高并发业务场景数据库使用以性能优先 四、字段设计规范 （14）必须把字段定义为NOT NULL并且提供默认值 解读： a）null的列使索引/索引统计/值比较都更加复杂，对MySQL来说更难优化 b）null 这种类型MySQL内部需要进行特殊处理，增加数据库处理记录的复杂性；同等条件下，表中有较多空字段的时候，数据库的处理性能会降低很多 c）null值需要更多的存储空，无论是表还是索引中每行中的null的列都需要额外的空间来标识 d）对null 的处理时候，只能采用is null或is not null，而不能采用=、in、&lt;、&lt;&gt;、!=、not in这些操作符号。如：where name!=’shenjian’，如果存在name为null值的记录，查询结果就不会包含name为null值的记录 （15）禁止使用TEXT、BLOB类型 解读：会浪费更多的磁盘和内存空间，非必要的大量的大字段查询会淘汰掉热数据，导致内存命中率急剧降低，影响数据库性能 （16）禁止使用小数存储货币 解读：使用整数吧，小数容易导致钱对不上 （17）必须使用varchar(20)存储手机号 解读： a）涉及到区号或者国家代号，可能出现+-() b）手机号会去做数学运算么？ c）varchar可以支持模糊查询，例如：like“138%” （18）禁止使用ENUM，可使用TINYINT代替 解读： a）增加新的ENUM值要做DDL操作 b）ENUM的内部实际存储就是整数，你以为自己定义的是字符串？ 五、索引设计规范 （19）单表索引建议控制在5个以内 （20）单索引字段数不允许超过5个 解读：字段超过5个时，实际已经起不到有效过滤数据的作用了 （21）禁止在更新十分频繁、区分度不高的属性上建立索引 解读： a）更新会变更B+树，更新频繁的字段建立索引会大大降低数据库性能 b）“性别”这种区分度不大的属性，建立索引是没有什么意义的，不能有效过滤数据，性能与全表扫描类似 （22）建立组合索引，必须把区分度高的字段放在前面 解读：能够更加有效的过滤数据 六、SQL使用规范 （23）禁止使用SELECT *，只获取必要的字段，需要显示说明列属性 解读： a）读取不需要的列会增加CPU、IO、NET消耗 b）不能有效的利用覆盖索引 c）使用SELECT *容易在增加或者删除字段后出现程序BUG （24）禁止使用INSERT INTO t_xxx VALUES(xxx)，必须显示指定插入的列属性 解读：容易在增加或者删除字段后出现程序BUG （25）禁止使用属性隐式转换 解读：SELECT uid FROM t_user WHERE phone=13800000000 会导致全表扫描，而不能命中phone索引，猜猜为什么？（这个线上问题不止出现过一次） （26）禁止在WHERE条件的属性上使用函数或者表达式 解读：SELECT uid FROM t_user WHERE from_unixtime(day)&gt;=’2017-01-15’ 会导致全表扫描 正确的写法是：SELECT uid FROM t_user WHERE day&gt;= unix_timestamp(‘2017-01-15 00:00:00’) （27）禁止负向查询，以及%开头的模糊查询 解读： a）负向查询条件：NOT、!=、&lt;&gt;、!&lt;、!&gt;、NOT IN、NOT LIKE等，会导致全表扫描 b）%开头的模糊查询，会导致全表扫描 （28）禁止大表使用JOIN查询，禁止大表使用子查询 解读：会产生临时表，消耗较多内存与CPU，极大影响数据库性能 （29）禁止使用OR条件，必须改为IN查询 解读：旧版本Mysql的OR查询是不能命中索引的，即使能命中索引，为何要让数据库耗费更多的CPU帮助实施查询优化呢？ （30）应用程序必须捕获SQL异常，并有相应处理 （31）同表的增删字段、索引合并一条DDL语句执行，提高执行效率，减少与数据库的交互。 总结：大数据量高并发的互联网业务，极大影响数据库性能的都不让用，不让用哟。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[linux-memory-notes]]></title>
    <url>%2F2019%2F11%2F18%2Flinux-memory-notes%2F</url>
    <content type="text"><![CDATA[用户空间内存，从低到高分别是五种不同的内存段。 只读段，包括代码和常量等。 数据段，包括全局变量等。 堆，包括动态分配的内存，从低地址开始向上增长。 文件映射段，包括动态库、共享内存等，从高地址开始向下增长。 栈，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 8 MB。在这五个内存段中，堆和文件映射段的内存是动态分配的。 调小数值 linux进程不被杀死 echo -16 &gt; /proc/$(pidof sshd)/oom_adj 注意不同版本的free输出可能会有所不同12345678910$ free total used free shared buff/cache availableMem: 8169348 263524 6875352 668 1030472 7611064Swap: 0 0 0[wkq.stb@s_wkq_b ~]$ free total used free shared buffers cachedMem: 16334056 16135076 198980 6948 130160 228828-/+ buffers/cache: 15776088 557968Swap: 8388604 1042220 7346384 第一列，total 是总内存大小； 第二列，used 是已使用内存的大小，包含了共享内存； 第三列，free 是未使用内存的大小； 第四列，shared 是共享内存的大小； 第五列，buff/cache 是缓存和缓冲区的大小； 最后一列，available 是新进程可用内存的大小。 123456789101112131415161718192021222324[wkq.stb@s_wkq_b ~]$ toptop - 14:49:12 up 151 days, 18:14, 2 users, load average: 0.00, 0.00, 0.00Tasks: 146 total, 1 running, 145 sleeping, 0 stopped, 0 zombieCpu0 : 0.3%us, 0.3%sy, 0.0%ni, 99.3%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu1 : 0.0%us, 0.3%sy, 0.0%ni, 99.7%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu2 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stCpu3 : 0.0%us, 0.0%sy, 0.0%ni,100.0%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stMem: 16334056k total, 16134976k used, 199080k free, 130144k buffersSwap: 8388604k total, 1042224k used, 7346380k free, 228824k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1802 search 20 0 17.9g 14g 5276 S 0.7 92.6 826:18.86 java 3914 wkq.stb 20 0 15024 1356 1004 R 0.3 0.0 0:00.02 top14363 root 20 0 359m 43m 4424 S 0.3 0.3 92:54.64 jdog-monitor.1. 1 root 20 0 19232 572 392 S 0.0 0.0 0:10.99 init 2 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kthreadd 3 root RT 0 0 0 0 S 0.0 0.0 4:08.02 migration/0 4 root 20 0 0 0 0 S 0.0 0.0 0:52.91 ksoftirqd/0 5 root RT 0 0 0 0 S 0.0 0.0 0:00.00 stopper/0 6 root RT 0 0 0 0 S 0.0 0.0 0:12.19 watchdog/0 7 root RT 0 0 0 0 S 0.0 0.0 3:15.61 migration/1 8 root RT 0 0 0 0 S 0.0 0.0 0:00.00 stopper/1 9 root 20 0 0 0 0 S 0.0 0.0 0:39.68 ksoftirqd/1 10 root RT 0 0 0 0 S 0.0 0.0 0:10.27 watchdog/1 12345678910111213141516171819202122232425262728293031# 按下M切换到内存排序[wkq.stb@s_wkq_b ~]$ toptop - 14:49:55 up 151 days, 18:15, 2 users, load average: 0.00, 0.00, 0.00Tasks: 146 total, 1 running, 145 sleeping, 0 stopped, 0 zombieCpu(s): 0.1%us, 0.2%sy, 0.0%ni, 99.8%id, 0.0%wa, 0.0%hi, 0.0%si, 0.0%stMem: 16334056k total, 16134976k used, 199080k free, 130144k buffersSwap: 8388604k total, 1042224k used, 7346380k free, 228824k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1802 search 20 0 17.9g 14g 5276 S 0.3 92.6 826:19.00 java 5945 jd.dev 20 0 7653m 244m 5092 S 0.3 1.5 275:00.32 java 1515 root 20 0 825m 81m 4124 S 0.0 0.5 718:10.97 jcloudhids14363 root 20 0 359m 43m 4424 S 0.3 0.3 92:54.68 jdog-monitor.1.13321 root 20 0 748m 20m 5996 S 0.0 0.1 128:42.30 jdog-kunlunmirr 3757 root 20 0 97.7m 3936 2976 S 0.0 0.0 0:00.00 sshd30572 root 20 0 97.7m 3932 2976 S 0.0 0.0 0:00.01 sshd 1164 root 20 0 249m 3688 568 S 0.0 0.0 0:09.71 rsyslogd30637 wkq.stb 20 0 59980 3480 2536 S 0.0 0.0 0:05.60 ssh 1448 root 20 0 186m 3004 968 S 0.0 0.0 2:19.88 python30574 wkq.stb 20 0 97.7m 2256 1280 S 0.0 0.0 0:08.71 sshd 3759 wkq.stb 20 0 97.7m 2252 1272 S 0.0 0.0 0:00.00 sshd 3760 wkq.stb 20 0 105m 1968 1504 S 0.0 0.0 0:00.01 bash30575 wkq.stb 20 0 105m 1960 1508 S 0.0 0.0 0:00.00 bash 1481 root 20 0 175m 1472 1004 S 0.0 0.0 92:43.68 AgentMonitor 3958 wkq.stb 20 0 15024 1360 1004 R 0.7 0.0 0:00.06 top 862 root 20 0 22356 932 644 S 0.0 0.0 24:56.74 qemu-ga 1284 nscd 20 0 549m 920 576 S 0.0 0.0 10:17.28 nscd 1091 root 20 0 9120 700 552 S 0.0 0.0 0:00.25 dhclient 1574 root 20 0 158m 612 476 S 0.0 0.0 1:49.17 jcloudhidsupdat 1440 root 20 0 114m 608 388 S 0.0 0.0 0:39.52 crond 1 root 20 0 19232 572 392 S 0.0 0.0 0:10.99 init top 输出界面的顶端，也显示了系统整体的内存使用情况，这些数据跟 free 类似。 接着看下面的内容，跟内存相关的几列数据，比如 VIRT、RES、SHR 以及 %MEM 等。这些数据，包含了进程最重要的几个内存使用情况，我们挨个来看。 VIRT 是进程虚拟内存的大小，只要是进程申请过的内存，即便还没有真正分配物理内存，也会计算在内。 RES 是常驻内存的大小，也就是进程实际使用的物理内存大小，但不包括 Swap 和共享内存。 SHR 是共享内存的大小，比如与其他进程共同使用的共享内存、加载的动态链接库以及程序的代码段等。 %MEM 是进程使用物理内存占系统总内存的百分比。 /proc/meminfo 12# 清理文件页、目录项、Inodes等各种缓存$ echo 3 &gt; /proc/sys/vm/drop_caches 123456# 每隔1秒输出1组数据$ vmstat 1procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----r b swpd free buff cache si so bi bo in cs us sy id wa st0 0 0 7743608 1112 92168 0 0 0 0 52 152 0 1 100 0 00 0 0 7743608 1112 92168 0 0 0 0 36 92 0 0 100 0 0 12345678910[jd.stb@s_legend_b ~]$ vmstat 1procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 1042220 199476 130192 228868 0 0 0 5 1 0 0 0 100 0 0 0 0 1042220 199452 130192 228888 0 0 0 0 370 572 0 0 100 0 0 0 0 1042220 199452 130192 228888 0 0 0 0 311 569 0 0 100 0 0 0 0 1042220 199452 130192 228888 0 0 0 0 360 591 0 0 100 0 0 0 0 1042220 199256 130192 228888 0 0 0 4 658 1096 0 0 99 0 0 0 0 1042220 199188 130200 228888 0 0 0 116 388 635 0 0 100 0 0 0 0 1042220 199188 130200 228888 0 0 0 0 351 582 0 0 100 0 0 $ dd if=/dev/urandom of=/tmp/file bs=1M count=500 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172[jd.stb@s_legend_b ~]$ vmstat 1procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 1042220 196704 130204 229204 0 0 0 5 1 0 0 0 100 0 0 0 0 1042220 196548 130204 229204 0 0 0 60 385 611 0 0 100 0 0 0 0 1042220 196604 130204 229204 0 0 0 4 308 559 0 0 100 0 0 0 0 1042220 196604 130204 229204 0 0 0 0 335 571 0 0 100 0 0 0 0 1042220 196612 130204 229204 0 0 0 4 315 580 0 0 100 0 0 0 0 1042220 196612 130204 229204 0 0 0 0 338 573 0 0 100 0 0 0 0 1042220 196628 130204 229204 0 0 0 0 342 592 0 0 100 0 0 0 0 1042220 196636 130204 229204 0 0 0 40 361 609 0 0 100 0 0 0 0 1042220 196636 130204 229204 0 0 0 0 364 670 0 0 100 0 0 0 0 1042220 196636 130204 229204 0 0 0 0 352 584 0 0 100 0 0 0 0 1042220 196636 130204 229204 0 0 0 0 321 580 0 0 100 0 0 0 0 1042220 196636 130204 229204 0 0 0 0 351 592 0 0 100 0 0 0 0 1042220 196636 130204 229204 0 0 0 72 627 1205 0 0 100 0 0 0 0 1042220 196636 130204 229204 0 0 0 0 504 908 0 0 100 0 0 0 0 1042220 196636 130204 229204 0 0 0 0 563 958 0 0 100 0 0 0 0 1042220 196636 130204 229204 0 0 0 0 492 855 0 0 100 0 0 0 0 1042220 196636 130204 229204 0 0 0 0 684 1392 0 1 100 0 0 0 0 1042220 196636 130204 229204 0 0 0 100 343 570 0 0 100 0 0 0 0 1042220 196636 130204 229204 0 0 0 0 295 560 0 0 100 0 0 1 0 1042220 196636 130204 229204 0 0 52 0 566 634 0 4 96 0 0 1 0 1042220 193884 130204 231304 0 0 0 52 1304 558 0 25 75 0 0 1 0 1042220 190908 130204 234376 0 0 0 0 1324 531 0 25 75 0 0 1 0 1042220 187808 130204 237300 0 0 0 0 1278 518 0 25 75 0 0 1 0 1042220 185824 130204 239496 0 0 0 0 1368 567 0 25 75 0 0 1 0 1042220 182724 130204 242420 0 0 0 8 1322 577 0 25 75 0 0 1 0 1042220 180616 130212 244616 0 0 0 116 1558 701 0 25 75 0 0 1 0 1042220 177640 130212 247540 0 0 0 4 1355 576 0 25 75 0 0 1 0 1042220 174540 130212 250612 0 0 0 8 1425 584 0 25 75 0 0 1 0 1042220 172432 130212 252808 0 0 0 0 1298 540 0 25 75 0 0 1 0 1042220 169456 130212 255732 0 0 0 0 1338 560 0 25 75 0 0 1 0 1042220 167348 130212 257928 0 0 0 56 1348 589 0 25 75 0 0 1 0 1042220 164248 130212 260852 0 0 0 4 1388 590 0 25 75 0 0 1 0 1042220 160756 130212 263924 0 0 0 0 1662 666 0 27 72 0 1 1 0 1042220 158652 130212 266120 0 0 0 0 1345 535 0 25 75 0 0procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 1 0 1042220 155552 130212 269044 0 0 0 0 1288 532 0 25 75 0 0 1 0 1042220 153568 130212 271240 0 0 0 32 1421 652 0 26 73 0 1 1 0 1045056 168572 125980 263304 0 2836 0 2836 1394 532 0 26 74 0 0 1 0 1045056 166588 125980 265200 0 0 0 12 1309 491 0 25 75 0 0 1 0 1045056 163496 125980 268272 0 0 0 0 1231 466 0 25 75 0 0 1 0 1045056 160520 125980 271196 0 0 0 0 1309 490 0 25 75 0 0 1 0 1045056 158412 125980 273392 0 0 0 32 1292 538 0 25 75 0 0 1 0 1045056 155436 125980 276316 0 0 0 0 1371 555 0 25 75 0 0 1 0 1045056 153212 125980 278512 0 0 0 12 1307 551 0 25 75 0 0 1 0 1047004 167100 120572 271568 0 1948 0 1948 1418 574 0 26 74 0 0 1 0 1047004 164124 120572 274640 0 0 0 0 1242 489 0 25 75 0 0 1 0 1047004 162016 120572 276836 0 0 0 28 1318 524 0 25 75 0 0 1 0 1047004 158916 120572 279760 0 0 0 0 1266 486 0 25 75 0 0 1 0 1047004 156932 120572 281956 0 0 0 0 1345 598 0 25 74 0 1 1 0 1047004 153832 120572 284884 0 0 0 0 1243 476 0 25 75 0 0 1 0 1047408 166728 113704 279076 0 404 0 404 1356 506 0 26 74 0 0 1 0 1047408 164620 113704 281272 0 0 0 28 1252 498 0 25 75 0 0 1 0 1047408 161644 113704 284196 0 0 0 0 1350 584 0 25 75 0 0 1 0 1047408 159040 113708 286400 0 0 4 86096 1480 620 0 27 72 0 1 1 0 1047408 156064 113708 289324 0 0 0 0 1394 638 0 25 75 0 0 1 0 1047408 153956 113708 291520 0 0 0 0 1290 537 0 25 75 0 0 1 0 1047720 166960 105696 287132 0 312 128 344 2060 958 1 27 70 0 3 1 0 1047720 163984 105696 290056 0 0 0 0 1336 550 0 25 75 0 0 1 0 1047720 161876 105696 292252 0 0 0 0 1335 525 0 25 75 0 0 1 0 1047720 158776 105696 295176 0 0 0 0 1262 504 0 25 75 0 0 1 0 1047720 156792 105696 297372 0 0 0 4 1351 541 0 25 74 0 0 1 0 1047720 153692 105696 300224 0 0 0 52 1338 542 0 25 75 0 0 1 0 1047792 166340 97976 295228 0 72 0 72 1368 535 0 26 75 0 0 1 0 1047792 164240 97976 297424 0 0 0 8 1258 501 0 25 75 0 0 0 0 1047792 164256 97976 298300 0 0 0 0 849 548 0 13 87 0 0 0 0 1047792 164388 97976 298448 0 0 0 0 266 515 0 0 100 0 0 0 0 1047792 164388 97976 298448 0 0 0 0 404 578 0 0 100 0 0 0 0 1047792 164332 97980 298452 0 0 0 32 341 599 0 0 100 0 0 Buffer 是对磁盘数据的缓存，而 Cache 是文件数据的缓存，它们既会用在读请求中，也会用在写请求中。 如何统计出所有进程的物理内存使用量呢？ 哪些区域会内存泄露 栈内存由系统自动分配和管理。一旦程序运行超出了这个局部变量的作用域，栈内存就会被系统自动回收，所以不会产生内存泄漏的问题。 再比如，很多时候，我们事先并不知道数据大小，所以你就要用到标准库函数malloc()_，_ 在程序中动态分配内存。这时候，系统就会从内存空间的堆中分配内存。 堆内存由应用程序自己来分配和管理。除非程序退出，这些堆内存并不会被系统自动释放，而是需要应用程序明确调用库函数free()来释放它们。如果应用程序没有正确释放堆内存，就会造成内存泄漏。这是两个栈和堆的例子，那么，其他内存段是否也会导致内存泄漏呢？ 只读段，包括程序的代码和常量，由于是只读的，不会再去分配新的内存，所以也不会产生内存泄漏。 数据段，包括全局变量和静态变量，这些变量在定义时就已经确定了大小，所以也不会产生内存泄漏。 最后一个内存映射段，包括动态链接库和共享内存，其中共享内存由程序动态分配和管理。所以，如果程序在分配后忘了回收，就会导致跟堆内存类似的泄漏问题。 内存泄漏的危害非常大，这些忘记释放的内存，不仅应用程序自己不能访问，系统也不能把它们再次分配给其他应用。内存泄漏不断累积，甚至会耗尽系统内存。 检测内存泄露 用 top 或 ps 来观察进程的内存使用情况，然后找出内存使用一直增长的进程，最后再通过 pmap 查看进程的内存分布。 memleak 可以跟踪系统或指定进程的内存分配、释放请求，然后定期输出一个未释放内存和相应调用栈的汇总情况（默认 5 秒）。 12345678910111213# -a 表示显示每个内存分配请求的大小以及地址# -p 指定案例应用的PID号$ /usr/share/bcc/tools/memleak -a -p $(pidof app)WARNING: Couldn't find .text section in /appWARNING: BCC can't handle sym look ups for /app addr = 7f8f704732b0 size = 8192 addr = 7f8f704772d0 size = 8192 addr = 7f8f704712a0 size = 8192 addr = 7f8f704752c0 size = 8192 32768 bytes in 4 allocations from stack [unknown] [app] [unknown] [app] start_thread+0xdb [libpthread-2.27.so] 从 memleak 的输出可以看到，案例应用在不停地分配内存，并且这些分配的地址没有被回收。 free()调用，释放函数fibonacci()分配的内存，修复了内存泄漏的问题。就这个案例而言，还有没有其他更好的修复方法呢？ Linux Swap Linux 的 Swap 机制。Swap 把这些不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。 Swap 原理 Swap 说白了就是把一块磁盘空间或者一个本地文件（以下讲解以磁盘为例），当成内存来使用。它包括换出和换入两个过程。 换出，就是把进程暂时不用的内存数据存储到磁盘中，并释放这些数据占用的内存。 换入，则是在进程再次访问这些内存的时候，把它们从磁盘读到内存中来。Swap 其实是把系统的可用内存变大了。这样，即使服务器的内存不足，也可以运行大内存的应用程序。 有一个专门的内核线程用来定期回收内存，也就是 kswapd0。为了衡量内存的使用情况，kswapd0 定义了三个内存阈值（watermark，也称为水位），分别是页最小阈值（pages_min）、页低阈值（pages_low）和页高阈值（pages_high）。剩余内存，则使用 pages_free 表示。 kswapd0 定期扫描内存的使用情况，并根据剩余内存落在这三个阈值的空间位置，进行内存的回收操作。 剩余内存小于页最小阈值，说明进程可用内存都耗尽了，只有内核才可以分配内存。 剩余内存落在页最小阈值和页低阈值中间，说明内存压力比较大，剩余内存不多了。这时 kswapd0 会执行内存回收，直到剩余内存大于高阈值为止。 剩余内存落在页低阈值和页高阈值中间，说明内存有一定压力，但还可以满足新内存请求。 剩余内存大于页高阈值，说明剩余内存比较多，没有内存压力。 直接内存访问区（DMA）、普通内存区（NORMAL）、伪内存区（MOVABLE） 内存既包括了文件页，又包括了匿名页。 对文件页的回收，当然就是直接回收缓存，或者把脏页写回磁盘后再回收。 而对匿名页的回收，其实就是通过 Swap 机制，把它们写入磁盘后再释放内存。 不过，你可能还有一个问题。既然有两种不同的内存回收机制，那么在实际回收内存时，到底该先回收哪一种呢？ 其实，Linux 提供了一个 /proc/sys/vm/swappiness 选项，用来调整使用 Swap 的积极程度。 swappiness 的范围是 0-100，数值越大，越积极使用 Swap，也就是更倾向于回收匿名页；数值越小，越消极使用 Swap，也就是更倾向于回收文件页。虽然 swappiness 的范围是 0-100，不过要注意，这并不是内存的百分比，而是调整 Swap 积极程度的权重，即使你把它设置成 0，当剩余内存 + 文件页小于页高阈值时，还是会发生 Swap。 在内存资源紧张时，Linux 通过直接内存回收和定期扫描的方式，来释放文件页和匿名页，以便把内存分配给更需要的进程使用。文件页的回收比较容易理解，直接清空，或者把脏数据写回磁盘后再释放。而对匿名页的回收，需要通过 Swap 换出到磁盘中，下次访问时，再从磁盘换入到内存中。 12345678# 创建Swap文件$ fallocate -l 8G /mnt/swapfile# 修改权限只有根用户可以访问$ chmod 600 /mnt/swapfile# 配置Swap文件$ mkswap /mnt/swapfile# 开启Swap$ swapon /mnt/swapfile 12# 关闭 swap$ swapoff -a 12# 关闭 Swap 后再重新打开，也是一种常用的 Swap 空间清理方法$ swapoff -a &amp;&amp; swapon -a 查看 swappiness 的配置 cat /proc/sys/vm/swappiness 1234567# 按VmSwap使用量对进程排序，输出进程名称、进程ID以及SWAP用量$ for file in /proc/*/status ; do awk '/VmSwap|Name|^Pid/&#123;printf $2 " " $3&#125;END&#123; print ""&#125;' $file; done | sort -k 3 -n -r | headdockerd 2226 10728 kBdocker-containe 2251 8516 kBsnapd 936 4020 kBnetworkd-dispat 911 836 kBpolkitd 1004 44 kB 内存性能指标 快速定位系统内存问题 为了迅速定位内存问题，我通常会先运行几个覆盖面比较大的性能工具，比如 free、top、vmstat、pidstat 等。具体的分析思路主要有这几步。 先用 free 和 top，查看系统整体的内存使用情况。 再用 vmstat 和 pidstat，查看一段时间的趋势，从而判断出内存问题的类型。 最后进行详细分析，比如内存分配分析、缓存 / 缓冲区分析、具体进程的内存使用分析等。 References[1] 01| 基础篇：Linux内存是怎么工作的？[2] 02 | 基础篇：怎么理解内存中的Buffer和Cache？[3] 03 | 案例篇：如何利用系统缓存优化程序的运行效率？[4] 04 | 案例篇：内存泄漏了，我该如何定位和处理？[5] 05 | 案例篇：为什么系统的Swap变高了（上）[6] 06 | 案例篇：为什么系统的Swap变高了？（下）[7] 07 | 套路篇：如何“快准狠”找到系统内存的问题？]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gradle-download-install]]></title>
    <url>%2F2019%2F11%2F18%2Fgradle-download-install%2F</url>
    <content type="text"><![CDATA[export PATH=$PATH:/Users/weikeqin1/SoftWare/gradle-6.0/bin %GRADLE_HOME%/bin Errorgradle SSL peer shut down incorrectly123456789Caused by: org.gradle.internal.resource.transport.http.HttpRequestException: Could not GET &apos;https://d29vzk4ow07wi7.cloudfront.net/70915a3f0ef4243d4630ea23219f5445fac82700?response-content-disposition=attachment%3Bfilename%3D%22gradle-core-1.5.0.jar%22&amp;Policy=eyJTdGF0ZW1lbnQiOiBbeyJSZXNvdXJjZSI6Imh0dHAqOi8vZDI5dnprNG93MDd3aTcuY2xvdWRmcm9udC5uZXQvNzA5MTVhM2YwZWY0MjQzZDQ2MzBlYTIzMjE5ZjU0NDVmYWM4MjcwMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0JmaWxlbmFtZSUzRCUyMmdyYWRsZS1jb3JlLTEuNS4wLmphciUyMiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTU3NDA0OTI4NH0sIklwQWRkcmVzcyI6eyJBV1M6U291cmNlSXAiOiIwLjAuMC4wLzAifX19XX0_&amp;Signature=PHZOaWL0~35J849nq~FwS02Mi5fX8vhbhJRz5scXG1-19xso8v0xWPj6oyh7Ku9q8~I6ypEAdaI1eNDGQwl2VOcffRUm5fDXlclmdZUB4O~ZgA535OgXD3iv4n0Ht37uIrbsnpPad1WHy7qUFZzlS1BDA9hxDMLnXACL7rNeNiWea-Q8t5hwbCGkKi6ucdOSa8quedY7iH~biP0HxeKcQY242-HFjbByCaa8jZTcghdgIVVBnrIA8almCFttc6hTa1jy2OcTvWzm22Ty~jvfTKeBW9H3ApHty4BRHeyo7oU4YHOJCRMwtMuynQWgGwGH13zRnX~3HHfcka8L1qigEw__&amp;Key-Pair-Id=APKAIFKFWOMXM2UMTSFA&apos;. at org.gradle.internal.resource.transport.http.HttpClientHelper.performRequest(HttpClientHelper.java:99)Caused by: javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:994) at org.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:436)Caused by: java.io.EOFException: SSL peer shut down incorrectly at sun.security.ssl.InputRecord.read(InputRecord.java:505) 在 jcenter() 下 添加 maven { url &quot;http://jcenter.bintray.com&quot; } 即可 12345678910111213141516171819202122// Top-level build file where you can add configuration options common to all sub-projects/modules.buildscript &#123; repositories &#123; maven&#123; url &apos;http://maven.aliyun.com/nexus/content/groups/public/&apos; &#125; maven&#123; url &apos;http://maven.aliyun.com/nexus/content/repositories/jcenter&apos;&#125; &#125; dependencies &#123; //classpath &apos;com.android.tools.build:gradle:1.+&apos; classpath &apos;com.android.tools.build:gradle:2.4.0-alpha6&apos; // NOTE: Do not place your application dependencies here; they belong // in the individual module build.gradle files &#125;&#125;allprojects &#123; repositories &#123; maven&#123; url &apos;http://maven.aliyun.com/nexus/content/groups/public/&apos; &#125; maven&#123; url &apos;http://maven.aliyun.com/nexus/content/repositories/jcenter&apos;&#125; &#125;&#125; The android gradle plugin version 2.4.0-alpha6 is too old, please update to the latest version.123The android gradle plugin version 2.4.0-alpha6 is too old, please update to the latest version.To override this check from the command line please set the ANDROID_DAILY_OVERRIDE environment variable to &quot;9f2969bc0d30c60a0b363c667a7529c45a3f76d7&quot; References[1] gradle.org[2] gradle配置国内镜像]]></content>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch-error-notes]]></title>
    <url>%2F2019%2F11%2F13%2Felasticsearch-error-notes%2F</url>
    <content type="text"><![CDATA[(1) index_not_found_exception123456789101112131415161718192021&#123; "error": &#123; "root_cause": [ &#123; "type": "index_not_found_exception", "reason": "no such index", "resource.type": "index_or_alias", "resource.id": "test_20181204", "index_uuid": "_na_", "index": "test_20181204" &#125; ], "type": "index_not_found_exception", "reason": "no such index", "resource.type": "index_or_alias", "resource.id": "test_20181204", "index_uuid": "_na_", "index": "test_20181204" &#125;, "status": 404&#125; 问题原因： 没有对应索引 可能是没创建，可能是创建了被删除了 创建对应的索引即可 (2) illegal_argument_exception Fielddata is disabled on text fields by default1234567891011121314151617181920212223242526272829303132333435$ curl -X GET "localhost:9200/megacorp/employee/_search" -H 'Content-Type: application/json' -d' &#123; "aggs": &#123; "all_interests": &#123; "terms": &#123; "field": "interests" &#125; &#125; &#125; &#125; '&#123; "error":&#123; "root_cause":[ &#123; "type":"illegal_argument_exception", "reason":"Fielddata is disabled on text fields by default. Set fielddata=true on [interests] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead." &#125; ], "type":"search_phase_execution_exception", "reason":"all shards failed", "phase":"query", "grouped":true, "failed_shards":[ &#123; "shard":0, "index":"megacorp", "node":"gjy4N2RCQ4mLxp1lxdyZ8w", "reason":&#123; "type":"illegal_argument_exception", "reason":"Fielddata is disabled on text fields by default. Set fielddata=true on [interests] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead." &#125; &#125; ] &#125;, "status":400&#125; 解决办法: 5.x后对排序，聚合这些操作用单独的数据结构(fielddata)缓存到内存里了，需要单独开启，官方解释在此 fielddata 简单来说就是修改如下配置 1234567891011$ curl -X PUT "localhost:9200/megacorp/_mapping/employee/" -H 'Content-Type: application/json' -d' &#123; "properties": &#123; "interests": &#123; "type": "text", "fielddata": true &#125; &#125; &#125; '&#123;"acknowledged":true&#125; (3) remote_transport_exception illegal_argument_exception1234567891011121314151617181920212223242526272829303132333435$ curl -X POST "localhost:9200/website/blog/1/_update?pretty" -H 'Content-Type: application/json' -d'&gt; &#123;&gt; "script" : "ctx._source.tags+=new_tag",&gt; "params" : &#123;&gt; "new_tag" : "search"&gt; &#125;&gt; &#125; '&#123; "error" : &#123; "root_cause" : [ &#123; "type" : "remote_transport_exception", "reason" : "[node-1][127.0.0.1:9300][indices:data/write/update[s]]" &#125; ], "type" : "illegal_argument_exception", "reason" : "failed to execute script", "caused_by" : &#123; "type" : "script_exception", "reason" : "compile error", "script_stack" : [ "ctx._source.tags+=new_tag", " ^---- HERE" ], "script" : "ctx._source.tags+=new_tag", "lang" : "painless", "caused_by" : &#123; "type" : "illegal_argument_exception", "reason" : "Variable [new_tag] is not defined." &#125; &#125; &#125;, "status" : 400&#125; (4) maybe these locations are not writable or multiple nodes were started without increasing1234567891011[2020-01-21T09:45:14,313][ERROR][o.e.b.Bootstrap ] [elasticsearch_001_data] Exceptionjava.lang.IllegalStateException: failed to obtain node locks, tried [[/Users/weikeqin1/SoftWare/elasticsearch-6.6.2/data]] with lock id [0]; maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])? at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:297) ~[elasticsearch-6.6.2.jar:6.6.2] ... [2020-01-21T09:45:14,331][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [elasticsearch_001_data] uncaught exception in thread [main]org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: failed to obtain node locks, tried [[/Users/weikeqin1/SoftWare/elasticsearch-6.6.2/data]] with lock id [0]; maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])? at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:163) ~[elasticsearch-6.6.2.jar:6.6.2] ... Caused by: java.lang.IllegalStateException: failed to obtain node locks, tried [[/Users/weikeqin1/SoftWare/elasticsearch-6.6.2/data]] with lock id [0]; maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])? at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:297) ~[elasticsearch-6.6.2.jar:6.6.2] ... (5) Failed to parse value [1] as only [true] or [false] are allowed.1234567891011121314151617181920$ curl -XPUT &apos;http://localhost:9200/us/user/1?pretty=1&apos; -d &apos;&gt; &#123;&gt; &quot;email&quot; : &quot;john@smith.com&quot;,&gt; &quot;name&quot; : &quot;John Smith&quot;,&gt; &quot;username&quot; : &quot;@john&quot;&gt; &#125;&gt; &apos;&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Failed to parse value [1] as only [true] or [false] are allowed.&quot; &#125; ], &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Failed to parse value [1] as only [true] or [false] are allowed.&quot; &#125;, &quot;status&quot;: 400&#125; change pretty=1 to pretty=true failed-to-parse-value-1-as-only-true-or-false-are-allowed (6) Rejecting mapping update to [gb] as the final mapping would have more than 1 type123456789101112131415161718192021$ curl -XPUT 'http://localhost:9200/gb/tweet/3?pretty=true' -H 'Content-Type: application/json' -d '&gt; &#123;&gt; "date" : "2014-09-13",&gt; "name" : "Mary Jones",&gt; "tweet" : "Elasticsearch means full text search has never been so easy",&gt; "user_id" : 2&gt; &#125;&gt; '&#123; "error" : &#123; "root_cause" : [ &#123; "type" : "illegal_argument_exception", "reason" : "Rejecting mapping update to [gb] as the final mapping would have more than 1 type: [tweet, user]" &#125; ], "type" : "illegal_argument_exception", "reason" : "Rejecting mapping update to [gb] as the final mapping would have more than 1 type: [tweet, user]" &#125;, "status" : 400&#125; unable-to-create-index-with-more-that-1-type-in-6-x ElasticSearch 中的索引与类型的前生今世 load_test_data.sh （7） parsing_exception1234567891011121314151617&#123; "error": &#123; "root_cause": [ &#123; "type": "parsing_exception", "reason": "[terms] malformed query, expected [END_OBJECT] but found [FIELD_NAME]", "line": 1, "col": 86 &#125; ], "type": "parsing_exception", "reason": "[terms] malformed query, expected [END_OBJECT] but found [FIELD_NAME]", "line": 1, "col": 86 &#125;, "status": 400&#125; (8) invalid_type_name_exception12345678910111213&#123; "error": &#123; "root_cause": [ &#123; "type": "invalid_type_name_exception", "reason": "Document mapping type name can't start with '_', found: [_setting]" &#125; ], "type": "invalid_type_name_exception", "reason": "Document mapping type name can't start with '_', found: [_setting]" &#125;, "status": 400&#125; 语法错误 (9) [ElasticsearchException[Elasticsearch exception [type=mapper_parsing_exception, reason=failed to parse field [stdSpecialAttrMap] of type [text]]]; nested: ElasticsearchException[Elasticsearch exception [type=illegal_state_exception, reason=Can’t get text on a START_OBJECT at 1:1831]];]1[ElasticsearchException[Elasticsearch exception [type=mapper_parsing_exception, reason=failed to parse field [stdSpecialAttrMap] of type [text]]]; nested: ElasticsearchException[Elasticsearch exception [type=illegal_state_exception, reason=Can&apos;t get text on a START_OBJECT at 1:1831]];] Could not index event to Elasticsearch 动态索引没有显示指定字段，es不知道如何处理 stdSpecialAttrMap.attr_* (10) The difference between max_gram and min_gram in NGram Tokenizer must be less than or equal to: [1] but was [49]. This limit can be set by changing the [index.max_ngram_diff] index level setting.12345678910&#123; "reason":"The difference between max_gram and min_gram in NGram Tokenizer must be less than or equal to: [1] but was [49]. This limit can be set by changing the [index.max_ngram_diff] index level setting.", "type":"illegal_argument_exception", "root_cause":[ &#123; "reason":"The difference between max_gram and min_gram in NGram Tokenizer must be less than or equal to: [1] but was [49]. This limit can be set by changing the [index.max_ngram_diff] index level setting.", "type":"illegal_argument_exception" &#125; ]&#125; 从es 6 到 es7 配置变了，需要加个 &quot;max_ngram_diff&quot;: &quot;50&quot; 的配置 trying-to-set-the-max-gram-and-min-gram-in-elasticsearch 11 Root mapping definition has unsupported parameters: [docs : {properties={username={type=keyword}, address={type=keyword, fields={search={analyzer=ngram_min1_max50, type=text}}}}}]1234567891011121314&#123; "reason": "Failed to parse mapping [_doc]: Root mapping definition has unsupported parameters: [docs : &#123;properties=&#123;username=&#123;type=keyword&#125;, address=&#123;type=keyword, fields=&#123;search=&#123;analyzer=ngram_min1_max50, type=text&#125;&#125;&#125;&#125;&#125;]", "caused_by": &#123; "reason": "Root mapping definition has unsupported parameters: [docs : &#123;properties=&#123;username=&#123;type=keyword&#125;, address=&#123;type=keyword, fields=&#123;search=&#123;analyzer=ngram_min1_max50, type=text&#125;&#125;&#125;&#125;&#125;]", "type": "mapper_parsing_exception" &#125;, "type": "mapper_parsing_exception", "root_cause": [ &#123; "reason": "Root mapping definition has unsupported parameters: [docs : &#123;properties=&#123;username=&#123;type=keyword&#125;, address=&#123;type=keyword, fields=&#123;search=&#123;analyzer=ngram_min1_max50, type=text&#125;&#125;&#125;&#125;&#125;]", "type": "mapper_parsing_exception" &#125; ]&#125; es7 mapping 里不需要设置type References[1] fielddata[2] failed-to-parse-value-1-as-only-true-or-false-are-allowed[3] unable-to-create-index-with-more-that-1-type-in-6-x[4] ElasticSearch 中的索引与类型的前生今世[5] load_test_data.sh[6] trying-to-set-the-max-gram-and-min-gram-in-elasticsearch]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL SQL笔记]]></title>
    <url>%2F2019%2F11%2F06%2Fmysql-sql-notes%2F</url>
    <content type="text"><![CDATA[查询MySQL版本 SELECT VERSION(); 1234567mysql&gt; SELECT VERSION();+------------+| VERSION() |+------------+| 5.7.26-log |+------------+1 row in set (0.01 sec) 查看当前库正在使用的表 SHOW OPEN TABLES WHERE in_use &gt; 0 ; 12mysql&gt; SHOW OPEN TABLES WHERE in_use &gt; 0 ;Empty set (0.13 sec) 1234567mysql&gt; SHOW OPEN TABLES WHERE in_use &gt; 0 ;+----------+-------+--------+-------------+| Database | Table | In_use | Name_locked |+----------+-------+--------+-------------+| test | t | 1 | 0 |+----------+-------+--------+-------------+1 row in set (0.00 sec) 查看正在运行的线程 SHOW PROCESSLIST ; 123456789mysql&gt; SHOW PROCESSLIST ;+------+------+-----------------+------------+---------+------+----------+------------------+| Id | User | Host | db | Command | Time | State | Info |+------+------+-----------------+------------+---------+------+----------+------------------+| 2622 | root | localhost:55861 | dataserver | Sleep | 38 | | NULL || 2623 | root | localhost:55862 | NULL | Sleep | 59 | | NULL || 2628 | root | localhost | NULL | Query | 0 | starting | SHOW PROCESSLIST |+------+------+-----------------+------------+---------+------+----------+------------------+3 rows in set (0.01 sec) 12345678910111213141516171819202122232425262728293031323334353637mysql&gt; SHOW TABLE STATUS ;+---------------------------+--------+---------+------------+------+----------------+-------------+--------------------+--------------+-----------+----------------+---------------------+---------------------+------------+-------------------+----------+--------------------+-----------------------------------------+| Name | Engine | Version | Row_format | Rows | Avg_row_length | Data_length | Max_data_length | Index_length | Data_free | Auto_increment | Create_time | Update_time | Check_time | Collation | Checksum | Create_options | Comment |+---------------------------+--------+---------+------------+------+----------------+-------------+--------------------+--------------+-----------+----------------+---------------------+---------------------+------------+-------------------+----------+--------------------+-----------------------------------------+| columns_priv | MyISAM | 10 | Fixed | 0 | 0 | 0 | 241505530017742847 | 4096 | 0 | NULL | 2019-07-05 10:31:46 | 2019-07-05 10:31:46 | NULL | utf8_bin | NULL | | Column privileges || db | MyISAM | 10 | Fixed | 2 | 488 | 976 | 137359788634800127 | 5120 | 0 | NULL | 2019-07-05 10:31:46 | 2019-07-05 10:31:48 | NULL | utf8_bin | NULL | | Database privileges || engine_cost | InnoDB | 10 | Dynamic | 2 | 8192 | 16384 | 0 | 0 | 0 | NULL | 2019-07-05 10:31:51 | NULL | NULL | utf8_general_ci | NULL | stats_persistent=0 | || event | MyISAM | 10 | Dynamic | 0 | 0 | 0 | 281474976710655 | 2048 | 0 | NULL | 2019-07-05 10:31:46 | 2019-07-05 10:31:46 | NULL | utf8_general_ci | NULL | | Events || func | MyISAM | 10 | Fixed | 0 | 0 | 0 | 162974011515469823 | 1024 | 0 | NULL | 2019-07-05 10:31:46 | 2019-07-05 10:31:46 | NULL | utf8_bin | NULL | | User defined functions || general_log | CSV | 10 | Dynamic | 2 | 0 | 0 | 0 | 0 | 0 | NULL | NULL | NULL | NULL | utf8_general_ci | NULL | | General log || gtid_executed | InnoDB | 10 | Dynamic | 0 | 0 | 16384 | 0 | 0 | 0 | NULL | 2019-07-05 10:31:51 | NULL | NULL | latin1_swedish_ci | NULL | | || help_category | InnoDB | 10 | Dynamic | 41 | 399 | 16384 | 0 | 16384 | 0 | NULL | 2019-07-05 10:31:51 | NULL | NULL | utf8_general_ci | NULL | stats_persistent=0 | help categories || help_keyword | InnoDB | 10 | Dynamic | 728 | 135 | 98304 | 0 | 81920 | 0 | NULL | 2019-07-05 10:31:51 | NULL | NULL | utf8_general_ci | NULL | stats_persistent=0 | help keywords || help_relation | InnoDB | 10 | Dynamic | 1276 | 51 | 65536 | 0 | 0 | 0 | NULL | 2019-07-05 10:31:51 | NULL | NULL | utf8_general_ci | NULL | stats_persistent=0 | keyword-topic relation || help_topic | InnoDB | 10 | Dynamic | 618 | 2571 | 1589248 | 0 | 81920 | 4194304 | NULL | 2019-07-05 10:31:51 | NULL | NULL | utf8_general_ci | NULL | stats_persistent=0 | help topics || innodb_index_stats | InnoDB | 10 | Dynamic | 4895 | 297 | 1458176 | 0 | 0 | 4194304 | NULL | 2019-07-05 10:31:51 | 2019-11-26 16:57:16 | NULL | utf8_bin | NULL | stats_persistent=0 | || innodb_table_stats | InnoDB | 10 | Dynamic | 418 | 235 | 98304 | 0 | 0 | 0 | NULL | 2019-07-05 10:31:51 | 2019-11-26 16:57:16 | NULL | utf8_bin | NULL | stats_persistent=0 | || ndb_binlog_index | MyISAM | 10 | Dynamic | 0 | 0 | 0 | 281474976710655 | 1024 | 0 | NULL | 2019-07-05 10:31:46 | 2019-07-05 10:31:46 | NULL | latin1_swedish_ci | NULL | | || plugin | InnoDB | 10 | Dynamic | 0 | 0 | 16384 | 0 | 0 | 0 | NULL | 2019-07-05 10:31:51 | NULL | NULL | utf8_general_ci | NULL | stats_persistent=0 | MySQL plugins || proc | MyISAM | 10 | Dynamic | 48 | 6261 | 300528 | 281474976710655 | 4096 | 0 | NULL | 2019-07-05 10:31:46 | 2019-07-05 10:31:49 | NULL | utf8_general_ci | NULL | | Stored Procedures || procs_priv | MyISAM | 10 | Fixed | 0 | 0 | 0 | 266275327968280575 | 4096 | 0 | NULL | 2019-07-05 10:31:46 | 2019-07-05 10:31:46 | NULL | utf8_bin | NULL | | Procedure privileges || proxies_priv | MyISAM | 10 | Fixed | 1 | 837 | 837 | 235594555506819071 | 9216 | 0 | NULL | 2019-07-05 10:31:48 | 2019-07-05 10:31:48 | NULL | utf8_bin | NULL | | User proxy privileges || server_cost | InnoDB | 10 | Dynamic | 6 | 2730 | 16384 | 0 | 0 | 0 | NULL | 2019-07-05 10:31:51 | NULL | NULL | utf8_general_ci | NULL | stats_persistent=0 | || servers | InnoDB | 10 | Dynamic | 0 | 0 | 16384 | 0 | 0 | 0 | NULL | 2019-07-05 10:31:51 | NULL | NULL | utf8_general_ci | NULL | stats_persistent=0 | MySQL Foreign Servers table || slave_master_info | InnoDB | 10 | Dynamic | 0 | 0 | 16384 | 0 | 0 | 0 | NULL | 2019-07-05 10:31:51 | NULL | NULL | utf8_general_ci | NULL | stats_persistent=0 | Master Information || slave_relay_log_info | InnoDB | 10 | Dynamic | 0 | 0 | 16384 | 0 | 0 | 0 | NULL | 2019-07-05 10:31:51 | NULL | NULL | utf8_general_ci | NULL | stats_persistent=0 | Relay Log Information || slave_worker_info | InnoDB | 10 | Dynamic | 0 | 0 | 16384 | 0 | 0 | 0 | NULL | 2019-07-05 10:31:51 | NULL | NULL | utf8_general_ci | NULL | stats_persistent=0 | Worker Information || slow_log | CSV | 10 | Dynamic | 2 | 0 | 0 | 0 | 0 | 0 | NULL | NULL | NULL | NULL | utf8_general_ci | NULL | | Slow log || tables_priv | MyISAM | 10 | Fixed | 2 | 947 | 1894 | 266556802944991231 | 9216 | 0 | NULL | 2019-07-05 10:31:46 | 2019-07-05 10:31:48 | NULL | utf8_bin | NULL | | Table privileges || time_zone | InnoDB | 10 | Dynamic | 0 | 0 | 16384 | 0 | 0 | 0 | 1 | 2019-07-05 10:31:51 | NULL | NULL | utf8_general_ci | NULL | stats_persistent=0 | Time zones || time_zone_leap_second | InnoDB | 10 | Dynamic | 0 | 0 | 16384 | 0 | 0 | 0 | NULL | 2019-07-05 10:31:51 | NULL | NULL | utf8_general_ci | NULL | stats_persistent=0 | Leap seconds information for time zones || time_zone_name | InnoDB | 10 | Dynamic | 0 | 0 | 16384 | 0 | 0 | 0 | NULL | 2019-07-05 10:31:51 | NULL | NULL | utf8_general_ci | NULL | stats_persistent=0 | Time zone names || time_zone_transition | InnoDB | 10 | Dynamic | 0 | 0 | 16384 | 0 | 0 | 0 | NULL | 2019-07-05 10:31:51 | NULL | NULL | utf8_general_ci | NULL | stats_persistent=0 | Time zone transitions || time_zone_transition_type | InnoDB | 10 | Dynamic | 0 | 0 | 16384 | 0 | 0 | 0 | NULL | 2019-07-05 10:31:51 | NULL | NULL | utf8_general_ci | NULL | stats_persistent=0 | Time zone transition types || user | MyISAM | 10 | Dynamic | 5 | 123 | 616 | 281474976710655 | 4096 | 0 | NULL | 2019-07-05 10:31:46 | 2019-07-23 20:26:04 | NULL | utf8_bin | NULL | | Users and global privileges |+---------------------------+--------+---------+------------+------+----------------+-------------+--------------------+--------------+-----------+----------------+---------------------+---------------------+------------+-------------------+----------+--------------------+-----------------------------------------+31 rows in set (0.00 sec) MySQL删除重复数据 一不小心犯了一个错误，导致数据库中的数据重复了，想把数据库中重复的数据删除，发现时间长了，mysql语句都忘了，复习一遍。 id name age email 1 lisi 12 lisi@gmail.com 2 lihua 24 lisi@gmail.com 3 wang3 36 lihua@gmail.com 4 zhao4 32 lihua@gmail.com 5 zhao5 22 lihua@gmail.com 6 zhao6 22 zhao6@gmail.com 数据如上，想删除邮箱重复的数据，并且只保留一条。 建表语句123456789101112DROP TABLE IF EXISTS `test_table`;CREATE TABLE `test_table` (`id` tinyint(4) NOT NULL AUTO_INCREMENT ,`name` varchar(8) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL ,`age` tinyint(2) NULL DEFAULT NULL ,`email` varchar(16) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL ,PRIMARY KEY (`id`))ENGINE=InnoDBDEFAULT CHARACTER SET=utf8mb4 COLLATE=utf8mb4_general_ciAUTO_INCREMENT=7; 插入数据语句123456INSERT INTO `test_table` (`id`, `name`, `age`, `email`) VALUES (1, 'lisi', 12, 'lisi@gmail.com');INSERT INTO `test_table` (`id`, `name`, `age`, `email`) VALUES (2, 'lihua', 24, 'lisi@gmail.com');INSERT INTO `test_table` (`id`, `name`, `age`, `email`) VALUES (3, 'wang3', 36, 'lihua@gmail.com');INSERT INTO `test_table` (`id`, `name`, `age`, `email`) VALUES (4, 'zhao4', 22, 'lihua@gmail.com');INSERT INTO `test_table` (`id`, `name`, `age`, `email`) VALUES (5, 'zhao5', 32, 'lihua@gmail.com');INSERT INTO `test_table` (`id`, `name`, `age`, `email`) VALUES (6, 'zhao6', 22, 'zhao6@gmail.com'); 查看数据重复次数1select id, email, count(email) as count from test_table group by email having count(email) &gt; 1 order by count desc; 1select id, 字段, count(字段) as count from 表名 group by 字段 having count(字段) &gt; 1 order by count desc; 查询邮箱重复的数据(所有的，比较耗时)123# id是主键select id, email from test_table where email in (select email from test_table group by email having count(email) &gt; 1) 1select id, 字段 from 表名 where 字段 in (select 字段 from 表名 group by 字段 having count(字段) &gt; 1) 删除邮箱重复的数据，保留id最小的一条oracle sqlserver 可以用下面这条语句，但是mysql用这条语句会报错123delete from test_table where email in (select email from test_table group by email having count(email) &gt; 1)and id not in (select min(id) as id from test_table group by email having count(email) &gt; 1) 执行报错：1093 - You can’t specify target table ‘student’ for update in FROM clause原因是：更新数据时使用了查询，而查询的数据又做了更新的条件，mysql不支持这种方式。oracel和msserver都支持这种方式。 怎么办，再加一层封装。如下： 12345delete from test_table -- 重复的数据where email in (select email from (select email from test_table group by email having count(email) &gt; 1) a )-- 重复数据里去除id最小的and id not in (select id from (select min(id) as id from test_table group by email having count(email) &gt; 1) b ) 12345delete from 表名 -- 重复的数据where 重复字段 in (select 重复字段 from (select 重复字段 from 表名 group by 重复字段 having count(重复字段) &gt; 1) a )-- 重复数据里去除id最小的and 主键 not in (select 主键 from (select min(主键) as id from 表名 group by 重复字段 having count(重复字段) &gt; 1) b ) 123delete from 表名 where 主键ID not in (select * from (select max(主键ID) from 表名 group by 重复字段 having count(重复字段) &gt; 1) as b);delete from un_visited_url where id not in ( select * from (select max(id) from un_visited_url group by unique_key having count(unique_key)&gt;1 ) as b ); 测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657582019-11-06T09:12:00.044126Z 850 Connect root@localhost on using TCP/IP2019-11-06T09:12:00.114183Z 850 Query SET NAMES utf8mb42019-11-06T09:12:00.139959Z 850 Query show variables like &apos;profiling&apos;2019-11-06T09:12:00.224086Z 850 Query SHOW DATABASES2019-11-06T09:12:00.248403Z 850 Query show variables like &apos;lower_case_table_names&apos;2019-11-06T09:12:00.266839Z 850 Query select SCHEMA_NAME, DEFAULT_CHARACTER_SET_NAME, DEFAULT_COLLATION_NAME from INFORMATION_SCHEMA.SCHEMATA2019-11-06T09:12:02.202479Z 851 Connect root@localhost on using TCP/IP2019-11-06T09:12:02.213393Z 851 Query SET NAMES utf8mb42019-11-06T09:12:02.220314Z 851 Query USE `test`2019-11-06T09:12:02.223651Z 851 Query select @@character_set_database2019-11-06T09:12:02.242049Z 851 Query SET NAMES utf8mb42019-11-06T09:12:02.256033Z 851 Query select @@collation_database2019-11-06T09:12:02.280248Z 851 Query USE `test`2019-11-06T09:12:02.290210Z 851 Query SET NAMES utf8mb42019-11-06T09:12:02.293509Z 851 Query SHOW FULL TABLES WHERE Table_Type != &apos;VIEW&apos;2019-11-06T09:12:02.766408Z 851 Query SHOW TABLE STATUS2019-11-06T09:18:52.613285Z 854 Connect root@localhost on using TCP/IP2019-11-06T09:18:52.618798Z 854 Query SET NAMES utf8mb42019-11-06T09:18:52.626539Z 854 Query USE `test`2019-11-06T09:18:52.645761Z 854 Query select * from `test`.`student` limit 0,1002019-11-06T09:18:52.707339Z 854 Query show columns from `test`.`student`2019-11-06T09:18:52.823659Z 854 Query show index from `test`.`student`2019-11-06T09:18:52.848332Z 854 Query show create table `test`.`student`2019-11-06T09:19:32.386415Z 851 Query show engines2019-11-06T09:19:32.390662Z 851 Query SELECT DISTINCT(TABLESPACE_NAME) FROM information_schema.FILES WHERE NOT ISNULL(TABLESPACE_NAME)2019-11-06T09:19:32.429012Z 851 Query SHOW CHARACTER SET2019-11-06T09:19:32.432583Z 851 Query SHOW COLLATION2019-11-06T09:19:32.470973Z 855 Connect root@localhost on using TCP/IP2019-11-06T09:19:32.474990Z 855 Query SET NAMES utf8mb42019-11-06T09:19:32.479401Z 855 Query SHOW DATABASES2019-11-06T09:19:32.479732Z 851 Query show columns from `test`.`student`2019-11-06T09:19:32.482259Z 851 Query show index from `test`.`student`2019-11-06T09:19:32.485231Z 851 Query select column_name, column_default from information_schema.columns where table_schema=&apos;test&apos; and table_name=&apos;student&apos;2019-11-06T09:19:32.487267Z 851 Query SHOW CREATE TABLE `student`2019-11-06T09:19:32.493814Z 851 Query show index from `test`.`student`2019-11-06T09:19:32.494839Z 851 Query show create table `test`.`student`2019-11-06T09:19:32.497089Z 851 Query select trigger_name, action_timing, event_manipulation, event_object_schema, event_object_table, action_statement from information_schema.triggers where event_object_schema = &apos;test&apos; and event_object_table = &apos;student&apos;2019-11-06T09:19:32.498658Z 851 Query show table status like &apos;student&apos;2019-11-06T09:19:32.511728Z 851 Query SELECT *FROM information_schema.PARTITIONS WHERE TABLE_SCHEMA = &apos;test&apos; AND TABLE_NAME = &apos;student&apos;2019-11-06T09:19:32.513854Z 851 Query show create table `student`2019-11-06T09:19:32.549343Z 851 Query SHOW DATABASES# 刷新表2019-11-06T09:20:32.824748Z 851 Query USE `test`2019-11-06T09:20:32.825658Z 851 Query SET NAMES utf8mb42019-11-06T09:20:32.825941Z 851 Query SHOW FULL TABLES WHERE Table_Type != &apos;VIEW&apos;2019-11-06T09:20:32.956968Z 851 Query SHOW TABLE STATUS# 刷新数据库2019-11-06T09:20:58.442659Z 851 Query USE `test`2019-11-06T09:20:58.444201Z 851 Query SET NAMES utf8mb42019-11-06T09:20:58.444490Z 851 Query select @@character_set_database2019-11-06T09:20:58.444837Z 851 Query SET NAMES utf8mb42019-11-06T09:20:58.445188Z 851 Query select @@collation_database2019-11-06T09:21:04.888733Z 850 Query SHOW DATABASES2019-11-06T09:21:04.891149Z 850 Query select SCHEMA_NAME, DEFAULT_CHARACTER_SET_NAME, DEFAULT_COLLATION_NAME from INFORMATION_SCHEMA.SCHEMATA mysql in()查询结果按in集合顺序显示的方法 mysql in()查询结果按in集合顺序显示的方法 order by field (id, 2, 1, 3) References[1] Mysql删除重复记录，保留id最小的一条]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java-stream]]></title>
    <url>%2F2019%2F11%2F04%2Fjava-stream%2F</url>
    <content type="text"><![CDATA[Stream 作为 Java 8 的一大亮点，它与 java.io 包里的 InputStream 和 OutputStream 是完全不同的概念。它也不同于 StAX 对 XML 解析的 Stream，也不是 Amazon Kinesis 对大数据实时处理的 Stream。Java 8 中的 Stream 是对集合（Collection）对象功能的增强，它专注于对集合对象进行各种非常便利、高效的聚合操作（aggregate operation），或者大批量数据操作 (bulk data operation)。Stream API 借助于同样新出现的 Lambda 表达式，极大的提高编程效率和程序可读性。同时它提供串行和并行两种模式进行汇聚操作，并发模式能够充分利用多核处理器的优势，使用 fork/join 并行方式来拆分任务和加速处理过程。通常编写并行代码很难而且容易出错, 但使用 Stream API 无需编写一行多线程的代码，就可以很方便地写出高性能的并发程序。所以说，Java 8 中首次出现的 java.util.stream 是一个函数式语言+多核时代综合影响的产物。 什么是流Stream 不是集合元素，它不是数据结构并不保存数据，它是有关算法和计算的，它更像一个高级版本的 Iterator。原始版本的 Iterator，用户只能显式地一个一个遍历元素并对其执行某些操作；高级版本的 Stream，用户只要给出需要对其包含的元素执行什么操作，比如 “过滤掉长度大于 10 的字符串”、“获取每个字符串的首字母”等，Stream 会隐式地在内部进行遍历，做出相应的数据转换。 Stream 就如同一个迭代器（Iterator），单向，不可往复，数据只能遍历一次，遍历过一次后即用尽了，就好比流水从面前流过，一去不复返。 而和迭代器又不同的是，Stream 可以并行化操作，迭代器只能命令式地、串行化操作。顾名思义，当使用串行方式去遍历时，每个 item 读完后再读下一个 item。而使用并行去遍历时，数据会被分成多个段，其中每一个都在不同的线程中处理，然后将结果一起输出。Stream 的并行操作依赖于 Java7 中引入的 Fork/Join 框架（JSR166y）来拆分任务和加速处理过程。Java 的并行 API 演变历程基本如下： 1.0-1.4 中的 java.lang.Thread5.0 中的 java.util.concurrent6.0 中的 Phasers 等7.0 中的 Fork/Join 框架8.0 中的 LambdaStream 的另外一大特点是，数据源本身可以是无限的。 流的构成当我们使用一个流的时候，通常包括三个基本步骤： 获取一个数据源（source）→ 数据转换→执行操作获取想要的结果，每次转换原有 Stream 对象不改变，返回一个新的 Stream 对象（可以有多次转换），这就允许对其操作可以像链条一样排列，变成一个管道。 有多种方式生成 Stream Source： 从 Collection 和数组 Collection.stream() Collection.parallelStream() Arrays.stream(T array) or Stream.of() 从 BufferedReader java.io.BufferedReader.lines() 静态工厂 java.util.stream.IntStream.range() java.nio.file.Files.walk() 自己构建 java.util.Spliterator 其它 Random.ints() BitSet.stream() Pattern.splitAsStream(java.lang.CharSequence) JarFile.stream() 流的操作类型分为两种： Intermediate：一个流可以后面跟随零个或多个 intermediate 操作。其目的主要是打开流，做出某种程度的数据映射/过滤，然后返回一个新的流，交给下一个操作使用。这类操作都是惰性化的（lazy），就是说，仅仅调用到这类方法，并没有真正开始流的遍历。Terminal：一个流只能有一个 terminal 操作，当这个操作执行后，流就被使用“光”了，无法再被操作。所以这必定是流的最后一个操作。Terminal 操作的执行，才会真正开始流的遍历，并且会生成一个结果，或者一个 side effect。在对于一个 Stream 进行多次转换操作 (Intermediate 操作)，每次都对 Stream 的每个元素进行转换，而且是执行多次，这样时间复杂度就是 N（转换次数）个 for 循环里把所有操作都做掉的总和吗？其实不是这样的，转换操作都是 lazy 的，多个转换操作只会在 Terminal 操作的时候融合起来，一次循环完成。我们可以这样简单的理解，Stream 里有个操作函数的集合，每次转换操作就是把转换函数放入这个集合中，在 Terminal 操作的时候循环 Stream 对应的集合，然后对每个元素执行所有的函数。 还有一种操作被称为 short-circuiting。用以指： 对于一个 intermediate 操作，如果它接受的是一个无限大（infinite/unbounded）的 Stream，但返回一个有限的新 Stream。对于一个 terminal 操作，如果它接受的是一个无限大的 Stream，但能在有限的时间计算出结果。当操作一个无限大的 Stream，而又希望在有限时间内完成操作，则在管道内拥有一个 short-circuiting 操作是必要非充分条件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531import lombok.Data;import org.junit.Test;import java.util.*;import java.util.function.Function;import java.util.function.Supplier;import java.util.stream.Collectors;import java.util.stream.IntStream;import java.util.stream.Stream;/** * Stream 方法 * * &lt;pre&gt; * * 巧用Java8中的Stream，让集合操作飞起来！ https://mp.weixin.qq.com/s/fx8XvRjcevXJMRzrWeHx-A * java8的Stream对集合操作飞起来 https://juejin.im/post/5d5e2616f265da03b638b28a * Java 8 Stream https://www.runoob.com/java/java8-streams.html * Java 8 中的 Streams API 详解 https://www.ibm.com/developerworks/cn/java/j-lo-java8streamapi/index.html * * Stream作为java8的新特性，基于lambda表达式，是对集合对象功能的增强，它专注于对集合对象进行各种高效、便利的聚合操作或者大批量的数据操作，提高了编程效率和代码可读性。 * Stream的原理：将要处理的元素看做一种流，流在管道中传输，并且可以在管道的节点上处理，包括过滤筛选、去重、排序、聚合等。元素流在管道中经过中间操作的处理，最后由最终操作得到前面处理的结果。 * * 集合有两种方式生成流： * stream() − 为集合创建串行流 * parallelStream() - 为集合创建并行流 * * 中间操作主要有以下方法（此类型方法返回的都是Stream）：map (mapToInt, flatMap 等)、 filter、 distinct、 sorted、 peek、 limit、 skip、 parallel、 sequential、 unordered * 终止操作主要有以下方法：forEach、 forEachOrdered、 toArray、 reduce、 collect、 min、 max、 count、 anyMatch、 allMatch、 noneMatch、 findFirst、 findAny、 iterator * * &lt;/pre&gt; * * @author: weikeqin@gmail.com **/public class StreamSample &#123; @Test public void getStream() &#123; // 1. Individual values Stream stream = Stream.of("a", "b", "c"); // 2. Arrays String[] strArray = new String[]&#123;"a", "b", "c"&#125;; stream = Stream.of(strArray); stream = Arrays.stream(strArray); // 3. Collections List&lt;String&gt; list = Arrays.asList(strArray); stream = list.stream(); stream.forEach(System.out::println); // 数值流的构造 IntStream.of(new int[]&#123;1, 2, 3&#125;).forEach(System.out::println); IntStream.range(1, 3).forEach(System.out::println); IntStream.rangeClosed(1, 3).forEach(System.out::println); &#125; /** * */ @Test public void streamExample() &#123; List&lt;Student&gt; students = getList(); List&lt;Long&gt; res = students.parallelStream() .filter(x -&gt; "北京".equals(x.getAddress())) .sorted(Comparator.comparing(Student::getAge).reversed()) .map(Student::getId) .collect(Collectors.toList()); res.forEach(System.out::println); &#125; /** * 把所有的单词转换为大写 */ @Test public void strToUpperCase() &#123; List&lt;String&gt; strs = getStringList(); strs.stream(). map(String::toUpperCase). collect(Collectors.toList()). forEach(System.out::println); &#125; /** * 平方数 */ @Test public void cal() &#123; List&lt;Integer&gt; nums = Arrays.asList(1, 2, 3, 4); nums.stream(). map(n -&gt; n * n). collect(Collectors.toList()). forEach(System.out::println); &#125; /** * 一对多 */ @Test public void listTest() &#123; Stream&lt;List&lt;Integer&gt;&gt; inputStream = Stream.of( Arrays.asList(1), Arrays.asList(2, 3), Arrays.asList(4, 5, 6) ); inputStream. flatMap((childList) -&gt; childList.stream()). forEach(System.out::println); &#125; /** * 筛选 */ @Test public void filterTest() &#123; List&lt;Student&gt; students = getList(); // 筛选地址是浙江的 List&lt;Student&gt; streamStudents = students.stream().filter(s -&gt; "浙江".equals(s.getAddress())).collect(Collectors.toList()); streamStudents.forEach(System.out::println); System.out.println("----------------"); // 筛选年龄大于15的 List&lt;Student&gt; students2 = students.stream().filter(s -&gt; s.getAge() &gt; 15).collect(Collectors.toList()); students2.forEach(x -&gt; &#123; System.out.println(x); &#125;); System.out.println("----------------"); List&lt;Student&gt; students3 = students.stream().filter(s -&gt; s.getAge() == 18).collect(Collectors.toList()); students3.forEach(System.out::println); &#125; /** * peek 对每个元素执行操作并返回一个新的 Stream */ @Test public void test1() &#123; Stream.of("one", "two", "three", "four") .filter(e -&gt; e.length() &gt; 3) .peek(e -&gt; System.out.println("Filtered value: " + e)) .map(String::toUpperCase) .peek(e -&gt; System.out.println("Mapped value: " + e)) .collect(Collectors.toList()) .forEach(System.out::println); &#125; /** * Optional 的两个用例 */ @Test public void optionalUse() &#123; String text = "abcd"; //text = null; Optional.ofNullable(text).ifPresent(System.out::println); int i = Optional.ofNullable(text).map(String::length).orElse(-1); System.out.println("text 长度：" + i); &#125; /** * reduce 的用例 */ @Test public void reduceUse() &#123; // 字符串连接，concat = "ABCD" String concat = Stream.of("A", "B", "C", "D").reduce("_", String::concat); System.out.println(concat); // 求最小值，minValue = -3.0 double minValue = Stream.of(-1.5, 1.0, -3.0, -2.0).reduce(Double.MAX_VALUE, Double::min); System.out.println("最小值: " + minValue); // 求和，sumValue = 10, 有起始值 int sumValue = Stream.of(1, 2, 3, 4).reduce(0, Integer::sum); System.out.println("和: " + sumValue); // 求和，sumValue = 10, 无起始值 sumValue = Stream.of(1, 2, 3, 4).reduce(Integer::sum).get(); System.out.println("和: " + sumValue); // 过滤，字符串连接，concat = "ace" concat = Stream.of("a", "B", "c", "D", "e", "F"). filter(x -&gt; x.compareTo("Z") &gt; 0). reduce("", String::concat); System.out.println("过滤后的字符串: " + concat); &#125; /** * 获取数据 * * @return */ public List&lt;Student&gt; getList() &#123; Student s1 = new Student(1L, "张三", 13, "浙江"); Student s2 = new Student(2L, "李四", 31, "湖北"); Student s3 = new Student(3L, "王五", 17, "北京"); Student s4 = new Student(4L, "赵六", 18, "山西"); Student s5 = new Student(5L, "田七", 18, "北京"); List&lt;Student&gt; students = new ArrayList&lt;&gt;(); students.add(s1); students.add(s2); students.add(s3); students.add(s4); students.add(s5); return students; &#125; /** * @return */ public List&lt;Student&gt; getList2() &#123; List&lt;Student&gt; students = getList(); Student s6 = new Student(); Student s7 = null; students.add(s6); students.add(s7); return students; &#125; /** * @return */ public List&lt;String&gt; getStringList() &#123; List&lt;String&gt; list = Arrays.asList("a", "aa", "b", "c", "A", "AA", "B", "C", "1", "11", "2", "中", "中国", "众", "众人", "终", "终于", "#", " ", " a", " A", " 中"); System.out.println("list size：" + list.size()); return list; &#125; /** * list转map */ @Test public void listToMap() &#123; List&lt;Student&gt; list = getList(); // 方法一 // Map&lt;id, Student&gt; Map&lt;Long, Student&gt; map = list.stream().collect(Collectors.toMap(Student::getId, Function.identity())); System.out.println(map); // 方法二 // Map&lt;id, Student&gt; Map&lt;Long, Student&gt; map2 = list.stream().collect(Collectors.toMap(s -&gt; s.getId(), v -&gt; v)); System.out.println(map2); // 方法三 // Duplicated Key (List转Map 重复key问题) Map&lt;Long, Student&gt; map3 = list.stream().collect(Collectors.toMap(s -&gt; s.getId(), v -&gt; v, (oldValue, newValue) -&gt; oldValue)); System.out.println(map3); // 方法四 // Map&lt;id, name&gt; Map&lt;Long, String&gt; idNameMap = list.stream().collect(Collectors.toMap(Student::getId, Student::getName)); System.out.println(idNameMap); &#125; /** * 集合处理 * &lt;p&gt; * list 转 list * list 转 set */ @Test public void listDeal() &#123; List&lt;Student&gt; list = getList(); List&lt;String&gt; nameList = list.stream().map(x -&gt; "姓名：" + x.getName()).collect(Collectors.toList()); nameList.forEach(System.out::println); Set&lt;String&gt; nameSet = list.stream().map(x -&gt; "姓名：" + x.getName()).collect(Collectors.toSet()); nameSet.forEach(System.out::println); &#125; /** * 字符串排序 */ @Test public void strSort() &#123; List&lt;String&gt; list = getStringList(); list.stream().sorted().forEach(System.out::println); &#125; /** * 对象排序 */ @Test public void listSort() &#123; List&lt;Student&gt; list = getList(); Collections.shuffle(list); list.forEach(System.out::println); System.out.println("------------------"); Map&lt;Long, Student&gt; map = list.stream().sorted(Comparator.comparingLong(Student::getId).reversed()).collect(Collectors.toMap(k -&gt; k.getId(), v -&gt; v, (o, n) -&gt; o)); System.out.println(map); System.out.println("------------------"); Map&lt;Long, Student&gt; map2 = list.stream().sorted(Comparator.comparingLong(Student::getId).reversed()).collect(Collectors.toMap(Student::getId, v -&gt; v, (o, n) -&gt; o)); System.out.println(map2); &#125; /** * */ @Test public void strDistinct() &#123; List&lt;String&gt; list = Arrays.asList("a", "b", "c", "d", "a", "a", "b"); list.forEach(System.out::println); System.out.println("---------"); list.stream().distinct().forEach(System.out::println); &#125; /** * 去重 */ @Test public void disticnt() &#123; List&lt;Student&gt; student = getList(); // 集合去重（引用对象） student.stream().distinct().forEach(System.out::println); System.out.println("---------"); student.forEach(System.out::println); System.out.println("------------------"); student.addAll(student); student.forEach(System.out::println); System.out.println("------------------"); // 集合去重（引用对象） student.stream().distinct().forEach(System.out::println); &#125; /** * */ @Test public void limit() &#123; List&lt;String&gt; list = getStringList(); list.stream().forEach(System.out::println); System.out.println("------------------"); list.stream().limit(2).forEach(System.out::println); &#125; /** * 集合skip 删除前n个元素 */ @Test public void skip() &#123; List&lt;String&gt; list = getStringList(); list.stream().forEach(System.out::println); System.out.println("------------------"); list.stream().skip(2).forEach(System.out::println); &#125; /** * 集合reduce,将集合中每个元素聚合成一条数据 */ @Test public void listReduce() &#123; List&lt;String&gt; list = Arrays.asList("欢", "迎", "你"); String str = list.stream().reduce("北京", (a, b) -&gt; a + b); System.out.println(str); &#125; /** * */ @Test public void min() &#123; List&lt;Student&gt; list = getList(); Student minAgeStudent = list.stream().filter(x -&gt; (x != null &amp;&amp; x.getAge() &gt; 0)).filter(x -&gt; x.getAge() != null).min((x, y) -&gt; Integer.compare(x.getAge(), y.getAge())).get(); System.out.println(minAgeStudent); &#125; /** * &lt;pre&gt; * Operator != cannot be applied to 'int', 'null' * 数字类型为int时不能用 != null * &lt;/pre&gt; */ @Test public void testOperator() &#123; List&lt;Student&gt; list = getList2(); list.stream().filter(x -&gt; (x != null &amp;&amp; x.getAge() != null)).forEach(System.out::println); System.out.println("------------------"); // list.stream().filter(x -&gt; x.getId() == 1).collect(Collectors.toList()); &#125; /** * &lt;pre&gt; * anyMatch：Stream 中任意一个元素符合传入的 predicate，返回 true * allMatch：Stream 中全部元素符合传入的 predicate，返回 true * noneMatch：Stream 中没有一个元素符合传入的 predicate，返回 true * &lt;/pre&gt; */ @Test public void match() &#123; List&lt;Student&gt; list = getList(); Boolean anyMatch = list.stream().anyMatch(x -&gt; "北京".equals(x.getAddress())); System.out.println("anyMatch res: " + anyMatch); Boolean allMatch = list.stream().anyMatch(x -&gt; x.getAge() &gt; 3); System.out.println("allMatch res: " + allMatch); Boolean noMatch = list.stream().noneMatch(x -&gt; x.getAge() &gt; 35); System.out.println("noMatch res: " + noMatch); &#125; @Test public void parallelTest() &#123; List&lt;String&gt; strs = getStringList(); Long count = strs.parallelStream().filter(s -&gt; s.length() == 1).count(); System.out.println("count: " + count); &#125; @Test public void test() &#123; List&lt;Integer&gt; numbers = Arrays.asList(1, 3, 2, 2, 3, 7, 3, 5, 11, 13, 17); IntSummaryStatistics stats = numbers.stream().mapToInt((x) -&gt; x).summaryStatistics(); System.out.println("列表中最大的数 : " + stats.getMax()); System.out.println("列表中最小的数 : " + stats.getMin()); System.out.println("所有数之和 : " + stats.getSum()); System.out.println("平均数 : " + stats.getAverage()); &#125; /** * */ @Test public void mySupplierTest() &#123; Stream.generate(new PersonSupplier()). limit(10). forEach(p -&gt; System.out.println(p.getName() + ", " + p.getAge())); &#125; /** * 生成等差数列 */ @Test public void getIterate() &#123; Stream.iterate(0, n -&gt; n + 3).limit(10).forEach(x -&gt; System.out.print(x + " ")); &#125; /** * */ @Test public void groupByTest() &#123; Map&lt;Integer, List&lt;Student&gt;&gt; personGroups = Stream.generate(new PersonSupplier()). limit(100). collect(Collectors.groupingBy(Student::getAge)); Iterator it = personGroups.entrySet().iterator(); while (it.hasNext()) &#123; Map.Entry&lt;Integer, List&lt;Student&gt;&gt; persons = (Map.Entry) it.next(); System.out.println("Age " + persons.getKey() + " = " + persons.getValue().size()); &#125; &#125; private class PersonSupplier implements Supplier&lt;Student&gt; &#123; private int index = 0; private Random random = new Random(); @Override public Student get() &#123; return new Student(index++, "StormTestUser" + index, random.nextInt(100)); &#125; &#125;&#125;@Dataclass Student &#123; private Long id; private String name; private Integer age; private String address; private Integer num; public Student() &#123; &#125; /** * @param id * @param name * @param age * @param address */ public Student(Long id, String name, int age, String address) &#123; this.id = id; this.name = name; this.age = age; this.address = address; &#125; public Student(int i, String s, int nextInt) &#123; this.num = i; this.name = s; this.age = nextInt; &#125;&#125; References[1] Java 8 中的 Streams API 详解[2] 巧用Java8中的Stream，让集合操作飞起来[3] java8的Stream对集合操作飞起来[4] Java 8 Stream]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java-lambda]]></title>
    <url>%2F2019%2F11%2F04%2Fjava-lambda%2F</url>
    <content type="text"><![CDATA[Java8 Lambda 表达式产生的背景和用法，以及 Lambda 表达式与匿名类的不同等。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import org.junit.Test;import java.util.Arrays;import java.util.Comparator;import java.util.List;import java.util.function.BiConsumer;import java.util.function.Consumer;import java.util.function.Predicate;import java.util.stream.Collectors;/** * Lambda 学习 * * &lt;pre&gt; * 深入浅出 Java 8 Lambda 表达式 http://blog.oneapm.com/apm-tech/226.html * 关于Java Lambda表达式看这一篇就够了 https://objcoding.com/2019/03/04/lambda/ * Java 8 lambda 最佳实践 https://wizardforcel.gitbooks.io/java8-tutorials/content/Java%208%20lambda%20%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.html * * &lt;/pre&gt; * * @author: weikeqin1@jd.com * @date: 2019-10-22 20:33 **/public class LambdaSample &#123; @Test public void threadStartOld() &#123; Worker w = new Worker(); Thread t = new Thread(w); t.start(); &#125; @Test public void threadStartOld2() &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("Hello from thread"); &#125; &#125;).start(); &#125; @Test public void threadStartNew() &#123; new Thread(() -&gt; System.out.println("abcd")).start(); // Runnable r = () -&gt; System.out.println("hello world"); &#125; public void test1() &#123; Consumer&lt;Integer&gt; c = (x) -&gt; &#123; System.out.println(x); &#125;; BiConsumer&lt;Integer, String&gt; b = (Integer x, String y) -&gt; System.out.println(x + " : " + y); Predicate&lt;String&gt; p = (String s) -&gt; &#123; return s == null; &#125;; &#125;&#125;class Worker implements Runnable &#123; @Override public void run() &#123; System.out.println("abcd"); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * */@Testpublic void arrCompareOld() &#123; String[] strs = new String[]&#123;"d", "c", "b", "a"&#125;; Arrays.stream(strs).forEach(System.out::println); Arrays.sort(strs, new Comparator&lt;String&gt;() &#123; @Override public int compare(String o1, String o2) &#123; return o1.compareTo(o2); &#125; &#125;); Arrays.stream(strs).forEach(System.out::println);&#125;/** * */@Testpublic void arrCompareNew() &#123; String[] strs = new String[]&#123;"d", "c", "b", "a"&#125;; Arrays.stream(strs).forEach(System.out::println); Arrays.sort(strs, (o1, o2) -&gt; &#123; //return o1.compareTo(o2); return Integer.compare(o1.length(), o2.length()); &#125;); Arrays.stream(strs).forEach(System.out::println);&#125;/** * */@Testpublic void arrCompareNew2() &#123; String[] strs = new String[]&#123;"d", "c", "b", "a"&#125;; Arrays.stream(strs).forEach(System.out::println); //参数调用其类的相同方法，再简化 Arrays.sort(strs, Comparator.comparingInt(String::length)); Arrays.stream(strs).forEach(System.out::println);&#125; 12345678910111213141516171819202122/** * 方法的引用 */@Testpublic void methodUse() &#123; String[] strs = &#123;"a", "b", "1", "2"&#125;; List&lt;String&gt; list = Arrays.asList(strs); list.forEach(x -&gt; &#123; System.out.println(x); &#125;); /** :: 实际上面这对对象方法的引用 */ List&lt;String&gt; strLen = list.stream().map(String::trim).collect(Collectors.toList()); strLen.forEach(System.out::println); /** 方法的引用 */ List&lt;String&gt; strLen2 = list.stream().map(s -&gt; s.trim()).collect(Collectors.toList()); strLen2.forEach(x -&gt; &#123; System.out.println(x); &#125;);&#125; 123456789101112131415@Testpublic void calOld() &#123; //Old way: List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7); for (Integer n : list) &#123; int x = n * n; System.out.println(x); &#125;&#125;@Testpublic void calNew() &#123; List&lt;Integer&gt; list = Arrays.asList(1, 2, 3, 4, 5, 6, 7); list.stream().map((x) -&gt; x * x).forEach(System.out::println);&#125; References[1] 深入浅出 Java 8 Lambda 表达式[2] 关于Java Lambda表达式看这一篇就够了[3] Java 8 lambda 最佳实践]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis]]></title>
    <url>%2F2019%2F10%2F21%2Fredis-use%2F</url>
    <content type="text"><![CDATA[Redis教程-菜鸟教程java 中 Redis 5大基本类型的用法后端开发都应该掌握的Redis基础想不到！面试官问我：Redis 内存满了怎么办？掌握这些 Redis 技巧，百亿数据量不在话下！ Redis 的各项功能解决了哪些问题Redis在游戏开发中的几种应用场景 一份完整的阿里云 Redis 开发规范 Redis 数据结构和主要命令十二张图详解Redis的数据结构和对象系统Redis 为什么这么快为什么 Redis 单线程能支撑高并发Redis为什么这么快？一文深入了解Redis内存模型！为什么我们做分布式使用RedisRedis大量数据测试 扫盲，为什么分布式一定要有Redis?分布式缓存 Java 框架分布式数据缓存中的一致性哈希算法 高并发编程之高并发场景：秒杀（无锁、排他锁、乐观锁、redis缓存的逐步演变）基于 Redis 的分布式锁到底安全吗Redlock：Redis分布式锁最牛逼的实现不用找了，基于 Redis 的分布式锁实战来了 Tendis 基于 binlog 的主从同步方案 MongoDB、HBase、Redis 等 NoSQL 优劣势、应用场景 - 芋道源码]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql-index]]></title>
    <url>%2F2019%2F09%2F09%2Fmysql-index%2F</url>
    <content type="text"><![CDATA[转自 04 | 深入浅出索引（上） MySQL实战45讲 05 | 深入浅出索引（下） MySQL实战45讲 MySQL 不走索引的情况 转自 18 | 为什么这些SQL语句逻辑相同，性能却差异巨大？ 案例一：条件字段函数操作 对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。 假设你现在维护了一个交易系统，其中交易记录表 tradelog 包含交易流水号（tradeid）、交易员 id（operator）、交易时间（t_modified）等字段。为了便于描述，我们先忽略其他字段。这个表的建表语句如下：123456789mysql&gt; CREATE TABLE `tradelog` ( `id` int(11) NOT NULL, `tradeid` varchar(32) DEFAULT NULL, `operator` int(11) DEFAULT NULL, `t_modified` datetime DEFAULT NULL, PRIMARY KEY (`id`), KEY `tradeid` (`tradeid`), KEY `t_modified` (`t_modified`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 假设，现在已经记录了从 2016 年初到 2018 年底的所有数据，运营部门有一个需求是，要统计发生在所有年份中 7 月份的交易记录总数。这个逻辑看上去并不复杂，你的 SQL 语句可能会这么写： 案例二：隐式类型转换 数据类型转换的规则是什么？ 为什么有数据类型转换，就需要走全索引扫描？ 这里有一个简单的方法，看 select “10” &gt; 9 的结果： 如果规则是“将字符串转成数字”，那么就是做数字比较，结果应该是 1； 如果规则是“将数字转成字符串”，那么就是做字符串比较，结果应该是 0。 select * from tradelog where tradeid=110717; 就知道对于优化器来说，这个语句相当于： select * from tradelog where CAST(tradid AS signed int) = 110717; 现在，我留给你一个小问题，id 的类型是 int，如果执行下面这个语句，是否会导致全表扫描呢？ select * from tradelog where id=”83126”; 案例三：隐式字符编码转换123456789101112131415161718192021222324mysql&gt; CREATE TABLE `trade_detail` ( `id` int(11) NOT NULL, `tradeid` varchar(32) DEFAULT NULL, `trade_step` int(11) DEFAULT NULL, /* 操作步骤 */ `step_info` varchar(32) DEFAULT NULL, /* 步骤信息 */ PRIMARY KEY (`id`), KEY `tradeid` (`tradeid`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into tradelog values(1, 'aaaaaaaa', 1000, now());insert into tradelog values(2, 'aaaaaaab', 1000, now());insert into tradelog values(3, 'aaaaaaac', 1000, now());insert into trade_detail values(1, 'aaaaaaaa', 1, 'add');insert into trade_detail values(2, 'aaaaaaaa', 2, 'update');insert into trade_detail values(3, 'aaaaaaaa', 3, 'commit');insert into trade_detail values(4, 'aaaaaaab', 1, 'add');insert into trade_detail values(5, 'aaaaaaab', 2, 'update');insert into trade_detail values(6, 'aaaaaaab', 3, 'update again');insert into trade_detail values(7, 'aaaaaaab', 4, 'commit');insert into trade_detail values(8, 'aaaaaaac', 1, 'add');insert into trade_detail values(9, 'aaaaaaac', 2, 'update');insert into trade_detail values(10, 'aaaaaaac', 3, 'update again');insert into trade_detail values(11, 'aaaaaaac', 4, 'commit'); 在这个执行计划里，是从 tradelog 表中取 tradeid 字段，再去 trade_detail 表里查询匹配字段。因此，我们把 tradelog 称为驱动表，把 trade_detail 称为被驱动表，把 tradeid 称为关联字段。 select from trade_detail where tradeid=$L2.tradeid.value; select from trade_detail where CONVERT(traideid USING utf8mb4)=$L2.tradeid.value; 参照前面的两个例子，你肯定就想到了，字符集 utf8mb4 是 utf8 的超集，所以当这两个类型的字符串在做比较的时候，MySQL 内部的操作是，先把 utf8 字符串转成 utf8mb4 字符集，再做比较。 CONVERT() 函数，在这里的意思是把输入的字符串转成 utf8mb4 字符集。 mysql&gt;select l.operator from tradelog l , trade_detail d where d.tradeid=l.tradeid and d.id=4; alter table trade_detail modify tradeid varchar(32) CHARACTER SET utf8mb4 default null; mysql&gt; select d.* from tradelog l , trade_detail d where d.tradeid=CONVERT(l.tradeid USING utf8) and l.id=2; References[1] 04 | 深入浅出索引（上） MySQL实战45讲[2] 05 | 深入浅出索引（下） MySQL实战45讲]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 事务]]></title>
    <url>%2F2019%2F09%2F09%2Fmysql-transaction%2F</url>
    <content type="text"><![CDATA[转自 03 | 事务隔离：为什么你改了我还看不见？ MySQL实战45讲 20 | 幻读是什么，幻读有什么问题？MySQL实战45讲 提到事务，你肯定不陌生，和数据库打交道的时候，我们总是会用到事务。最经典的例子就是转账，你要给朋友小王转 100 块钱，而此时你的银行卡只有 100 块钱。 转账过程具体到程序里会有一系列的操作，比如查询余额、做加减法、更新余额等，这些操作必须保证是一体的，不然等程序查完之后，还没做减法之前，你这 100 块钱，完全可以借着这个时间差再查一次，然后再给另外一个朋友转账，如果银行这么整，不就乱了么？这时就要用到“事务”这个概念了。 简单来说，事务就是要保证一组数据库操作，要么全部成功，要么全部失败。在 MySQL 中，事务支持是在引擎层实现的。你现在知道，MySQL 是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如 MySQL 原生的 MyISAM 引擎就不支持事务，这也是 MyISAM 被 InnoDB 取代的重要原因之一。 今天的文章里，我将会以 InnoDB 为例，剖析 MySQL 在事务支持方面的特定实现，并基于原理给出相应的实践建议，希望这些案例能加深你对 MySQL 事务原理的理解。 隔离性与隔离级别 提到事务，你肯定会想到 ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），今天我们就来说说其中 I，也就是“隔离性”。 当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。 在谈隔离级别之前，你首先要知道，你隔离得越严实，效率就会越低。因此很多时候，我们都要在二者之间寻找一个平衡点。SQL 标准的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。下面我逐一为你解释： 1. 读未提交(read uncommitted) 是指，一个事务还没提交时，它做的变更就能被别的事务看到。 2. 读提交(read committed) 是指，一个事务提交之后，它做的变更才会被其他事务看到。 3. 可重复读(repeatable read) 是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。 4. 串行化(serializable)，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 读未提交：别人改数据的事务尚未提交，我在我的事务中也能读到。 读已提交：别人改数据的事务已经提交，我在我的事务中才能读到。 可重复读：别人改数据的事务已经提交，我在我的事务中也不去读。 串行：我的事务尚未提交，别人就别想改数据。 这4种隔离级别，并行性能依次降低，安全性依次提高。 在当前隔离级别是否会出现对应情况 脏读(dirty read) 不可重复读(non-repeatable read) 幻读(phantom read) 读未提交(read uncommitted) 会 会 会 读提交(read committed) 不会 会 会 可重复读(repeatable read) 不会 不会 会 串行化(serializable) 不会 不会 不会 其中“读提交”和“可重复读”比较难理解，所以我用一个例子说明这几种隔离级别。假设数据表 T 中只有一列，其中一行的值为 1，下面是按照时间顺序执行两个事务的行为。 12mysql&gt; create table T(c int) engine=InnoDB;insert into T(c) values(1); 事务A 事务B 启动事务 查询得到值1 启动事务 查询得到值1 将1改成2 查询得到值V1 提交事务B 查询得到值V2 提交事务A 查询得到值V3 我们来看看在不同的隔离级别下，事务 A 会有哪些不同的返回结果，也就是图里面 V1、V2、V3 的返回值分别是什么。 若隔离级别是“读未提交”， 则 V1 的值就是 2。这时候事务 B 虽然还没有提交，但是结果已经被 A 看到了。因此，V2、V3 也都是 2。 若隔离级别是“读提交”，则 V1 是 1，V2 的值是 2。事务 B 的更新在提交后才能被 A 看到。所以， V3 的值也是 2。 若隔离级别是“可重复读”，则 V1、V2 是 1，V3 是 2。之所以 V2 还是 1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。 若隔离级别是“串行化”，则在事务 B 执行“将 1 改成 2”的时候，会被锁住。直到事务 A 提交后，事务 B 才可以继续执行。所以从 A 的角度看， V1、V2 值是 1，V3 的值是 2。 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个 SQL 语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。 我们可以看到在不同的隔离级别下，数据库行为是有所不同的。Oracle 数据库的默认隔离级别其实就是“读提交”，因此对于一些从 Oracle 迁移到 MySQL 的应用，为保证数据库隔离级别的一致，你一定要记得将 MySQL 的隔离级别设置为“读提交”。 配置的方式是，将启动参数 transaction-isolation 的值设置成 READ-COMMITTED。你可以用 show variables 来查看当前的值。 1234567891011mysql&gt; show variables like 'transaction_isolation';+-----------------------+----------------+| Variable_name | Value |+-----------------------+----------------+| transaction_isolation | READ-COMMITTED |+-----------------------+----------------+ 总结来说，存在即合理，哪个隔离级别都有它自己的使用场景，你要根据自己的业务情况来定。我想 你可能会问那什么时候需要“可重复读”的场景呢 ？我们来看一个数据校对逻辑的案例。 假设你在管理一个个人银行账户表。一个表存了每个月月底的余额，一个表存了账单明细。这时候你要做数据校对，也就是判断上个月的余额和当前余额的差额，是否与本月的账单明细一致。你一定希望在校对过程中，即使有用户发生了一笔新的交易，也不影响你的校对结果。 这时候使用“可重复读”隔离级别就很方便。事务启动时的视图可以认为是静态的，不受其他事务更新的影响。 事务隔离的实现 理解了事务的隔离级别，我们再来看看事务隔离具体是怎么实现的。这里我们展开说明“可重复读”。 在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。 假设一个值从 1 被按顺序改成了 2、3、4，在回滚日志里面就会有类似下面的记录。 当前值是 4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的 read-view。如图中看到的，在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于 read-view A，要得到 1，就必须将当前值依次执行图中所有的回滚操作得到。 同时你会发现，即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的。 你一定会问，回滚日志总不能一直保留吧，什么时候删除呢？答案是，在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除。 什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的 read-view 的时候。 基于上面的说明，我们来讨论一下为什么建议你尽量不要使用长事务。 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。 在 MySQL 5.5 及以前的版本，回滚日志是跟数据字典一起放在 ibdata 文件里的，即使长事务最终提交，回滚段被清理，文件也不会变小。我见过数据只有 20GB，而回滚段有 200GB 的库。最终只好为了清理回滚段，重建整个库。 除了对回滚段的影响，长事务还占用锁资源，也可能拖垮整个库。 事务的启动方式 如前面所述，长事务有这些潜在风险，我当然是建议你尽量避免。其实很多时候业务开发同学并不是有意使用长事务，通常是由于误用所致。MySQL 的事务启动方式有以下几种： 显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。 set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。 有些客户端连接框架会默认连接成功后先执行一个 set autocommit=0 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。 因此，我会建议你总是使用 set autocommit=1, 通过显式语句的方式来启动事务。 但是有的开发同学会纠结“多一次交互”的问题。对于一个需要频繁使用事务的业务，第二种方式每个事务在开始时都不需要主动执行一次 “begin”，减少了语句的交互次数。如果你也有这个顾虑，我建议你使用 commit work and chain 语法。 在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。 你可以在 information_schema 库的 innodb_trx 这个表中查询长事务，比如下面这个语句，用于查找持续时间超过 60s 的事务。 1select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))&gt;60 牛刀小试 如果你是业务开发负责人同时也是数据库负责人，你会有什么方案来避免出现或者处理这种情况呢？ 在开发过程中，尽可能的减小事务范围，少用长事务，如果无法避免，保证逻辑日志空间足够用，并且支持动态日志空间增长。监控Innodb_trx表，发现长事务报警。 References[1] 03 | 事务隔离：为什么你改了我还看不见？ MySQL实战45讲[2] 20 | 幻读是什么，幻读有什么问题？MySQL实战45讲 [3] 《高性能MySQL》 O’REILLY]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字节码增强技术探索]]></title>
    <url>%2F2019%2F09%2F08%2Fjava-bytecode-enhancement%2F</url>
    <content type="text"><![CDATA[转 字节码增强技术探索 References[1] 字节码增强技术探索]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis的各项功能解决了哪些问题]]></title>
    <url>%2F2019%2F09%2F08%2Fredis-solve-what-problem%2F</url>
    <content type="text"><![CDATA[Redis的各项功能解决了哪些问题？ 先看一下Redis是一个什么东西。官方简介解释到：Redis是一个基于BSD开源的项目，是一个把结构化的数据放在内存中的一个存储系统，你可以把它作为数据库，缓存和消息中间件来使用。同时支持strings，lists，hashes，sets，sorted sets，bitmaps，hyperloglogs和geospatial indexes等数据类型。它还内建了复制，lua脚本，LRU，事务等功能，通过redis sentinel实现高可用，通过redis cluster实现了自动分片。以及事务，发布/订阅，自动故障转移等等。 Zset、发布订阅、消息队列、Redis的分布式锁、日志系统、消息队列、数据清洗 redis常用5中value类型 String 字符串 session uuid VFS in memory (小文件) 数值 限流器 点击率 统计 bitmap 用户统计 统计用户登陆天数 统计当天活跃用户 权限 (linux 文件权限) 二进制 list 有序放入 同向 栈 异向 队列 数组 评论 分页 抢红包 hash set sorted set 实时排序 References[1] Redis 的各项功能解决了哪些问题？[2] Redis 的各项功能解决了哪些问题？]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>cache</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 锁表后快速解决方法 及 锁表原因]]></title>
    <url>%2F2019%2F09%2F05%2Fmysql-lock-table-solution%2F</url>
    <content type="text"><![CDATA[前几天同事在晚上上线的时候执行sql语句造成锁表，想总结一下以避免后续发生。 (1) 遇到锁表快速解决办法 依次执行1-6步，运行第6步生成的语句即可。 如果特别着急，运行 1 2 6 步 以及第6步生成的kill语句 即可。 1. 第1步 查看表是否在使用。 show open tables where in_use &gt; 0 ;如果查询结果为空。则证明表没有在使用。结束。12mysql&gt; show open tables where in_use &gt; 0 ;Empty set (0.00 sec) 如果查询结果不为空，继续后续的步骤。1234567mysql&gt; show open tables where in_use &gt; 0 ;+----------+-------+--------+-------------+| Database | Table | In_use | Name_locked |+----------+-------+--------+-------------+| test | t | 1 | 0 |+----------+-------+--------+-------------+1 row in set (0.00 sec) 2. 第2步 查看数据库当前的进程，看一下有无正在执行的慢SQL记录线程。 show processlist; show processlist 是显示用户正在运行的线程，需要注意的是，除了 root 用户能看到所有正在运行的线程外，其他用户都只能看到自己正在运行的线程，看不到其它用户正在运行的线程。 SHOW PROCESSLIST shows which threads are running. If you have the PROCESS privilege, you can see all threads. Otherwise, you can see only your own threads (that is, threads associated with the MySQL account that you are using). If you do not use the FULL keyword, only the first 100 characters of each statement are shown in the Info field. 3. 第3步 当前运行的所有事务 SELECT * FROM information_schema.INNODB_TRX; 4. 第4步 当前出现的锁 SELECT * FROM information_schema.INNODB_LOCKs; 5. 第5步 锁等待的对应关系 SELECT * FROM information_schema.INNODB_LOCK_waits; 看事务表INNODB_TRX，里面是否有正在锁定的事务线程，看看ID是否在show processlist里面的sleep线程中，如果是，就证明这个sleep的线程事务一直没有commit或者rollback而是卡住了，我们需要手动kill掉。 搜索的结果是在事务表发现了很多任务，这时候最好都kill掉。 6. 第6步 批量删除事务表中的事务 这里用的方法是：通过information_schema.processlist表中的连接信息生成需要处理掉的MySQL连接的语句临时文件，然后执行临时文件中生成的指令。12345SELECT concat('KILL ',id,';') FROM information_schema.processlist p INNER JOIN information_schema.INNODB_TRX x ON p.id=x.trx_mysql_thread_id WHERE db='test'; 记得修改对应的数据库名。 这个语句执行后结果如下：12345678mysql&gt; SELECT concat('KILL ',id,';') FROM information_schema.processlist p INNER JOIN information_schema.INNODB_TRX x ON p.id=x.trx_mysql_thread_id WHERE db='test';+------------------------+| concat('KILL ',id,';') |+------------------------+| KILL 42; || KILL 40; |+------------------------+2 rows in set (0.00 sec) 执行结果里的两个kill语句即可解决锁表。 首先问几个问题： MySQL里有哪些锁？ 如何造成锁表？ 如何造成死锁？ 全局锁加锁方法的执行命令是什么?主要的应用场景是什么? 做整库备份时为什么要加全局锁? MySQL的自带备份工具, 使用什么参数可以确保一致性视图, 在什么场景下不适用? 不建议使用set global readonly = true的方法加全局锁有哪两点原因? 表级锁有哪两种类型? 各自的使用场景是什么? MDL中读写锁之间的互斥关系怎样的? 如何安全的给小表增加字段? (2) 复盘 自己创建了一个测试的表t，插入了两条数据。然后手动构造锁表和死锁模拟。 测试的时候用的root用户测试 show processlist 是显示用户正在运行的线程，需要注意的是，除了 root 用户能看到所有正在运行的线程外，其他用户都只能看到自己正在运行的线程，看不到其它用户正在运行的线程。 SHOW PROCESSLIST shows which threads are running. If you have the PROCESS privilege, you can see all threads. Otherwise, you can see only your own threads (that is, threads associated with the MySQL account that you are using). If you do not use the FULL keyword, only the first 100 characters of each statement are shown in the Info field. (2.1) 创建表t并插入2条数据12345CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB; 1234INSERT INTO `t` (id, c)VALUES (1, 1),(2, 1); (2.2) 准备多个shell模拟锁表可以看到我打开三个shell，用root创建了三个连接，分别是 15 40 41 12345678910111213141516171819202122232425262728293031323334353637383940414243mysql&gt; show processlist ;+----+-------------+-----------------+------+---------+------+--------------------------+------------------+----------+| Id | User | Host | db | Command | Time | State | Info | Progress |+----+-------------+-----------------+------+---------+------+--------------------------+------------------+----------+| 1 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 2 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 3 | system user | | NULL | Daemon | NULL | InnoDB purge coordinator | NULL | 0.000 || 4 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 5 | system user | | NULL | Daemon | NULL | InnoDB shutdown handler | NULL | 0.000 || 15 | root | localhost:49914 | NULL | Query | 0 | Init | show processlist | 0.000 |+----+-------------+-----------------+------+---------+------+--------------------------+------------------+----------+6 rows in set (0.00 sec)mysql&gt;mysql&gt; show processlist ;+----+-------------+-----------------+------+---------+------+--------------------------+------------------+----------+| Id | User | Host | db | Command | Time | State | Info | Progress |+----+-------------+-----------------+------+---------+------+--------------------------+------------------+----------+| 1 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 2 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 3 | system user | | NULL | Daemon | NULL | InnoDB purge coordinator | NULL | 0.000 || 4 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 5 | system user | | NULL | Daemon | NULL | InnoDB shutdown handler | NULL | 0.000 || 15 | root | localhost:49914 | NULL | Query | 0 | Init | show processlist | 0.000 || 40 | root | localhost:50872 | test | Sleep | 41 | | NULL | 0.000 |+----+-------------+-----------------+------+---------+------+--------------------------+------------------+----------+7 rows in set (0.00 sec)mysql&gt;mysql&gt; show processlist ;+----+-------------+-----------------+------+---------+------+--------------------------+------------------+----------+| Id | User | Host | db | Command | Time | State | Info | Progress |+----+-------------+-----------------+------+---------+------+--------------------------+------------------+----------+| 1 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 2 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 3 | system user | | NULL | Daemon | NULL | InnoDB purge coordinator | NULL | 0.000 || 4 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 5 | system user | | NULL | Daemon | NULL | InnoDB shutdown handler | NULL | 0.000 || 15 | root | localhost:49914 | NULL | Query | 0 | Init | show processlist | 0.000 || 40 | root | localhost:50872 | test | Sleep | 64 | | NULL | 0.000 || 41 | root | localhost:50888 | test | Sleep | 5 | | NULL | 0.000 |+----+-------------+-----------------+------+---------+------+--------------------------+------------------+----------+8 rows in set (0.00 sec) (2.3) 模拟锁表 在第一个shell里观察 在第二个shell里执行 start transaction; delete from t where c=1 ; 故意打开事务，然后执行语句不提交，占用写锁。 在第三个shell里执行 delete from t where c=1 ; 执行删除语句，造成锁表。 这个时候 session3在等待session2释放写锁。这个时候已经锁表了。 如果再在 第三个shell里执行 delete from t where c=2 ; 在 第二个shell里执行 delete from t where c=1 ; 就会相互等待，造成死锁。 (2.4) 锁表后查看 然后在第一个shell里查看 (2.4.1)查看是否锁表 可以看到下面的查询语句有结果，确实是锁表了。1234567mysql&gt; show open tables where in_use &gt; 0 ;+----------+-------+--------+-------------+| Database | Table | In_use | Name_locked |+----------+-------+--------+-------------+| test | t | 1 | 0 |+----------+-------+--------+-------------+1 row in set (0.00 sec) (2.4.2) 查看数据库当前的进程1234567891011121314mysql&gt; show processlist ;+----+-------------+-----------------+------+---------+------+--------------------------+-------------------------+----------+| Id | User | Host | db | Command | Time | State | Info | Progress |+----+-------------+-----------------+------+---------+------+--------------------------+-------------------------+----------+| 1 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 2 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 3 | system user | | NULL | Daemon | NULL | InnoDB purge coordinator | NULL | 0.000 || 4 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 5 | system user | | NULL | Daemon | NULL | InnoDB shutdown handler | NULL | 0.000 || 15 | root | localhost:49914 | NULL | Query | 0 | Init | show processlist | 0.000 || 40 | root | localhost:50872 | test | Sleep | 15 | | NULL | 0.000 || 41 | root | localhost:50888 | test | Query | 11 | Updating | delete from t where c=1 | 0.000 |+----+-------------+-----------------+------+---------+------+--------------------------+-------------------------+----------+8 rows in set (0.00 sec) (2.4.3) 当前运行的所有事务1234567891011mysql&gt; SELECT * FROM information_schema.INNODB_TRX;+--------+-----------+---------------------+-----------------------+---------------------+------------+---------------------+-------------------------+---------------------+-------------------+-------------------+------------------+-----------------------+-----------------+-------------------+-------------------------+---------------------+-------------------+------------------------+----------------------------+------------------+----------------------------+| trx_id | trx_state | trx_started | trx_requested_lock_id | trx_wait_started | trx_weight | trx_mysql_thread_id | trx_query | trx_operation_state | trx_tables_in_use | trx_tables_locked | trx_lock_structs | trx_lock_memory_bytes | trx_rows_locked | trx_rows_modified | trx_concurrency_tickets | trx_isolation_level | trx_unique_checks | trx_foreign_key_checks | trx_last_foreign_key_error | trx_is_read_only | trx_autocommit_non_locking |+--------+-----------+---------------------+-----------------------+---------------------+------------+---------------------+-------------------------+---------------------+-------------------+-------------------+------------------+-----------------------+-----------------+-------------------+-------------------------+---------------------+-------------------+------------------------+----------------------------+------------------+----------------------------+| 23312 | LOCK WAIT | 2019-09-05 23:16:18 | 23312:78:3:2 | 2019-09-05 23:16:18 | 2 | 41 | delete from t where c=1 | starting index read | 1 | 1 | 2 | 1136 | 1 | 0 | 0 | REPEATABLE READ | 1 | 1 | NULL | 0 | 0 || 23311 | RUNNING | 2019-09-05 23:16:13 | NULL | NULL | 3 | 40 | NULL | NULL | 0 | 1 | 2 | 1136 | 3 | 1 | 0 | REPEATABLE READ | 1 | 1 | NULL | 0 | 0 |+--------+-----------+---------------------+-----------------------+---------------------+------------+---------------------+-------------------------+---------------------+-------------------+-------------------+------------------+-----------------------+-----------------+-------------------+-------------------------+---------------------+-------------------+------------------------+----------------------------+------------------+----------------------------+2 rows in set (0.00 sec) (2.4.4) 当前出现的锁12345678mysql&gt; SELECT * FROM information_schema.INNODB_LOCKs;+--------------+-------------+-----------+-----------+------------+------------+------------+-----------+----------+-----------+| lock_id | lock_trx_id | lock_mode | lock_type | lock_table | lock_index | lock_space | lock_page | lock_rec | lock_data |+--------------+-------------+-----------+-----------+------------+------------+------------+-----------+----------+-----------+| 23312:78:3:2 | 23312 | X | RECORD | `test`.`t` | PRIMARY | 78 | 3 | 2 | 1 || 23311:78:3:2 | 23311 | X | RECORD | `test`.`t` | PRIMARY | 78 | 3 | 2 | 1 |+--------------+-------------+-----------+-----------+------------+------------+------------+-----------+----------+-----------+2 rows in set (0.00 sec) (2.4.5) 锁等待的对应关系1234567mysql&gt; SELECT * FROM information_schema.INNODB_LOCK_waits;+-------------------+-------------------+-----------------+------------------+| requesting_trx_id | requested_lock_id | blocking_trx_id | blocking_lock_id |+-------------------+-------------------+-----------------+------------------+| 23312 | 23312:78:3:2 | 23311 | 23311:78:3:2 |+-------------------+-------------------+-----------------+------------------+1 row in set (0.00 sec) (2.4.6) 删除事务表中的事务1234567mysql&gt; SELECT p.id, p.time, i.trx_id, i.trx_state, p.info FROM INFORMATION_SCHEMA.PROCESSLIST p, INFORMATION_SCHEMA.INNODB_TRX i WHERE p.id = i.trx_mysql_thread_id AND i.trx_state = 'LOCK WAIT';+----+------+--------+-----------+-------------------------+| id | time | trx_id | trx_state | info |+----+------+--------+-----------+-------------------------+| 41 | 27 | 23312 | LOCK WAIT | delete from t where c=1 |+----+------+--------+-----------+-------------------------+1 row in set (0.01 sec) (2.4.7) kill掉锁表的语句 这儿有两种观点，一种是只kill掉后面等待的那个语句。还有一种是把两个语句都kill掉。这个根据实际情况处理。 12345mysql&gt; kill 41 ;Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT p.id, p.time, i.trx_id, i.trx_state, p.info FROM INFORMATION_SCHEMA.PROCESSLIST p, INFORMATION_SCHEMA.INNODB_TRX i WHERE p.id = i.trx_mysql_thread_id AND i.trx_state = 'LOCK WAIT';Empty set (0.01 sec) 杀掉41 12345678910111213mysql&gt; show processlist ;+----+-------------+-----------------+------+---------+------+--------------------------+------------------+----------+| Id | User | Host | db | Command | Time | State | Info | Progress |+----+-------------+-----------------+------+---------+------+--------------------------+------------------+----------+| 1 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 2 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 3 | system user | | NULL | Daemon | NULL | InnoDB purge coordinator | NULL | 0.000 || 4 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 5 | system user | | NULL | Daemon | NULL | InnoDB shutdown handler | NULL | 0.000 || 15 | root | localhost:49914 | NULL | Query | 0 | Init | show processlist | 0.000 || 40 | root | localhost:50872 | test | Sleep | 56 | | NULL | 0.000 |+----+-------------+-----------------+------+---------+------+--------------------------+------------------+----------+7 rows in set (0.00 sec) 然后到第3个shell窗口查看，可以看到12mysql&gt; delete from t where c=1 ;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction 因为第3个shell里执行的语句被kill掉了。 到这儿可以看到死锁解决了。 但其实有个问题。第3个shell里的语句被kill掉了。但第2个shell里的语句还在执行。如果第二个shell里的事务不提交或者kill，在第3个shell里执行删除语句还会造成锁表。 第二种观点的办法1234567891011SELECT p.id, p.time, x.trx_id, x.trx_state, p.info FROM INFORMATION_SCHEMA.PROCESSLIST p, INFORMATION_SCHEMA.INNODB_TRX x WHERE p.id = x.trx_mysql_thread_id ; 123456789mysql&gt; SELECT p.id, p.time, x.trx_id, x.trx_state, p.info FROM INFORMATION_SCHEMA.PROCESSLIST p, INFORMATION_SCHEMA.INNODB_TRX x WHEREp.id = x.trx_mysql_thread_id ;+----+------+--------+-----------+-------------------------+| id | time | trx_id | trx_state | info |+----+------+--------+-----------+-------------------------+| 42 | 3 | 23317 | LOCK WAIT | delete from t where c=1 || 40 | 1792 | 23311 | RUNNING | NULL |+----+------+--------+-----------+-------------------------+2 rows in set (0.01 sec) 然后同时杀掉 40 42 就可以。 (3) MySQL中的锁 数据库锁设计的初衷是处理并发问题。作为多用户共享的资源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访问规则的重要数据结构。根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。 (3.1) 全局锁 全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 全局锁的典型使用场景是，做全库逻辑备份。 也就是把整库每个表都 select 出来存成文本。 风险： 1.如果在主库备份，在备份期间不能更新，业务停摆 2.如果在从库备份，备份期间不能执行主库同步的binlog，导致主从延迟 官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。 你一定在疑惑，有了mysqldump这个功能，为什么还需要 FTWRL 呢？一致性读是好，但前提是引擎要支持这个隔离级别。比如，对于 MyISAM 这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用 FTWRL 命令了。 比如，对于 MyISAM 这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用 FTWRL 命令了。 single-transaction 方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法。这往往是 DBA 要求业务开发人员使用 InnoDB 替代 MyISAM 的原因之一。 全局锁主要用在逻辑备份过程中。对于全部是 InnoDB 引擎的库，建议你选择使用 –single-transaction 参数，对应用会更友好。 (3.2) 表级锁 MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。 表锁是在Server层实现的。ALTER TABLE之类的语句会使用表锁，忽略存储引擎的锁机制。 表锁的语法是 lock tables … read/write。 与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。 举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。 在还没有出现更细粒度的锁的时候，表锁是最常用的处理并发的方式。而对于 InnoDB 这种支持行锁的引擎，一般不使用 lock tables 命令来控制并发，毕竟锁住整个表的影响面还是太大。 另一类表级的锁是 MDL（metadata lock)。 MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。 因此，在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 你现在应该知道了，事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。 给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。在对大表操作的时候，你肯定会特别小心，以免对线上服务造成影响。而实际上，即使是小表，操作不慎也会出问题。在修改表的时候会持有MDL写锁，如果这个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。 如何安全地给表加字段？ 首先我们要解决长事务，事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。 这时候 kill 可能未必管用，因为新的请求马上就来了。比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程。 MariaDB 已经合并了 AliSQL 的这个功能，所以这两个开源分支目前都支持 DDL NOWAIT/WAIT n 这个语法。12ALTER TABLE tbl_name NOWAIT add column ...ALTER TABLE tbl_name WAIT N add column ... MDL 是并发情况下维护数据的一致性,在表上有事务的时候,不可以对元数据经行写入操作,并且这个是在server层面实现的 当你做 dml 时候增加的 MDL 读锁, update table set id=Y where id=X; 并且由于隔离级别的原因 读锁之间不冲突 当你DDL 时候 增加对表的写锁, 同时操作两个alter table 操作 这个要出现等待情况。 但是 如果是 dml 与ddl 之间的交互 就更容易出现不可读写情况,这个情况容易session 爆满,session是占用内存的,也会导致内存升高 MDL 释放的情况就是 事务提交. (3.3) 行锁 MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的，这也是 MyISAM 被 InnoDB 替代的重要原因之一。 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。 知道了这个设定，对我们使用事务有什么帮助呢？那就是，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。 假设你负责实现一个电影票在线交易业务，顾客 A 要在影院 B 购买电影票。我们简化一点，这个业务需要涉及到以下操作： 从顾客 A 账户余额中扣除电影票价； 给影院 B 的账户余额增加这张电影票价； 记录一条交易日志。 试想如果同时有另外一个顾客 C 要在影院 B 买票，那么这两个事务冲突的部分就是语句 2 了。因为它们要更新同一个影院账户的余额，需要修改同一行数据。根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你把语句 2 安排在最后，比如按照 3、1、2 这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。 当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。这里我用数据库中的行锁举个例子。 当出现死锁以后，有两种策略： 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。 在 InnoDB 中，innodb_lock_wait_timeout 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。 你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。 (4) 可能遇到的问题(4.1) 备份一般都会在备库上执行，你在用–single-transaction 方法做逻辑备份的过程中，如果主库上的一个小表做了一个 DDL，比如给一个表上加了一列。这时候，从备库上会看到什么现象呢？ 备份一般都会在备库上执行，你在用–single-transaction 方法做逻辑备份的过程中，如果主库上的一个小表做了一个 DDL，比如给一个表上加了一列。这时候，从备库上会看到什么现象呢？ 假设这个 DDL 是针对表 t1 的， 这里我把备份过程中几个关键的语句列出来：123456789101112Q1:SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ;Q2:START TRANSACTION WITH CONSISTENT SNAPSHOT；/* other tables */Q3:SAVEPOINT sp;/* 时刻 1 */Q4:show create table `t1`;/* 时刻 2 */Q5:SELECT * FROM `t1`;/* 时刻 3 */Q6:ROLLBACK TO SAVEPOINT sp;/* 时刻 4 *//* other tables */ 在备份开始的时候，为了确保 RR（可重复读）隔离级别，再设置一次 RR 隔离级别 (Q1);启动事务，这里用 WITH CONSISTENT SNAPSHOT 确保这个语句执行完就可以得到一个一致性视图（Q2)；设置一个保存点，这个很重要（Q3）；show create 是为了拿到表结构 (Q4)，然后正式导数据 （Q5），回滚到 SAVEPOINT sp，在这里的作用是释放 t1 的 MDL 锁 （Q6）。当然这部分属于“超纲”，上文正文里面都没提到。DDL 从主库传过来的时间按照效果不同，我打了四个时刻。题目设定为小表，我们假定到达后，如果开始执行，则很快能够执行完成。 参考答案如下： 如果在 Q4 语句执行之前到达，现象：没有影响，备份拿到的是 DDL 后的表结构。 如果在“时刻 2”到达，则表结构被改过，Q5 执行的时候，报 Table definition has changed, please retry transaction，现象：mysqldump 终止； 如果在“时刻 2”和“时刻 3”之间到达，mysqldump 占着 t1 的 MDL 读锁，binlog 被阻塞，现象：主从延迟，直到 Q6 执行完成。 从“时刻 4”开始，mysqldump 释放了 MDL 读锁，现象：没有影响，备份拿到的是 DDL 前的表结构。 (4.2) 删数据问题 如果你要删除一个表里面的前 10000 行数据，有以下三种方法可以做到： 第一种，直接执行 delete from T limit 10000; 第二种，在一个连接中循环执行 20 次 delete from T limit 500; 第三种，在 20 个连接中同时执行 delete from T limit 500。 你会选择哪一种方法呢？为什么呢？ 方案一，事务相对较长，则占用锁的时间较长，会导致其他客户端等待资源时间较长。 方案二，串行化执行，将相对长的事务分成多次相对短的事务，则每次事务占用锁的时间相对较短，其他客户端在等待相应资源的时间也较短。这样的操作，同时也意味着将资源分片使用（每次执行使用不同片段的资源），可以提高并发性。 方案三，人为自己制造锁竞争，加剧并发量。 (4.3) 问题31.如何在死锁发生时,就把发生的sql语句抓出来？2.在使用连接池的情况下,连接会复用.比如一个业务使用连接set sql_select_limit=1,释放掉以后.其他业务复用该连接时,这个参数也生效.请问怎么避免这种情况,或者怎么禁止业务set session？3.很好奇双11的成交额,是通过redis累加的嘛？4.不会改源码能成为专家嘛？ show engine innodb status 里面有信息，不过不是很全… 5.7的reset_connection接口可以考虑一下 用redis的话，为了避免超卖需要增加了很多机制来保证。修改都在数据库里执行就方便点。前提是要解决热点问题 我认识几位处理问题和分析问题经验非常丰富的专家，不用懂源码，但是原理还是要很清楚的 (4.4) 转义导致死锁问题前天在开发中，还遇到过一次死锁，是在一个批处理中，要删除1000条数据，5个线程，200条数据commit一次，sol：delete from 表A where id =15426169754750004759008 STORAGEDB(id是主键)我同事解决了，说原因是id 是char 类型，但是没有加单引号，所以没有进入id索引中，然后锁表了，所以导致死锁。 这个问题的出现，应该是人为只要并发导致锁冲突吧？但是为什么不加单引号会死锁，加了单引号就能正常跑呢？ References[1] Mysql 锁、事务强制手动kill/释放[2] 19 | 为什么我只查一行的语句，也执行这么慢？MySQL实战45讲 [3] 06 | 全局锁和表锁 ：给表加个字段怎么有这么多阻碍？MySQL实战45讲 [4] 07 | 行锁功过：怎么减少行锁对性能的影响？MySQL实战45讲[5] mysql 5.7 lock-tables[6] 《高性能MySQL》 O’REILLY[7] mysql-show-open-tables[8] mysql-show-processlist]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java核心技术36讲 杨晓峰]]></title>
    <url>%2F2019%2F08%2F18%2Fjava-core-technology-36-lectures%2F</url>
    <content type="text"><![CDATA[转自 Java核心技术36讲 杨晓峰 第1讲 | 谈谈你对Java平台的理解？ 模块一 Java基础 (14讲) 第1讲 | 谈谈你对Java平台的理解？ 谈谈你对 Java 平台的理解？“Java 是解释执行”，这句话正确吗？ Java 本身是一种面向对象的语言，最显著的特性有两个方面，一是所谓的“书写一次，到处运行”（Write once, run anywhere），能够非常容易地获得跨平台能力；另外就是 垃圾收集 （GC, Garbage Collection），Java 通过垃圾收集器（Garbage Collector）回收分配内存，大部分情况下，程序员不需要自己操心内存的分配和回收。 我们日常会接触到 JRE（Java Runtime Environment）或者 JDK（Java Development Kit）。 JRE，也就是 Java 运行环境，包含了 JVM 和 Java 类库，以及一些模块等。而 JDK 可以看作是 JRE 的一个超集，提供了更多工具，比如编译器、各种诊断工具等。 对于“Java 是解释执行”这句话，这个说法不太准确。我们开发的 Java 的源代码，首先通过 Javac 编译成为字节码（bytecode），然后，在运行时，通过 Java 虚拟机（JVM）内嵌的解释器将字节码转换成为最终的机器码。但是常见的 JVM，比如我们大多数情况使用的 Oracle JDK 提供的 Hotspot JVM，都提供了 JIT（Just-In-Time）编译器，也就是通常所说的动态编译器，JIT 能够在运行时将热点代码编译成机器码，这种情况下部分热点代码就属于 对于 Java 平台的理解，可以从很多方面简明扼要地谈一下，例如：Java 语言特性，包括泛型、Lambda 等语言特性；基础类库，包括集合、IO/NIO、网络、并发、安全等基础类库。对于我们日常工作应用较多的类库，面试前可以系统化总结一下，有助于临场发挥。 或者谈谈 JVM 的一些基础概念和机制，比如 Java 的类加载机制，常用版本 JDK（如 JDK 8）内嵌的 Class-Loader，例如 Bootstrap、 Application 和 Extension Class-loader；类加载大致过程：加载、验证、链接、初始化（这里参考了周志明的《深入理解 Java 虚拟机》，非常棒的 JVM 上手书籍）；自定义 Class-Loader 等。还有垃圾收集的基本原理，最常见的垃圾收集器，如 SerialGC、Parallel GC、 CMS、 G1 等，对于适用于什么样的工作负载最好也心里有数。这些都是可以扩展开的领域，我会在后面的专栏对此进行更系统的介绍。 当然还有 JDK 包含哪些工具或者 Java 领域内其他工具等，如编译器、运行时环境、安全工具、诊断和监控工具等。这些基本工具是日常工作效率的保证，对于我们工作在其他语言平台上，同样有所帮助，很多都是触类旁通的。 Java特性: 面向对象（封装，继承，多态） 平台无关性（JVM运行.class文件） 语言（泛型，Lambda） 类库（集合，并发，网络，IO/NIO） JRE（Java运行环境，JVM，类库） JDK（Java开发工具，包括JRE，javac，诊断工具） Java是解析运行吗？ 不正确！ 1，Java源代码经过Javac编译成.class文件 2，.class文件经JVM解析或编译运行。 （1）解析:.class文件经过JVM内嵌的解析器解析执行。 （2）编译:存在JIT编译器（Just In Time Compile 即时编译器）把经常运行的代码作为”热点代码”编译与本地平台相关的机器码，并进行各种层次的优化。 （3）AOT编译器: Java 9提供的直接将所有代码编译成机器码执行。 Java平台中有两大核心： Java语言本身、JDK中所提供的核心类库和相关工具 Java虚拟机以及其他包含的GC Java语言本身、JDK中所提供的核心类库和相关工具从事Java平台的开发，掌握Java语言、核心类库以及相关工具是必须的，我觉得这是基础中的基础。 对语言本身的了解，需要开发者非常熟悉语言的语法结构；而Java又是一种面对对象的语言，这又需要开发者深入了解面对对象的设计理念；Java核心类库包含集合类、线程相关类、IO、NIO、J.U.C并发包等；JDK提供的工具包含：基本的编译工具、虚拟机性能检测相关工具等。 Java虚拟机Java语言具有跨平台的特性，也正是因为虚拟机的存在。Java源文件被编译成字节码，被虚拟机加载后执行。这里隐含的意思有两层：1）大部分情况下，编程者只需要关心Java语言本身，而无需特意关心底层细节。包括对内存的分配和回收，也全权交给了GC。2）对于虚拟机而言，只要是符合规范的字节码，它们都能被加载执行，当然，能正常运行的程序光满足这点是不行的，程序本身需要保证在运行时不出现异常。所以，Scala、Kotlin、Jython等语言也可以跑在虚拟机上。 围绕虚拟机的效率问题展开，将涉及到一些优化技术，例如：JIT、AOT。因为如果虚拟机加载字节码后，完全进行解释执行，这势必会影响执行效率。所以，对于这个运行环节，虚拟机会进行一些优化处理，例如JIT技术，会将某些运行特别频繁的代码编译成机器码。而AOT技术，是在运行前，通过工具直接将字节码转换为机器码。 1，JVM的内存模型，堆、栈、方法区；字节码的跨平台性；对象在JVM中的强引用，弱引用，软引用，虚引用，是否可用finalise方法救救它？；双亲委派进行类加载，什么是双亲呢？双亲就是多亲，一份文档由我加载，然后你也加载，这份文档在JVM中是一样的吗？；多态思想是Java需要最核心的概念，也是面向对象的行为的一个最好诠释；理解方法重载与重写在内存中的执行流程，怎么定位到这个具体方法的。2，发展流程，JDK5(重写bug)，JDK6(商用最稳定版)，JDK7(switch的字符串支持)，JDK8(函数式编程)，一直在发展进化。3，理解祖先类Object，它的行为是怎样与现实生活连接起来的。4，理解23种设计模式，因为它是道与术的结合体。 解释执行和编译执行有何区别 一个是同声传译，一个是放录音 这种基于运行分析，进行热点代码编译的设计，是因为绝大多数的程序都表现为“小部分的热点耗费了大多数的资源”。只有这样才能做到，在某些场景下，一个需要跑在运行时上的语言，可以比直接编译成机器码的语言更“快” Java平台包括java语言，class文件结构，jvm，api类库，第三方库，各种编译、监控和诊断工具等。Java语言是一种面向对象的高级语言；通过平台中立的class文件格式和屏蔽底层硬件差异的jvm实现‘一次编写，到处运行’；通过‘垃圾收集器’管理内存的分配和回收。jvm通过使用class文件这种中间表示和具体语言解耦，使得任何在源码早期编译过程中以class文件为中间表示或者能够转换成class文件的具体语言，都能运行jvm之上，也就可以使用jvm的各种特性。api类库主要包含集合、IO/NIO、网络、并发等。第三方库包括各种商业机构和开源社区的java库，如spring、mybatis等。各种工具如javac、jconsole、jmap、jstack等。 第2讲 | Exception和Error有什么区别？ 模块一 Java基础 (14讲) 第2讲 | Exception和Error有什么区别？ 请对比 Exception 和 Error，另外，运行时异常与一般异常有什么区别？ Exception 和 Error 都是继承了 Throwable 类，在 Java 中只有 Throwable 类型的实例才可以被抛出（throw）或者捕获（catch），它是异常处理机制的基本组成类型。 Exception 和 Error 体现了 Java 平台设计者对不同异常情况的分类。Exception 是程序正常运行中，可以预料的意外情况，可能并且应该被捕获，进行相应处理。 Error 是指在正常情况下，不大可能出现的情况，绝大部分的 Error 都会导致程序（比如 JVM 自身）处于非正常的、不可恢复状态。既然是非正常情况，所以不便于也不需要捕获，常见的比如 OutOfMemoryError 之类，都是 Error 的子类。 Exception 又分为可检查（checked）异常和不检查（unchecked）异常。 可检查异常在源代码里必须显式地进行捕获处理，这是编译期检查的一部分。前面不可查的 Error，是 Throwable 不是 Exception。 不检查异常就是所谓的运行时异常，类似 NullPointerException、ArrayIndexOutOfBoundsException 之类，通常是可以编码避免的逻辑错误，具体根据需要来判断是否需要捕获，并不会在编译期强制要求。 异常处理的两个基本原则 尽量不要捕获类似 Exception 这样的通用异常，而是应该捕获特定异常。 (不方便维护) 不要生吞（swallow）异常。 (不方便排查问题)(很可能会导致非常难以诊断的诡异情况。)异常以及异常的堆栈信息输出到日志里。 一课一练 对于异常处理编程，不同的编程范式也会影响到异常处理策略，比如，现在非常火热的反应式编程（Reactive Stream），因为其本身是异步、基于事件机制的，所以出现异常情况，决不能简单抛出去；另外，由于代码堆栈不再是同步调用那种垂直的结构，这里的异常处理和日志需要更加小心，我们看到的往往是特定 executor 的堆栈，而不是业务方法调用关系。对于这种情况，你有什么好的办法吗？ 评论 毛毛熊 NoClassDefFoundError 和 ClassNotFoundException 有什么区别？ NoClassDefFoundError是一个错误(Error)，而ClassNOtFoundException是一个异常，在Java中对于错误和异常的处理是不同的，我们可以从异常中恢复程序但却不应该尝试从错误中恢复程序。 ClassNotFoundException 的产生原因： (1).Java支持使用Class.forName方法来动态地加载类，任意一个类的类名如果被作为参数传递给这个方法都将导致该类被加载到JVM内存中，如果这个类在类路径中没有被找到，那么此时就会在运行时抛出ClassNotFoundException异常。 (2).Java支持使用反射方式在运行时动态加载类，例如使用Class.forName方法来动态地加载类时，可以将类名作为参数传递给上述方法从而将指定类加载到JVM内存中，如果这个类在类路径中没有被找到，那么此时就会在运行时抛出ClassNotFoundException异常。解决该问题需要确保所需的类连同它依赖的包存在于类路径中，常见问题在于类名书写错误。 (3).当一个类已经某个类加载器加载到内存中了，此时另一个类加载器又尝试着动态地从同一个包中加载这个类。通过控制动态类加载过程，可以避免上述情况发生。 NoClassDefFoundError产生的原因在于： 如果JVM或者ClassLoader实例尝试加载（可以通过正常的方法调用，也可能是使用new来创建新的对象）类的时候却找不到类的定义。要查找的类在编译的时候是存在的，运行的时候却找不到了。这个时候就会导致NoClassDefFoundError. 造成该问题的原因可能是打包过程漏掉了部分类，或者jar包出现损坏或者篡改。解决这个问题的办法是查找那些在开发期间存在于类路径下但在运行期间却不在类路径下的类。 评论 公号-代码荣耀在Java世界里，异常的出现让我们编写的程序运行起来更加的健壮，同时为程序在调试、运行期间发生的一些意外情况，提供了补救机会；即使遇到一些严重错误而无法弥补，异常也会非常忠实的记录所发生的这一切。以下是文章心得感悟: 1 不要推诿或延迟处理异常，就地解决最好，并且需要实实在在的进行处理，而不是只捕捉，不动作。 2 一个函数尽管抛出了多个异常，但是只有一个异常可被传播到调用端。最后被抛出的异常时唯一被调用端接收的异常，其他异常都会被吞没掩盖。如果调用端要知道造成失败的最初原因，程序之中就绝不能掩盖任何异常。 3 不要在finally代码块中处理返回值。 4 按照我们程序员的惯性认知：当遇到return语句的时候，执行函数会立刻返回。但是，在Java语言中，如果存在finally就会有例外。除了return语句，try代码块中的break或continue语句也可能使控制权进入finally代码块。 5 请勿在try代码块中调用return、break或continue语句。万一无法避免，一定要确保finally的存在不会改变函数的返回值。 6 函数返回值有两种类型：值类型与对象引用。对于对象引用，要特别小心，如果在finally代码块中对函数返回的对象成员属性进行了修改，即使不在finally块中显式调用return语句，这个修改也会作用于返回值上。 7 勿将异常用于控制流。 8 如无必要，勿用异常。 评论 adrian-jser 假如你开车上山，车坏了，你拿出工具箱修一修，修好继续上路（Exception被捕获，从异常中恢复，继续程序的运行），车坏了，你不知道怎么修，打电话告诉修车行，告诉你是什么问题，要车行过来修。（在当前的逻辑背景下，你不知道是怎么样的处理逻辑，把异常抛出去到更高的业务层来处理）。你打电话的时候，要尽量具体，不能只说我车动不了了。那修车行很难定位你的问题。（要补货特定的异常，不能捕获类似Exception的通用异常）。还有一种情况是，你开车上山，山塌了，这你还能修吗？（Error：导致你的运行环境进入不正常的状态，很难恢复）思考问题：由于代码堆栈不再是同步调用那种垂直的结构，这里的异常处理和日志需要更加小心，我们看到的往往是特定 executor 的堆栈，而不是业务方法调用关系。对于这种情况，你有什么好的办法吗？业务功能模块分配ID，在记录日志是将前后模块的ID进行调用关系的串联，一并跟任务信息，进行日志保存。 第3讲 | 谈谈final、finally、 finalize有什么不同？ 模块一 Java基础 (14讲) 第3讲 | 谈谈final、finally、 finalize有什么不同？ 谈谈 final、finally、 finalize 有什么不同？ final 可以用来修饰类、方法、变量，分别有不同的意义，final 修饰的 class 代表不可以继承扩展，final 的变量是不可以修改的，而 final 的方法也是不可以重写的（override）。 finally 则是 Java 保证重点代码一定要被执行的一种机制。我们可以使用 try-finally 或者 try-catch-finally 来进行类似关闭 JDBC 连接、保证 unlock 锁等动作。 finalize 是基础类 java.lang.Object 的一个方法，它的设计目的是保证对象在被垃圾收集前完成特定资源的回收。finalize 机制现在已经不推荐使用，并且在 JDK 9 开始被标记为 deprecated。 注意，final 不是 immutable！ final 只能约束对象引用不可以被赋值，但是对象行为不被 final 影响。添加元素等操作是完全正常的。如果我们真的希望对象本身是不可变的，那么需要相应的类支持不可变的行为。在上面这个例子中， 如果我们真的希望对象本身是不可变的，那么需要相应的类支持不可变的行为。 第4讲 | 强引用、软引用、弱引用、幻象引用有什么区别？ 模块一 Java基础 (14讲) 第4讲 | 强引用、软引用、弱引用、幻象引用有什么区别？ 强可达（Strongly Reachable），就是当一个对象可以有一个或多个线程可以不通过各种引用访问到的情况。比如，我们新创建一个对象，那么创建它的线程对它就是强可达。 软可达（Softly Reachable），就是当我们只能通过软引用才能访问到对象的状态。 弱可达（Weakly Reachable），类似前面提到的，就是无法通过强引用或者软引用访问，只能通过弱引用访问时的状态。这是十分临近 finalize 状态的时机，当弱引用被清除的时候，就符合 finalize 的条件了。 幻象可达（Phantom Reachable），上面流程图已经很直观了，就是没有强、软、弱引用关联，并且 finalize 过了，只有幻象引用指向这个对象的时候。 当然，还有一个最后的状态，就是不可达（unreachable），意味着对象可以被清除了。 评论Jerry银银 强引用：项目中到处都是。 软引用：图片缓存框架中，“内存缓存”中的图片是以这种引用来保存，使得JVM在发生OOM之前，可以回收这部分缓存 虚引用：在静态内部类中，经常会使用虚引用。例如，一个类发送网络请求，承担callback的静态内部类，则常以虚引用的方式来保存外部类(宿主类)的引用，当外部类需要被JVM回收时，不会因为网络请求没有及时回来，导致外部类不能被回收，引起内存泄漏 幽灵引用：这种引用的get()方法返回总是null，所以，可以想象，在平常的项目开发肯定用的少。但是根据这种引用的特点，我想可以通过监控这类引用，来进行一些垃圾清理的动作。不过具体的场景，还是希望峰哥举几个稍微详细的实战性的例子？ 公号-代码荣耀在Java语言中，除了基本数据类型外，其他的都是指向各类对象的对象引用；Java中根据其生命周期的长短，将引用分为4类。 1 强引用 特点：我们平常典型编码Object obj = new Object()中的obj就是强引用。通过关键字new创建的对象所关联的引用就是强引用。 当JVM内存空间不足，JVM宁愿抛出OutOfMemoryError运行时错误（OOM），使程序异常终止，也不会靠随意回收具有强引用的“存活”对象来解决内存不足的问题。对于一个普通的对象，如果没有其他的引用关系，只要超过了引用的作用域或者显式地将相应（强）引用赋值为 null，就是可以被垃圾收集的了，具体回收时机还是要看垃圾收集策略。 2 软引用 特点：软引用通过SoftReference类实现。 软引用的生命周期比强引用短一些。只有当 JVM 认为内存不足时，才会去试图回收软引用指向的对象：即JVM 会确保在抛出 OutOfMemoryError 之前，清理软引用指向的对象。软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收器回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中。后续，我们可以调用ReferenceQueue的poll()方法来检查是否有它所关心的对象被回收。如果队列为空，将返回一个null,否则该方法返回队列中前面的一个Reference对象。 应用场景：软引用通常用来实现内存敏感的缓存。如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。 3 弱引用 弱引用通过WeakReference类实现。 弱引用的生命周期比软引用短。在垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现了具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。由于垃圾回收器是一个优先级很低的线程，因此不一定会很快回收弱引用的对象。弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。 应用场景：弱应用同样可用于内存敏感的缓存。 4 虚引用 特点：虚引用也叫幻象引用，通过PhantomReference类来实现。无法通过虚引用访问对象的任何属性或函数。幻象引用仅仅是提供了一种确保对象被 finalize 以后，做某些事情的机制。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。虚引用必须和引用队列 （ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。ReferenceQueue queue = new ReferenceQueue ();PhantomReference pr = new PhantomReference (object, queue);程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将要被垃圾回收。如果程序发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象的内存被回收之前采取一些程序行动。 应用场景：可用来跟踪对象被垃圾回收器回收的活动，当一个虚引用关联的对象被垃圾收集器回收之前会收到一条系统通知。 第5讲 | String、StringBuffer、StringBuilder有什么区别？ 模块一 Java基础 (14讲) 第5讲 | String、StringBuffer、StringBuilder有什么区别？ String 是 Java 语言非常基础和重要的类，提供了构造和管理字符串的各种基本逻辑。它是典型的 Immutable 类，被声明成为 final class，所有属性也都是 final 的。也由于它的不可变性，类似拼接、裁剪字符串等动作，都会产生新的 String 对象。由于字符串操作的普遍性，所以相关操作的效率往往对应用性能有明显影响。 StringBuffer 是为解决上面提到拼接产生太多中间对象的问题而提供的一个类，我们可以用 append 或者 add 方法，把字符串添加到已有序列的末尾或者指定位置。StringBuffer 本质是一个线程安全的可修改字符序列，它保证了线程安全，也随之带来了额外的性能开销，所以除非有线程安全的需要，不然还是推荐使用它的后继者，也就是 StringBuilder。 StringBuilder 是 Java 1.5 中新增的，在能力上和 StringBuffer 没有本质区别，但是它去掉了线程安全的部分，有效减小了开销，是绝大部分情况下进行字符串拼接的首选。 关于今天我们讨论的题目你做到心中有数了吗？限于篇幅有限，还有很多字符相关的问题没有来得及讨论，比如编码相关的问题。可以思考一下，很多字符串操作，比如 getBytes()/String​(byte[] bytes) 等都是隐含着使用平台默认编码，这是一种好的实践吗？是否有利于避免乱码？ 第6讲 | 动态代理是基于什么原理？ 模块一 Java基础 (14讲) 第6讲 | 动态代理是基于什么原理？ 评论肖一林提一些建议：应该从两条线讲这个问题，一条从代理模式，一条从反射机制。不要老担心篇幅限制讲不清问题，废话砍掉一些，深层次的内在原理多讲些（比如asm），容易自学的扩展知识可以用链接代替代理模式（通过代理静默地解决一些业务无关的问题，比如远程、安全、事务、日志、资源关闭……让应用开发者可以只关心他的业务） 静态代理：事先写好代理类，可以手工编写，也可以用工具生成。缺点是每个业务类都要对应一个代理类，非常不灵活。 动态代理：运行时自动生成代理对象。缺点是生成代理代理对象和调用代理方法都要额外花费时间。 JDK动态代理：基于Java反射机制实现，必须要实现了接口的业务类才能用这种办法生成代理对象。新版本也开始结合ASM机制。 cglib动态代理：基于ASM机制实现，通过生成业务类的子类作为代理类。Java 发射机制的常见应用：动态代理（AOP、RPC）、提供第三方开发者扩展能力（Servlet容器，JDBC连接）、第三方组件创建对象（DI）……我水平比较菜，希望多学点东西，希望比免费知识层次更深些，也不光是为了面试，所以提提建议。 公号-代码荣耀反射与动态代理原理 1 关于反射反射最大的作用之一就在于我们可以不在编译时知道某个对象的类型，而在运行时通过提供完整的”包名+类名.class”得到。注意：不是在编译时，而是在运行时。 功能：•在运行时能判断任意一个对象所属的类。•在运行时能构造任意一个类的对象。•在运行时判断任意一个类所具有的成员变量和方法。•在运行时调用任意一个对象的方法。说大白话就是，利用Java反射机制我们可以加载一个运行时才得知名称的class，获悉其构造方法，并生成其对象实体，能对其fields设值并唤起其methods。 应用场景：反射技术常用在各类通用框架开发中。因为为了保证框架的通用性，需要根据配置文件加载不同的对象或类，并调用不同的方法，这个时候就会用到反射——运行时动态加载需要加载的对象。 特点：由于反射会额外消耗一定的系统资源，因此如果不需要动态地创建一个对象，那么就不需要用反射。另外，反射调用方法时可以忽略权限检查，因此可能会破坏封装性而导致安全问题。 2 动态代理为其他对象提供一种代理以控制对这个对象的访问。在某些情况下，一个对象不适合或者不能直接引用另一个对象，而代理对象可以在两者之间起到中介的作用（可类比房屋中介，房东委托中介销售房屋、签订合同等）。所谓动态代理，就是实现阶段不用关心代理谁，而是在运行阶段才指定代理哪个一个对象（不确定性）。如果是自己写代理类的方式就是静态代理（确定性）。 组成要素：(动态)代理模式主要涉及三个要素：其一：抽象类接口其二：被代理类（具体实现抽象接口的类）其三：动态代理类：实际调用被代理类的方法和属性的类 实现方式:实现动态代理的方式很多，比如 JDK 自身提供的动态代理，就是主要利用了反射机制。还有其他的实现方式，比如利用字节码操作机制，类似 ASM、CGLIB（基于 ASM）、Javassist 等。举例，常可采用的JDK提供的动态代理接口InvocationHandler来实现动态代理类。其中invoke方法是该接口定义必须实现的，它完成对真实方法的调用。通过InvocationHandler接口，所有方法都由该Handler来进行处理，即所有被代理的方法都由InvocationHandler接管实际的处理任务。此外，我们常可以在invoke方法实现中增加自定义的逻辑实现，实现对被代理类的业务逻辑无侵入。 第7讲 | int和Integer有什么区别？？ 模块一 Java基础 (14讲) 第7讲 | int和Integer有什么区别？ 评论12对象由三部分组成，对象头，对象实例，对齐填充。其中对象头一般是十六个字节，包括两部分，第一部分有哈希码，锁状态标志，线程持有的锁，偏向线程id，gc分代年龄等。第二部分是类型指针，也就是对象指向它的类元数据指针，可以理解，对象指向它的类。对象实例就是对象存储的真正有效信息，也是程序中定义各种类型的字段包括父类继承的和子类定义的，这部分的存储顺序会被虚拟机和代码中定义的顺序影响（这里问一下，这个被虚拟机影响是不是就是重排序？？如果是的话，我知道的volatile定义的变量不会被重排序应该就是这里不会受虚拟机影响吧？？）。第三部分对齐填充只是一个类似占位符的作用，因为内存的使用都会被填充为八字节的倍数。 Kyle节选自《深入理解JAVA虚拟机》：在HotSpot虚拟机中，对象在内存中存储的布局可以分为3块区域：对象头（Header）、实例数据（Instance Data）和对齐填充（Padding）。 HotSpot虚拟机的对象头包括两部分信息，第一部分用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等，这部分数据的长度在32位和64位的虚拟机（未开启压缩指针）中分别为32bit和64bit，官方称它为”Mark Word”。 对象头的另外一部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。并不是所有的虚拟机实现都必须在对象数据上保留类型指针，换句话说，查找对象的元数据信息并不一定要经过对象本身，这点将在2.3.3节讨论。另外，如果对象是一个Java数组，那在对象头中还必须有一块用于记录数组长度的数据，因为虚拟机可以通过普通Java对象的元数据信息确定Java对象的大小，但是从数组的元数据中却无法确定数组的大小。 接下来的实例数据部分是对象真正存储的有效信息，也是在程序代码中所定义的各种类型的字段内容。无论是从父类继承下来的，还是在子类中定义的，都需要记录起来。 第三部分对齐填充并不是必然存在的，也没有特别的含义，它仅仅起着占位符的作用。由于HotSpot VM的自动内存管理系统要求对象起始地址必须是8字节的整数倍，换句话说，就是对象的大小必须是8字节的整数倍。 公号-代码荣耀1 int和Integer JDK1.5引入了自动装箱与自动拆箱功能，Java可根据上下文，实现int/Integer,double/Double,boolean/Boolean等基本类型与相应对象之间的自动转换，为开发过程带来极大便利。 最常用的是通过new方法构建Integer对象。但是，基于大部分数据操作都是集中在有限的、较小的数值范围，在JDK1.5 中新增了静态工厂方法 valueOf，其背后实现是将int值为-128 到 127 之间的Integer对象进行缓存，在调用时候直接从缓存中获取，进而提升构建对象的性能，也就是说使用该方法后，如果两个对象的int值相同且落在缓存值范围内，那么这个两个对象就是同一个对象；当值较小且频繁使用时，推荐优先使用整型池方法（时间与空间性能俱佳）。 2 注意事项 [1] 基本类型均具有取值范围，在大数大数的时候，有可能会出现越界的情况。[2] 基本类型转换时，使用声明的方式。例：long result= 1234567890 24 365；结果值一定不会是你所期望的那个值，因为1234567890 24已经超过了int的范围，如果修改为：long result= 1234567890L 24 365；就正常了。[3] 慎用基本类型处理货币存储。如采用double常会带来差距，常采用BigDecimal、整型（如果要精确表示分，可将值扩大100倍转化为整型）解决该问题。[4] 优先使用基本类型。原则上，建议避免无意中的装箱、拆箱行为，尤其是在性能敏感的场合，[5] 如果有线程安全的计算需要，建议考虑使用类型AtomicInteger、AtomicLong 这样的线程安全类。部分比较宽的基本数据类型，比如 float、double，甚至不能保证更新操作的原子性，可能出现程序读取到只更新了一半数据位的数值。 行者 Mark Word:标记位 4字节，类似轻量级锁标记位，偏向锁标记位等。 Class对象指针:4字节，指向对象对应class对象的内存地址。 对象实际数据:对象所有成员变量。 对齐:对齐填充字节，按照8个字节填充。 Integer占用内存大小，4+4+4+4=16字节。作者回复: 不错，如果是64位不用压缩指针，对象头会变大，还可能有对齐开销 George计算对象大小可通过dump内存之后用memory analyze分析 作者回复: 嗯，也可以利用：jol，jmap，或者instrument api（Java agent）等等 第8讲 | 对比Vector、ArrayList、LinkedList有何区别？ 模块一 Java基础 (14讲) 第8讲 | 对比Vector、ArrayList、LinkedList有何区别？ 评论公号-代码荣耀Vector、ArrayList、LinkedList均为线型的数据结构，但是从实现方式与应用场景中又存在差别。 1 底层实现方式ArrayList内部用数组来实现；LinkedList内部采用双向链表实现；Vector内部用数组实现。 2 读写机制ArrayList在执行插入元素是超过当前数组预定义的最大值时，数组需要扩容，扩容过程需要调用底层System.arraycopy()方法进行大量的数组复制操作；在删除元素时并不会减少数组的容量（如果需要缩小数组容量，可以调用trimToSize()方法）；在查找元素时要遍历数组，对于非null的元素采取equals的方式寻找。 LinkedList在插入元素时，须创建一个新的Entry对象，并更新相应元素的前后元素的引用；在查找元素时，需遍历链表；在删除元素时，要遍历链表，找到要删除的元素，然后从链表上将此元素删除即可。Vector与ArrayList仅在插入元素时容量扩充机制不一致。对于Vector，默认创建一个大小为10的Object数组，并将capacityIncrement设置为0；当插入元素数组大小不够时，如果capacityIncrement大于0，则将Object数组的大小扩大为现有size+capacityIncrement；如果capacityIncrement&lt;=0,则将Object数组的大小扩大为现有大小的2倍。 3 读写效率 ArrayList对元素的增加和删除都会引起数组的内存分配空间动态发生变化。因此，对其进行插入和删除速度较慢，但检索速度很快。 LinkedList由于基于链表方式存放数据，增加和删除元素的速度较快，但是检索速度较慢。 4 线程安全性 ArrayList、LinkedList为非线程安全；Vector是基于synchronized实现的线程安全的ArrayList。 需要注意的是：单线程应尽量使用ArrayList，Vector因为同步会有性能损耗；即使在多线程环境下，我们可以利用Collections这个类中为我们提供的synchronizedList(List list)方法返回一个线程安全的同步列表对象。 问题回答 利用PriorityBlockingQueue或Disruptor可实现基于任务优先级为调度策略的执行调度系统。 第9讲 | 对比Hashtable、HashMap、TreeMap有什么不同？ 模块一 Java基础 (14讲) 第9讲 | 对比Hashtable、HashMap、TreeMap有什么不同？ 对比 Hashtable、HashMap、TreeMap 有什么不同？ 天凉好个秋 解决哈希冲突的常用方法有： 开放定址法基本思想是：当关键字key的哈希地址p=H（key）出现冲突时，以p为基础，产生另一个哈希地址p1，如果p1仍然冲突，再以p为基础，产生另一个哈希地址p2，…，直到找出一个不冲突的哈希地址pi ，将相应元素存入其中。 再哈希法这种方法是同时构造多个不同的哈希函数：Hi=RH1（key） i=1，2，…，k当哈希地址Hi=RH1（key）发生冲突时，再计算Hi=RH2（key）……，直到冲突不再产生。这种方法不易产生聚集，但增加了计算时间。 链地址法这种方法的基本思想是将所有哈希地址为i的元素构成一个称为同义词链的单链表，并将单链表的头指针存在哈希表的第i个单元中，因而查找、插入和删除主要在同义词链中进行。链地址法适用于经常进行插入和删除的情况。 建立公共溢出区这种方法的基本思想是：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。 公号-代码荣耀Hashtable、HashMap、TreeMap心得 三者均实现了Map接口，存储的内容是基于key-value的键值对映射，一个映射不能有重复的键，一个键最多只能映射一个值。 （1） 元素特性HashTable中的key、value都不能为null；HashMap中的key、value可以为null，很显然只能有一个key为null的键值对，但是允许有多个值为null的键值对；TreeMap中当未实现 Comparator 接口时，key 不可以为null；当实现 Comparator 接口时，若未对null情况进行判断，则key不可以为null，反之亦然。 （2）顺序特性HashTable、HashMap具有无序特性。TreeMap是利用红黑树来实现的（树中的每个节点的值，都会大于或等于它的左子树种的所有节点的值，并且小于或等于它的右子树中的所有节点的值），实现了SortMap接口，能够对保存的记录根据键进行排序。所以一般需要排序的情况下是选择TreeMap来进行，默认为升序排序方式（深度优先搜索），可自定义实现Comparator接口实现排序方式。 （3）初始化与增长方式初始化时：HashTable在不指定容量的情况下的默认容量为11，且不要求底层数组的容量一定要为2的整数次幂；HashMap默认容量为16，且要求容量一定为2的整数次幂。扩容时：Hashtable将容量变为原来的2倍加1；HashMap扩容将容量变为原来的2倍。 （4）线程安全性HashTable其方法函数都是同步的（采用synchronized修饰），不会出现两个线程同时对数据进行操作的情况，因此保证了线程安全性。也正因为如此，在多线程运行环境下效率表现非常低下。因为当一个线程访问HashTable的同步方法时，其他线程也访问同步方法就会进入阻塞状态。比如当一个线程在添加数据时候，另外一个线程即使执行获取其他数据的操作也必须被阻塞，大大降低了程序的运行效率，在新版本中已被废弃，不推荐使用。HashMap不支持线程的同步，即任一时刻可以有多个线程同时写HashMap;可能会导致数据的不一致。如果需要同步（1）可以用 Collections的synchronizedMap方法；（2）使用ConcurrentHashMap类，相较于HashTable锁住的是对象整体， ConcurrentHashMap基于lock实现锁分段技术。首先将Map存放的数据分成一段一段的存储方式，然后给每一段数据分配一把锁，当一个线程占用锁访问其中一个段的数据时，其他段的数据也能被其他线程访问。ConcurrentHashMap不仅保证了多线程运行环境下的数据访问安全性，而且性能上有长足的提升。 (5)一段话HashMapHashMap基于哈希思想，实现对数据的读写。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，让后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap在每个链表节点中储存键值对对象。当两个不同的键对象的hashcode相同时，它们会储存在同一个bucket位置的链表中，可通过键对象的equals()方法用来找到键值对。如果链表大小超过阈值（TREEIFY_THRESHOLD, 8），链表就会被改造为树形结构。 第10讲 | 如何保证集合是线程安全的? ConcurrentHashMap如何实现高效地线程安全？ 模块一 Java基础 (14讲) 第10讲 | 如何保证集合是线程安全的? ConcurrentHashMap如何实现高效地线程安全？ 明翼1.7put加锁通过分段加锁segment，一个hashmap里有若干个segment，每个segment里有若干个桶，桶里存放K-V形式的链表，put数据时通过key哈希得到该元素要添加到的segment，然后对segment进行加锁，然后在哈希，计算得到给元素要添加到的桶，然后遍历桶中的链表，替换或新增节点到桶中 size分段计算两次，两次结果相同则返回，否则对所以段加锁重新计算 1.8put CAS 加锁1.8中不依赖与segment加锁，segment数量与桶数量一致；首先判断容器是否为空，为空则进行初始化利用volatile的sizeCtl作为互斥手段，如果发现竞争性的初始化，就暂停在那里，等待条件恢复，否则利用CAS设置排他标志（U.compareAndSwapInt(this, SIZECTL, sc, -1)）;否则重试对key hash计算得到该key存放的桶位置，判断该桶是否为空，为空则利用CAS设置新节点否则使用synchronize加锁，遍历桶中数据，替换或新增加点到桶中最后判断是否需要转为红黑树，转换之前判断是否需要扩容 size利用LongAdd累加计算 第34讲 | 有人说“Lambda能让Java程序慢30倍”，你怎么看？ 模块四 Java性能基础 第34讲 | 有人说“Lambda能让Java程序慢30倍”，你怎么看 如果 Stream 使用不当，会让你的代码慢 5 倍 基准测试框架 JMH JMH上手步骤 JMH样例代码 JMH 是由 Hotspot JVM 团队专家开发的，除了支持完整的基准测试过程，包括预热、运行、统计和报告等，还支持 Java 和其他 JVM 语言。更重要的是，它针对 Hotspot JVM 提供了各种特性，以保证基准测试的正确性，整体准确性大大优于其他框架，并且，JMH 还提供了用近乎白盒的方式进行 Profiling 等工作的能力。 使用 JMH 也非常简单，你可以直接将其依赖加入Maven工程 1234567mvn archetype:generate \ -DinteractiveMode=false \ -DarchetypeGroupId=org.openjdk.jmh \ -DarchetypeArtifactId=jmh-java-benchmark-archetype \ -DgroupId=org.sample \ -DartifactId=test \ -Dversion=1.0 一课一练 我们在项目中需要评估系统的容量，以计划和保证其业务支撑能力，谈谈你的思路是怎么样的？常用手段有哪些？ 第35讲 | JVM优化Java代码时都做了什么？ 模块四 Java性能基础 JVM 在对代码执行的优化可分为运行时（runtime）优化和即时编译器（JIT）优化。 运行时优化主要是解释执行和动态编译通用的一些机制，比如说锁机制（如偏斜锁）、内存分配机制（如 TLAB）等。除此之外，还有一些专门用于优化解释执行效率的，比如说模版解释器、内联缓存（inline cache，用于优化虚方法调用的动态绑定）。 JVM 的即时编译器优化是指将热点代码以方法为单位转换成机器码，直接运行在底层硬件之上。它采用了多种优化方式，包括静态编译器可以使用的如方法内联、逃逸分析，也包括基于程序运行 profile 的投机性优化（speculative/optimistic optimization）。这个怎么理解呢？比如我有一条 instanceof 指令，在编译之前的执行过程中，测试对象的类一直是同一个，那么即时编译器可以假设编译之后的执行过程中还会是这一个类，并且根据这个类直接返回 instanceof 的结果。如果出现了其他类，那么就抛弃这段编译后的机器码，并且切换回解释执行。 一课一练 如何程序化验证 final 关键字是否会影响性能？ 第36讲 | 谈谈MySQL支持的事务隔离级别，以及悲观锁和乐观锁的原理和应用场景？第36讲 | 谈谈MySQL支持的事务隔离级别，以及悲观锁和乐观锁的原理和应用场景？ 模块5 Java应用开发扩展 (4讲) MySQL 支持的事务隔离级别，以及悲观锁和乐观锁的原理和应用场景 mysql（innodb）的可重复读隔离级别下，为什么可以认为不会出现幻像读呢? 从技术角度实现，mysq用mvcc做了snapshot，如果是locking read文档明确指出了会做区间锁定 乐观锁 悲观锁 脏读 幻读 不可重复读 CAS MVCC 隔离级别 锁队列 2PC TCC 一课一练 从架构设计的角度，可以将 MyBatis 分为哪几层？每层都有哪些主要模块？ mybatis架构自下而上分为基础支撑层、数据处理层、API接口层这三层。 基础支撑层，主要是用来做连接管理、事务管理、配置加载、缓存管理等最基础组件，为上层提供最基础的支撑。数据处理层，主要是用来做参数映射、sql解析、sql执行、结果映射等处理，可以理解为请求到达，完成一次数据库操作的流程。API接口层，主要对外提供API，提供诸如数据的增删改查、获取配置等接口。 第37讲 | 谈谈Spring Bean的生命周期和作用域？第37讲 | 谈谈Spring Bean的生命周期和作用域？模块5 Java应用开发扩展 (4讲) 谈谈 Spring Bean 的生命周期和作用域？ Spring Bean 生命周期比较复杂，可以分为创建和销毁两个过程。 首先，创建 Bean 会经过一系列的步骤，主要包括： 实例化 Bean 对象。 设置 Bean 属性。 如果我们通过各种 Aware 接口声明了依赖关系，则会注入 Bean 对容器基础设施层面的依赖。具体包括 BeanNameAware、BeanFactoryAware 和 ApplicationContextAware，分别会注入 Bean ID、Bean Factory 或者 ApplicationContext。 调用 BeanPostProcessor 的前置初始化方法 postProcessBeforeInitialization。 如果实现了 InitializingBean 接口，则会调用 afterPropertiesSet 方法。 调用 Bean 自身定义的 init 方法。 调用 BeanPostProcessor 的后置初始化方法 postProcessAfterInitialization。 创建过程完毕。 第二，Spring Bean 的销毁过程会依次调用 DisposableBean 的 destroy 方法和 Bean 自身定制的 destroy 方法。 广义上的 Spring 已经成为了一个庞大的生态系统，例如： Spring Boot，通过整合通用实践，更加自动、智能的依赖管理等，Spring Boot 提供了各种典型应用领域的快速开发基础，所以它是以应用为中心的一个框架集合。 Spring Cloud，可以看作是在 Spring Boot 基础上发展出的更加高层次的框架，它提供了构建分布式系统的通用模式，包含服务发现和服务注册、分布式配置管理、负载均衡、分布式诊断等各种子系统，可以简化微服务系统的构建。 当然，还有针对特定领域的 Spring Security、Spring Data 等。 Spring容器初始化开始:1.[BeanFactoryPostProcessor]接口实现类的构造器2.[BeanFactoryPostProcessor]的postProcessorBeanFactory方法3.[BeanPostProcessor]接口实现类的构造器4.[InstantiationAwareBeanPostProcessorAdapter]构造器5.[InstantiationAwareBeanPostProcessorAdapter]的postProcessBeforeInstantiation方法(从这里开始初始化bean)6.[Bean]的构造器7.[InstantiationAwareBeanPostProcessorAdapter]的postProcessAfterInstantiation8.[InstantiationAwareBeanPostProcessorAdapter]的postProcessPropertyValues方法9.[Bean]属性注入，setter方方法10.[Bean]如果实现了各种XXXaware接口，依次调用各个setXXX(如BeanNameAware.setBeanName(),BeanFactoryAware.setBeanFactory())11.[BeanPostProcessor]的postProcessBeforeInitialization方法12.[InstantiationAwareBeanPostProcessorAdapter]的postProcessBeforeInitialization方法13.[Bean]自定义的init-method14.[Bean]如果实现了InitializingBean接口，此时会调用它的afterPropertiesSet方法15.[BeanPostProcessor]的postProcessAfterInitialization方法(此时bean初始化完成)16.[InstantiationAwareBeanPostProcessorAdapter]的postProcessInitialization方法(到这里容器初始化完成)17.业务逻辑bean的使用 Bean的销毁过程:1.[DisposableBean]的destory方法2.[Bean]自定义的destory-method方法 说明:如果有多个bean需要初始化，会循环执行5–15。 感觉本篇文章跑题了呢，关于生命周期，只讨论了初始化过程和销毁过程，那么什么时候引发的初始化呢？什么时候触发销毁操作呢？spring容器管理的bean是在容器运行过程中不会被销毁吧？ 我来讲讲吧:首先理解scope: Singleton(单例) 在整个应用中,只创建bean的一个实例 Propotype(原型) 每次注入或者通过Spring应用上下文获取的时候,都会创建一个新的bean实例。 Session(会话) 在Web应用中,为每个会话创建一个bean实例。 Request(请求) 在Web应用中,为每个请求创建一个bean实例。 他们是什么时候创建的:1一个单例的bean,而且lazy-init属性为false(默认),在Application Context创建的时候构造2一个单例的bean,lazy-init属性设置为true,那么,它在第一次需要的时候被构造.3 其他scope的bean,都是在第一次需要使用的时候创建 他们是什么时候销毁的:1 单例的bean始终 存在与application context中, 只有当 application 终结的时候,才会销毁2 和其他scope相比,Spring并没有管理prototype实例完整的生命周期,在实例化,配置,组装对象交给应用后,spring不再管理.只要bean本身不持有对另一个资源（如数据库连接或会话对象）的引用，只要删除了对该对象的所有引用或对象超出范围，就会立即收集垃圾.3 Request: 每次客户端请求都会创建一个新的bean实例,一旦这个请求结束,实例就会离开scope,被垃圾回收.4 Session: 如果用户结束了他的会话,那么这个bean实例会被GC. 一课一练 请介绍一下 Spring 声明式事务的实现机制，可以考虑将具体过程画图。 第38讲 | 对比Java标准NIO类库，你知道Netty是如何实现更高性能的吗？第38讲 | 对比Java标准NIO类库，你知道Netty是如何实现更高性能的吗？模块5 Java应用开发扩展 (4讲) 你知道 Netty 是如何实现更高性能的吗 单独从性能角度，Netty 在基础的 NIO 等类库之上进行了很多改进，例如： 更加优雅的 Reactor 模式实现、灵活的线程模型、利用 EventLoop 等创新性的机制，可以非常高效地管理成百上千的 Channel。 充分利用了 Java 的 Zero-Copy 机制，并且从多种角度，“斤斤计较”般的降低内存分配和回收的开销。例如，使用池化的 Direct Buffer 等技术，在提高 IO 性能的同时，减少了对象的创建和销毁；利用反射等技术直接操纵 SelectionKey，使用数组而不是 Java 容器等。 使用更多本地代码。例如，直接利用 JNI 调用 Open SSL 等方式，获得比 Java 内建 SSL 引擎更好的性能。 在通信协议、序列化等其他角度的优化。 推荐阅读 Norman Maurer 等编写的《Netty 实战》（Netty In Action），如果你想系统学习 Netty，它会是个很好的入门参考。 考点： Reactor 模式和 Netty 线程模型。 Pipelining、EventLoop 等部分的设计实现细节。 Netty 的内存管理机制、引用计数 等特别手段。 有的时候面试官也喜欢对比 Java 标准 NIO API，例如，你是否知道 Java NIO 早期版本中的 Epoll 空转问题 ，以及 Netty 的解决方式等。 一课一练 Netty 的线程模型是什么样的？ 第39讲 | 谈谈常用的分布式ID的设计方案？Snowflake是否受冬令时切换影响？第39讲 | 谈谈常用的分布式ID的设计方案？Snowflake是否受冬令时切换影响？ 模块5 Java应用开发扩展 (4讲) 常用的分布式 ID 的设计方案？ Snowflake 是否受冬令时切换影响？]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[orientdb-config]]></title>
    <url>%2F2019%2F08%2F18%2Forientdb-config%2F</url>
    <content type="text"><![CDATA[orientdb配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271orientdb &#123;db=test&#125;&gt; CONFIGREMOTE SERVER CONFIGURATION+----+---------------------------------------------------+---------------------------------------------------------------------------------------+|# |NAME |VALUE |+----+---------------------------------------------------+---------------------------------------------------------------------------------------+|0 |network.binary.maxLength |16384 ||1 |distributed.autoRemoveOfflineServers |0 ||2 |index.sbtreeBonsaiToEmbeddedThreshold |-1 ||3 |server.database.path | ||4 |security.userPasswordSaltCacheSize |500 ||5 |distributed.dumpStatsEvery |0 ||6 |distributed.asynchResponsesTimeout |15000 ||7 |sbtree.maxEmbeddedValueSize |40960 ||8 |storage.diskCache.writeCacheFlushInactivityInterval|60000 ||9 |network.http.installDefaultCommands |true ||10 |tx.commit.synch |false ||11 |security.userPasswordSaltIterations |65536 ||12 |statement.cacheSize |100 ||13 |security.userPasswordDefaultAlgorithm |PBKDF2WithHmacSHA256 ||14 |network.socketTimeout |15000 ||15 |cache.local.impl |com.orientechnologies.orient.core.cache.ORecordCacheWeakRefs ||16 |memory.directMemory.preallocate |true ||17 |client.channel.minPool |1 ||18 |network.socketBufferSize |0 ||19 |distributed.publishNodeStatusEvery |10000 ||20 |storage.walPerformanceStatisticsInterval |10 ||21 |storage.trackChangedRecordsInWAL |false ||22 |log.console.level |info ||23 |query.parallelResultQueueSize |20000 ||24 |environment.dumpCfgAtStartup |false ||25 |sequence.retryDelay |200 ||26 |distributed.backupDirectory |../backup/databases ||27 |storage.configuration.syncOnUpdate |true ||28 |network.retryDelay |500 ||29 |db.makeFullCheckpointOnIndexChange |true ||30 |network.http.charset |utf-8 ||31 |command.cache.enabled |false ||32 |distributed.concurrentTxAutoRetryDelay |1000 ||33 |sbtreebonsai.bucketSize |2 ||34 |storage.useWAL |true ||35 |network.http.streaming |false ||36 |storage.openFiles.limit |-1 ||37 |storage.wal.readCacheSize |1000 ||38 |db.mvcc.throwfast |false ||39 |index.embeddedToSbtreeBonsaiThreshold |40 ||40 |db.validation |true ||41 |server.openAllDatabasesAtStartup |false ||42 |storage.autoCloseDelay |20 ||43 |storage.diskCache.keepState |false ||44 |distributed.txAliveTimeout |30000 ||45 |storage.diskCache.printCacheStatistics |false ||46 |storage.diskCache.printFileRemoveStatistics |true ||47 |command.cache.maxResultsetSize |500 ||48 |profiler.autoDump.type |full ||49 |distributed.asynchQueueSize |0 ||50 |db.use.distributedVersion |false ||51 |network.http.serverInfo |OrientDB Server v.3.0.22 - Veloce (build 8afc634a2ea9c351898ae57dee8d739ff4851252, b...||52 |client.ci.ciphertransform |AES/CBC/PKCS5Padding ||53 |storage.wal.reportAfterOperationsDuringRestore |10000 ||54 |server.log.dumpClientExceptionFullStackTrace |false ||55 |jna.disable.system.library |true ||56 |storage.trackFileAccess |true ||57 |distributed.checkIntegrityLastTxs |16 ||58 |environment.concurrent |true ||59 |network.token.secretKey | ||60 |index.manual.lazyUpdates |1 ||61 |tx.autoRetry |1 ||62 |storage.compressionMethod |nothing ||63 |storage.wal.restore.batchSize |10000 ||64 |query.limitThresholdTip |10000 ||65 |distributed.deployDbTaskCompression |7 ||66 |index.ignoreNullValuesDefault |false ||67 |storage.diskCache.pinnedPages |20 ||68 |storage.diskCache.bufferSize |5245 ||69 |distributed.deployDbTaskTimeout |1200000 ||70 |client.ssl.trustStorePass | ||71 |memory.directMemory.safeMode |true ||72 |distributed.responseChannels |1 ||73 |storage.diskCache.exclusiveFlushBoundary |0.9 ||74 |orient.initInServletContextListener |true ||75 |storage.wal.segmentBufferSize |32 ||76 |file.lock |true ||77 |network.http.jsonResponseError |true ||78 |query.parallelAuto |false ||79 |log.console.ansi |auto ||80 |network.token.expireTimeout |60 ||81 |distributed.commandTaskTimeout |120000 ||82 |cloud.base.url |cloud.orientdb.com ||83 |distributed.maxStartupDelay |10000 ||84 |distributed.conflictResolverRepairerBatch |1000 ||85 |sbtree.maxKeySize |10240 ||86 |network.binary.readResponse.maxTimes |20 ||87 |file.trackFileClose |false ||88 |network.http.useToken |false ||89 |memory.leftToOS |12% ||90 |sql.graphConsistencyMode |tx ||91 |storage.makeFullCheckpointAfterClusterCreate |true ||92 |nonTX.clusters.sync.immediately |manindex ||93 |storage.encryptionKey |&lt;hidden&gt; ||94 |storage.wal.commitTimeout |1000 ||95 |network.retry |5 ||96 |storage.wal.allowDirectIO |true ||97 |query.remoteResultSet.pageSize |1000 ||98 |distributed.commandLongTaskTimeout |86400000 ||99 |storage.lowestFreeListBound |16 ||100 |profiler.memoryCheckInterval |120000 ||101 |storage.wal.shutdownTimeout |10000 ||102 |storage.diskCache.exclusiveBoundary |0.7 ||103 |distributed.queueMaxSize |10000 ||104 |client.ssl.trustStore | ||105 |document.binaryMapping |0 ||106 |index.auto.synchronousAutoRebuild |true ||107 |network.http.sessionExpireTimeout |900 ||108 |storage.diskCache.writeCachePart |5 ||109 |storage.record.lockTimeout |2000 ||110 |log.file.level |info ||111 |storage.diskCache.printFlushTillSegmentStatistics |true ||112 |distributed.checkHealthCanOfflineServer |false ||113 |file.deleteDelay |10 ||114 |memory.chunk.size |2147483647 ||115 |network.binary.loadBalancing.enabled |false ||116 |network.binary.loadBalancing.timeout |2000 ||117 |db.pool.idleTimeout |0 ||118 |command.cache.minExecutionTime |10 ||119 |storage.wal.syncOnPageFlush |true ||120 |sbtreebonsai.linkBagCache.size |100000 ||121 |server.cache.staticFile |false ||122 |record.downsizing.enabled |true ||123 |distributed.backupTryIncrementalFirst |true ||124 |client.krb5.ccname | ||125 |ridBag.embeddedToSbtreeBonsaiThreshold |40 ||126 |jvm.gc.delayForOptimize |600 ||127 |db.document.serializer |ORecordSerializerBinary ||128 |file.deleteRetry |50 ||129 |storage.exclusiveFileAccess |true ||130 |memory.directMemory.trackMode |false ||131 |network.http.maxLength |1000000 ||132 |query.maxHeapElementsAllowedPerOp |500000 ||133 |server.log.dumpClientExceptionLevel |FINE ||134 |query.scanBatchSize |1000 ||135 |storage.diskCache.writeCachePageTTL |86400 ||136 |sbtreebonsai.linkBagCache.evictionSize |1000 ||137 |db.makeFullCheckpointOnSchemaChange |true ||138 |storage.wal.segmentsInterval |30 ||139 |tx.useLog |true ||140 |distributed.conflictResolverRepairerChain |majority,content,version ||141 |distributed.heartbeatTimeout |10000 ||142 |network.lockTimeout |15000 ||143 |db.pool.acquireTimeout |60000 ||144 |distributed.purgeResponsesTimerDelay |15000 ||145 |profiler.enabled |false ||146 |query.remoteResultSet.sendExecutionPlan |false ||147 |tx.log.synch |false ||148 |storage.encryptionMethod |nothing ||149 |storage.wal.maxSegmentSizePercent |5 ||150 |storage.diskCache.diskFreeSpaceCheckInterval |5 ||151 |storage.wal.minSegSize |6144 ||152 |client.channel.maxPool |100 ||153 |query.scanPrefetchPages |20 ||154 |query.live.support |true ||155 |server.channel.cleanDelay |5000 ||156 |script.pool.maxSize |20 ||157 |client.channel.dbReleaseWaitTimeout |10000 ||158 |storage.wal.cacheSize |65536 ||159 |environment.allowJVMShutdown |true ||160 |cloud.project.token | ||161 |distributed.crudTaskTimeout |3000 ||162 |client.krb5.ktname | ||163 |distributed.conflictResolverRepairerCheckEvery |5000 ||164 |tx.pageCacheSize |12 ||165 |command.timeout |0 ||166 |client.ci.keystore.file | ||167 |index.auto.rebuildAfterNotSoftClose |true ||168 |storage.wal.path | ||169 |network.requestTimeout |3600000 ||170 |storage.diskCache.diskFreeSpaceLimit |-8 ||171 |query.parallelMinimumRecords |300000 ||172 |network.binary.minProtocolVersion |26 ||173 |query.scanThresholdTip |50000 ||174 |index.cursor.prefetchSize |10000 ||175 |memory.directMemory.onlyAlignedMemoryAccess |true ||176 |storage.useTombstones |false ||177 |distributed.dbWorkerThreads |0 ||178 |sequence.maxRetry |100 ||179 |object.saveOnlyDirty |false ||180 |sbtree.maxDepth |64 ||181 |memory.pool.limit |2147483647 ||182 |server.security.file | ||183 |storage.pessimisticLock |false ||184 |storage.autoCloseAfterDelay |false ||185 |storage.makeFullCheckpointAfterCreate |false ||186 |storage.diskCache.diskFreeSpaceCheckIntervalInPages|2048 ||187 |db.pool.min |0 ||188 |client.credentialinterceptor | ||189 |distributed.commandQuickTaskTimeout |5000 ||190 |distributed.checkHealthEvery |10000 ||191 |network.binary.debug |false ||192 |cloud.project.id | ||193 |storage.wal.maxSize |-1 ||194 |security.createDefaultUsers |true ||195 |memory.useUnsafe |true ||196 |distributed.requestChannels |1 ||197 |storage.diskCache.cacheStatisticsInterval |10 ||198 |server.backwardCompatibility |true ||199 |db.pool.max |100 ||200 |profiler.autoDump.interval |0 ||201 |index.txMode |FULL ||202 |index.flushAfterCreate |true ||203 |storage.diskCache.chunkSize |256 ||204 |storage.callFsync |true ||205 |client.connection.strategy | ||206 |client.ssl.enabled |false ||207 |storage.wal.fullCheckpointShutdownTimeout |600 ||208 |client.krb5.config | ||209 |client.ci.keystore.password | ||210 |storage.wal.fuzzyCheckpointInterval |300 ||211 |class.minimumClusters |0 ||212 |storage.internal.journaled.tx.streaming.port | ||213 |client.connection.fetchHostList |true ||214 |storage.wal.maxSegmentSize |-1 ||215 |distributed.deployChunkTaskTimeout |60000 ||216 |client.ssl.keyStore | ||217 |nonTX.recordUpdate.synch |false ||218 |network.http.jsonp |false ||219 |storage.wal.bufferSize |64 ||220 |storage.diskCache.walSizeToStopFlush |2147483648 ||221 |storage.diskCache.printFlushFileStatistics |true ||222 |distributed.atomicLockTimeout |100 ||223 |ridBag.embeddedDefaultSize |4 ||224 |network.retry.strategy |auto ||225 |environment.lockManager.concurrency.level |32 ||226 |storage.diskCache.pageSize |64 ||227 |storageProfiler.intervalBetweenSnapshots |100 ||228 |storage.diskCache.writeCacheShutdownTimeout |30 ||229 |client.connectionPool.waitTimeout |5000 ||230 |profiler.maxValues |200 ||231 |distributed.localQueueSize |10000 ||232 |storage.componentsLock.cache |10000 ||233 |storage.lockTimeout |0 ||234 |storage.wal.fuzzyCheckpointShutdownWait |600 ||235 |distributed.concurrentTxMaxAutoRetry |15 ||236 |index.durableInNonTxMode |true ||237 |hashTable.slitBucketsBuffer.length |1500 ||238 |storage.useCHMCache |true ||239 |index.auto.lazyUpdates |10000 ||240 |sbtreebonsai.freeSpaceReuseTrigger |0 ||241 |profiler.config | ||242 |client.ci.keyalgorithm |AES ||243 |tx.trackAtomicOperations |false ||244 |network.token.encryptionAlgorithm |HmacSHA256 ||245 |ridBag.sbtreeBonsaiToEmbeddedToThreshold |-1 ||246 |client.ssl.keyStorePass | ||247 |command.cache.evictStrategy |PER_CLUSTER ||248 |oauth2.secretkey | ||249 |storage.cluster.usecrc32 |false ||250 |storage.makeFullCheckpointAfterOpen |true ||251 |network.maxConcurrentSessions |1000 ||252 |storage.wal.fileAutoCloseInterval |10 ||253 |memory.leftToContainer |12% ||254 |distributed.queueTimeout |500000 ||255 |db.pool.idleCheckDelay |0 ||256 |storage.diskCache.checksumMode |StoreAndSwitchReadOnlyMode ||257 |storage.diskCache.writeCachePageFlushInterval |25 ||258 |storageProfiler.cleanUpInterval |5000 ||259 |storage.printWALPerformanceStatistics |false ||260 |storage.diskCache.writeCacheFlushLockTimeout |-1 ||261 |storage.diskCache.walSizeToStartFlush |6442450944 |+----+---------------------------------------------------+---------------------------------------------------------------------------------------+orientdb &#123;db=test&#125;&gt; References[1] orientdb-docs]]></content>
      <categories>
        <category>orientdb</category>
      </categories>
      <tags>
        <tag>orientdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java-jdbc-prepared-statement]]></title>
    <url>%2F2019%2F08%2F18%2Fjava-jdbc-prepared-statement%2F</url>
    <content type="text"><![CDATA[在写neo4j和orientdb的通用方法时，忽然想到jdbc，然后就想试试mysql neo4j orientdb几个数据库jdbc连接方式里的 prepartdStatement一不一样。 问题的来源来自以下代码12345678910111213141516171819202122232425262728List&lt;Map&lt;String, Object&gt;&gt; list = new ArrayList();try (PreparedStatement pst = conn.prepareStatement(sql); ResultSet rs = pst.executeQuery();) &#123; List&lt;String&gt; fields = new ArrayList&lt;&gt;(); while (rs.next()) &#123; if (fields.isEmpty()) &#123; ResultSetMetaData metaData = rs.getMetaData(); // 查询出的字段 int count = metaData.getColumnCount(); for (int i = 1; i &lt;= count; i++) &#123; fields.add(metaData.getColumnName(i)); &#125; &#125; Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); for (String field : fields) &#123; map.put(field, rs.getObject(field)); &#125; // T r = JSONObject.parseObject(JSON.toString(map), Object.class); list.add(map); &#125;&#125; catch (SQLException e) &#123; throw new SQLException(e);&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495/** * @param sql 查询语句 * @param params 占位符 参数 * @param conn 连接 * @return */@Overridepublic Iterator&lt;Map&lt;String, Object&gt;&gt; query(String sql, Map&lt;Integer, Object&gt; params, Connection conn) throws SQLException &#123; // final PreparedStatement statement = conn.prepareStatement(sql); // 设置参数 setParameters(statement, params); // 执行查询并获得结果 final ResultSet result = statement.executeQuery(); // 封装返回 return new Iterator&lt;Map&lt;String, Object&gt;&gt;() &#123; boolean hasNext = result.next(); // 所有字段 public List&lt;String&gt; columns; // 字段个数 public int columnsCount; /** * * * @return */ @Override public boolean hasNext() &#123; return hasNext; &#125; /** * 获得所有字段&lt;br&gt; * 第一次会查询出所有字段，第二 第三次 直接用columns * * @return * @throws SQLException */ private List&lt;String&gt; getColumns() throws SQLException &#123; if (columns != null) &#123; return columns; &#125; ResultSetMetaData metaData = result.getMetaData(); // 查询出的字段 int count = metaData.getColumnCount(); List&lt;String&gt; cols = new ArrayList&lt;&gt;(count); for (int i = 1; i &lt;= count; i++) &#123; cols.add(metaData.getColumnName(i)); &#125; columnsCount = cols.size(); return columns = cols; &#125; /** * * @return */ @Override public Map&lt;String, Object&gt; next() &#123; try &#123; if (hasNext) &#123; // Map&lt;String, Object&gt; map = new LinkedHashMap&lt;&gt;(columnsCount); for (String col : getColumns()) &#123; map.put(col, result.getObject(col)); &#125; hasNext = result.next(); if (!hasNext) &#123; result.close(); statement.close(); &#125; return map; &#125; else &#123; throw new NoSuchElementException(); &#125; &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; &#125; /** * */ @Override public void remove() &#123; &#125; &#125;;&#125; 然后查看对应的源代码mysql-connector-java-5.1.40.jarneo4j-jdbc-3.4.0.jarorientdb-jdbc-3.0.22.jar MySQL prepart 测试MySQL client server prepart 测试 MySQL server 配置开启 all_query log 在命令行执行以下语句 12345678910111213PREPARE stmt1 FROM 'SELECT SQRT(POW(?,2) + POW(?,2)) AS hypotenuse';SET @a = 3;SET @b = 4;EXECUTE stmt1 USING @a, @b;EXECUTE stmt1 USING @a, @b;SET @a = 6;SET @b = 8;EXECUTE stmt1 USING @a, @b;SET @s = 'SELECT SQRT(POW(?,2) + POW(?,2)) AS hypotenuse';PREPARE stmt2 FROM @s;SET @a = 6;SET @b = 8;EXECUTE stmt2 USING @a, @b; all_query.log输出如下 123456789101112131415161718192019-08-14T12:24:02.934322Z 1042 Query PREPARE stmt1 FROM ...2019-08-14T12:24:02.934412Z 1042 Prepare SELECT SQRT(POW(?,2) + POW(?,2)) AS hypotenuse2019-08-14T12:24:02.934762Z 1042 Query SET @a = 32019-08-14T12:24:02.935089Z 1042 Query SET @b = 42019-08-14T12:24:02.935404Z 1042 Query EXECUTE stmt1 USING @a, @b2019-08-14T12:24:02.935449Z 1042 Execute SELECT SQRT(POW(3,2) + POW(4,2)) AS hypotenuse2019-08-14T12:24:02.935949Z 1042 Query EXECUTE stmt1 USING @a, @b2019-08-14T12:24:02.935994Z 1042 Execute SELECT SQRT(POW(3,2) + POW(4,2)) AS hypotenuse2019-08-14T12:24:02.936388Z 1042 Query SET @a = 62019-08-14T12:24:02.936938Z 1042 Query SET @b = 82019-08-14T12:24:02.937319Z 1042 Query EXECUTE stmt1 USING @a, @b2019-08-14T12:24:02.937358Z 1042 Execute SELECT SQRT(POW(6,2) + POW(8,2)) AS hypotenuse2019-08-14T12:24:02.937791Z 1042 Query SET @s = &apos;SELECT SQRT(POW(?,2) + POW(?,2)) AS hypotenuse&apos;2019-08-14T12:24:02.938083Z 1042 Query PREPARE stmt2 FROM @s2019-08-14T12:24:02.938187Z 1042 Prepare SELECT SQRT(POW(?,2) + POW(?,2)) AS hypotenuse2019-08-14T12:24:02.938518Z 1042 Query SET @a = 62019-08-14T12:24:02.938804Z 1042 Query SET @b = 82019-08-14T12:24:02.939095Z 1042 Query EXECUTE stmt2 USING @a, @b2019-08-14T12:24:02.939130Z 1042 Execute SELECT SQRT(POW(6,2) + POW(8,2)) AS hypotenuse 确实是使用了Prepare 不过从这个结果看不出Prepare提高了多少性能 通过程序测试Prepare大概提高了30%的性能，语句不同，参数不通，测试结果会有差异。 jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;characterEncoding=UTF-8&amp;autoReconnect=true&amp;useSSL=false&amp;useServerPrepStmts=true com.mysql.jdbc.ConnectionImpl.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public PreparedStatement prepareStatement(String sql) throws SQLException &#123; return this.prepareStatement(sql, 1003, 1007);&#125;public PreparedStatement prepareStatement(String sql, int autoGenKeyIndex) throws SQLException &#123; PreparedStatement pStmt = this.prepareStatement(sql); ((com.mysql.jdbc.PreparedStatement)pStmt).setRetrieveGeneratedKeys(autoGenKeyIndex == 1); return pStmt;&#125;public PreparedStatement prepareStatement(String sql, int resultSetType, int resultSetConcurrency) throws SQLException &#123; synchronized(this.getConnectionMutex()) &#123; this.checkClosed(); com.mysql.jdbc.PreparedStatement pStmt = null; boolean canServerPrepare = true; String nativeSql = this.getProcessEscapeCodesForPrepStmts() ? this.nativeSQL(sql) : sql; if (this.useServerPreparedStmts &amp;&amp; this.getEmulateUnsupportedPstmts()) &#123; canServerPrepare = this.canHandleAsServerPreparedStatement(nativeSql); &#125; if (this.useServerPreparedStmts &amp;&amp; canServerPrepare) &#123; // // 从缓存中获取 pst if (this.getCachePreparedStatements()) &#123; synchronized(this.serverSideStatementCache) &#123; pStmt = (ServerPreparedStatement)this.serverSideStatementCache.remove(sql); if (pStmt != null) &#123; ((ServerPreparedStatement)pStmt).setClosed(false); // 清理上次留下的参数 ((com.mysql.jdbc.PreparedStatement)pStmt).clearParameters(); &#125; if (pStmt == null) &#123; // 向 Server 提交 SQL 预编译 try &#123; pStmt = ServerPreparedStatement.getInstance(this.getMultiHostSafeProxy(), nativeSql, this.database, resultSetType, resultSetConcurrency); if (sql.length() &lt; this.getPreparedStatementCacheSqlLimit()) &#123; ((ServerPreparedStatement)pStmt).isCached = true; &#125; ((com.mysql.jdbc.PreparedStatement)pStmt).setResultSetType(resultSetType); ((com.mysql.jdbc.PreparedStatement)pStmt).setResultSetConcurrency(resultSetConcurrency); &#125; catch (SQLException var13) &#123; if (!this.getEmulateUnsupportedPstmts()) &#123; throw var13; &#125; pStmt = (com.mysql.jdbc.PreparedStatement)this.clientPrepareStatement(nativeSql, resultSetType, resultSetConcurrency, false); if (sql.length() &lt; this.getPreparedStatementCacheSqlLimit()) &#123; this.serverSideStatementCheckCache.put(sql, Boolean.FALSE); &#125; &#125; &#125; &#125; &#125; else &#123; // // 向 Server 提交 SQL 预编译 try &#123; pStmt = ServerPreparedStatement.getInstance(this.getMultiHostSafeProxy(), nativeSql, this.database, resultSetType, resultSetConcurrency); ((com.mysql.jdbc.PreparedStatement)pStmt).setResultSetType(resultSetType); ((com.mysql.jdbc.PreparedStatement)pStmt).setResultSetConcurrency(resultSetConcurrency); &#125; catch (SQLException var12) &#123; if (!this.getEmulateUnsupportedPstmts()) &#123; throw var12; &#125; pStmt = (com.mysql.jdbc.PreparedStatement)this.clientPrepareStatement(nativeSql, resultSetType, resultSetConcurrency, false); &#125; &#125; &#125; else &#123; pStmt = (com.mysql.jdbc.PreparedStatement)this.clientPrepareStatement(nativeSql, resultSetType, resultSetConcurrency, false); &#125; return (PreparedStatement)pStmt; &#125;&#125; 从代码里可以看到，服务(代码里)缓存了解析编译的语句，如果有直接拿来用。 Neo4j Neo4j 连接方式 有 neo4j-jdbc-driver neo4j-jdbc-bolt neo4j-jdbc-http org.neo4j.jdbc.Neo4jPreparedStatement.java 12345678910111213/** * Default constructor with connection and statement. * * @param connection The JDBC connection * @param rawStatement The prepared statement */protected Neo4jPreparedStatement(Neo4jConnection connection, String rawStatement) &#123; super(connection); this.statement = PreparedStatementBuilder.replacePlaceholders(rawStatement); this.parametersNumber = PreparedStatementBuilder.namedParameterCount(statement); this.parameters = new HashMap&lt;&gt;(this.parametersNumber); this.batchParameters = new ArrayList&lt;&gt;();&#125; org.neo4j.jdbc.utils.PreparedStatementBuilder12345678910111213141516171819202122/** * This method return a String that is the original raw string with all valid placeholders replaced with neo4j curly brackets notation for parameters. * &lt;br&gt; * i.e. MATCH n RETURN n WHERE n.name = ? is transformed in MATCH n RETURN n WHERE n.name = &#123;1&#125; * * @param raw The string to be translated. * @return The string with the placeholders replaced. */public static String replacePlaceholders(String raw) &#123; int index = 1; String digested = raw; String regex = "\\?(?=[^\"]*(?:\"[^\"]*\"[^\"]*)*$)"; Matcher matcher = Pattern.compile(regex).matcher(digested); while (matcher.find()) &#123; digested = digested.replaceFirst(regex, "&#123;" + index + "&#125;"); index++; &#125; return digested;&#125; neo4j-jdbc 里对PreparedStatement里的语句仅仅是把占位符组装成一个cypher语句，没有做预编译处理 Orientdb com.orientechnologies.orient.jdbc.OrientJdbcPreparedStatement.java 1234567891011121314public OrientJdbcPreparedStatement(OrientJdbcConnection iConnection, String sql) &#123; this(iConnection, 1003, 1007, 1, sql);&#125;public OrientJdbcPreparedStatement(OrientJdbcConnection iConnection, int resultSetType, int resultSetConcurrency, String sql) throws SQLException &#123; this(iConnection, resultSetType, resultSetConcurrency, 1, sql);&#125;public OrientJdbcPreparedStatement(OrientJdbcConnection iConnection, int resultSetType, int resultSetConcurrency, int resultSetHoldability, String sql) &#123; super(iConnection, resultSetType, resultSetConcurrency, resultSetHoldability); this.sql = sql; this.params = new HashMap();&#125; orientdb-jdbc jar包里没有对PreparedStatement的语句做预编译处理 References[1] JDBC PreparedStatement 实现原理【推荐阅读】[2] mysql-JDBC源码解析[3] mysql sql-syntax-prepared-statements[4] github mysql-connector-j[5] github neo4j-jdbc [6] neo4j-jdbc doc[7] github orientdb-jdbc[8] orientdb-docs]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jdbc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java 并发]]></title>
    <url>%2F2019%2F08%2F18%2Fjava-concurrent%2F</url>
    <content type="text"><![CDATA[Java并发 References[1] 忘掉 Java 并发，先听完这个故事。。。[2] [一个故事搞懂Java并发编程] (https://www.cnblogs.com/flashsun/p/11017431.html )]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka-notes]]></title>
    <url>%2F2019%2F08%2F12%2Fkafka-notes%2F</url>
    <content type="text"><![CDATA[kafka详解 References[1] Kafka的架构原理，你真的理解吗？[2] 为什么Kafka速度那么快[3] 我以为我对Kafka很了解，直到我看了这篇文章[4] 震惊了，原来这才是Kafka的“真面目”！]]></content>
      <categories>
        <category>mq</category>
      </categories>
      <tags>
        <tag>mq</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBASE 安装使用]]></title>
    <url>%2F2019%2F07%2F26%2Fhbase-install-config-note%2F</url>
    <content type="text"><![CDATA[下载 hbase所有版本下载地址 https://www.apache.org/dyn/closer.lua/hbase/2.2.0/hbase-2.2.0-bin.tar.gz http://mirror.bit.edu.cn/apache/hbase/2.2.0/hbase-2.2.0-bin.tar.gz http://mirrors.tuna.tsinghua.edu.cn/apache/hbase/2.2.0/hbase-2.2.0-bin.tar.gz 下载完解压 配置 修改 bin/hbase-env.sh 文件 设置JAVA_HOME export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home/ 修改 bin/hbase-site.xml 文件12345678910111213141516171819&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;file:///Users/weikeqin1/SoftWare/hbase-2.2.0/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/Users/weikeqin1/SoftWare/hbase-2.2.0/data_zookeeper&lt;/value&gt; &lt;/property&gt; &lt;!-- use localhost zookeeper --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;localhost&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动hbase ./bin/start-hbase.sh12ZBMAC-C02PGMT0F:bin weikeqin1$ ./start-hbase.shrunning master, logging to /Users/weikeqin1/SoftWare/hbase-2.2.0/bin/../logs/hbase-weikeqin1-master-ZBMAC-C02PGMT0F.local.out 验证是否启动成功 jps -l 找到 pid org.apache.hadoop.hbase.master.HMaster 说明启动成功，也可以用 ./hbase shell 验证 hbase 语句查看所有命名空间 list_namespace 创建命名空间 create_namespace &#39;ns_h&#39; 查看所有表 list 查看指定命名空间下的表 list_namespace_tables &#39;ns_h&#39; 新建表 create &#39;ns_h:user_xyj&#39;, &#39;cf1&#39;, &#39;cf2&#39; 新建一个以命名空间ns_h的表 user_xyj，列族为cf1,cf2。 删除表 disable &#39;ns_h:user_xyj&#39; drop &#39;ns_h:user_xyj&#39; 查看表内容 scan &#39;ns_h:user_xyj&#39;, {LIMIT=&gt;5} 查看前5行数据 插入 put &#39;namespace:table1&#39;, &#39;rowkey_1&#39;, &#39;cf1:field1&#39;, &#39;field1_value&#39; 1234put &apos;ns_h:user_xyj&apos;,&apos;rowkey_1&apos;,&apos;cf1:name&apos;,&apos;孙悟空&apos;put &apos;ns_h:user_xyj&apos;,&apos;rowkey_1&apos;,&apos;cf1:sex&apos;,&apos;男&apos;put &apos;ns_h:user_xyj&apos;,&apos;rowkey_1&apos;,&apos;cf1:age&apos;,&apos;18&apos;put &apos;ns_h:user_xyj&apos;,&apos;rowkey_1&apos;,&apos;cf2:address&apos;,&apos;北京市朝阳区&apos; hbase shell12345678ZBMAC-C02PGMT0F:bin weikeqin1$ ./hbase shell2019-07-26 18:35:23,037 WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableHBase ShellUse &quot;help&quot; to get list of supported commands.Use &quot;exit&quot; to quit this interactive shell.For Reference, please visit: http://hbase.apache.org/2.0/book.html#shellVersion 2.2.0, rUnknown, Tue Jun 11 04:30:30 UTC 2019Took 0.0038 seconds 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869hbase(main):001:0&gt; helpHBase Shell, version 2.2.0, rUnknown, Tue Jun 11 04:30:30 UTC 2019Type &apos;help &quot;COMMAND&quot;&apos;, (e.g. &apos;help &quot;get&quot;&apos; -- the quotes are necessary) for help on a specific command.Commands are grouped. Type &apos;help &quot;COMMAND_GROUP&quot;&apos;, (e.g. &apos;help &quot;general&quot;&apos;) for help on a command group.COMMAND GROUPS: Group name: general Commands: processlist, status, table_help, version, whoami Group name: ddl Commands: alter, alter_async, alter_status, clone_table_schema, create, describe, disable, disable_all, drop, drop_all, enable, enable_all, exists, get_table, is_disabled, is_enabled, list, list_regions, locate_region, show_filters Group name: namespace Commands: alter_namespace, create_namespace, describe_namespace, drop_namespace, list_namespace, list_namespace_tables Group name: dml Commands: append, count, delete, deleteall, get, get_counter, get_splits, incr, put, scan, truncate, truncate_preserve Group name: tools Commands: assign, balance_switch, balancer, balancer_enabled, catalogjanitor_enabled, catalogjanitor_run, catalogjanitor_switch, cleaner_chore_enabled, cleaner_chore_run, cleaner_chore_switch, clear_block_cache, clear_compaction_queues, clear_deadservers, close_region, compact, compact_rs, compaction_state, compaction_switch, decommission_regionservers, flush, is_in_maintenance_mode, list_deadservers, list_decommissioned_regionservers, major_compact, merge_region, move, normalize, normalizer_enabled, normalizer_switch, recommission_regionserver, rit, split, splitormerge_enabled, splitormerge_switch, stop_master, stop_regionserver, trace, unassign, wal_roll, zk_dump Group name: replication Commands: add_peer, append_peer_exclude_namespaces, append_peer_exclude_tableCFs, append_peer_namespaces, append_peer_tableCFs, disable_peer, disable_table_replication, enable_peer, enable_table_replication, get_peer_config, list_peer_configs, list_peers, list_replicated_tables, remove_peer, remove_peer_exclude_namespaces, remove_peer_exclude_tableCFs, remove_peer_namespaces, remove_peer_tableCFs, set_peer_bandwidth, set_peer_exclude_namespaces, set_peer_exclude_tableCFs, set_peer_namespaces, set_peer_replicate_all, set_peer_serial, set_peer_tableCFs, show_peer_tableCFs, update_peer_config Group name: snapshots Commands: clone_snapshot, delete_all_snapshot, delete_snapshot, delete_table_snapshots, list_snapshots, list_table_snapshots, restore_snapshot, snapshot Group name: configuration Commands: update_all_config, update_config Group name: quotas Commands: disable_exceed_throttle_quota, disable_rpc_throttle, enable_exceed_throttle_quota, enable_rpc_throttle, list_quota_snapshots, list_quota_table_sizes, list_quotas, list_snapshot_sizes, set_quota Group name: security Commands: grant, list_security_capabilities, revoke, user_permission Group name: procedures Commands: list_locks, list_procedures Group name: visibility labels Commands: add_labels, clear_auths, get_auths, list_labels, set_auths, set_visibility Group name: rsgroup Commands: add_rsgroup, balance_rsgroup, get_rsgroup, get_server_rsgroup, get_table_rsgroup, list_rsgroups, move_namespaces_rsgroup, move_servers_namespaces_rsgroup, move_servers_rsgroup, move_servers_tables_rsgroup, move_tables_rsgroup, remove_rsgroup, remove_servers_rsgroupSHELL USAGE:Quote all names in HBase Shell such as table and column names. Commas delimitcommand parameters. Type &lt;RETURN&gt; after entering a command to run it.Dictionaries of configuration used in the creation and alteration of tables areRuby Hashes. They look like this: &#123;&apos;key1&apos; =&gt; &apos;value1&apos;, &apos;key2&apos; =&gt; &apos;value2&apos;, ...&#125;and are opened and closed with curley-braces. Key/values are delimited by the&apos;=&gt;&apos; character combination. Usually keys are predefined constants such asNAME, VERSIONS, COMPRESSION, etc. Constants do not need to be quoted. Type&apos;Object.constants&apos; to see a (messy) list of all constants in the environment.If you are using binary keys or values and need to enter them in the shell, usedouble-quote&apos;d hexadecimal representation. For example: hbase&gt; get &apos;t1&apos;, &quot;key\x03\x3f\xcd&quot; hbase&gt; get &apos;t1&apos;, &quot;key\003\023\011&quot; hbase&gt; put &apos;t1&apos;, &quot;test\xef\xff&quot;, &apos;f1:&apos;, &quot;\x01\x33\x40&quot;The HBase shell is the (J)Ruby IRB with the above HBase-specific commands added.For more on the HBase Shell, see http://hbase.apache.org/book.htmlhbase(main):002:0&gt; exitZBMAC-C02PGMT0F:bin weikeqin1$ 创建表12345hbase(main):001:0&gt; create &apos;student&apos;, &apos;description&apos;, &apos;course&apos;Created table studentTook 1.3820 seconds=&gt; Hbase::Table - studenthbase(main):002:0&gt; 信息明细1234567hbase(main):003:0* list &apos;student&apos;TABLEstudent1 row(s)Took 0.0411 seconds=&gt; [&quot;student&quot;]hbase(main):004:0&gt; 12hbase(main):005:0* list studentNameError: undefined local variable or method `student&apos; for main:Object 123456789101112131415161718192021hbase(main):007:0* put &apos;student&apos;, &apos;row1&apos;, &apos;descroption:age&apos;, &apos;18&apos;ERROR: org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column family descroption does not exist in region student,,1564137475186.c45878edd2192b0caaac80b33c2f8915. in table &apos;student&apos;, &#123;NAME =&gt; &apos;course&apos;, VERSIONS =&gt; &apos;1&apos;, EVICT_BLOCKS_ON_CLOSE =&gt; &apos;false&apos;, NEW_VERSION_BEHAVIOR =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, CACHE_DATA_ON_WRITE =&gt; &apos;false&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, CACHE_INDEX_ON_WRITE =&gt; &apos;false&apos;, IN_MEMORY =&gt; &apos;false&apos;, CACHE_BLOOMS_ON_WRITE =&gt; &apos;false&apos;, PREFETCH_BLOCKS_ON_OPEN =&gt; &apos;false&apos;, COMPRESSION =&gt; &apos;NONE&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;&#125;, &#123;NAME =&gt; &apos;description&apos;, VERSIONS =&gt; &apos;1&apos;, EVICT_BLOCKS_ON_CLOSE =&gt; &apos;false&apos;, NEW_VERSION_BEHAVIOR =&gt; &apos;false&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, CACHE_DATA_ON_WRITE =&gt; &apos;false&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, TTL =&gt; &apos;FOREVER&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, CACHE_INDEX_ON_WRITE =&gt; &apos;false&apos;, IN_MEMORY =&gt; &apos;false&apos;, CACHE_BLOOMS_ON_WRITE =&gt; &apos;false&apos;, PREFETCH_BLOCKS_ON_OPEN =&gt; &apos;false&apos;, COMPRESSION =&gt; &apos;NONE&apos;, BLOCKCACHE =&gt; &apos;true&apos;, BLOCKSIZE =&gt; &apos;65536&apos;&#125; at org.apache.hadoop.hbase.regionserver.HRegion.doBatchMutate(HRegion.java:4308) at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:3096) at org.apache.hadoop.hbase.regionserver.RSRpcServices.mutate(RSRpcServices.java:2870) at org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:42000) at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:413) at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:132) at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:324) at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:304)For usage try &apos;help &quot;put&quot;&apos;Took 0.4988 secondshbase(main):008:0&gt; put &apos;student&apos;, &apos;row1&apos;, &apos;description:age&apos;, &apos;18&apos;Took 0.0229 secondshbase(main):009:0&gt;hbase(main):010:0* put &apos;student&apos;, &apos;row1&apos;, &apos;description:name&apos;, &apos;liu&apos;Took 0.0127 secondshbase(main):011:0&gt; put &apos;student&apos;, &apos;row1&apos;, &apos;course:chinese&apos;, &apos;100&apos; References[1] Mac下安装HBase及详解]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
        <tag>nosql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java BIO NIO AIO]]></title>
    <url>%2F2019%2F07%2F08%2Fjava-bio-nio-aio%2F</url>
    <content type="text"><![CDATA[转自 漫画：一文学会面试中常问的 IO 问题！ 什么是BIO、NIO和AIO？三者有什么区别？具体如何使用? 输入/输出是信息处理系统（例如计算机）与外部世界（可能是人类或另一信息处理系统）之间的通信。 输入是系统接收的信号或数据，输出则是从其发送的信号或数据。 在Java中，提供了一些列API，可以供开发者来读写外部数据或文件。我们称这些API为Java IO。 IO是Java中比较重要，且比较难的知识点，主要是因为随着Java的发展，目前有三种IO共存。分别是BIO、NIO和AIO。 Java BIO BIO 全称Block-IO 是一种同步且阻塞的通信模式。是一个比较传统的通信方式，模式简单，使用方便。但并发处理能力低，通信耗时，依赖网速。 Java NIO Java NIO，全称 Non-Block IO ，是Java SE 1.4版以后，针对网络传输效能优化的新功能。是一种非阻塞同步的通信模式。 NIO 与原来的 I/O 有同样的作用和目的, 他们之间最重要的区别是数据打包和传输的方式。原来的 I/O 以流的方式处理数据，而 NIO 以块的方式处理数据。 面向流的 I/O 系统一次一个字节地处理数据。一个输入流产生一个字节的数据，一个输出流消费一个字节的数据。 面向块的 I/O 系统以块的形式处理数据。每一个操作都在一步中产生或者消费一个数据块。按块处理数据比按(流式的)字节处理数据要快得多。但是面向块的 I/O 缺少一些面向流的 I/O 所具有的优雅性和简单性。 Java AIO Java AIO，全称 Asynchronous IO，是异步非阻塞的IO。是一种非阻塞异步的通信模式。 在NIO的基础上引入了新的异步通道的概念，并提供了异步文件通道和异步套接字通道的实现。 三种IO的区别 首先，我们站在宏观的角度，重新画一下重点： BIO （Blocking I/O）：同步阻塞I/O模式。 NIO （New I/O）：同步非阻塞模式。 AIO （Asynchronous I/O）：异步非阻塞I/O模型。 那么，同步阻塞、同步非阻塞、异步非阻塞都是怎么回事呢？关于这部分内容也可以查看《漫话：如何给女朋友解释什么是IO中的阻塞、非阻塞、同步、异步？》。 同步阻塞模式：这种模式下，我们的工作模式是先来到厨房，开始烧水，并坐在水壶面前一直等着水烧开。 同步非阻塞模式：这种模式下，我们的工作模式是先来到厨房，开始烧水，但是我们不一直坐在水壶前面等，而是回到客厅看电视，然后每隔几分钟到厨房看一下水有没有烧开。 异步非阻塞I/O模型：这种模式下，我们的工作模式是先来到厨房，开始烧水，我们不一一直坐在水壶前面等，也不隔一段时间去看一下，而是在客厅看电视，水壶上面有个开关，水烧开之后他会通知我。 阻塞VS非阻塞：人是否坐在水壶前面一直等。 同步VS异步：水壶是不是在水烧开之后主动通知人。 试用场景 BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解。 NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。 AIO方式适用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持。 References[1] 漫画：一文学会面试中常问的 IO 问题！[2] 10个最高频的Java NIO面试题剖析！]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[REST VS RPC]]></title>
    <url>%2F2019%2F07%2F03%2Frest-vs-rpc%2F</url>
    <content type="text"><![CDATA[小型的项目用什么语言都行，爱用什么用什么。但是，真正的企业级架构就不一样了，其中并不仅仅只是 RESTful API 或 RPC，还有各种配套设施和控制系统，比如：应用网关，服务发现、配置中心、健康检查、服务监控、服务治理（熔断、限流、幂等、重试、隔离、事务补偿）、Tracing监控、SOA/ESB、CQRS、EDA…… 本文不会告诉你必须用什么，但是会告诉你两者的优缺点。 rest 以 springcloud为例，rpc以dubbo为例。 从性能上来讲，在数据量小（单个请求小于20K）并发大的场景下，rpc的性能比restful api的性能要好。 从上手、使用、维护 方面来说，springcloud更容易上手、维护。 各组件配置使用运行流程： 1、请求统一通过API网关（Zuul）来访问内部服务. 2、网关接收到请求后，从注册中心（Eureka）获取可用服务 3、由Ribbon进行均衡负载后，分发到后端具体实例 4、微服务之间通过Feign进行通信处理业务 5、Hystrix负责处理服务超时熔断 6、Turbine监控服务间的调用和熔断相关指标 springcloud提供了微服务所需的全套组件，降低了架构和开发成本，解决了微服务的多个痛点问题。具体如下： 注册中心 配置中心 服务间的调用方式 链路追踪 服务的限流、降级、熔断 如果使用dubbo，配置中心、分布式跟踪这些内容都需要自己去集成，无形中增加了难度。 References[1] 微服务架构的基础框架选择：Spring Cloud还是Dubbo？[2] 比较spring cloud和dubbo，各自的优缺点是什么[3] 中小型互联网公司微服务实践-经验和教训[4] SpringBoot和SpringCloud为什么会那么火？对开发有哪些帮助？[5] 项目中为什么首先spring cloud，而不是dubbo[6] Microservices-微服务原文]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springcloud笔记]]></title>
    <url>%2F2019%2F07%2F03%2Fspringcloud-notes%2F</url>
    <content type="text"><![CDATA[各组件配置使用运行流程： 1、请求统一通过API网关（Zuul）来访问内部服务. 2、网关接收到请求后，从注册中心（Eureka）获取可用服务 3、由Ribbon进行均衡负载后，分发到后端具体实例 4、微服务之间通过Feign进行通信处理业务 5、Hystrix负责处理服务超时熔断 6、Turbine监控服务间的调用和熔断相关指标 springcloud提供了微服务所需的全套组件，降低了架构和开发成本，解决了微服务的多个痛点问题。具体如下： 注册中心 配置中心 服务间的调用方式 链路追踪 服务的限流、降级、熔断 References[1] spring-cloud[2] springcloud中文网[3] springcloud(一)：大话Spring Cloud-纯洁的微笑[4] 史上最简单的 SpringCloud 教程 | 终章-方志朋[5] Spring Cloud 从入门到精通[6] Spring Cloud中国社区]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[orientdb 常用语句]]></title>
    <url>%2F2019%2F06%2F30%2Forientdb-sql%2F</url>
    <content type="text"><![CDATA[orientdb启动后可以在 http://localhost:2480 打开前端管理界面操作，也可以在shell里操作 orientdb的很多语句很像mysql和neo4j，学习的时候可以用来对比 常用语句 orientdb mysql 连接数据库 CONNECT remote:127.0.0.1 root root mysql -u root -p root 查看所有数据库 LIST DATABASES show databases 创建数据库 CREATE DATABASE remote:127.0.0.1/db_test_wkq root root create database db_test_wkq 使用某个数据库 CONNECT remote:127.0.0.1/db_test_wkq root root use db_test_wkq 查询节点类型(表)个数 LIST CLASSES select expand(classes) from metadata:schema show tables 创建某类节点 CREATE CLASS TestVertex extends V create table table1 查看某个类型的结构 INFO CLASS OUser desc table1 查看某个类型的数据 BROWSE CLASS OUser select * from OUser limit 20 CREATE DATABASE &lt;database-url&gt; [&lt;user&gt;] [&lt;password&gt;] [&lt;storage-type&gt;] [&lt;db-type&gt;] [&lt;[options]&gt;] 创建数据库 CREATE DATABASE PLOCAL:/usr/local/orientdb/databases/db_test_wkq CREATE DATABASE remote:127.0.0.1/db_test_wkq root root plocal graph orientdb mysql 新增节点(CRUD) insert into v1 (brand, name) values (“宝马”, “车型1”) insert into v1 (brand, name) values (“宝马”, “车型1”) 新增节点(Graph) CREATE VERTEX V1 SET brand = ‘宝马’, name = ‘车型1’ 更新节点(CRUD) UPDATE Employee SET local=TRUE WHERE city=’London’ UPDATE Employee SET local=TRUE WHERE city=’London’ 更新节点(Graph) UPDATE Employee MERGE { local : TRUE } WHERE city=’London’ 删除节点 DELETE FROM Employee WHERE city &lt;&gt; ‘London’ DELETE FROM Employee WHERE city &lt;&gt; ‘London’ 新增边 CREATE EDGE E FROM #22:33 TO #22:55 CONTENT { “name”: “Jay”, “surname”: “Miner” } 更新边 UPDATE EDGE hasAssignee SET foo = ‘bar’ UPSERT WHERE id = 56 删除边 DELETE EDGE #22:38482 查询 MATCH {class: Person, as: people, where: (name = ‘John’)} RETURN people 查看某个类型的数据 SELECT FROM Person 基础操作连接数据库 // mysql -u root -p root CONNECT remote:localhost root root 查看有哪些库 mysql show databses ; orientdb list databases ; 1234567orientdb &#123;server=remote:localhost&#125;&gt; list databases;Found 3 databases:* test* demodb* test_wkq 创建数据库 create database remote:localhost/mydb root root plocal graph -restore=/tmp/backup create database remote:localhost/test root root 1234567orientdb &#123;server=remote:localhost/test&#125;&gt; create database remote:localhost/test root adminCreating database [remote:localhost/test] using the storage type [PLOCAL]...Database created successfully.Current database is: remote:localhost/testorientdb &#123;db=test&#125;&gt; list databases; 使用某个库// user test_wkq123orientdb &#123;server=remote:localhost&#125;&gt; CONNECT remote:localhost/test_wkq root rootConnecting to database [remote:localhost/test_wkq] with user 'root'...OK 查看节点类型个数// show tables; 12345678910111213141516171819202122232425262728293031orientdb &#123;db=test_wkq&#125;&gt; LIST CLASSES;CLASSES +----+-------------------+-------------+---------------------------------------------------------+-----+ |# |NAME |SUPER-CLASSES|CLUSTERS |COUNT| +----+-------------------+-------------+---------------------------------------------------------+-----+ |0 |_studio | |_studio(17),_studio_1(18),_studio_2(19),_studio_3(20) | 1| |1 |E | |e(13),e_1(14),e_2(15),e_3(16) | 0| |2 |FriendOf |[E] |friendof(25),friendof_1(26),friendof_2(27),friendof_3(28)| 2| |3 |OFunction | |ofunction(6) | 0| |4 |OGeometryCollection|[OShape] |- | 0| |5 |OIdentity | |- | 0| |6 |OLineString |[OShape] |- | 0| |7 |OMultiLineString |[OShape] |- | 0| |8 |OMultiPoint |[OShape] |- | 0| |9 |OMultiPolygon |[OShape] |- | 0| |10 |OPoint |[OShape] |- | 0| |11 |OPolygon |[OShape] |- | 0| |12 |ORectangle |[OShape] |- | 0| |13 |ORestricted | |- | 0| |14 |ORole |[OIdentity] |orole(4) | 3| |15 |OSchedule | |oschedule(8) | 0| |16 |OSequence | |osequence(7) | 0| |17 |OShape | |- | 0| |18 |OTriggered | |- | 0| |19 |OUser |[OIdentity] |ouser(5) | 3| |20 |Person |[V] |person(21),person_1(22),person_2(23),person_3(24) | 3| |21 |V | |v(9),v_1(10),v_2(11),v_3(12) | 0| +----+-------------------+-------------+---------------------------------------------------------+-----+ | |TOTAL | | | 12| +----+-------------------+-------------+---------------------------------------------------------+-----+ 创建某种节点orientdb {db=test_wkq}&gt; CREATE CLASS Table1 extends V Class created successfully. orientdb&gt; CREATE CLASS Student Class created successfully. Total classes in database now: 15 赋值orientdb&gt; CREATE PROPERTY Student.name STRING Property created successfully with id=1 orientdb&gt; CREATE PROPERTY Student.surname STRING Property created successfully with id=2 orientdb&gt; CREATE PROPERTY Student.birthDate DATE Property created successfully with id=3 查看某个类型的结构// desc OUserorientdb {db=test_wkq}&gt; INFO CLASS OUser CLASS ‘OUser’ Records…………..: 3Super classes……..: [OIdentity]Default cluster……: ouser (id=5)Supported clusters…: ouser(5)Cluster selection….: round-robinOversize………….: 0.0 PROPERTIES+—-+——–+—————–+———+——–+——–+—-+—-+——-+——-+|# |NAME |LINKED-TYPE/CLASS|MANDATORY|READONLY|NOT-NULL|MIN |MAX |COLLATE|DEFAULT|+—-+——–+—————–+———+——–+——–+—-+—-+——-+——-+|0 |password| |true |false |true | | |default| ||1 |roles |ORole |false |false |false | | |default| ||2 |name | |true |false |true |1 | |ci | ||3 |status | |true |false |true | | |default| |+—-+——–+—————–+———+——–+——–+—-+—-+——-+——-+ INDEXES (1 altogether)+—-+———-+———-+|# |NAME |PROPERTIES|+—-+———-+———-+|0 |OUser.name|[name] |+—-+———-+———-+ orientdb&gt; ALTER PROPERTY Student.name MIN 3Property updated successfully orientdb&gt; ALTER PROPERTY Student.name MANDATORY true 查看某个类型的数据(默认最多显示20条)// select * from OUser limit 20 ;orientdb {db=test_wkq}&gt; BROWSE CLASS OUser +—-+—-+——+——+——+——+—————————————————————————————————————–+|# |@RID|@CLASS|name |status|roles |password |+—-+—-+——+——+——+——+—————————————————————————————————————–+|0 |#5:0|OUser |admin |ACTIVE|[#4:0]|{PBKDF2WithHmacSHA256}AFD543552898111E1C874350E7A09398E20A15B098E2010E:E42843B52BE6EDA36DA6282920E75876C7DDFBA…||1 |#5:1|OUser |reader|ACTIVE|[#4:1]|{PBKDF2WithHmacSHA256}9339EAE81055AE1CA400CBC511AAD42DE5E416841B709D53:D2F810E5E1D2EFA7E7E7DA65E7D1338F2F76904…||2 |#5:2|OUser |writer|ACTIVE|[#4:2]|{PBKDF2WithHmacSHA256}67CED97F7AF9C6B445E120CF4B06EA4D40A745DB330D42D6:A5B4D1A71C166578EDF78B4873A5F74E1D5330B…|+—-+—-+——+——+——+——+—————————————————————————————————————–+ 查看第一条记录// select * from table order by id asc limit 1 ;orientdb {db=test_wkq}&gt; DISPLAY RECORD 0 DOCUMENT @class:OUser @rid:#5:0 @version:1+—-+——–+—————————————————————————————————————————–+|# |NAME |VALUE |+—-+——–+—————————————————————————————————————————–+|0 |password|{PBKDF2WithHmacSHA256}AFD543552898111E1C874350E7A09398E20A15B098E2010E:E42843B52BE6EDA36DA6282920E75876C7DDFBA4A135503E:65536||1 |roles |[#4:0] ||2 |name |admin ||3 |status |ACTIVE |+—-+——–+—————————————————————————————————————————–+ orientdb {db=test_wkq}&gt; SELECT Version FROM OUser; +—-+———+|# |Version|+—-+———+|0 | ||1 | ||2 | |+—-+———+ 3 item(s) found. Query executed in 0.029 sec(s).orientdb {db=test_wkq}&gt; 合并重复节点1234select expand($c)let $a = ( SELECT EXPAND(out(&apos;E1&apos;).out(&apos;E3&apos;)) FROM V1 WHERE id = &lt;someIdThatV1Has&gt;),$b = ( SELECT EXPAND(out(&apos;E1&apos;).out(&apos;E2&apos;).out(&apos;E3&apos;)) FROM V1 WHERE id = &lt;someIdThatV1Has&gt;),$c = unionAll( $a, $b ) 查询所有类别 select expand(classes) from metadata:schema 查询某个类别(表)结构1234567select expand(properties) from ( select expand(classes) from metadata:schema) where name = &apos;OUser&apos;select customFields from ( select expand(classes) from metadata:schema) where name=&quot;OUser&quot; 查询所有索引 select expand(indexes) from metadata:indexmanager Querying database metadata SELECT FROM metadata:database Querying storage metadata SELECT FROM metadata:storage // 创建节点类型CREATE CLASS V1 EXTENDS V// 插入节点CREATE VERTEX V1 SET brand = ‘maruti’, name = ‘swift’ CREATE EDGE USER_RELATION_FRIEND FROM (SELECT FROM userList where id = ${input.FROM_ID}) TO (SELECT FROM userList where id = ${input.TO_ID}) set weight=${input.WEIGHT}”} SSELECT SELECT FROM OUser SELECT SELECT FROM CLUSTER:Ouser 记录ID要对一个或多个记录ID执行，请使用标识符作为目标 SELECT FROM #10:3SELECT FROM [#10:1, #10:30, #10:5] 索引要对索引执行查询，请INDEX为目标名称添加前缀。 SELECT VALUE FROM INDEX:dictionary WHERE key=&#39;Jay&#39; 模糊匹配 SELECT FROM OUser WHERE name LIKE &#39;l%&#39; 排序 SELECT FROM Employee WHERE city=&#39;Rome&#39; ORDER BY surname ASC, name ASC 聚合 SELECT SUM(salary) FROM Employee WHERE age &lt; 40 GROUP BY job 分页 SELECT FROM Employee WHERE gender=&#39;male&#39; LIMIT 20 分页-2 SELECT FROM Employee WHERE gender=&#39;male&#39; LIMIT 20SELECT FROM Employee WHERE gender=&#39;male&#39; SKIP 20 LIMIT 20SELECT FROM Employee WHERE gender=&#39;male&#39; SKIP 40 LIMIT 20 INSERT INSERT语句将新数据添加到类和集群。OrientDB支持三种语法形式，用于将新数据插入数据库。 标准的ANSI-92语法 INSERT INTO Employee(name, surname, gender) VALUES(&#39;Jay&#39;, &#39;Miner&#39;, &#39;M&#39;) 简化的ANSI-92语法 INSERT INTO Employee SET name=&#39;Jay&#39;, surname=&#39;Miner&#39;, gender=&#39;M&#39; JSON语法 INSERT INTO Employee CONTENT {name : &#39;Jay&#39;, surname : &#39;Miner&#39;, gender : &#39;M&#39;} UPDATE 该UPDATE语句更改了类和集群中现有数据的值。在OrientDB中，有两种形式的语法用于更新数据库上的数据。 标准的ANSI-92语法 UPDATE Employee SET local=TRUE WHERE city=&#39;London&#39; 与MERGE关键字一起使用的JSON语法，它将更改与当前记录合并 UPDATE Employee MERGE { local : TRUE } WHERE city=&#39;London&#39; DELETE DELETE FROM Employee WHERE city &lt;&gt; &#39;London&#39; CREATE INDEX T.id ON T(id) UNIQUE Tutorial-Working-with-graphs 在1.4.x版本中，OrientDB开始将一些边缘作为轻量级边缘进行管理。轻量级边缘没有记录ID，但物理存储为顶点内的链接。请注意，OrientDB仅在边缘没有属性时才使用轻量级边缘，否则它使用标准边缘。 从逻辑的角度来看，Lightweight Edges是所有效果中的边缘，因此所有图形函数都可以使用它们。这是为了提高性能并减少磁盘空间。 由于轻量级边缘不作为数据库中的单独记录存在，因此某些查询将无法按预期工作。例如， orientdb&gt; SELECT FROM E对于大多数情况，使用边连接顶点，因此该查询不会特别导致任何问题。但是，它不会在结果集中返回Lightweight Edges。如果您需要直接查询边缘（包括没有属性的边缘），请禁用“轻量级边缘”功能。 要禁用轻量级边缘功能，请执行以下命令。 orientdb&gt; ALTER DATABASE CUSTOM useLightweightEdges=FALSE您只需要执行一次此命令。OrientDB现在生成新边缘作为标准边缘，而不是轻量级边缘。请注意，这不会影响现有边缘。 orientdb&gt; CREATE PROPERTY Owns.out LINK Person orientdb&gt; CREATE PROPERTY Owns.in LINK Car References[1] SQL-Commands[2] Console-Commands[3] queries-demo[4] Tutorial-Working-with-graphs]]></content>
      <categories>
        <category>orientdb</category>
      </categories>
      <tags>
        <tag>orientdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM类加载机制]]></title>
    <url>%2F2019%2F06%2F01%2Fjvm-class-loader%2F</url>
    <content type="text"><![CDATA[首先问几个问题 JVM类加载机制解决了什么问题？ JVM类加载机制的过程？ Java的代码如果要加密不让别人获取到有什么办法？ Spring是怎么找到加了@Component等注解的类? (1) JVM加载class过程 一个类型从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期将会经历加载(Loading)、验证(Verification)、准备(Preparation)、解析(Resolution)、初始化(Initialization)、使用(Using)和卸载(Unloading)七个阶段。 其中验证、准备、解析三个部分统称为连接(Linking)。 加载、验证、准备、初始化和卸载这五个阶段的顺序是确定，而解析阶段则不一定：它在某些情况下可以在初始化阶段之后再开始。 (1.1) 加载 在加载阶段，Java虚拟机需要完成以下三件事情： 通过一个类的全限定名来获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在内存中生成一个代表这个类的 java.lang.Class 对象，作为方法区这个类的各种数据的访问入口。 (1.2) 验证 验证的目的是确保Class文件的字节流中包含的信息符合《Java虚拟机规范》的全部约束要求，并且不会危害虚拟机自身的安全。从整体上看，验证阶段大致上会完成下面四个阶段的检验动作：文件格式验证、元数据验证、字节码验证和符号引用验证。 (1.2.1) 文件格式验证 验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理，如： 是否以魔数0xCAFEBABE开头 主、次版本号是否在当前Java虚拟机接受范围之内 常量池的常量中是否有不被支持的常量类型(检查常量tag标志) 指向常量的各种索引值中是否有指向不存在的常量或不符合类型的常量 CONSTANT_Utf8_info型的常量中是否有不符合UTF-8编码的数据 Class文件中各个部分及文件本身是否有被删除的或附加的其他信息 (1.2.2) 元数据验证 对字节码描述的信息进行语义分析，以保证其描述的信息符合《Java语言规范》的要求，如： 这个类是否有父类(除了java.lang.Object之外，所有的类都应当有父类) 这个类的父类是否继承了不允许被继承的类(被final修饰的类) 如果这个类不是抽象类，是否实现了其父类或接口之中要求实现的所有方法 类中的字段、方法是否与父类产生矛盾(例如覆盖了父类的final字段，或者出现不符合规则的方法重载，例如方法参数都一致，但返回值类型却不同等) (1.2.3) 字节码验证 通过数据流分析和控制流分析，确定程序语义是合法的、符合逻辑的，如： 保证任意时刻操作数栈的数据类型与指令代码序列都能配合工作，例如不会出现类似于“在操作栈放置了一个int类型的数据，使用时却按long类型来加载入本地变量表中”这样的情况保证任何跳转指令都不会跳转到方法体以外的字节码指令上保证方法体中的类型转换总是有效的，例如父类=子类对象是合法的，返回来就是非法的 (1.2.4) 符号引用验证 符号引用验证发生在虚拟机将符号引用转化为直接引用的时候，即解析阶段。主要目的是检查该类是否缺少或者被禁止访问它依赖的某些外部类、方法、字段等资源，如： 符号引用中通过字符串描述的全限定名是否能找到对应的类 在指定类中是否存在符合方法的字段描述符及简单名称所描述的方法和字段 符号引用中的类、字段、方法的可访问性(private、protected、public、)是否可被当前类访问 (1.3) 准备 准备阶段是正式为类中定义的变量(即静态变量，被static修饰的变量)分配内存并设置类变量初始值的阶段，当类变量被final修饰时，在准备阶段就直接会被复制，不是使用初始值，如： public static int a = 123; public static final int B = 123; 在准备阶段a的值是0，B的值是123。 下面是，基本数据类型的初始值：类型|默认值—|—| int | 0 long | 0L byte | (byte)0 short | (short)0 char| ‘\u000’ float | 0.0f double | 0.0d boolean | false reference| null (1.4) 解析 解析阶段是Java虚拟机将常量池内的符号引用替换为直接引用的过程。 符号引用(Symbolic References)：符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可，引用的目标并不一定是已经加载到虚拟机内存当中的资源； 直接引用(Direct References)：直接引用是可以直接指向目标的指针、相对偏移量或者是一个能间接定位到目标的句柄，如果有了直接引用，那引用的目标必定已经在虚拟机的内存中存在； (1.5) 初始化初始化阶段就是执行类构造器 ()方法的过程，它是真正开始执行Java代码的阶段，比如给类属性赋真实的值。 public static int a = 123;在初始化阶段后，a的值才等于123。 (1) 方法是由编译器自动收集类中的所有类变量(static变量)的赋值动作和静态语句块(static{}块)中的语句合并产生的，收集顺序是源文件中的代码顺序；(2) 方法不是必须的，如果我们的源文件中没有静态语句块和静态属性的赋值，那么久不会有() 方法。(3) 方法在多线程情况下会通过加锁的方式来保证同步，并且只会被执行一次子类() 方法执行之前需要保证先执行父类的() 方法，所以Object类的() 方法是第一个执行的 (1.5) 使用(1.6) 卸载 (2) 类加载器 对于任意一个类，都必须由加载它的类加载器和这个类本身一起共同确立其在Java虚拟机中的唯一性，每一个类加载器，都拥有一个独立的类名称空间。 (2.1) 类加载器类型 启动类加载器(Bootstrap Class Loader) 启动类加载器(Bootstrap Class Loader)：负责加载存放在lib目录，或者被-Xbootclasspath参数所指定的路径中存放的，而且是Java虚拟机能够识别的(按照文件名识别，如rt.jar、tools.jar，名字不符合的类库即使放在lib目录中也不会被加载)类库加载到虚拟机的内存中； 扩展类加载器(Extension Class Loader) 扩展类加载器(Extension Class Loader)：这个类加载器是在类sun.misc.Launcher$ExtClassLoader中以Java代码的形式实现的。它负责加载\lib\ext目录中，或者被java.ext.dirs系统变量所指定的路径中所有的类库； 应用程序类加载器(Application Class Loader) 应用程序类加载器(Application Class Loader)：这个类加载器由sun.misc.Launcher$AppClassLoader来实现，也称为”系统类加载器”。它负责加载用户类路径(ClassPath)上所有的类库，开发者同样可以直接在代码中使用这个类加载器； (2.2) 双亲委派模型如图中展示的各种类加载器之间的层次关系被称为类加载器的“双亲委派模型(ParentsDelegation Model)”。双亲委派模型要求除了顶层的启动类加载器外，其余的类加载器都应有自己的父类加载器。不过这里类加载器之间的父子关系一般不是以继承(Inheritance)的关系来实现的，而是通常使用组合(Composition)关系来复用父加载器的代码。 双亲委派模型的工作过程是：所有类的加载都委托给父加载器去完成，当父加载器无法加载这个类的时候，子加载器才会尝试加载。 双亲委派模型最大的好处就是Java中的类随着它的类加载器一起具备了一种带有优先级的层次关系，保证同一个类只会被一个加载器加载。 (2.2.1) 双亲委派模型的实现先检查请求加载的类型是否已经被加载过，若没有则调用父加载器的loadClass()方法，若父加载器为空则默认使用启动类加载器作为父加载器。假如父类加载器加载失败，抛出ClassNotFoundException异常的话，才调用自己的findClass()方法尝试进行加载。 (2.2.2) 破坏双亲委派模型 双亲委派模型主要出现过3次较大规模“被破坏”的情况： 在1.2之前，由于实现自定义类加载器只有覆盖loadClass()方法，导致了双亲委派模型的破坏，在1.2之后引入了findClass()方法之后得以解决。基础类型无法调用回用户的代码，如JNDI、JDBC、JCE、JAXB和JBI等，他们的接口定义是基础类型，但是他们的实现是各各厂商，这就导致了基础类型需要调用用户代码。后来引入线程上下文类加载器(Thread Context ClassLoader)得以解决。双亲委派模型的第三次“被破坏”是由于用户对程序动态性的追求而导致的，这里所说的“动态性”指的是一些非常“热”门的名词：代码热替换(Hot Swap)、模块热部署(HotDeployment)等。 (2.2.3) 自定义类加载器 自定义类加载器需要继承ClassLoader类，为了不破坏双亲委派模型，自定义类加器建议覆盖findClass()方法，不建议覆盖loadClass()方法。下面是我实现的一个加载加密class文件、防止反编译核心代码的类加载器。 (3) 遇到的问题org.springframework.beans.factory.BeanDefinitionStoreException: Failed to read candidate component class1org.springframework.beans.factory.BeanDefinitionStoreException: Failed to read candidate component class: URL [jar:file:/cn/wkq/java/test/classtest.jar!/com/xx/yy/HelloServiceImpl.class]; nested exception is java.lang.ArrayIndexOutOfBoundsException: Spring在扫描指定包路径下的类时，并不会一一用类加载器加载它们，而是自己把类文件当成普通文件从本地磁盘中读进来变成一个字节数组（并没有经过JVM类加载过程），然后用ASM去解析这个字节数组得到这个类的元数据，然后判断这个类的元数据里面是否有@Component等相关Spring的注解。如果有的话后面才会进一步使用类加载器去加载这个类，没有的话就不会尝试去加载。 References[1] 《深入理解Java虚拟机：JVM高级特性与最佳实践》 周志明[2] 深入理解JVM - 类加载机制[3] 80%以上Javaer可能不知道的一个Spring知识点]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[秒杀系统]]></title>
    <url>%2F2019%2F05%2F13%2Fseckill-system%2F</url>
    <content type="text"><![CDATA[秒杀系统 秒杀其实主要解决两个问题，一个是并发读，一个是并发写 并发读的核心优化理念是尽量减少用户到服务端来“读”数据，或者让他们读更少的数据；并发写的处理原则也一样，它要求我们在数据库层面独立出来一个库，做特殊的处理。另外，我们还要针对秒杀系统做一些保护，针对意料之外的情况设计兜底方案，以防止最坏的情况发生。 所谓“稳”，就是整个系统架构要满足高可用，流量符合预期时肯定要稳定，就是超出预期时也同样不能掉链子，你要保证秒杀活动顺利完成，即秒杀商品顺利地卖出去，这个是最基本的前提。 然后就是“准”，就是秒杀 10 台 iPhone，那就只能成交 10 台，多一台少一台都不行。一旦库存不对，那平台就要承担损失，所以“准”就是要求保证数据的一致性。 最后再看“快”，“快”其实很好理解，它就是说系统的性能要足够高，否则你怎么支撑这么大的流量呢？不光是服务端要做极致的性能优化，而且在整个请求链路上都要做协同的优化，每个地方快一点，整个系统就完美了。 所以从技术角度上看“稳、准、快”，就对应了我们架构上的高可用、一致性和高性能的要求，我们的专栏也将主要围绕这几个方面来展开，具体如下。 秒杀涉及大量的并发读和并发写，因此支持高并发访问这点非常关键。本专栏将从设计数据的动静分离方案、热点的发现与隔离、请求的削峰与分层过滤、服务端的极致优化这 4 个方面重点介绍。 秒杀中商品减库存的实现方式同样关键。可想而知，有限数量的商品在同一时刻被很多倍的请求同时来减库存，减库存又分为“拍下减库存”“付款减库存”以及预扣等几种，在大并发更新的过程中都要保证数据的准确性，其难度可想而知。因此，我将用一篇文章来专门讲解如何设计秒杀减库存方案。 虽然我介绍了很多极致的优化思路，但现实中总难免出现一些我们考虑不到的情况，所以要保证系统的高可用和正确性，我们还要设计一个 PlanB 来兜底，以便在最坏情况发生时仍然能够从容应对。专栏的最后，我将带你思考可以从哪些环节来设计兜底方案。 架构原则 架构原则：“4 要 1 不要” 1. 数据要尽量少 为啥“数据要尽量少”呢？ 1) 数据在网络上传输需要时间 2) 不管是请求数据还是返回数据都需要服务器做处理 3) 服务器在写网络时通常都要做压缩和字符编码，这些都非常消耗 CPU 所以减少传输的数据量可以显著减少 CPU 的使用。 例如，我们可以简化秒杀页面的大小，去掉不必要的页面装修效果，等等。 其次，“数据要尽量少”还要求系统依赖的数据能少就少，包括系统完成某些业务逻辑需要读取和保存的数据，这些数据一般是和后台服务以及数据库打交道的。调用其他服务会涉及数据的序列化和反序列化，而这也是 CPU 的一大杀手，同样也会增加延时。而且，数据库本身也容易成为一个瓶颈，所以和数据库打交道越少越好，数据越简单、越小则越好。 2. 请求数要尽量少 用户请求的页面返回后，浏览器渲染这个页面还要包含其他的额外请求，比如说，这个页面依赖的 CSS/JavaScript、图片，以及 Ajax 请求等等都定义为“额外请求”，这些额外请求应该尽量少。因为浏览器每发出一个请求都多少会有一些消耗，例如建立连接要做三次握手，有的时候有页面依赖或者连接数限制，一些请求（例如 JavaScript）还需要串行加载等。另外，如果不同请求的域名不一样的话，还涉及这些域名的 DNS 解析，可能会耗时更久。所以你要记住的是，减少请求数可以显著减少以上这些因素导致的资源消耗。 例如，减少请求数最常用的一个实践就是合并 CSS 和 JavaScript 文件，把多个 JavaScript 文件合并成一个文件，在 URL 中用逗号隔开（??module-preview/index.xtpl.js,module-jhs/index.xtpl.js,module-focus/index.xtpl.js）。这种方式在服务端仍然是单个文件各自存放，只是服务端会有一个组件解析这个 URL，然后动态把这些文件合并起来一起返回。 3. 路径要尽量短(调用链) 所谓“路径”，就是用户发出请求到返回数据这个过程中，需求经过的中间的节点数。 通常，这些节点可以表示为一个系统或者一个新的 Socket 连接（比如代理服务器只是创建一个新的 Socket 连接来转发请求）。每经过一个节点，一般都会产生一个新的 Socket 连接。 所以缩短请求路径不仅可以增加可用性，同样可以有效提升性能（减少中间节点可以减少数据的序列化与反序列化），并减少延时（可以减少网络传输耗时）。 要缩短访问路径有一种办法，就是多个相互强依赖的应用合并部署在一起，把远程过程调用（RPC）变成 JVM 内部之间的方法调用。在《大型网站技术架构演进与性能优化》一书中，我也有一章介绍了这种技术的详细实现。 4. 依赖要尽量少 所谓依赖，指的是要完成一次用户请求必须依赖的系统或者服务，这里的依赖指的是强依赖。 举个例子，比如说你要展示秒杀页面，而这个页面必须强依赖商品信息、用户信息，还有其他如优惠券、成交列表等这些对秒杀不是非要不可的信息（弱依赖），这些弱依赖在紧急情况下就可以去掉。 要减少依赖，我们可以给系统进行分级，比如 0 级系统、1 级系统、2 级系统、3 级系统，0 级系统如果是最重要的系统，那么 0 级系统强依赖的系统也同样是最重要的系统，以此类推。 注意，0 级系统要尽量减少对 1 级系统的强依赖，防止重要的系统被不重要的系统拖垮。例如支付系统是 0 级系统，而优惠券是 1 级系统的话，在极端情况下可以把优惠券给降级，防止支付系统被优惠券这个 1 级系统给拖垮。 5. 不要有单点 系统中的单点可以说是系统架构上的一个大忌，因为单点意味着没有备份，风险不可控，我们设计分布式系统最重要的原则就是“消除单点”。 那如何避免单点呢？我认为关键点是避免将服务的状态和机器绑定，即把服务无状态化，这样服务就可以在机器中随意移动。 把秒杀系统独立出来单独打造一个系统，这样可以有针对性地做优化，例如这个独立出来的系统就减少了店铺装修的功能，减少了页面的复杂度； 在系统部署上也独立做一个机器集群，这样秒杀的大流量就不会影响到正常的商品购买集群的机器负载； 将热点数据（如库存数据）单独放到一个缓存系统中，以提高“读性能”； 增加秒杀答题，防止有秒杀器抢单。 例如秒杀的场景来说，不同QPS量级下瓶颈也会不一样， 10w级别可能瓶颈就在数据读取上，通过增加缓存一般就能解决， 如果要到100w那么，可能服务端的网络可能都是瓶颈，所以要把大部分的静态数据放到cdn上甚至缓存在浏览器里 所以要做架构升级，还是主要要分析在预估的QPS下，整个系统的瓶颈会在什么地方，要针对这起瓶颈来重新设计架构方案 详情、购物车、交易、优惠、库存、物流 References[0] 开篇词 | 秒杀系统架构设计都有哪些关键点？[1] 01 | 设计秒杀系统时应该注意的5个架构原则[2] 02 | 如何才能做好动静分离？有哪些方案可选？[3] 03 | 二八原则：有针对性地处理好系统的“热点数据”[4] 04 | 流量削峰这事应该怎么做？[5] 05 | 影响性能的因素有哪些？又该如何提高系统的性能？[6] 06 | 秒杀系统“减库存”设计的核心逻辑[7] 07 | 准备Plan B：如何设计兜底方案?[8] 08 | 答疑解惑：缓存失效的策略应该怎么定？[9] 《大型网站技术架构演进》]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL order by 原理]]></title>
    <url>%2F2019%2F05%2F06%2Fmysql-order-by%2F</url>
    <content type="text"><![CDATA[转自 16 | “order by”是怎么工作的？ 16 “order by”是怎么工作的？ select city,name,age from t where city=&#39;杭州&#39; order by name limit 1000 ; 全字段排序 以我们前面举例用过的市民表为例，假设你要查询城市是“杭州”的所有人名字，并且按照姓名排序返回前 1000 个人的姓名、年龄。 Extra 这个字段中的“Using filesort”表示的就是需要排序，MySQL 会给每个线程分配一块内存用于排序，称为 sort_buffer。 通常情况下，这个语句执行流程如下所示 ： 1.初始化 sort_buffer，确定放入 name、city、age 这三个字段； 2.从索引 city 找到第一个满足 city=’杭州’条件的主键 id，也就是图中的 ID_X； 3.到主键 id 索引取出整行，取 name、city、age 三个字段的值，存入 sort_buffer 中； 4.从索引 city 取下一个记录的主键 id； 5.重复步骤 3、4 直到 city 的值不满足查询条件为止，对应的主键 id 也就是图中的 ID_Y； 6.对 sort_buffer 中的数据按照字段 name 做快速排序； 7.按照排序结果取前 1000 行返回给客户端。 sort_buffer_size，就是 MySQL 为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。 可以用下面的方法，来确定一个排序语句是否使用了临时文件。123456789101112131415161718/* 打开 optimizer_trace，只对本线程有效 */SET optimizer_trace='enabled=on'; /* @a 保存 Innodb_rows_read 的初始值 */select VARIABLE_VALUE into @a from performance_schema.session_status where variable_name = 'Innodb_rows_read'; /* 执行语句 */select city, name,age from t where city='杭州' order by name limit 1000; /* 查看 OPTIMIZER_TRACE 输出 */SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G /* @b 保存 Innodb_rows_read 的当前值 */select VARIABLE_VALUE into @b from performance_schema.session_status where variable_name = 'Innodb_rows_read'; /* 计算 Innodb_rows_read 差值 */select @b-@a; number_of_tmp_files 表示的是，排序过程中使用的临时文件数。你一定奇怪，为什么需要 12 个文件？内存放不下时，就需要使用外部排序，外部排序一般使用归并排序算法。可以这么简单理解，MySQL 将需要排序的数据分成 12 份，每一份单独排序后存在这些临时文件中。然后把这 12 个有序文件再合并成一个有序的大文件。 rowid 排序 MySQL 认为排序的单行长度太大会怎么做呢？ max_length_for_sort_data，是 MySQL 中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL 就认为单行太大，要换一个算法。 1.初始化 sort_buffer，确定放入两个字段，即 name 和 id； 2.从索引 city 找到第一个满足 city=’杭州’条件的主键 id，也就是图中的 ID_X； 3.到主键 id 索引取出整行，取 name、id 这两个字段，存入 sort_buffer 中； 4.从索引 city 取下一个记录的主键 id； 5.重复步骤 3、4 直到不满足 city=’杭州’条件为止，也就是图中的 ID_Y； 6.对 sort_buffer 中的数据按照字段 name 进行排序； 7.遍历排序结果，取前 1000 行，并按照 id 的值回到原表中取出 city、name 和 age 三个字段返回给客户端。 全字段排序 VS rowid 排序 如果 MySQL 实在是担心排序内存太小，会影响排序效率，才会采用 rowid 排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据。 如果 MySQL 认为内存足够大，会优先选择全字段排序，把需要的字段都放到 sort_buffer 中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据。 这也就体现了 MySQL 的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。 对于 InnoDB 表来说，rowid 排序会要求回表多造成磁盘读，因此不会被优先选择。 1.从索引 (city,name) 找到第一个满足 city=’杭州’条件的主键 id； 2.到主键 id 索引取出整行，取 name、city、age 三个字段的值，作为结果集的一部分直接返回； 3.从索引 (city,name) 取下一个记录主键 id； 4.重复步骤 2、3，直到查到第 1000 条记录，或者是不满足 city=’杭州’条件时循环结束。 假设你的表里面已经有了 city_name(city, name) 这个联合索引，然后你要查杭州和苏州两个城市中所有的市民的姓名，并且按名字排序，显示前 100 条记录。如果 SQL 查询语句是这么写的 ： select * from t where city in (&#39;杭州&#39;,&quot; 苏州 &quot;) order by name limit 100; 那么，这个语句执行的时候会有排序过程吗，为什么？ 如果业务端代码由你来开发，需要实现一个在数据库端不需要排序的方案，你会怎么实现呢？ 进一步地，如果有分页需求，要显示第 101 页，也就是说语句最后要改成 “limit 10000,100”， 你的实现方法又会是什么呢？ 1) 需要排序，虽然有 (city,name) 联合索引，对于单个 city 内部，name 是递增的。但是由于这条 SQL 语句不是要单独地查一个 city 的值，而是同时查了”杭州”和” 苏州 “两个城市，因此所有满足条件的 name 就不是递增的了。也就是说，这条 SQL 语句需要排序。 2) 执行 select from t where city=“杭州” order by name limit 100; 这个语句是不需要排序的，客户端用一个长度为 100 的内存数组 A 保存结果。 执行 select from t where city=“苏州” order by name limit 100; 用相同的方法，假设结果被存进了内存数组 B。 现在 A 和 B 是两个有序数组，然后你可以用归并排序的思想，得到 name 最小的前 100 值，就是我们需要的结果了。 select from (select from t where city = ‘杭州’ limit 100union all select * from t where city = ‘苏州’ limit 100) as tt order by name limit 100 select * from t join(select id from t where city in(‘杭州’,’苏州’) order by name limit 10000,100) t_idon t.id=t_id.id; 3) select from t where city=” 杭州 “ order by name limit 10100; select from t where city=” 苏州 “ order by name limit 10100。 然后，再用归并排序的方法取得按 name 顺序第 10001~10100 的 name、id 的值，然后拿着这 100 个 id 到数据库中去查出所有记录。 3）对分页的优化。 没有特别好的办法。如果业务允许不提供排序功能，不提供查询最后一页，只能一页一页的翻，基本上前几页的数据已经满足客户需求。 为了意义不大的功能优化，可能会得不偿失。 如果一定要优化可以 select id from t where city in (‘杭州’,” 苏州 “) order by name limit 10000,100 因为有city\name索引，上面的语句走覆盖索引就可以完成，不用回表。 最后使用 select * from t where id in (); 取得结果 对于这个优化方法，我不好确定的是临界点，前几页直接查询就可以，最后几页使用这个优化方法。 但是中间的页码应该怎么选择不太清楚 References[1] 16 | “order by”是怎么工作的？]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么我只查一行的语句，也执行这么慢]]></title>
    <url>%2F2019%2F05%2F06%2Fmysql-slow-reason%2F</url>
    <content type="text"><![CDATA[19 | 为什么我只查一行的语句，也执行这么慢？ 需要说明的是，如果 MySQL 数据库本身就有很大的压力，导致数据库服务器 CPU 占用率很高或 ioutil（IO 利用率）很高，这种情况下所有语句的执行都有可能变慢，不属于我们今天的讨论范围。 为了便于描述，我还是构造一个表，基于这个表来说明今天的问题。这个表有两个字段 id 和 c，并且我在里面插入了 10 万行记录。 12345678910111213141516171819mysql&gt; CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB;delimiter ;;create procedure idata()begin declare i int; set i=1; while(i&lt;=100000)do insert into t values(i,i); set i=i+1; end while;end;;delimiter ;call idata(); 第一类：查询长时间不返回 select * from t where id=1; 一般碰到这种情况的话，大概率是表 t 被锁住了。接下来分析原因的时候，一般都是首先执行一下 show processlist 命令，看看当前语句处于什么状态。 等 MDL 锁 出现 这个状态表示的是，现在有一个线程正在表 t 上请求或者持有 MDL 写锁，把 select 语句堵住了。 等 flush mysql&gt; select * from information_schema.processlist where id=1; Waiting for table flush 等行锁 mysql&gt; select * from t where id=1 lock in share mode; –怎么查出是谁占着这个写锁。如果你用的是 MySQL 5.7 版本，可以通过 sys.innodb_lock_waits 表查到。 mysql&gt; select * from t sys.innodb_lock_waits where locked_table=&#39;test&#39;.&#39;t&#39;\G 第二类：查询慢 mysql&gt; select * from t where c=50000 limit 1; References[1] 19 | 为什么我只查一行的语句，也执行这么慢？]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go入门笔记]]></title>
    <url>%2F2019%2F04%2F10%2Fgolang%2F</url>
    <content type="text"><![CDATA[(1) 安装 go 安装包下载地址：(下面三个都可以) https://studygolang.com/dl https://golang.google.cn/dl/ https://golang.org/dl/ 比较喜欢用压缩包，解压完配置就可以使用。 linux https://dl.google.com/go/go1.13.4.linux-amd64.tar.gz mac https://dl.google.com/go/go1.13.4.darwin-amd64.tar.gz mac https://dl.google.com/go/go1.13.4.darwin-amd64.pkg windows https://dl.google.com/go/go1.13.4.windows-amd64.msi windows https://dl.google.com/go/go1.13.4.windows-amd64.zip (2) 配置环境变量 go需要配置 GOROOT GOPATH GOBIN GOROOT 是go的安装目录 GOPATH 目录用来存放Go源码，Go的可运行文件，以及相应的编译之后的包文件。所以这个目录下面有三个子目录：src、bin、pkg。而且这个目录不能和 GOROOT 一样。当有多个GOPATH时，默认会将go get的内容放在第一个目录下。 GOPATH 目录约定有三个子目录： src 存放源代码（比如：.go .c .h .s等） pkg 编译后生成的文件（比如：.a） bin 编译后生成的可执行文件（为了方便，可以把此目录加入到 $PATH 变量中，如果有多个gopath，那么使用${GOPATH//://bin:}/bin添加所有的bin目录） (2.1) mac修改 .bash_profile 文件123export GOROOT=/usr/local/goexport PATH=$PATH:$GOROOT/binexport GOPATH=/Users/weikeqin1/gopath source .bash_profile 12$ go versiongo version go1.13.4 darwin/amd64 123456789101112131415161718192021222324252627282930313233$ go envGO111MODULE=&quot;&quot;GOARCH=&quot;amd64&quot;GOBIN=&quot;&quot;GOCACHE=&quot;/Users/weikeqin1/Library/Caches/go-build&quot;GOENV=&quot;/Users/weikeqin1/Library/Application Support/go/env&quot;GOEXE=&quot;&quot;GOFLAGS=&quot;&quot;GOHOSTARCH=&quot;amd64&quot;GOHOSTOS=&quot;darwin&quot;GONOPROXY=&quot;&quot;GONOSUMDB=&quot;&quot;GOOS=&quot;darwin&quot;GOPATH=&quot;/Users/weikeqin1/go&quot;GOPRIVATE=&quot;&quot;GOPROXY=&quot;https://proxy.golang.org,direct&quot;GOROOT=&quot;/usr/local/go&quot;GOSUMDB=&quot;sum.golang.org&quot;GOTMPDIR=&quot;&quot;GOTOOLDIR=&quot;/usr/local/go/pkg/tool/darwin_amd64&quot;GCCGO=&quot;gccgo&quot;AR=&quot;ar&quot;CC=&quot;clang&quot;CXX=&quot;clang++&quot;CGO_ENABLED=&quot;1&quot;GOMOD=&quot;&quot;CGO_CFLAGS=&quot;-g -O2&quot;CGO_CPPFLAGS=&quot;&quot;CGO_CXXFLAGS=&quot;-g -O2&quot;CGO_FFLAGS=&quot;-g -O2&quot;CGO_LDFLAGS=&quot;-g -O2&quot;PKG_CONFIG=&quot;pkg-config&quot;GOGCCFLAGS=&quot;-fPIC -m64 -pthread -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -fdebug-prefix-map=/var/folders/03/wwlrrsnn7b9761k_s31w4yqc0glkg3/T/go-build001757962=/tmp/go-build -gno-record-gcc-switches -fno-common&quot; (2.2) windows go1.12.3.windows-amd64.zip 解压完我放到 D:\ProfessionalSoftWare\golang\go 修改环境变量 配置 GO_ROOT 为 D:\ProfessionalSoftWare\golang\go 配置 PATH 在 PATH 后添加 %GO_ROOT%\bin 配置 GO_PATH 为 D:\ProfessionalSoftWare\golang\gopath 新打开一个cmd，输入 go env 结果如下： 123456789101112131415161718192021222324252627$ go envset GOARCH=amd64set GOBIN=set GOCACHE=C:\Users\WKQ\AppData\Local\go-buildset GOEXE=.exeset GOFLAGS=set GOHOSTARCH=amd64set GOHOSTOS=windowsset GOOS=windowsset GOPATH=D:\ProfessionalSoftWare\gopathset GOPROXY=set GORACE=set GOROOT=D:\ProfessionalSoftWare\golangset GOTMPDIR=set GOTOOLDIR=D:\ProfessionalSoftWare\golang\pkg\tool\windows_amd64set GCCGO=gccgoset CC=gccset CXX=g++set CGO_ENABLED=1set GOMOD=set CGO_CFLAGS=-g -O2set CGO_CPPFLAGS=set CGO_CXXFLAGS=-g -O2set CGO_FFLAGS=-g -O2set CGO_LDFLAGS=-g -O2set PKG_CONFIG=pkg-configset GOGCCFLAGS=-m64 -mthreads -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -fdebug-prefix-map=C:\Users\WKQ\AppData\Local\Temp\go-build355563370=/tmp/go-build -gno-record-gcc-switches (3) 牛刀小试 新建一个文件 hello.go ，文件内容如下：123456package mainimport &quot;fmt&quot;func main() &#123; fmt.Println(&quot;Hello, GO !&quot;)&#125; go run hello.go 输出 Hello, GO ! 12 go run hello.goHello, GO ! 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748$ goGo is a tool for managing Go source code.Usage: go &lt;command&gt; [arguments]The commands are: bug start a bug report build compile packages and dependencies clean remove object files and cached files doc show documentation for package or symbol env print Go environment information fix update packages to use new APIs fmt gofmt (reformat) package sources generate generate Go files by processing source get download and install packages and dependencies install compile and install packages and dependencies list list packages or modules mod module maintenance run compile and run Go program test test packages tool run specified go tool version print Go version vet report likely mistakes in packagesUse &quot;go help &lt;command&gt;&quot; for more information about a command.Additional help topics: buildmode build modes c calling between Go and C cache build and test caching environment environment variables filetype file types go.mod the go.mod file gopath GOPATH environment variable gopath-get legacy GOPATH go get goproxy module proxy protocol importpath import path syntax modules modules, module versions, and more module-get module-aware go get packages package lists and patterns testflag testing flags testfunc testing functionsUse &quot;go help &lt;topic&gt;&quot; for more information about that topic. (4) GOPATH GOPATH 对应的目录结构12345678910$GOPATH src |--github.com |-astaxie |-beedb pkg |--相应平台 |-github.com |--astaxie |beedb.a 项目对应的目录结构12345678910111213141516171819/ bin/ mathapp pkg/ 平台名/ 如：darwin_amd64、linux_amd64 mymath.a github.com/ astaxie/ beedb.a src/ mathapp main.go mymath/ sqrt.go github.com/ astaxie/ beedb/ beedb.go util.go (5) 可能遇到的问题(5.1) ‘go’ 不是内部或外部命令，也不是可运行的程序 或批处理文件。 如果出现 &#39;go&#39; 不是内部或外部命令，也不是可运行的程序 或批处理文件。 就是没有配置path环境变量的原因。 (6) 入门123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371// 单行注释/* 多行 注释 */// 导入包的子句在每个源文件的开头。// Main比较特殊，它用来声明可执行文件，而不是一个库。package main// Import语句声明了当前文件引用的包。import ( &quot;fmt&quot; // Go语言标准库中的包 &quot;io/ioutil&quot; // 包含一些输入输出函数 m &quot;math&quot; // 数学标准库，在此文件中别名为m &quot;net/http&quot; // 一个web服务器包 &quot;os&quot; // 系统底层函数，如文件读写 &quot;strconv&quot; // 字符串转换)// 函数声明：Main是程序执行的入口。// 不管你喜欢还是不喜欢，反正Go就用了花括号来包住函数体。func main() &#123; // 往标准输出打印一行。 // 用包名fmt限制打印函数。 fmt.Println(&quot;天坑欢迎你!&quot;) // 调用当前包的另一个函数。 beyondHello()&#125;// 函数可以在括号里加参数。// 如果没有参数的话，也需要一个空括号。func beyondHello() &#123; var x int // 变量声明，变量必须在使用之前声明。 x = 3 // 变量赋值。 // 可以用:=来偷懒，它自动把变量类型、声明和赋值都搞定了。 y := 4 sum, prod := learnMultiple(x, y) // 返回多个变量的函数 fmt.Println(&quot;sum:&quot;, sum, &quot;prod:&quot;, prod) // 简单输出 learnTypes() // 少于y分钟，学的更多！&#125;/* &lt;- 快看快看我是跨行注释_(:з」∠)_Go语言的函数可以有多个参数和 *多个* 返回值。在这个函数中， `x`、`y` 是参数，`sum`、`prod` 是返回值的标识符（可以理解为名字）且类型为int*/func learnMultiple(x, y int) (sum, prod int) &#123; return x + y, x * y // 返回两个值&#125;// 内置变量类型和关键词func learnTypes() &#123; // 短声明给你所想。 str := &quot;少说话多读书!&quot; // String类型 s2 := `这是一个可以换行的字符串` // 同样是String类型 // 非ascii字符。Go使用UTF-8编码。 g := &apos;Σ&apos; // rune类型，int32的别名，使用UTF-8编码 f := 3.14195 // float64类型，IEEE-754 64位浮点数 c := 3 + 4i // complex128类型，内部使用两个float64表示 // Var变量可以直接初始化。 var u uint = 7 // unsigned 无符号变量，但是实现依赖int型变量的长度 var pi float32 = 22. / 7 // 字符转换 n := byte(&apos;\n&apos;) // byte是uint8的别名 // 数组（Array）类型的大小在编译时即确定 var a4 [4] int // 有4个int变量的数组，初始为0 a3 := [...]int&#123;3, 1, 5&#125; // 有3个int变量的数组，同时进行了初始化 // Array和slice各有所长，但是slice可以动态的增删，所以更多时候还是使用slice。 s3 := []int&#123;4, 5, 9&#125; // 回去看看 a3 ，是不是这里没有省略号？ s4 := make([]int, 4) // 分配4个int大小的内存并初始化为0 var d2 [][]float64 // 这里只是声明，并未分配内存空间 bs := []byte(&quot;a slice&quot;) // 进行类型转换 // 切片（Slice）的大小是动态的，它的长度可以按需增长 // 用内置函数 append() 向切片末尾添加元素 // 要增添到的目标是 append 函数第一个参数， // 多数时候数组在原内存处顺次增长，如 s := []int&#123;1, 2, 3&#125; // 这是个长度3的slice s = append(s, 4, 5, 6) // 再加仨元素，长度变为6了 fmt.Println(s) // 更新后的数组是 [1 2 3 4 5 6] // 除了向append()提供一组原子元素（写死在代码里的）以外，我们 // 还可以用如下方法传递一个slice常量或变量，并在后面加上省略号， // 用以表示我们将引用一个slice、解包其中的元素并将其添加到s数组末尾。 s = append(s, []int&#123;7, 8, 9&#125;...) // 第二个参数是一个slice常量 fmt.Println(s) // 更新后的数组是 [1 2 3 4 5 6 7 8 9] p, q := learnMemory() // 声明p,q为int型变量的指针 fmt.Println(*p, *q) // * 取值 // Map是动态可增长关联数组，和其他语言中的hash或者字典相似。 m := map[string]int&#123;&quot;three&quot;: 3, &quot;four&quot;: 4&#125; m[&quot;one&quot;] = 1 // 在Go语言中未使用的变量在编译的时候会报错，而不是warning。 // 下划线 _ 可以使你“使用”一个变量，但是丢弃它的值。 _, _, _, _, _, _, _, _, _, _ = str, s2, g, f, u, pi, n, a3, s4, bs // 通常的用法是，在调用拥有多个返回值的函数时， // 用下划线抛弃其中的一个参数。下面的例子就是一个脏套路， // 调用os.Create并用下划线变量扔掉它的错误代码。 // 因为我们觉得这个文件一定会成功创建。 file, _ := os.Create(&quot;output.txt&quot;) fmt.Fprint(file, &quot;这句代码还示范了如何写入文件呢&quot;) file.Close() // 输出变量 fmt.Println(s, c, a4, s3, d2, m) learnFlowControl() // 回到流程控制&#125;// 和其他编程语言不同的是，go支持有名称的变量返回值。// 声明返回值时带上一个名字允许我们在函数内的不同位置// 只用写return一个词就能将函数内指定名称的变量返回func learnNamedReturns(x, y int) (z int) &#123; z = x * y return // z is implicit here, because we named it earlier.&#125;// Go全面支持垃圾回收。Go有指针，但是不支持指针运算。// 你会因为空指针而犯错，但是不会因为增加指针而犯错。func learnMemory() (p, q *int) &#123; // 返回int型变量指针p和q p = new(int) // 内置函数new分配内存 // 自动将分配的int赋值0，p不再是空的了。 s := make([]int, 20) // 给20个int变量分配一块内存 s[3] = 7 // 赋值 r := -2 // 声明另一个局部变量 return &amp;s[3], &amp;r // &amp; 取地址&#125;func expensiveComputation() int &#123; return 1e6&#125;func learnFlowControl() &#123; // If需要花括号，括号就免了 if true &#123; fmt.Println(&quot;这句话肯定被执行&quot;) &#125; // 用go fmt 命令可以帮你格式化代码，所以不用怕被人吐槽代码风格了， // 也不用容忍别人的代码风格。 if false &#123; // pout &#125; else &#123; // gloat &#125; // 如果太多嵌套的if语句，推荐使用switch x := 1 switch x &#123; case 0: case 1: // 隐式调用break语句，匹配上一个即停止 case 2: // 不会运行 &#125; // 和if一样，for也不用括号 for x := 0; x &lt; 3; x++ &#123; // ++ 自增 fmt.Println(&quot;遍历&quot;, x) &#125; // x在这里还是1。为什么？ // for 是go里唯一的循环关键字，不过它有很多变种 for &#123; // 死循环 break // 骗你的 continue // 不会运行的 &#125; // 用range可以枚举 array、slice、string、map、channel等不同类型 // 对于channel，range返回一个值， // array、slice、string、map等其他类型返回一对儿 for key, value := range map[string]int&#123;&quot;one&quot;: 1, &quot;two&quot;: 2, &quot;three&quot;: 3&#125; &#123; // 打印map中的每一个键值对 fmt.Printf(&quot;索引：%s, 值为：%d\n&quot;, key, value) &#125; // 如果你只想要值，那就用前面讲的下划线扔掉没用的 for _, name := range []string&#123;&quot;Bob&quot;, &quot;Bill&quot;, &quot;Joe&quot;&#125; &#123; fmt.Printf(&quot;你是。。 %s\n&quot;, name) &#125; // 和for一样，if中的:=先给y赋值，然后再和x作比较。 if y := expensiveComputation(); y &gt; x &#123; x = y &#125; // 闭包函数 xBig := func() bool &#123; return x &gt; 100 // x是上面声明的变量引用 &#125; fmt.Println(&quot;xBig:&quot;, xBig()) // true （上面把y赋给x了） x /= 1e5 // x变成10 fmt.Println(&quot;xBig:&quot;, xBig()) // 现在是false // 除此之外，函数体可以在其他函数中定义并调用， // 满足下列条件时，也可以作为参数传递给其他函数： // a) 定义的函数被立即调用 // b) 函数返回值符合调用者对类型的要求 fmt.Println(&quot;两数相加乘二: &quot;, func(a, b int) int &#123; return (a + b) * 2 &#125;(10, 2)) // Called with args 10 and 2 // =&gt; Add + double two numbers: 24 // 当你需要goto的时候，你会爱死它的！ goto lovelove: learnFunctionFactory() // 返回函数的函数多棒啊 learnDefer() // 对defer关键字的简单介绍 learnInterfaces() // 好东西来了！&#125;func learnFunctionFactory() &#123; // 空行分割的两个写法是相同的，不过第二个写法比较实用 fmt.Println(sentenceFactory(&quot;原谅&quot;)(&quot;当然选择&quot;, &quot;她！&quot;)) d := sentenceFactory(&quot;原谅&quot;) fmt.Println(d(&quot;当然选择&quot;, &quot;她！&quot;)) fmt.Println(d(&quot;你怎么可以&quot;, &quot;她？&quot;))&#125;// Decorator在一些语言中很常见，在go语言中，// 接受参数作为其定义的一部分的函数是修饰符的替代品func sentenceFactory(mystring string) func(before, after string) string &#123; return func(before, after string) string &#123; return fmt.Sprintf(&quot;%s %s %s&quot;, before, mystring, after) // new string &#125;&#125;func learnDefer() (ok bool) &#123; // defer表达式在函数返回的前一刻执行 defer fmt.Println(&quot;defer表达式执行顺序为后进先出（LIFO）&quot;) defer fmt.Println(&quot;\n这句话比上句话先输出，因为&quot;) // 关于defer的用法，例如用defer关闭一个文件， // 就可以让关闭操作与打开操作的代码更近一些 return true&#125;// 定义Stringer为一个接口类型，有一个方法Stringtype Stringer interface &#123; String() string&#125;// 定义pair为一个结构体，有x和y两个int型变量。type pair struct &#123; x, y int&#125;// 定义pair类型的方法，实现Stringer接口。func (p pair) String() string &#123; // p被叫做“接收器” // Sprintf是fmt包中的另一个公有函数。 // 用 . 调用p中的元素。 return fmt.Sprintf(&quot;(%d, %d)&quot;, p.x, p.y)&#125;func learnInterfaces() &#123; // 花括号用来定义结构体变量，:=在这里将一个结构体变量赋值给p。 p := pair&#123;3, 4&#125; fmt.Println(p.String()) // 调用pair类型p的String方法 var i Stringer // 声明i为Stringer接口类型 i = p // 有效！因为p实现了Stringer接口（类似java中的塑型） // 调用i的String方法，输出和上面一样 fmt.Println(i.String()) // fmt包中的Println函数向对象要它们的string输出，实现了String方法就可以这样使用了。 // （类似java中的序列化） fmt.Println(p) // 输出和上面一样，自动调用String函数。 fmt.Println(i) // 输出和上面一样。 learnVariadicParams(&quot;great&quot;, &quot;learning&quot;, &quot;here!&quot;)&#125;// 有变长参数列表的函数func learnVariadicParams(myStrings ...interface&#123;&#125;) &#123; // 枚举变长参数列表的每个参数值 // 下划线在这里用来抛弃枚举时返回的数组索引值 for _, param := range myStrings &#123; fmt.Println(&quot;param:&quot;, param) &#125; // 将可变参数列表作为其他函数的参数列表 fmt.Println(&quot;params:&quot;, fmt.Sprintln(myStrings...)) learnErrorHandling()&#125;func learnErrorHandling() &#123; // &quot;, ok&quot;用来判断有没有正常工作 m := map[int]string&#123;3: &quot;three&quot;, 4: &quot;four&quot;&#125; if x, ok := m[1]; !ok &#123; // ok 为false，因为m中没有1 fmt.Println(&quot;别找了真没有&quot;) &#125; else &#123; fmt.Print(x) // 如果x在map中的话，x就是那个值喽。 &#125; // 错误可不只是ok，它还可以给出关于问题的更多细节。 if _, err := strconv.Atoi(&quot;non-int&quot;); err != nil &#123; // _ discards value // 输出&quot;strconv.ParseInt: parsing &quot;non-int&quot;: invalid syntax&quot; fmt.Println(err) &#125; // 待会再说接口吧。同时， learnConcurrency()&#125;// c是channel类型，一个并发安全的通信对象。func inc(i int, c chan int) &#123; c &lt;- i + 1 // &lt;-把右边的发送到左边的channel。&#125;// 我们将用inc函数来并发地增加一些数字。func learnConcurrency() &#123; // 用make来声明一个slice，make会分配和初始化slice，map和channel。 c := make(chan int) // 用go关键字开始三个并发的goroutine，如果机器支持的话，还可能是并行执行。 // 三个都被发送到同一个channel。 go inc(0, c) // go is a statement that starts a new goroutine. go inc(10, c) go inc(-805, c) // 从channel中读取结果并打印。 // 打印出什么东西是不可预知的。 fmt.Println(&lt;-c, &lt;-c, &lt;-c) // channel在右边的时候，&lt;-是读操作。 cs := make(chan string) // 操作string的channel cc := make(chan chan string) // 操作channel的channel go func() &#123; c &lt;- 84 &#125;() // 开始一个goroutine来发送一个新的数字 go func() &#123; cs &lt;- &quot;wordy&quot; &#125;() // 发送给cs // Select类似于switch，但是每个case包括一个channel操作。 // 它随机选择一个准备好通讯的case。 select &#123; case i := &lt;-c: // 从channel接收的值可以赋给其他变量 fmt.Println(&quot;这是……&quot;, i) case &lt;-cs: // 或者直接丢弃 fmt.Println(&quot;这是个字符串！&quot;) case &lt;-cc: // 空的，还没作好通讯的准备 fmt.Println(&quot;别瞎想&quot;) &#125; // 上面c或者cs的值被取到，其中一个goroutine结束，另外一个一直阻塞。 learnWebProgramming() // Go很适合web编程，我知道你也想学！&#125;// http包中的一个简单的函数就可以开启web服务器。func learnWebProgramming() &#123; // ListenAndServe第一个参数指定了监听端口，第二个参数是一个接口，特定是http.Handler。 go func() &#123; err := http.ListenAndServe(&quot;:8080&quot;, pair&#123;&#125;) fmt.Println(err) // 不要无视错误。 &#125;() requestServer()&#125;// 使pair实现http.Handler接口的ServeHTTP方法。func (p pair) ServeHTTP(w http.ResponseWriter, r *http.Request) &#123; // 使用http.ResponseWriter返回数据 w.Write([]byte(&quot;Y分钟golang速成!&quot;))&#125;func requestServer() &#123; resp, err := http.Get(&quot;http://localhost:8080&quot;) fmt.Println(err) defer resp.Body.Close() body, err := ioutil.ReadAll(resp.Body) fmt.Printf(&quot;\n服务器消息： `%s`&quot;, string(body))&#125; References[1] build-web-application-with-golang/zh/preface.md[2] the-way-to-go_ZH_CN《Go 入门指南》[2] Go 编程语言-中文[3] 如何使用Go编程-中文[4] Command go-中文[5] GO 指南-中文[6] Go 语言之旅-中文[7] Go 维基-英文[8] 包文档-中文[9] 语言规范-中文[10] Go 内存模型-中文[11] go源代码[12] 一刻钟学会Go语言[13] Learn X in Y minutes[14] 再见，Python！你好，Go 语言]]></content>
      <categories>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybaits笔记]]></title>
    <url>%2F2019%2F03%2F29%2Fmybaits-notes%2F</url>
    <content type="text"><![CDATA[1234driverClassName=com.mysql.jdbc.Driverurl=jdbc:mysql://localhost:3306/dataserver?useUnicode=true&amp;characterEncoding=UTF-8&amp;autoReconnect=true&amp;useSSL=false&amp;allowMultiQueries=trueusername=adminpassword=admin Java中使用123456789101112CREATE TABLE `user_info` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键', `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `update_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间', `is_delete` tinyint(11) DEFAULT '0' COMMENT '0:未删除 1:已删除', `user_name` varchar(32) DEFAULT '' COMMENT '用户名', `user_age` varchar(3) DEFAULT '' COMMENT '用户年龄', `user_address` varchar(128) DEFAULT '' COMMENT '用户地址', `user_tel` varchar(16) DEFAULT '' COMMENT '用户电话', `user_type` tinyint(4) DEFAULT '0' COMMENT '用户等级 0:普通 1:高级', PRIMARY KEY (`id`) USING BTREE) ENGINE=InnoDB COMMENT='用户信息表'; 增1234567891011121314&lt;insert id="insert" parameterType="cn.wkq.domain.UsrInfoDo"&gt; &lt;!-- WARNING - @mbggenerated This element is automatically generated by MyBatis Generator, do not modify. --&gt; insert into user_info (id, create_time, update_time, is_delete, user_name, user_age, user_address, user_tel, user_type ) values (#&#123;id,jdbcType=INTEGER&#125;, #&#123;createTime,jdbcType=TIMESTAMP&#125;, #&#123;updateTime,jdbcType=TIMESTAMP&#125;, #&#123;isDelete,jdbcType=TINYINT&#125;, #&#123;userName,jdbcType=VARCHAR&#125;, #&#123;userAge,jdbcType=VARCHAR&#125;, #&#123;userAddress,jdbcType=VARCHAR&#125;, #&#123;userTel,jdbcType=VARCHAR&#125;, #&#123;userType,jdbcType=TINYINT&#125; )&lt;/insert&gt; 12345678910111213&lt;insert id="batchInsert"&gt; insert into user_info (id, create_time, update_time, is_delete, user_name, user_age, user_address, user_tel, user_type ) &lt;foreach collection="list" separator="," item="item"&gt; ( #&#123;item.id,jdbcType=INTEGER&#125;, #&#123;item.createTime,jdbcType=TIMESTAMP&#125;, #&#123;item.updateTime,jdbcType=TIMESTAMP&#125;, #&#123;item.isDelete,jdbcType=TINYINT&#125;, #&#123;item.userName,jdbcType=VARCHAR&#125;, #&#123;item.userAge,jdbcType=VARCHAR&#125;, #&#123;item.userAddress,jdbcType=VARCHAR&#125;, #&#123;item.userTel,jdbcType=VARCHAR&#125;, #&#123;item.userType,jdbcType=TINYINT&#125; ) &lt;/foreach&gt;&lt;/insert&gt; 注意：foreach 里 配置 separator=&quot;,&quot;，不需要在sql结尾加 , 删12345678&lt;delete id="deleteByPrimaryKey" parameterType="java.lang.Integer"&gt; &lt;!-- WARNING - @mbggenerated This element is automatically generated by MyBatis Generator, do not modify. --&gt; delete from user_info where id = #&#123;id,jdbcType=INTEGER&#125;&lt;/delete&gt; 改12345678910111213141516&lt;update id="updateByPrimaryKey" parameterType="cn.wkq.domain.UsrInfoDo"&gt; &lt;!-- WARNING - @mbggenerated This element is automatically generated by MyBatis Generator, do not modify. --&gt; update user_info set create_time = #&#123;createTime,jdbcType=TIMESTAMP&#125;, update_time = #&#123;updateTime,jdbcType=TIMESTAMP&#125;, is_delete = #&#123;isDelete,jdbcType=TINYINT&#125;, user_name = #&#123;userName,jdbcType=VARCHAR&#125;, user_age = #&#123;userAge,jdbcType=VARCHAR&#125;, user_address = #&#123;userAddress,jdbcType=VARCHAR&#125;, user_tel = #&#123;userTel,jdbcType=VARCHAR&#125;, user_type = #&#123;userType,jdbcType=TINYINT&#125; where id = #&#123;id,jdbcType=INTEGER&#125;&lt;/update&gt; 12345678910111213141516171819202122232425262728293031323334&lt;update id="updateByPrimaryKeySelective" parameterType="cn.wkq.domain.UsrInfoDo"&gt; &lt;!-- WARNING - @mbggenerated This element is automatically generated by MyBatis Generator, do not modify. --&gt; update user_info &lt;set&gt; &lt;if test="createTime != null"&gt; create_time = #&#123;createTime,jdbcType=TIMESTAMP&#125;, &lt;/if&gt; &lt;if test="updateTime != null"&gt; update_time = #&#123;updateTime,jdbcType=TIMESTAMP&#125;, &lt;/if&gt; &lt;if test="isDelete != null"&gt; is_delete = #&#123;isDelete,jdbcType=TINYINT&#125;, &lt;/if&gt; &lt;if test="userName != null"&gt; user_name = #&#123;userName,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="userAge != null"&gt; user_age = #&#123;userAge,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="userAddress != null"&gt; user_address = #&#123;userAddress,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="userTel != null"&gt; user_tel = #&#123;userTel,jdbcType=VARCHAR&#125;, &lt;/if&gt; &lt;if test="userType != null"&gt; user_type = #&#123;userType,jdbcType=TINYINT&#125;, &lt;/if&gt; &lt;/set&gt; where id = #&#123;id,jdbcType=INTEGER&#125;&lt;/update&gt; 12345678910111213&lt;update id="batchUpdateAllColumns"&gt; &lt;foreach collection="list" separator=";" item="item"&gt; update user_info set create_time = #&#123;createTime,jdbcType=TIMESTAMP&#125;, update_time = #&#123;updateTime,jdbcType=TIMESTAMP&#125;, user_name = #&#123;userName,jdbcType=VARCHAR&#125;, user_age = #&#123;userAge,jdbcType=VARCHAR&#125;, user_address = #&#123;userAddress,jdbcType=VARCHAR&#125;, user_tel = #&#123;userTel,jdbcType=VARCHAR&#125;, user_type = #&#123;userType,jdbcType=TINYINT&#125; where id = #&#123;id,jdbcType=INTEGER&#125; &lt;/foreach&gt;&lt;/update&gt; 注意，foreach里配置了 separator=&quot;;&quot; ，不要的update语句后加 ; ，否则会报错此种写法需要在 jdbc url 里配置 &amp;allowMultiQueries=true 12345678&lt;update id="batchDelete"&gt; UPDATE car_category SET is_delete = 1 WHERE id in &lt;foreach collection="ids" item="item" separator="," open="(" close=")"&gt; #&#123;item&#125; &lt;/foreach&gt;&lt;/update&gt; 查12345678&lt;select id="getDataById" resultType="cn.wkq.domain.UsrInfoDo"&gt; SELECT id, create_time, update_time, user_name FROM user_info WHERE id &gt;= #&#123;startId&#125; &lt;![CDATA[ AND id &lt;= #&#123;endId&#125; ]]&gt;&lt;/select&gt; 123456789101112131415161718&lt;sql id="Base_Column_List"&gt; &lt;!-- WARNING - @mbggenerated This element is automatically generated by MyBatis Generator, do not modify. --&gt; id, create_time, update_time, user_name, user_age, user_address, user_tel, user_type&lt;/sql&gt;&lt;select id="selectByPrimaryKey" resultMap="BaseResultMap" parameterType="java.lang.Integer"&gt; &lt;!-- WARNING - @mbggenerated This element is automatically generated by MyBatis Generator, do not modify. --&gt; select &lt;include refid="Base_Column_List"/&gt; from user_info where id = #&#123;id,jdbcType=INTEGER&#125;&lt;/select&gt; 123456789101112&lt;select id="getPageable" resultType="cn.wkq.domain.UsrInfoDo"&gt; select &lt;include refid="Base_Column_List"/&gt; from user_info &lt;if test="limit != null"&gt; limit &lt;if test="offset != null"&gt; $&#123;offset&#125;, &lt;/if&gt; $&#123;limit&#125; &lt;/if&gt;&lt;/select&gt; 遇到的问题; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ‘;123456789### Cause: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &apos;; update tablexxx set control_type = &apos;C&apos;, manufacturers &apos; at line 1; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &apos;; update tablexxx set control_type = &apos;C&apos;, manufacturers &apos; at line 1 at org.springframework.jdbc.support.SQLErrorCodeSQLExceptionTranslator.doTranslate(SQLErrorCodeSQLExceptionTranslator.java:230) ~[spring-jdbc-4.3.20.RELEASE.jar:4.3.20.RELEASE] 被队友坑了，jdbc里配置的 &amp;amp;allowMultiQueries=true 导致 &amp;allowMultiQueries=true 没有生效 mybatis批量更新报错问题解决 Error updating database. Cause: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ‘update1234567891011121314151617181920org.springframework.jdbc.BadSqlGrammarException:### Error updating database. Cause: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &apos;update tablexxx set control_type = &apos;C&apos;, manufacturers = &apos;克莱&apos; at line 210### The error may involve defaultParameterMap### The error occurred while setting parameters### SQL: 报错的SQL有546664字符，大概有547KB### Cause: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &apos;update tablexxx set control_type = &apos;C&apos;, manufacturers = &apos;克莱&apos; at line 210; bad SQL grammar []; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &apos;update tablexxx set control_type = &apos;C&apos;, manufacturers = &apos;克莱&apos; at line 210 at org.springframework.jdbc.support.SQLErrorCodeSQLExceptionTranslator.doTranslate(SQLErrorCodeSQLExceptionTranslator.java:230) ~[spring-jdbc-4.3.20.RELEASE.jar:4.3.20.RELEASE] ...Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &apos;update tablexxx set control_type = &apos;C&apos;, manufacturers = &apos;克莱&apos; at line 210 at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_20] ... 检查 jdbc url配置里是否有 &amp;allowMultiQueries=true检查 mysql xml 里是否在 foreach里 配置了 separator=&quot;;&quot; sql语句里又写了 ;检查 mysql 版本 org.apache.ibatis.exceptions.TooManyResultsException: Expected one result (or null) to be returned by selectOne(), but found: 21org.mybatis.spring.MyBatisSystemException: nested exception is org.apache.ibatis.exceptions.TooManyResultsException: Expected one result (or null) to be returned by selectOne(), but found: 2 limit 1 Cause: java.sql.SQLException: Value ‘0000-00-00 00:00:00’ can not be represented as java.sql.Timestamp12345org.springframework.dao.TransientDataAccessResourceException: Error attempting to get column &apos;gmt_create&apos; from result set. Cause: java.sql.SQLException: Value &apos;0000-00-00 00:00:00&apos; can not be represented as java.sql.Timestamp; SQL []; Value &apos;0000-00-00 00:00:00&apos; can not be represented as java.sql.Timestamp; nested exception is java.sql.SQLException: Value &apos;0000-00-00 00:00:00&apos; can not be represented as java.sql.Timestamp at org.springframework.jdbc.support.SQLStateSQLExceptionTranslator.doTranslate(SQLStateSQLExceptionTranslator.java:108) ~[spring-jdbc-4.2.4.RELEASE.jar:4.2.4.RELEASE]Caused by: java.sql.SQLException: Value &apos;0000-00-00 00:00:00&apos; can not be represented as java.sql.Timestamp 数据问题 代码里可以用String (不推荐) References[1] mybatis简介-官方中文文档[2] mybatis批量更新报错问题解决[3] 自动生成mapper-xml和对应数据库实体类]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka 安装 使用]]></title>
    <url>%2F2019%2F03%2F26%2Fkafka-install-use%2F</url>
    <content type="text"><![CDATA[http://kafka.apache.org/downloads (1) 安装 使用kafka时需要zookeeper，所以需要安装 kafka 和 zookeeper (1.1) zookeeper安装 下载 apache-zookeeper-3.5.5-bin.tar.gz 解压 tar -zxvf apache-zookeeper-3.5.5-bin.tar.gz 配置 cp -rf conf/zoo_sample.cfg conf/zoo.cfg 启动 ./bin/zkServer.sh start 12345ZBMAC-C02PGMT0F:apache-zookeeper-3.5.5-bin weikeqin1$ ./bin/zkServer.sh start/usr/bin/javaZooKeeper JMX enabled by defaultUsing config: /Users/weikeqin1/SoftWare/apache-zookeeper-3.5.5-bin/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 12345ZBMAC-C02PGMT0F:apache-zookeeper-3.5.5-bin weikeqin1$ ./bin/zkServer.sh stop/usr/bin/javaZooKeeper JMX enabled by defaultUsing config: /Users/weikeqin1/SoftWare/apache-zookeeper-3.5.5-bin/bin/../conf/zoo.cfgStopping zookeeper ... STOPPED (1.2) kafka安装 下载 kafka_2.11-0.10.1.0.tgz 解压 tar -zxvf kafka_2.11-0.10.1.0.tgz 配置 修改 conf/server.properties 12broker.id=1log.dir=/tmp/kafka-logs/logs-1 启动 ./bin/kafka-server-start.sh config/server.properties 验证 (1.3) 使用 (1.3.1) 创建topic ./bin/kafka-topics.sh --create --topic topic_test --replication-factor 1 --partitions 1 --zookeeper localhost:2181 123ZBMAC-C02PGMT0F:kafka_2.11-0.10.1.0 weikeqin1$ ./bin/kafka-topics.sh --create --topic topic_test --replication-factor 1 --partitions 1 --zookeeper localhost:2181WARNING: Due to limitations in metric names, topics with a period (&apos;.&apos;) or underscore (&apos;_&apos;) could collide. To avoid issues it is best to use either, but not both.Created topic &quot;topic_test&quot;. (1.3.2) 查看所有 topics ./bin/kafka-topics.sh --list --zookeeper localhost:2181 123ZBMAC-C02PGMT0F:kafka_2.11-0.10.1.0 weikeqin1$ ./bin/kafka-topics.sh --list --zookeeper localhost:2181testtopic_test (1.3.3) 启动 Producer 生产消息 ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic topic_test (1.3.4) 启动 Consumer 消费消息 ./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic topic_test --from-beginning (1.3.5) 删除Topic ./bin/kafka-run-class.sh kafka.admin.TopicCommand --delete --topic topic_test --zookeeper localhost:2181 123ZBMAC-C02PGMT0F:kafka_2.11-0.10.1.0 weikeqin1$ ./bin/kafka-run-class.sh kafka.admin.TopicCommand --delete --topic topic_test --zookeeper localhost:2181Topic topic_test is marked for deletion.Note: This will have no impact if delete.topic.enable is not set to true. (1.3.6) 查看 Topic 的offset ./bin/kafka-consumer-offset-checker.sh --zookeeper localhost:2181 --topic topic_test --group consumer (1.3.7) 查看描述 topics 信息 ./bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic topic_test 123ZBMAC-C02PGMT0F:kafka_2.11-0.10.1.0 weikeqin1$ ./bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic topic_testTopic:topic_test PartitionCount:1 ReplicationFactor:1 Configs: Topic: topic_test Partition: 0 Leader: 1 Replicas: 1 Isr: 1 第一行给出了所有分区的摘要，每个附加行给出了关于一个分区的信息。 由于我们只有一个分区，所以只有一行。 “Leader”: 是负责给定分区的所有读取和写入的节点。 每个节点将成为分区随机选择部分的领导者。 “Replicas”: 是复制此分区日志的节点列表，无论它们是否是领导者，或者即使他们当前处于活动状态。 “Isr”: 是一组“同步”副本。这是复制品列表的子集，当前活着并被引导到领导者。 (2) 原理介绍(2.1) 消息队列（Message Queue) Message Queue消息传送系统提供传送服务。消息传送依赖于大量支持组件，这些组件负责处理连接服务、消息的路由和传送、持久性、安全性以及日志记录。消息服务器可以使用一个或多个代理实例。 JMS（Java Messaging Service）是Java平台上有关面向消息中间件(MOM)的技术规范，它便于消息系统中的Java应用程序进行消息交换,并且通过提供标准的产生、发送、接收消息的接口简化企业应用的开发，翻译为Java消息服务。 (2.2) 消息队列分类 消息队列分类：点对点和发布/订阅两种： 1、点对点： 消息生产者生产消息发送到queue中，然后消息消费者从queue中取出并且消费消息。 消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 2、发布/订阅： 消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。 (2.3) 消息队列对比 1、RabbitMQ：支持的协议多，非常重量级消息队列，对路由(Routing)，负载均衡(Loadbalance)或者数据持久化都有很好的支持。 2、ZeroMQ：号称最快的消息队列系统，尤其针对大吞吐量的需求场景，擅长的高级/复杂的队列，但是技术也复杂，并且只提供非持久性的队列。 3、ActiveMQ：Apache下的一个子项，类似ZeroMQ，能够以代理人和点对点的技术实现队列。 4、Redis：是一个key-Value的NOSql数据库，但也支持MQ功能，数据量较小，性能优于RabbitMQ，数据超过10K就慢的无法忍受。 (2.4) Kafka简介 Kafka是分布式发布-订阅消息系统,它最初由 LinkedIn 公司开发，使用 Scala语言编写,之后成为 Apache 项目的一部分。在Kafka集群中，没有“中心主节点”的概念，集群中所有的服务器都是对等的，因此，可以在不做任何配置的更改的情况下实现服务器的的添加与删除，同样的消息的生产者和消费者也能够做到随意重启和机器的上下线。 (2.5) Kafka术语介绍 1、消息生产者：即：Producer，是消息的产生的源头，负责生成消息并发送到Kafka服务器上。 2、消息消费者：即：Consumer，是消息的使用方，负责消费Kafka服务器上的消息。 3、主题：即：Topic，由用户定义并配置在Kafka服务器，用于建立生产者和消息者之间的订阅关系：生产者发送消息到指定的Topic下，消息者从这个Topic下消费消息。 4、消息分区：即：Partition，一个Topic下面会分为很多分区，例如：“kafka-test”这个Topic下可以分为6个分区，分别由两台服务器提供，那么通常可以配置为让每台服务器提供3个分区，假如服务器ID分别为0、1，则所有的分区为0-0、0-1、0-2和1-0、1-1、1-2。Topic物理上的分组，一个 topic可以分为多个 partition，每个 partition 是一个有序的队列。partition中的每条消息都会被分配一个有序的 id（offset）。 5、Broker：即Kafka的服务器，用户存储消息，Kafa集群中的一台或多台服务器统称为 broker。 6、消费者分组：Group，用于归组同类消费者，在Kafka中，多个消费者可以共同消息一个Topic下的消息，每个消费者消费其中的部分消息，这些消费者就组成了一个分组，拥有同一个分组名称，通常也被称为消费者集群。 7、Offset：消息存储在Kafka的Broker上，消费者拉取消息数据的过程中需要知道消息在文件中的偏移量，这个偏移量就是所谓的Offset。 (3) 使用 (Java代码) kafka 使用 0.10.1.0，由于java python 等都要用，选一个各个语言支持比较好的版本，所以选 0.10.1.0 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.10.1.0&lt;/version&gt;&lt;/dependency&gt; (3.1) 生产者代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.Producer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Properties;/** * 生产者 * * &lt;pre&gt; * https://blog.csdn.net/m0_37739193/article/details/78396773 * &lt;/pre&gt; * * @author: weikeqin.cn@gmail.com * @date: 2019-03-26 **/public class kafkaProducerExample &#123; /** * 此配置是 Producer 在确认一个请求发送完成之前需要收到的反馈信息的数量。 * acks=0 如果设置为0，则 producer 不会等待服务器的反馈。该消息会被立刻添加到 socket buffer 中并认为已经发送完成。在这种情况下，服务器是否收到请求是没法保证的，并且参数retries也不会生效（因为客户端无法获得失败信息）。每个记录返回的 offset 总是被设置为-1。 * acks=1 如果设置为1，leader节点会将记录写入本地日志，并且在所有 follower 节点反馈之前就先确认成功。在这种情况下，如果 leader 节点在接收记录之后，并且在 follower 节点复制数据完成之前产生错误，则这条记录会丢失。 * acks=all 如果设置为all，这就意味着 leader 节点会等待所有同步中的副本确认之后再确认这条记录是否发送完成。只要至少有一个同步副本存在，记录就不会丢失。这种方式是对请求传递的最有效保证。acks=-1与acks=all是等效的。 */ private final static String ACKS = "all"; public static void main(String[] args) &#123; Properties props = new Properties(); props.put("bootstrap.servers", "127.0.0.1:9092"); props.put("acks", "all"); props.put("retries", 3); props.put("batch.size", 16384); props.put("linger.ms", 1); props.put("buffer.memory", 33554432); props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); String topic = "test"; Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 100; i++) &#123; producer.send(new ProducerRecord&lt;&gt;(topic, Integer.toString(i), Integer.toString(i)));// producer.send(new ProducerRecord&lt;&gt;("test", "Hello" + i)); &#125; producer.close(); &#125;&#125; (3.2)消费者代码1234567891011121314151617181920212223242526272829303132333435363738394041//import lombok.extern.slf4j.Slf4j;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.apache.kafka.clients.consumer.ConsumerRecords;import org.apache.kafka.clients.consumer.KafkaConsumer;import java.util.Arrays;import java.util.Properties;/** * 消费者 * * @author: weikeqin.cn@gmail.com * @date: 2019-03-26 **///@Slf4jpublic class KafkaConsumerExample &#123; public static void main(String[] args) &#123; Properties props = new Properties(); props.put("bootstrap.servers", "127.0.0.1:9092"); props.put("group.id", "test"); props.put("enable.auto.commit", "true"); props.put("auto.commit.interval.ms", "1000"); props.put("session.timeout.ms", "30000"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); String topic = "test"; KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList(topic)); while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; //log.info("offset = &#123;&#125;, key = &#123;&#125;, value = &#123;&#125;", record.offset(), record.key(), record.value()); System.out.println(record.offset() + " " + record.key() + " " + record.value()) &#125; &#125; &#125;&#125; (4)管理 Kafka-manager Kafka教程(三)Kafka-manager安装 https://github.com/yahoo/kafka-manager References[1] Kafka教程(一)Kafka入门教程[2] Kafka安装及快速入门[3] Kafka安装配置[4] Kafka安装教程（详细过程）[5] Kafka使用Java进行Producer和Consumer编程[6] Kafka教程(二)Kafka集群环境安装[7] Kafka教程(三)Kafka-manager安装[8] kafka学习非常详细的经典教程]]></content>
      <categories>
        <category>mq</category>
      </categories>
      <tags>
        <tag>mq</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql 架构 及 语句执行流程]]></title>
    <url>%2F2019%2F03%2F17%2Fmysql-architecture%2F</url>
    <content type="text"><![CDATA[转自 01 | 基础架构：一条SQL查询语句是如何执行的？ - MySQL实战45讲 02 | 日志系统：一条SQL更新语句是如何执行的？ - MySQL实战45讲 问大家一个问题。SELECT * FROM t WHERE id = 1 ; 以及 UPDATE t set c = 2 where id = 1 ; 这两个语句是怎么执行的。 假设表结构是12345CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB; 大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。 第一层是连接处理、授权认证、安全等。这是大多数基于网络的客户端/服务器的工具或者服务都有的架构。 第二层是 Server 层，包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 第三层是 存储引擎层，负责数据的存储和提取。每个存储引擎都有它的优势和劣势。服务器通过API与存储引擎层通信。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。 (1) 连接器 第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令一般是这么写的： mysql -h $ip -P $port -u $user -p 连接命令中的 mysql 是客户端工具，用来跟服务端建立连接。在完成经典的 TCP 握手后，连接器就要开始认证你的身份，这个时候用的就是你输入的用户名和密码。 如果用户名或密码不对，你就会收到一个”Access denied for user”的错误，然后客户端程序结束执行。 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。 这就意味着，一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。 连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在 show processlist 命令中看到它。其中的 Command 列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。 123456789101112mysql&gt; show processlist ;+----+-------------+-----------------+------+---------+------+--------------------------+------------------+----------+| Id | User | Host | db | Command | Time | State | Info | Progress |+----+-------------+-----------------+------+---------+------+--------------------------+------------------+----------+| 1 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 2 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 3 | system user | | NULL | Daemon | NULL | InnoDB purge coordinator | NULL | 0.000 || 4 | system user | | NULL | Daemon | NULL | InnoDB purge worker | NULL | 0.000 || 5 | system user | | NULL | Daemon | NULL | InnoDB shutdown handler | NULL | 0.000 || 15 | root | localhost:49914 | NULL | Query | 0 | Init | show processlist | 0.000 |+----+-------------+-----------------+------+---------+------+--------------------------+------------------+----------+6 rows in set (0.00 sec) 客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数 wait_timeout 控制的，默认值是 8 小时。 在用的时候一般会在程序的配置文件里配置 validationQuery=SELECT 1 就是因为这个原因。 如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。 数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。 建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。 但是全部使用长连接后，你可能会发现，有些时候 MySQL 占用内存涨得特别快，这是因为 MySQL 在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是 MySQL 异常重启了。 怎么解决这个问题呢？你可以考虑以下两种方案。 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 (2) 查询缓存 连接建立完成后，你就可以执行 select 语句了。执行逻辑就会来到第二步：查询缓存。 MySQL 拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以 key-value 对的形式，被直接缓存在内存中。key 是查询的语句，value 是查询的结果。如果你的查询能够直接在这个缓存中找到 key，那么这个 value 就会被直接返回给客户端。 如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查询命中缓存，MySQL 不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。 但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。 好在 MySQL 也提供了这种“按需使用”的方式。你可以将参数 query_cache_type 设置成 DEMAND，这样对于默认的 SQL 语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用 SQL_CACHE 显式指定，像下面这个语句一样： select SQL_CACHE * from T where ID=10； 需要注意的是，MySQL 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻底没有这个功能了。 (3) 分析器 如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL 需要知道你要做什么，因此需要对 SQL 语句做解析。 分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。 MySQL 从你输入的”select”这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名 T”，把字符串“ID”识别成“列 ID”。 做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法。 如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如下面这个语句 select 少打了开头的字母“s”。 123mysql&gt; elect * from t where ID=1;ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'elect * from t where ID=1' at line 1 (4) 优化器 经过了分析器，MySQL 就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的 join： mysql&gt; select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20; 既可以先从表 t1 里面取出 c=10 的记录的 ID 值，再根据 ID 值关联到表 t2，再判断 t2 里面 d 的值是否等于 20。 也可以先从表 t2 里面取出 d=20 的记录的 ID 值，再根据 ID 值关联到 t1，再判断 t1 里面 c 的值是否等于 10。 这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。 (5) 执行器 MySQL 通过分析器知道了你要做什么，通过优化器知道了该怎么做，于是就进入了执行器阶段，开始执行语句。 开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示 (在工程实现上，如果命中查询缓存，会在查询缓存返回结果的时候，做权限验证。查询也会在优化器之前调用 precheck 验证权限)。 123mysql&gt; select * from T where ID=10;ERROR 1142 (42000): SELECT command denied to user 'b'@'localhost' for table 'T' 如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 至此，这个语句就执行完成了。 对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。 你会在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。 在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此 引擎扫描行数跟 rows_examined 并不是完全相同的。 MySQL的架构这儿就完了。下面是两个比较重要的模块。 不知道你还记不记得《孔乙己》这篇文章，酒店掌柜有一个粉板，专门用来记录客人的赊账记录。如果赊账的人不多，那么他可以把顾客名和账目写在板上。但如果赊账的人多了，粉板总会有记不下的时候，这个时候掌柜一定还有一个专门记录赊账的账本。 如果有人要赊账或者还账的话，掌柜一般有两种做法： 一种做法是直接把账本翻出来，把这次赊的账加上去或者扣除掉； 另一种做法是先在粉板上记下这次的账，等打烊以后再把账本翻出来核算。 在生意红火柜台很忙时，掌柜一定会选择后者，因为前者操作实在是太麻烦了。首先，你得找到这个人的赊账总额那条记录。你想想，密密麻麻几十页，掌柜要找到那个名字，可能还得带上老花镜慢慢找，找到之后再拿出算盘计算，最后再将结果写回到账本上。 这整个过程想想都麻烦。相比之下，还是先在粉板上记一下方便。你想想，如果掌柜没有粉板的帮助，每次记账都得翻账本，效率是不是低得让人难以忍受？ 同样，在 MySQL 里也有这个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了解决这个问题，MySQL 的设计者就用了类似酒店掌柜粉板的思路来提升更新效率。 而粉板和账本配合的整个过程，其实就是 MySQL 里经常说到的 WAL 技术，WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。 (6) 重要的日志模块：redo log （重做日志） 具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。 如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎么办呢？这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间。 与此类似，InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。 write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe 要理解 crash-safe 这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板上的数据明确赊账账目。 redo log 用于保证 crash-safe 能力。innodb_flush_log_at_trx_commit 这个参数设置成 1 的时候，表示每次事务的 redo log 都直接持久化到磁盘。这个参数建议你设置成 1，这样可以保证 MySQL 异常重启之后数据不丢失。 (7) 重要的日志模块：binlog （归档日志） MySQL 整体来看，其实就有两块：一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的粉板 redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。 最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。 这两种日志有以下三点不同。 1. redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 2. redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。 3. redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 Tips Redo log不是记录数据页“更新之后的状态”，而是记录这个页 “做了什么改动”。 Binlog有两种模式，statement 格式的话是记sql语句， row格式会记录行的内容，记两条，更新前和更新后都有。 redo log 记录 做了什么改动（比如把某个字段从0改成了1） binlog 记录 是怎么修改的（记录sql语句 或者 记录更新前后的行） 有了对这两个日志的概念性理解，我们再来看执行器和 InnoDB 引擎在执行这个简单的 update 语句时的内部流程。 1. 执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 2. 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 3.引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 4. 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。 (7.1) 两阶段提交 为什么必须有“两阶段提交”呢？这是为了让两份日志之间的逻辑一致。要说明这个问题，我们得从文章开头的那个问题说起：怎样让数据库恢复到半个月内任意一秒的状态？ 前面我们说过了，binlog 会记录所有的逻辑操作，并且是采用“追加写”的形式。如果你的 DBA 承诺说半个月内可以恢复，那么备份系统中一定会保存最近半个月的所有 binlog，同时系统会定期做整库备份。这里的“定期”取决于系统的重要性，可以是一天一备，也可以是一周一备。 当需要恢复到指定的某一秒时，比如某天下午两点发现中午十二点有一次误删表，需要找回数据，那你可以这么做： 1. 首先，找到最近的一次全量备份，如果你运气好，可能就是昨天晚上的一个备份，从这个备份恢复到临时库； 2. 然后，从备份的时间点开始，将备份的 binlog 依次取出来，重放到中午误删表之前的那个时刻。 这样你的临时库就跟误删之前的线上库一样了，然后你可以把表数据从临时库取出来，按需要恢复到线上库去。 好了，说完了数据恢复过程，我们回来说说，为什么日志需要“两阶段提交”。这里不妨用反证法来进行解释。 由于 redo log 和 binlog 是两个独立的逻辑，如果不用两阶段提交，要么就是先写完 redo log 再写 binlog，或者采用反过来的顺序。我们看看这两种方式会有什么问题。 仍然用前面的 update 语句 update T set c=c+1 where ID=2; 来做例子。假设当前 ID=2 的行，字段 c 的值是 0，再假设执行 update 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash，会出现什么情况呢？ 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。 但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。 然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。 可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。 不只是误操作后需要用这个过程来恢复数据。当你需要扩容的时候，也就是需要再多搭建一些备库来增加系统的读能力的时候，现在常见的做法也是用全量备份加上应用 binlog 来实现的，这个“不一致”就会导致你的线上出现主从数据库不一致的情况。 简单说，redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。 sync_binlog 这个参数设置成 1 的时候，表示每次事务的 binlog 都持久化到磁盘。这个参数我也建议你设置成 1，这样可以保证 MySQL 异常重启之后 binlog 不丢失。 (8) 执行一条查询语句的流程 连接器 连接数据库。 查询缓存 分析器 词法解析 语法解析 优化器 执行器 (9) 更新一条语句的流程 连接器 连接数据库。 清空对应表缓存 分析器 词法解析 语法解析 优化器 执行器 更新 redo log（重做日志）和 binlog（归档日志）。 (10) 牛刀小试(10.1) 面试题 MySQL的框架有几个组件, 各是什么作用? Server层和存储引擎层各是什么作用? you have an error in your SQL syntax 这个保存是在词法分析里还是在语法分析里报错? 对于表的操作权限验证在哪里进行? 执行器的执行查询语句的流程是什么样的? (10.2) 如果表 T 中没有字段 k，而你执行了这个语句 select * from T where k=1, 那肯定是会报“不存在这个列”的错误： “Unknown column ‘k’ in ‘where clause’”。你觉得这个错误是在我们上面提到的哪个阶段报出来的呢？ 答案是 分析器 办法：1234567891011121314151617安装完MySQL之后,使用Debug模式启动mysqld --debug --console &amp;后，mysql&gt; create database wxb;Query OK, 1 row affected (0.01 sec)mysql&gt; use wxb;Database changedmysql&gt; create table t(a int);Query OK, 0 rows affected (0.01 sec)mysql&gt; select * from t where k=1;ERROR 1054 (42S22): Unknown column &apos;k&apos; in &apos;where clause&apos;T@4: | | | | | | | | | error: error: 1054 message: &apos;Unknown column &apos;k&apos; in &apos;where clause&apos;&apos;Complete optimizer trace:答案就很清楚了 (10.3) 各个模块的功能1，连接 连接管理模块，接收请求；连接进程和用户模块，通过，连接线程和客户端对接2，查询 查询缓存 Query Cache 分析器，内建解析树，对其语法检查，先from，再on，再join，再where……；检查权限，生成新的解析树，语义检查（没有字段k在这里）等 优化器，将前面解析树转换成执行计划，并进行评估最优 执行器，获取锁，打开表，通过meta数据，获取数据3，返回结果 返回给连接进程和用户模块，然后清理，重新等待新的请求。 (10.4) 动画描述各个模块的功能连接器：门卫，想进请出示准入凭证（工牌、邀请证明一类）。“你好，你是普通员工，只能进入办公大厅，不能到高管区域”此为权限查询。分析器：“您需要在公司里面找一张头发是黑色的桌子？桌子没有头发啊！臣妾做不到”优化器：“要我在A B两个办公室找张三和李四啊？那我应该先去B办公室找李四，然后请李四帮我去A办公室找张三，因为B办公室比较近且李四知道张三具体工位在哪”执行器：“好了，找人的计划方案定了，开始行动吧，走你！糟糕，刚门卫大哥说了，我没有权限进B办公室” (10.5) 为什么对权限的检查不在优化器之前做？ 有些时候，SQL语句要操作的表不只是SQL字面上那些。比如如果有个触发器，得在执行器阶段（过程中）才能确定。优化器阶段前是无能为力的 (10.6) 在什么场景下，一天一备会比一周一备更有优势呢？或者说，它影响了这个数据库系统的哪个指标？(10.7) 假如要插入10万条数据，怎么写插入语句比较好 批量插入 insert into t (id, c) values (1,1), (2, 2) (10.7) 假如要更新10万条数据，怎么写插入语句比较好 批量更新 (10.8) 详细的执行过程 1. 首先客户端通过tcp/ip发送一条sql语句到server层的SQL interface 2. SQL interface接到该请求后，先对该条语句进行解析，验证权限是否匹配 3. 验证通过以后，分析器会对该语句分析,是否语法有错误等 4. 接下来是优化器器生成相应的执行计划，选择最优的执行计划 5. 之后会是执行器根据执行计划执行这条语句。在这一步会去open table,如果该table上有MDL，则等待。如果没有，则加在该表上加短暂的MDL(S)。(如果opend_table太大,表明open_table_cache太小。需要不停的去打开frm文件) 6. 进入到引擎层，首先会去innodb_buffer_pool里的data dictionary(元数据信息)得到表信息 7. 通过元数据信息,去lock info里查出是否会有相关的锁信息，并把这条update语句需要的锁信息写入到lock info里(锁这里还有待补充) 8. 然后涉及到的老数据通过快照的方式存储到innodb_buffer_pool里的undo page里,并且记录undo log修改的redo (如果data page里有就直接载入到undo page里，如果没有，则需要去磁盘里取出相应page的数据，载入到undo page里) 9. 在innodb_buffer_pool的data page做update操作。并把操作的物理数据页修改记录到redo log buffer里。由于update这个事务会涉及到多个页面的修改，所以redo log buffer里会记录多条页面的修改信息。因为group commit的原因，这次事务所产生的redo log buffer可能会跟随其它事务一同flush并且sync到磁盘上 10. 同时修改的信息，会按照event的格式,记录到binlog_cache中。(这里注意binlog_cache_size是transaction级别的,不是session级别的参数,一旦commit之后，dump线程会从binlog_cache里把event主动发送给slave的I/O线程) 11. 之后把这条sql,需要在二级索引上做的修改，写入到change buffer page，等到下次有其他sql需要读取该二级索引时，再去与二级索引做merge (随机I/O变为顺序I/O,但是由于现在的磁盘都是SSD,所以对于寻址来说,随机I/O和顺序I/O差距不大) 12. 此时update语句已经完成，需要commit或者rollback。这里讨论commit的情况 13. commit操作，由于存储引擎层与server层之间采用的是内部XA(保证两个事务的一致性,这里主要保证redo log和binlog的原子性),所以提交分为prepare阶段与commit阶段 14. prepare阶段,将事务的xid写入，将binlog_cache里的进行flush以及sync操作(大事务的话这步非常耗时) 15. commit阶段，由于之前该事务产生的redo log已经sync到磁盘了。所以这步只是在redo log里标记commit 16. 当binlog和redo log都已经落盘以后，如果触发了刷新脏页的操作，先把该脏页复制到doublewrite buffer里，把doublewrite buffer里的刷新到共享表空间，然后才是通过page cleaner线程把脏页写入到磁盘中 (10.9) 写redo日志也是写io（我理解也是外部存储）。同样耗费性能。怎么能做到优化呢1.写redo日志也是写io（我理解也是外部存储）。同样耗费性能。怎么能做到优化呢2.数据库只有redo commit 之后才会真正提交到数据库吗 Redolog是顺序写，并且可以组提交，还有别的一些优化，收益最大是是这两个因素；2.是这样，正常执行是要commit 才算完，但是崩溃恢复过程的话，可以接受“redolog prepare 并且binlog完整” 的情况 (10.10) redo log是为了快速响应SQL充当了粉板 1. redo log本身也是文件，记录文件的过程其实也是写磁盘，那和文中提到的离线写磁盘操作有何区别？ 2.响应一次SQL我理解是要同时操作两个日志文件？也就是写磁盘两次？ 写redo log是顺序写，不用去“找位置”，而更新数据需要找位置 其实是3次（redolog两次 binlog 1次）。不过在并发更新的时候会合并写 (10.11) binlog为什么说是逻辑日志呢？它里面有内容也会存储成物理文件，怎么说是逻辑而不是物理 这样理解 逻辑日志可以给别的数据库，别的引擎使用，已经大家都讲得通这个“逻辑”； 物理日志就只有“我”自己能用，别人没有共享我的“物理格式” (10.12) redo log 和 bin log 是否重复 redo 是引擎提供的，binlog 是server 自带的，文中提到前者用在crash的恢复，后者用于库的恢复。两者是否在某种程度上是重复的？如果在都是追加写的情况下，是否两种日志都能用于 crash 与 库 的恢复呢？ Crash-safe是崩溃恢复，就是原地满血复活；binlog恢复是制造一个影分身（副本）出来。 (10.11) 如果把 innodb_flush_log_at_trx_commit 设置成1每次都写入到磁盘，那不就等于是掌柜的每次记账都记到账本上嘛，那还要小黑板干嘛呢？ binlog是逻辑，redolog是物理，两者都能记录历史，如果发生异常情况binlog就可以恢复数据，为什么说只有redolog才能算是crash-safe了呢。 Redolog是顺序写，数据文件是随机写。虽然都写盘，顺序写还是快很多的 (10.12) 1、首先数据库更新操作都是基于内存页，更新的时候不会直接更新磁盘，如果内存有存在就直接更新内存，如果内存没有存在就从磁盘读取到内存，在更新内存，并且写redo log，目的是为了更新效率更快，等空闲时间在将其redo log所做的改变更新到磁盘中，innodb_flush_log_at_trx_commit设置为1时，也可以防止服务出现异常重启，数据不会丢失 2、redo log两阶段提交，是为了保证redo log和binlog的一致性，如果redo log写入成功处于prepare阶段，写binlog失败，事务回滚，redo log会回滚到操作之前的状态 3、redo log也是写磁盘，写redo log是顺序写，update直接更新磁盘，需要找到数据，再对此数据进行更新(随机写)。 (10.13) 开启了半同步复制after_sync以后，假设一个事务在已经把binlog sync到磁盘了,在传输binlog到从库上时,主库挂了。如果这时发生了主从切换,从库是没有这个事务的.但是挂掉的主库重新启起来,由于binlog已经fsync到磁盘,虽然引擎层未commit,但是会根据binlog来恢复这个事务.这个时候就是主从不一致了,那么和after_commit那就没区别呀。请问这个事务是恢复还是回滚呢？如果是回滚,内部又是怎么判断的呢？ 如果你设置来双MM，启动以后binlog还会传到新主库的。但是如果你设置的是单向的，那就会不一致了… References[1] 01 | 基础架构：一条SQL查询语句是如何执行的？ - MySQL实战45讲[2] 02 | 日志系统：一条SQL更新语句是如何执行的？ - MySQL实战45讲[3] 《高性能MySQL》 O’REILLY]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL优化点]]></title>
    <url>%2F2019%2F03%2F08%2Fmysql-optimization-notes%2F</url>
    <content type="text"><![CDATA[未完.. References[1] 超级全面的MySQL优化面试解析[2] MySQL优化面试[3] innodb-best-practices.html]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch 索引管理]]></title>
    <url>%2F2019%2F01%2F18%2Felasticsearch-index-management%2F</url>
    <content type="text"><![CDATA[如果你想禁止自动创建索引，你可以通过在 config/elasticsearch.yml 的每个节点下添加下面的配置： 2.0版本 action.auto_create_index: false 创建索引 curl -X PUT &quot;localhost:9200/dev_employee_20181221&quot; 123WKQ@WKQ-PC MINGW64 /d/WorkSpaces$ curl -X PUT &quot;localhost:9200/dev_employee_20181219&quot;&#123;&quot;acknowledged&quot;:true,&quot;shards_acknowledged&quot;:true,&quot;index&quot;:&quot;dev_employee_20181219&quot;&#125; 删除索引 // 删除一个 curl -X DELETE &quot;localhost:9200/dev_employee_20181221&quot; // 删除多个 curl -X DELETE &quot;localhost:9200/dev_employee_20181220,dev_employee_20181221&quot; curl -X DELETE &quot;localhost:9200/dev_employee_*&quot; // 删除所有 curl -X DELETE &quot;localhost:9200/_all&quot; curl -X DELETE &quot;localhost:9200/*&quot; 123WKQ@WKQ-PC MINGW64 /d/WorkSpaces$ curl -X DELETE &quot;localhost:9200/dev_employee_20181219&quot;&#123;&quot;acknowledged&quot;:true&#125; 索引设置 number_of_shards 每个索引的主分片数，默认值是 5 。这个配置在索引创建后不能修改。 number_of_replicas 每个主分片的副本数，默认值是 1 。对于活动的索引库，这个配置可以随时修改。 12345678curl -X PUT &quot;localhost:9200/my_temp_index&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot; : 1, &quot;number_of_replicas&quot; : 0 &#125;&#125;&apos; 12345curl -X PUT &quot;localhost:9200/my_temp_index/_settings&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;number_of_replicas&quot;: 1&#125;&apos; 配置分词器1234567891011121314curl -X PUT &quot;localhost:9200/spanish_docs&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;es_std&quot;: &#123; &quot;type&quot;: &quot;standard&quot;, &quot;stopwords&quot;: &quot;_spanish_&quot; &#125; &#125; &#125; &#125;&#125;&apos; 123curl -X GET &quot;localhost:9200/spanish_docs/_analyze?analyzer=es_std&quot; -H &apos;Content-Type: application/json&apos; -d&apos;El veloz zorro marrón&apos; 创建索引 curl -X PUT &quot;localhost:9200/dev_employee_20181221&quot; 123WKQ@WKQ-PC MINGW64 /d/WorkSpaces$ curl -X PUT &quot;localhost:9200/dev_employee_20181221&quot;&#123;&quot;acknowledged&quot;:true,&quot;shards_acknowledged&quot;:true,&quot;index&quot;:&quot;dev_employee_20181221&quot;&#125; 给索引添加别名 curl -X PUT &quot;localhost:9200/dev_employee_20181221/_alias/dev_employee&quot; 123WKQ@WKQ-PC MINGW64 /d/WorkSpaces$ curl -X PUT &quot;localhost:9200/dev_employee_20181221/_alias/dev_employee&quot;&#123;&quot;acknowledged&quot;:true&#125; 检测这个别名指向哪些索引 同一别名多个索引 curl -X GET &quot;localhost:9200/*/_alias/dev_employee&quot;123WKQ@WKQ-PC MINGW64 /d/WorkSpaces$ curl -X GET &quot;localhost:9200/*/_alias/dev_employee&quot;&#123;&quot;dev_employee_20181221&quot;:&#123;&quot;aliases&quot;:&#123;&quot;dev_employee&quot;:&#123;&#125;&#125;&#125;,&quot;dev_employee_20181220&quot;:&#123;&quot;aliases&quot;:&#123;&quot;dev_employee&quot;:&#123;&#125;&#125;&#125;&#125; 哪些别名指向这个索引 一个索引多个别名 curl -X GET &quot;localhost:9200/dev_employee_20181221/_alias/*&quot;123WKQ@WKQ-PC MINGW64 /d/WorkSpaces$ curl -X GET &quot;localhost:9200/dev_employee_20181221/_alias/*&quot;&#123;&quot;dev_employee_20181221&quot;:&#123;&quot;aliases&quot;:&#123;&quot;dev_employee&quot;:&#123;&#125;,&quot;sit_employee&quot;:&#123;&#125;&#125;&#125;&#125; 索引添加别名、 索引删除别名、 索引修改别名(索引不变，别名变了)、 索引切换(别名不变，索引换了)12345678curl -X POST &quot;localhost:9200/_aliases&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;actions&quot;: [ &#123; &quot;remove&quot;: &#123; &quot;index&quot;: &quot;dev_employee_20181220&quot;, &quot;alias&quot;: &quot;dev_employee&quot; &#125;&#125;, &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;dev_employee_20181221&quot;, &quot;alias&quot;: &quot;dev_employee&quot; &#125;&#125; ]&#125;&apos; 123456789101112131415161718192021222324252627282930313233343536373839404142#索引添加别名curl -X POST &quot;localhost:9200/_aliases&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;actions&quot;: [ &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;dev_employee_20181221&quot;, &quot;alias&quot;: &quot;dev_employee&quot; &#125;&#125; ]&#125;&apos; #索引删除别名curl -X POST &quot;localhost:9200/_aliases&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;actions&quot;: [ &#123; &quot;remove&quot;: &#123; &quot;index&quot;: &quot;dev_employee_20181220&quot;, &quot;alias&quot;: &quot;dev_employee&quot; &#125;&#125; ]&#125;&apos;#索引修改别名WKQ@WKQ-PC MINGW64 /d/WorkSpaces $ curl -X POST &quot;localhost:9200/_aliases&quot; -H &apos;Content-Type: application/json&apos; -d&apos; &#123; &quot;actions&quot;: [ &#123; &quot;remove&quot;: &#123; &quot;index&quot;: &quot;dev_employee_20181220&quot;, &quot;alias&quot;: &quot;dev_employee&quot; &#125;&#125;, &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;dev_employee_20181220&quot;, &quot;alias&quot;: &quot;dev_employee_backup&quot; &#125;&#125; ] &#125; &apos; &#123;&quot;acknowledged&quot;:true&#125;#索引切换(别名不变，索引换了) WKQ@WKQ-PC MINGW64 /d/WorkSpaces $ curl -X POST &quot;localhost:9200/_aliases&quot; -H &apos;Content-Type: application/json&apos; -d&apos; &#123; &quot;actions&quot;: [ &#123; &quot;remove&quot;: &#123; &quot;index&quot;: &quot;dev_employee_20181220&quot;, &quot;alias&quot;: &quot;dev_employee&quot; &#125;&#125;, &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;dev_employee_20181221&quot;, &quot;alias&quot;: &quot;dev_employee&quot; &#125;&#125; ] &#125; &apos; &#123;&quot;acknowledged&quot;:true&#125; References[1] 索引管理]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>index</tag>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch 游标 使用]]></title>
    <url>%2F2019%2F01%2F18%2Felasticsearch-scroll%2F</url>
    <content type="text"><![CDATA[碰到一个比较头疼的问题，MySQL数据丢失。 有两个办法，一个办法是让DBA找半年前的数据。另一个办法是保存了MySQL数据的ES里找。 由于数据量过万，而且ES设置了一次查询数据量最大10000，想想用 scroll 取数据会比较好。 (1) ElasticSearch 2.x(1.1) 查询索引有多少数据 localhost:9200/_nodes/stats/indices/search?pretty (1.2) 查看索引信息 curl -XGET &#39;http://127.0.0.1:9400/dev_index1_20190118/docs/_search?pretty&#39; (1.3) 使用游标123456curl -XGET &apos;http://127.0.0.1:9400/dev_index1_20190118/docs/_search?scroll=10m&apos; -d &apos; &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125;&#125;, &quot;sort&quot; : [&quot;_doc&quot;], &quot;size&quot;: 10000&#125;&apos; &gt;&gt; es_scroll_data_20190118_1w.txt (1.4) 不断取下一页12345curl -XGET &apos;http://127.0.0.1:9400/_search?scroll=10m&apos; -d &apos; &#123; &quot;scroll&quot;: &quot;10m&quot;, &quot;scroll_id&quot; : &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAANKLTFjdfVno3Y2hGU1BpOUYtMW54OTV3TEEAAAAAADSi1BY3X1Z6N2NoRlNQaTlGLTFueDk1d0xBAAAAAAA0otYWN19WejdjaEZTUGk5Ri0xbng5NXdMQQAAAAAANKLVFjdfVno3Y2hGU1BpOUYtMW54OTV3TEEAAAAAADvJpxZzcU9YSExnLVRTNk5RY3JfMlNuWU9n&quot;&#125;&apos; &gt;&gt; es_scroll_data_20190118_2w.txt 12345curl -XGET &apos;http://127.0.0.1:9400/_search?scroll=10m&apos; -d &apos; &#123; &quot;scroll&quot;: &quot;10m&quot;, &quot;scroll_id&quot; : &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAANKLTFjdfVno3Y2hGU1BpOUYtMW54OTV3TEEAAAAAADSi1BY3X1Z6N2NoRlNQaTlGLTFueDk1d0xBAAAAAAA0otYWN19WejdjaEZTUGk5Ri0xbng5NXdMQQAAAAAANKLVFjdfVno3Y2hGU1BpOUYtMW54OTV3TEEAAAAAADvJpxZzcU9YSExnLVRTNk5RY3JfMlNuWU9n&quot;&#125;&apos; &gt;&gt; es_scroll_data_20190118_3w.txt (2) ElasticSearch 5.6.x(2.1) 查询索引信息 localhost:9200/_nodes/stats/indices/search?pretty curl -XGET &#39;http://127.0.0.1:9400/dev_index1_20190118/docs/_search?pretty&#39; (2.2) 使用游标123456curl -XGET &apos;http://127.0.0.1:9400/dev_index1_20190118/docs/_search?scroll=10m&apos; -d &apos; &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125;&#125;, &quot;sort&quot; : [&quot;_doc&quot;], &quot;size&quot;: 10000&#125;&apos; &gt;&gt; es_scroll_data_20190118_1w.txt (2.3) 不断取下一页1234curl -XGET &apos;http://127.0.0.1:9400/_search?scroll=10m&apos; -d &apos; &#123; &quot;scroll_id&quot; : &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAANKLTFjdfVno3Y2hGU1BpOUYtMW54OTV3TEEAAAAAADSi1BY3X1Z6N2NoRlNQaTlGLTFueDk1d0xBAAAAAAA0otYWN19WejdjaEZTUGk5Ri0xbng5NXdMQQAAAAAANKLVFjdfVno3Y2hGU1BpOUYtMW54OTV3TEEAAAAAADvJpxZzcU9YSExnLVRTNk5RY3JfMlNuWU9n&quot;&#125;&apos; &gt;&gt; es_scroll_data_20190118_2w.txt 1234curl -XGET &apos;http://127.0.0.1:9400/_search?scroll=10m&apos; -d &apos; &#123; &quot;scroll_id&quot; : &quot;DnF1ZXJ5VGhlbkZldGNoBQAAAAAANKLTFjdfVno3Y2hGU1BpOUYtMW54OTV3TEEAAAAAADSi1BY3X1Z6N2NoRlNQaTlGLTFueDk1d0xBAAAAAAA0otYWN19WejdjaEZTUGk5Ri0xbng5NXdMQQAAAAAANKLVFjdfVno3Y2hGU1BpOUYtMW54OTV3TEEAAAAAADvJpxZzcU9YSExnLVRTNk5RY3JfMlNuWU9n&quot;&#125;&apos; &gt;&gt; es_scroll_data_20190118_3w.txt (3) 遇到的问题(3.1) Unknown key for a VALUE_STRING in [scroll_id].1234567891011121314151617&#123; "error": &#123; "root_cause": [ &#123; "type": "parsing_exception", "reason": "Unknown key for a VALUE_STRING in [scroll_id].", "line": 3, "col": 19 &#125; ], "type": "parsing_exception", "reason": "Unknown key for a VALUE_STRING in [scroll_id].", "line": 3, "col": 19 &#125;, "status": 400&#125; 第二次使用的 scroll_id 和第一次返回的 scroll_id 不一致导致 (3.2) Unknown key for a VALUE_STRING in [scroll]1234567891011121314151617&#123; "error": &#123; "root_cause": [ &#123; "type": "parsing_exception", "reason": "Unknown key for a VALUE_STRING in [scroll].", "line": 3, "col": 15 &#125; ], "type": "parsing_exception", "reason": "Unknown key for a VALUE_STRING in [scroll].", "line": 3, "col": 15 &#125;, "status": 400&#125; 第二次请求时 请求参数里多了 scroll 参数 (3.3) Batch size is too large, size must be less than or equal to: [10000] but was [1000000]. Scroll batch sizes cost as much memory as result windows so they are controlled by the [index.max_result_window] index level setting.1234567891011121314151617181920212223242526&#123; "error": &#123; "root_cause": [ &#123; "type": "query_phase_execution_exception", "reason": "Batch size is too large, size must be less than or equal to: [10000] but was [1000000]. Scroll batch sizes cost as much memory as result windows so they are controlled by the [index.max_result_window] index level setting." &#125; ], "type": "search_phase_execution_exception", "reason": "all shards failed", "phase": "query", "grouped": true, "failed_shards": [ &#123; "shard": 0, "index": "dev_index1_20190118", "node": "8XqKY198S823M78QA43F8g", "reason": &#123; "type": "query_phase_execution_exception", "reason": "Batch size is too large, size must be less than or equal to: [10000] but was [1000000]. Scroll batch sizes cost as much memory as result windows so they are controlled by the [index.max_result_window] index level setting." &#125; &#125; ] &#125;, "status": 500&#125; 设置的 size 过大，超过10000，配置文件里 index.max_result_window 最大为10000 (3.4) search_context_missing_exception1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677&#123; "error": &#123; "root_cause": [ &#123; "type": "search_context_missing_exception", "reason": "No search context found for id [3540965]" &#125;, &#123; "type": "search_context_missing_exception", "reason": "No search context found for id [3922089]" &#125;, &#123; "type": "search_context_missing_exception", "reason": "No search context found for id [3454995]" &#125;, &#123; "type": "search_context_missing_exception", "reason": "No search context found for id [3454996]" &#125;, &#123; "type": "search_context_missing_exception", "reason": "No search context found for id [3454994]" &#125; ], "type": "search_phase_execution_exception", "reason": "all shards failed", "phase": "query", "grouped": true, "failed_shards": [ &#123; "shard": -1, "index": null, "reason": &#123; "type": "search_context_missing_exception", "reason": "No search context found for id [3540965]" &#125; &#125;, &#123; "shard": -1, "index": null, "reason": &#123; "type": "search_context_missing_exception", "reason": "No search context found for id [3922089]" &#125; &#125;, &#123; "shard": -1, "index": null, "reason": &#123; "type": "search_context_missing_exception", "reason": "No search context found for id [3454995]" &#125; &#125;, &#123; "shard": -1, "index": null, "reason": &#123; "type": "search_context_missing_exception", "reason": "No search context found for id [3454996]" &#125; &#125;, &#123; "shard": -1, "index": null, "reason": &#123; "type": "search_context_missing_exception", "reason": "No search context found for id [3454994]" &#125; &#125; ], "caused_by": &#123; "type": "search_context_missing_exception", "reason": "No search context found for id [3454994]" &#125; &#125;, "status": 404&#125; 其实是超时了，scroll自动删除了 References[1] 游标查询 [2] scroll]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java-sort]]></title>
    <url>%2F2018%2F11%2F16%2Fjava-sort%2F</url>
    <content type="text"><![CDATA[遇到一个排序需求，按照数字、字母、汉字的顺序排序。 123456&lt;!-- https://mvnrepository.com/artifact/com.ibm.icu/icu4j --&gt;&lt;dependency&gt; &lt;groupId&gt;com.ibm.icu&lt;/groupId&gt; &lt;artifactId&gt;icu4j&lt;/artifactId&gt; &lt;version&gt;4.8&lt;/version&gt;&lt;/dependency&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546Collections.sort(datas, new Comparator&lt;TestVo&gt;() &#123; @Override public int compare(TestVo o1, TestVo o2) &#123; // if (o1.getField1() == null) &#123; return 1; &#125; else if (o2.getField1() == null) &#123; return -1; &#125; else if (!o1.getField1().equals(o2.getField1())) &#123; return com.ibm.icu.text.Collator.getInstance(com.ibm.icu.util.ULocale.SIMPLIFIED_CHINESE).compare(o1.getField1(), o2.getField1()); &#125; // if (o1.getField2() == null) &#123; return 1; &#125; else if (o2.getField2() == null) &#123; return -1; &#125; if (!o1.getField2().equals(o2.getField2())) &#123; return com.ibm.icu.text.Collator.getInstance(com.ibm.icu.util.ULocale.SIMPLIFIED_CHINESE).compare(o1.getField2(), o2.getField2()); &#125; // if (o1.getField3() == null) &#123; return 1; &#125; else if (o2.getField3() == null) &#123; return -1; &#125; if (!o1.getField3().equals(o2.getField3())) &#123; return com.ibm.icu.text.Collator.getInstance(com.ibm.icu.util.ULocale.SIMPLIFIED_CHINESE).compare(o1.getField3(), o2.getField3()); &#125; // if (o1.geField4() == null) &#123; return -1; &#125; else if (o2.geField4() == null) &#123; return 1; &#125; if (!o1.geField4().equals(o2.geField4())) &#123; return com.ibm.icu.text.Collator.getInstance(com.ibm.icu.util.ULocale.SIMPLIFIED_CHINESE).compare(o2.geField4(), o1.geField4()); &#125; return 0; &#125;&#125;); 12345678java.lang.IllegalArgumentException: Comparison method violates its general contract! at java.util.TimSort.mergeLo(TimSort.java:777) ~[?:1.8.0_71] at java.util.TimSort.mergeAt(TimSort.java:514) ~[?:1.8.0_71] at java.util.TimSort.mergeCollapse(TimSort.java:439) ~[?:1.8.0_71] at java.util.TimSort.sort(TimSort.java:245) ~[?:1.8.0_71] at java.util.Arrays.sort(Arrays.java:1512) ~[?:1.8.0_71] at java.util.ArrayList.sort(ArrayList.java:1454) ~[?:1.8.0_71] at java.util.Collections.sort(Collections.java:175) ~[?:1.8.0_71] JDK7中的Collections.Sort方法实现中，你的返回值需要严谨全面； 如果两个值是相等的，那么compare方法需要返回0，否则 可能 会在排序时抛错，而JDK6是没有这个限制的。 在在 JDK7 版本以上，Comparator 要满足自反性，传递性，对称性 说明： 1) 自反性：x，y 的比较结果和 y，x 的比较结果相反。 2) 传递性：x&gt;y,y&gt;z,则 x&gt;z。 3) 对称性：x=y,则 x,z 比较结果和 y，z 比较结果相同 文件排序文件按日期排序12345678910111213141516Arrays.sort(files, new Comparator&lt;File&gt;() &#123; public int compare(File f1, File f2) &#123; long diff = f1.lastModified() - f2.lastModified(); if (diff &gt; 0) return 1; else if (diff == 0) return 0; else return -1; &#125; public boolean equals(Object obj) &#123; return true; &#125;&#125;); 按照文件名称排序12345678910Arrays.sort(files, new Comparator&lt;File&gt;() &#123; @Override public int compare(File o1, File o2) &#123; if (o1.isDirectory() &amp;&amp; o2.isFile()) return -1; if (o1.isFile() &amp;&amp; o2.isDirectory()) return 1; return o1.getName().compareTo(o2.getName()); &#125;&#125;); 按照文件大小排序123456789101112131415Arrays.sort(files, new Comparator&lt;File&gt;() &#123; public int compare(File f1, File f2) &#123; long diff = f1.length() - f2.length(); if (diff &gt; 0) return 1; else if (diff == 0) return 0; else return -1; &#125; public boolean equals(Object obj) &#123; return true; &#125;&#125;); 1String number = name.replaceAll(&quot;[^\\d]&quot;, &quot;&quot;); References[1] Comparison method violates its general contract 问题的处理 [2] Comparison method violates its general contract! [3] file.listFiles()按文件名称、日期、大小排序方法 总结 [4] file.listFiles()按文件大小、名称、日期排序方法]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OrientDB 官方 etl 工具 导入 rdbms]]></title>
    <url>%2F2018%2F09%2F30%2Forientdb-etl-rdbms%2F</url>
    <content type="text"><![CDATA[这儿使用 用户 - 群组 举例。(可以认为是QQ用户和QQ群的关系) 一个用户有多个群组，一个群组有多个用户，用户和群组是多对多关系。 这儿使用OrientDb官方自带的etl rdbms导入数据。 吐槽：OrientDb ETL 工具 对多对多的关系支持不好，还需要自己写代码处理。(总感觉OrientDb是程序员思维的产品，功能可以用) Neo4j支持的就特别好，一个语句就解决了，瞬间感觉Neo4j好灵活。 1. 准备环境 下载MySQL的jar包放到OrientDb的lib目录下。 启动OrientDb 准备MySQL数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667-- ------------------------------ Table structure for group-- ----------------------------DROP TABLE IF EXISTS `group`;CREATE TABLE `group` ( `group_id` int(11) NOT NULL AUTO_INCREMENT, `group_name` varchar(8) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL, PRIMARY KEY (`group_id`) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 5 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Records of group-- ----------------------------INSERT INTO `group` VALUES (1, 'group1');INSERT INTO `group` VALUES (2, 'group2');INSERT INTO `group` VALUES (3, 'group3');INSERT INTO `group` VALUES (4, '组织4');-- ------------------------------ Table structure for person-- ----------------------------DROP TABLE IF EXISTS `person`;CREATE TABLE `person` ( `person_id` int(11) NOT NULL AUTO_INCREMENT, `person_name` varchar(8) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL, PRIMARY KEY (`person_id`) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 5 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Records of person-- ----------------------------INSERT INTO `person` VALUES (1, 'person1');INSERT INTO `person` VALUES (2, 'person2');INSERT INTO `person` VALUES (3, 'person3');INSERT INTO `person` VALUES (4, '人4');-- ------------------------------ Table structure for person_group-- ----------------------------DROP TABLE IF EXISTS `person_group`;CREATE TABLE `person_group` ( `id` int(11) NOT NULL AUTO_INCREMENT, `person_id` int(11) NULL DEFAULT NULL, `person_name` varchar(8) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL, `group_id` int(11) NULL DEFAULT NULL, `group_name` varchar(8) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NULL DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE) ENGINE = InnoDB AUTO_INCREMENT = 16 CHARACTER SET = utf8mb4 COLLATE = utf8mb4_general_ci ROW_FORMAT = Dynamic;-- ------------------------------ Records of person_group-- ----------------------------INSERT INTO `person_group` VALUES (1, 1, 'person1', 1, NULL);INSERT INTO `person_group` VALUES (2, 1, NULL, 2, NULL);INSERT INTO `person_group` VALUES (3, 1, NULL, 3, NULL);INSERT INTO `person_group` VALUES (4, 1, NULL, 4, NULL);INSERT INTO `person_group` VALUES (5, 2, NULL, 1, NULL);INSERT INTO `person_group` VALUES (6, 2, NULL, 2, NULL);INSERT INTO `person_group` VALUES (7, 2, NULL, 3, NULL);INSERT INTO `person_group` VALUES (8, 2, NULL, 4, NULL);INSERT INTO `person_group` VALUES (9, 3, NULL, 1, NULL);INSERT INTO `person_group` VALUES (10, 3, NULL, 2, NULL);INSERT INTO `person_group` VALUES (11, 3, NULL, 3, NULL);INSERT INTO `person_group` VALUES (12, 3, NULL, 4, NULL);INSERT INTO `person_group` VALUES (13, 4, NULL, 1, NULL);INSERT INTO `person_group` VALUES (14, 4, NULL, 2, NULL);INSERT INTO `person_group` VALUES (15, 4, NULL, 3, NULL); 2. 导入用户(Person)节点oetl.bat D:/WorkSpaces/data/orientdb/orientdb-etl-rdbms-Person.json 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&#123; &quot;config&quot;: &#123; &quot;log&quot;: &quot;debug&quot; &#125;, &quot;extractor&quot;: &#123; &quot;jdbc&quot;: &#123; &quot;driver&quot;: &quot;com.mysql.jdbc.Driver&quot;, &quot;url&quot;: &quot;jdbc:mysql://localhost:3306/test?useUnicode=true&quot;, &quot;userName&quot;: &quot;root&quot;, &quot;userPassword&quot;: &quot;root&quot;, &quot;query&quot;: &quot; select person_id, person_name from `person` ; &quot; &#125; &#125;, &quot;transformers&quot;: [ &#123; &quot;vertex&quot;: &#123; &quot;class&quot;: &quot;Person&quot; &#125; &#125; ], &quot;loader&quot;: &#123; &quot;orientdb&quot;: &#123; &quot;dbURL&quot;: &quot;remote:localhost/person_group&quot;, &quot;dbType&quot;: &quot;graph&quot;, &quot;dbUser&quot;: &quot;root&quot;, &quot;dbPassword&quot;: &quot;root&quot;, &quot;serverUser&quot;: &quot;root&quot;, &quot;serverPassword&quot;: &quot;root&quot;, &quot;classes&quot;: [ &#123; &quot;name&quot;: &quot;Person&quot;, &quot;extends&quot;: &quot;V&quot; &#125; ], &quot;indexes&quot;: [ &#123; &quot;class&quot;: &quot;Person&quot;, &quot;fields&quot;: [ &quot;person_id:integer&quot; ], &quot;type&quot;: &quot;NOTUNIQUE&quot; &#125;, &#123; &quot;class&quot;: &quot;Person&quot;, &quot;fields&quot;: [ &quot;person_name:string&quot; ], &quot;type&quot;: &quot;NOTUNIQUE&quot; &#125; ] &#125; &#125;&#125; 3. 导入群组(Group)节点 oetl.bat D:/WorkSpaces/data/orientdb/orientdb-etl-rdbms-Group.json 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&#123; &quot;config&quot;: &#123; &quot;log&quot;: &quot;debug&quot; &#125;, &quot;extractor&quot;: &#123; &quot;jdbc&quot;: &#123; &quot;driver&quot;: &quot;com.mysql.jdbc.Driver&quot;, &quot;url&quot;: &quot;jdbc:mysql://localhost:3306/test?useUnicode=true&quot;, &quot;userName&quot;: &quot;root&quot;, &quot;userPassword&quot;: &quot;root&quot;, &quot;query&quot;: &quot; select group_id, group_name from `group` ; &quot; &#125; &#125;, &quot;transformers&quot;: [ &#123; &quot;vertex&quot;: &#123; &quot;class&quot;: &quot;Group&quot; &#125; &#125; ], &quot;loader&quot;: &#123; &quot;orientdb&quot;: &#123; &quot;dbURL&quot;: &quot;remote:localhost/person_group&quot;, &quot;dbType&quot;: &quot;graph&quot;, &quot;dbUser&quot;: &quot;root&quot;, &quot;dbPassword&quot;: &quot;root&quot;, &quot;serverUser&quot;: &quot;root&quot;, &quot;serverPassword&quot;: &quot;root&quot;, &quot;classes&quot;: [ &#123; &quot;name&quot;: &quot;Group&quot;, &quot;extends&quot;: &quot;V&quot; &#125; ], &quot;indexes&quot;: [ &#123; &quot;class&quot;: &quot;Group&quot;, &quot;fields&quot;: [ &quot;group_id:integer&quot; ], &quot;type&quot;: &quot;NOTUNIQUE&quot; &#125;, &#123; &quot;class&quot;: &quot;Group&quot;, &quot;fields&quot;: [ &quot;group_name:string&quot; ], &quot;type&quot;: &quot;NOTUNIQUE&quot; &#125; ] &#125; &#125;&#125; 4. 建立关系 在建立多对多关系这块，OrientDb做的不好，etl工具不能直接建立多对多的关系，需要自己想办法处理。 或者多对多关系自己写代码解决。 我是先用节点保存关系，然后再写代码根据PersonGroup建立Person和Group关系，最后删除PersonGroup节点。曲线建关系，生产环境不推荐这么做。 oetl.bat D:/WorkSpaces/data/orientdb/orientdb-etl-rdbms-PersonGroup.json 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&#123; &quot;config&quot;: &#123; &quot;log&quot;: &quot;debug&quot; &#125;, &quot;extractor&quot;: &#123; &quot;jdbc&quot;: &#123; &quot;driver&quot;: &quot;com.mysql.jdbc.Driver&quot;, &quot;url&quot;: &quot;jdbc:mysql://localhost:3306/test?useUnicode=true&quot;, &quot;userName&quot;: &quot;root&quot;, &quot;userPassword&quot;: &quot;root&quot;, &quot;query&quot;: &quot; select id, person_id, group_id from person_group ; &quot; &#125; &#125;, &quot;transformers&quot;: [ &#123; &quot;vertex&quot;: &#123; &quot;class&quot;: &quot;PersonGroup&quot; &#125; &#125;, &#123; &quot;edge&quot;: &#123; &quot;class&quot;: &quot;Belong&quot;, &quot;joinFieldName&quot;: &quot;person_id&quot;, &quot;lookup&quot;: &quot;Person.person_id&quot;, &quot;direction&quot;: &quot;in&quot;, &quot;unresolvedLinkAction&quot;:&quot;CREATE&quot; &#125; &#125;, &#123; &quot;edge&quot;: &#123; &quot;class&quot;: &quot;Has&quot;, &quot;joinFieldName&quot;: &quot;group_id&quot;, &quot;lookup&quot;: &quot;Group.group_id&quot;, &quot;direction&quot;: &quot;out&quot;, &quot;unresolvedLinkAction&quot;:&quot;CREATE&quot; &#125; &#125; ], &quot;loader&quot;: &#123; &quot;orientdb&quot;: &#123; &quot;dbURL&quot;: &quot;remote:localhost/person_group&quot;, &quot;dbType&quot;: &quot;graph&quot;, &quot;dbUser&quot;: &quot;root&quot;, &quot;dbPassword&quot;: &quot;root&quot;, &quot;serverUser&quot;: &quot;root&quot;, &quot;serverPassword&quot;: &quot;root&quot;, &quot;classes&quot;: [ &#123; &quot;name&quot;: &quot;PersonGroup&quot;, &quot;extends&quot;: &quot;V&quot; &#125;, &#123; &quot;name&quot;: &quot;Belong&quot;, &quot;extends&quot;: &quot;E&quot; &#125;, &#123; &quot;name&quot;: &quot;Has&quot;, &quot;extends&quot;: &quot;E&quot; &#125; ], &quot;indexes&quot;: [ &#123; &quot;class&quot;: &quot;PersonGroup&quot;, &quot;fields&quot;: [ &quot;id:integer&quot; ], &quot;type&quot;: &quot;UNIQUE&quot; &#125;, &#123; &quot;class&quot;: &quot;PersonGroup&quot;, &quot;fields&quot;: [ &quot;person_id:integer&quot; ], &quot;type&quot;: &quot;NOTUNIQUE&quot; &#125;, &#123; &quot;class&quot;: &quot;PersonGroup&quot;, &quot;fields&quot;: [ &quot;group_id:integer&quot; ], &quot;type&quot;: &quot;NOTUNIQUE&quot; &#125; ] &#125; &#125;&#125;]]></content>
      <categories>
        <category>orientdb</category>
      </categories>
      <tags>
        <tag>orientdb</tag>
        <tag>database</tag>
        <tag>etl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OrientDB 官方 etl 工具 导入 csv文件]]></title>
    <url>%2F2018%2F09%2F30%2Forientdb-etl-csv%2F</url>
    <content type="text"><![CDATA[beers.csv: contains the beer recordsbreweries.csv: contains the breweries recordsbreweries_geocode.csv: contains the geocodes of the breweries. This file is not used in this Tutorialcategories.csv: contains the beer categoriesstyles.csv: contains the beer styles beers.csv：包含啤酒记录breweries.csv：包含啤酒厂记录breweries_geocode.csv：包含啤酒厂的地理编码。 本教程中未使用此文件categories.csv：包含啤酒类别styles.csv：包含啤酒风格 Nodes: Beer, Category, Style, Brewery;Relationships: HasCategory, HasStyle, HasBrewery. 节点：啤酒，类别，风格，啤酒厂;关系：有类别，有风格，有啤酒厂。 12345678910111213141516171819202122&#123; &quot;source&quot;: &#123; &quot;file&quot;: &#123; &quot;path&quot;: &quot;D:/WorkSpaces/data/orientdb/openbeerdb/categories.csv&quot; &#125; &#125;, &quot;extractor&quot;: &#123; &quot;csv&quot;: &#123;&#125; &#125;, &quot;transformers&quot;: [ &#123; &quot;vertex&quot;: &#123; &quot;class&quot;: &quot;Category&quot; &#125; &#125; ], &quot;loader&quot;: &#123; &quot;orientdb&quot;: &#123; &quot;dbURL&quot;: &quot;remote:localhost/openbeerdb&quot;, &quot;dbType&quot;: &quot;graph&quot;, &quot;dbUser&quot;: &quot;root&quot;, &quot;dbPassword&quot;: &quot;root&quot;, &quot;serverUser&quot;: &quot;root&quot;, &quot;serverPassword&quot;: &quot;root&quot;, &quot;classes&quot;: [ &#123;&quot;name&quot;: &quot;Category&quot;, &quot;extends&quot;: &quot;V&quot;&#125; ], &quot;indexes&quot;: [ &#123;&quot;class&quot;:&quot;Category&quot;, &quot;fields&quot;:[&quot;id:integer&quot;], &quot;type&quot;:&quot;UNIQUE&quot; &#125; ] &#125; &#125;&#125; 123456789101112131415161718192021222324&#123; &quot;source&quot;: &#123; &quot;file&quot;: &#123; &quot;path&quot;: &quot;D:/WorkSpaces/data/orientdb/openbeerdb/styles.csv&quot; &#125; &#125;, &quot;extractor&quot;: &#123; &quot;csv&quot;: &#123;&#125; &#125;, &quot;transformers&quot;: [ &#123; &quot;vertex&quot;: &#123; &quot;class&quot;: &quot;Style&quot; &#125; &#125;, &#123; &quot;edge&quot;: &#123; &quot;class&quot;: &quot;HasCategory&quot;, &quot;joinFieldName&quot;: &quot;cat_id&quot;, &quot;lookup&quot;: &quot;Category.id&quot; &#125; &#125; ], &quot;loader&quot;: &#123; &quot;orientdb&quot;: &#123; &quot;dbURL&quot;: &quot;remote:localhost/openbeerdb&quot;, &quot;dbType&quot;: &quot;graph&quot;, &quot;dbUser&quot;: &quot;root&quot;, &quot;dbPassword&quot;: &quot;root&quot;, &quot;serverUser&quot;: &quot;root&quot;, &quot;serverPassword&quot;: &quot;root&quot;, &quot;classes&quot;: [ &#123;&quot;name&quot;: &quot;Style&quot;, &quot;extends&quot;: &quot;V&quot;&#125;, &#123;&quot;name&quot;: &quot;HasCategory&quot;, &quot;extends&quot;: &quot;E&quot;&#125; ], &quot;indexes&quot;: [ &#123;&quot;class&quot;:&quot;Style&quot;, &quot;fields&quot;:[&quot;id:integer&quot;], &quot;type&quot;:&quot;UNIQUE&quot; &#125; ] &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930&#123; &quot;config&quot; : &#123; &quot;haltOnError&quot;: false &#125;, &quot;source&quot;: &#123; &quot;file&quot;: &#123; &quot;path&quot;: &quot;D:/WorkSpaces/data/orientdb/openbeerdb/beers.csv&quot; &#125; &#125;, &quot;extractor&quot;: &#123; &quot;csv&quot;: &#123; &quot;columns&quot;: [&quot;id&quot;,&quot;brewery_id&quot;,&quot;name&quot;,&quot;cat_id&quot;,&quot;style_id&quot;,&quot;abv&quot;,&quot;ibu&quot;,&quot;srm&quot;,&quot;upc&quot;,&quot;filepath&quot;,&quot;descript&quot;,&quot;last_mod&quot;], &quot;columnsOnFirstLine&quot;: true &#125; &#125;, &quot;transformers&quot;: [ &#123; &quot;vertex&quot;: &#123; &quot;class&quot;: &quot;Beer&quot; &#125; &#125;, &#123; &quot;edge&quot;: &#123; &quot;class&quot;: &quot;HasCategory&quot;, &quot;joinFieldName&quot;: &quot;cat_id&quot;, &quot;lookup&quot;: &quot;Category.id&quot; &#125; &#125;, &#123; &quot;edge&quot;: &#123; &quot;class&quot;: &quot;HasBrewery&quot;, &quot;joinFieldName&quot;: &quot;brewery_id&quot;, &quot;lookup&quot;: &quot;Brewery.id&quot; &#125; &#125;, &#123; &quot;edge&quot;: &#123; &quot;class&quot;: &quot;HasStyle&quot;, &quot;joinFieldName&quot;: &quot;style_id&quot;, &quot;lookup&quot;: &quot;Style.id&quot; &#125; &#125; ], &quot;loader&quot;: &#123; &quot;orientdb&quot;: &#123; &quot;dbURL&quot;: &quot;remote:localhost/openbeerdb&quot;, &quot;dbType&quot;: &quot;graph&quot;, &quot;dbUser&quot;: &quot;root&quot;, &quot;dbPassword&quot;: &quot;root&quot;, &quot;serverUser&quot;: &quot;root&quot;, &quot;serverPassword&quot;: &quot;root&quot;, &quot;classes&quot;: [ &#123;&quot;name&quot;: &quot;Beer&quot;, &quot;extends&quot;: &quot;V&quot;&#125;, &#123;&quot;name&quot;: &quot;HasCategory&quot;, &quot;extends&quot;: &quot;E&quot;&#125;, &#123;&quot;name&quot;: &quot;HasStyle&quot;, &quot;extends&quot;: &quot;E&quot;&#125;, &#123;&quot;name&quot;: &quot;HasBrewery&quot;, &quot;extends&quot;: &quot;E&quot;&#125; ], &quot;indexes&quot;: [ &#123;&quot;class&quot;:&quot;Beer&quot;, &quot;fields&quot;:[&quot;id:integer&quot;], &quot;type&quot;:&quot;UNIQUE&quot; &#125; ] &#125; &#125;&#125;]]></content>
      <categories>
        <category>orientdb</category>
      </categories>
      <tags>
        <tag>orientdb</tag>
        <tag>database</tag>
        <tag>etl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 问题 定位]]></title>
    <url>%2F2018%2F09%2F24%2Fjava-problem-locate%2F</url>
    <content type="text"><![CDATA[遇到问题第一反应，看日志，重现。 看日志，定位是传参的问题，还是程序逻辑，还是代码里内存未释放。 重现问题 分析问题用 JMX JMC jvisualvm 查看Java服务在运行过程中的内存、GC、线程等信息。VisualVM是Sun的一个OpenJDK项目，它是集成了多个JDK命令工具的一个可视化工具，它主要用来监控JVM的运行情况，可以用它来查看和浏览Heap Dump、Thread Dump、内存对象实例情况、GC执行情况、CPU消耗以及类的装载情况，也可以使用它来创建必要信息的日志。 也可以使用jstat命令来堆Java堆内存的使用情况进行统计 pid 3031jstat -gcutil 3031 1000 10 123456789101112[wkq@VM_77_25_centos ~]$ jstat -gcutil 3031 1000 10 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 0.00 100.00 27.27 78.98 97.75 97.20 155 1.642 0 0.000 1.642 0.00 100.00 27.27 78.98 97.75 97.20 155 1.642 0 0.000 1.642 0.00 100.00 27.27 78.98 97.75 97.20 155 1.642 0 0.000 1.642 0.00 100.00 27.27 78.98 97.75 97.20 155 1.642 0 0.000 1.642 0.00 100.00 27.27 78.98 97.75 97.20 155 1.642 0 0.000 1.642 0.00 100.00 27.27 78.98 97.75 97.20 155 1.642 0 0.000 1.642 0.00 100.00 27.27 78.98 97.75 97.20 155 1.642 0 0.000 1.642 0.00 100.00 27.27 78.98 97.75 97.20 155 1.642 0 0.000 1.642 0.00 100.00 27.27 78.98 97.75 97.20 155 1.642 0 0.000 1.642 0.00 100.00 27.27 78.98 97.75 97.20 155 1.642 0 0.000 1.642 YGC: 从启动到采样时Young Generation GC的次数 YGCT: 从启动到采样时Young Generation GC所用的时间 (s). FGC: 从启动到采样时Old Generation GC的次数. FGCT: 从启动到采样时Old Generation GC所用的时间 (s). GCT: 从启动到采样时GC所用的总时间 (s). 查看CPU 内存使用情况 使用 top 命令，查看CPU 内存使用情况123456789101112[wkq@VM_77_25_centos ~]$ toptop - 16:02:17 up 69 days, 23:08, 1 user, load average: 0.03, 0.03, 0.05Tasks: 68 total, 2 running, 66 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.3 us, 0.3 sy, 0.0 ni, 99.0 id, 0.3 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 1883844 total, 75024 free, 430228 used, 1378592 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 1266684 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 3031 wkq 20 0 2692228 354060 4156 S 0.3 18.8 477:30.26 java 16414 root 20 0 612232 13740 2380 S 0.3 0.7 21:40.11 barad_agent 1 root 20 0 41020 2900 1816 S 0.0 0.2 4:49.45 systemd ...... load 值 load average 代表1分钟、5分钟、15分钟的系统平均负载，从这三个数字，可以判断系统负荷是大还是小。 当CPU完全空闲的时候，平均负荷为0；当CPU工作量饱和的时候，平均负荷为1。 因此 load average 这三个数值越低，代表系统负荷越小。 单核CPU load值0.7比较合理 8核CPU load值 7 - 8 比较合理 如果电脑里只有一个CPU，把CPU看成一条单行桥，桥上只有一个车道，所有的车都必须从这个桥上通过。那么 系统负荷为0，代表桥上一辆车也没有 系统负荷0.5，意味着桥上一半路段上有车 系统负荷1，意味着桥上道路已经被车占满 系统负荷1.7，代表着在桥上车子已经满了（100%），同时还有70%的车子在等待从桥上通过 获取占用CPU最高的线程id top -p 12309 -H -p用于指定进程，-H用于获取每个线程的信息。从top输出的内容，可以看到有四个线程占用了非常高的CPU： 从top输出的内容，可以看到有四个线程占用了非常高的CPU： jstack是java虚拟机自带的一种堆栈跟踪工具，用于打印出给定的java进程ID或core file或远程调试服务的Java堆栈信息。使用下面命令，将java进程的堆栈信息打印到文件中 jstack -l 12309 &gt; stack.log 是哪些对象占用了内存 用VisualVM来生成headdump文件。 在机器上使用jmap命令来生成head dump文件。 jmap -dump:live,format=b,file=headInfo.hprof 12309 live这个参数表示我们需要抓取的是目前在生命周期内的内存对象，也就是说GC收不走的对象，在这种场景下，我们需要的就是这些内存的信息。生成了hprof文件后，可以拉回到本地，使用VisualVM来打开它进行分析。 使用下面的命令，来查看占用内存最多的类型： jmap -histo 12309 &gt; heap.log 能否对堆内对象进行查询如果能够分析出相似度高的字符串，那么有比较大的可能是这些字符串存在泄漏，从而可以缩小问题代码的范围。确实是有这么一种工具来对堆内的对象进行分析，也就是OQL（Object Query Language）,在VisualVM中可以对headdump文件执行对象查询，下面是一个示例，查找包含内容最多的List： select map(top(heap.objects(&#39;java.util.ArrayList&#39;), &#39;rhs.size - lhs.size&#39;, 5),&quot;toHtml(it)+&#39;=&#39;+it.size&quot;) References[1] 从一次线上故障思考 Java 问题定位思路[2] 从一次线上故障思考 Java 问题定位思路[3] automated-heap-dump-analysis-finding [] 记一次Full GC频繁排查过程]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拼写检查 纠错 推荐]]></title>
    <url>%2F2018%2F09%2F09%2Fspelling-correction%2F</url>
    <content type="text"><![CDATA[思路：排列组合(不是穷举)出编辑距离为1的词语，假定用户输入是错误的，用贝叶斯公式计算编辑距离最小的那个词(假设是正确的词)的概率。 可以优化 References[1] 基于海量词库的单词拼写检查、推荐到底是咋做的？[2] 贝叶斯推断及其互联网应用（三）：拼写检查[3] 编辑距离 (Edit distance) [4] 编辑距离算法[5] NLP-最小编辑距离[6] 命令行和搜索引擎等的关键词错误纠正（Did you mean…？）是如何实现的？]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java异常-性能分析]]></title>
    <url>%2F2018%2F08%2F13%2Fjava-exception-performance%2F</url>
    <content type="text"><![CDATA[遇到一个问题，Java里处理异常会有多慢。然后就自测+Google 曾经在给一个业务系统增加限流功能，使用的限流组件在流量超过阈值时，会直接抛异常，异常导致 CPU 占用率飙升。第一次遇到这样的情况，让我们不得不思考，异常怎么会对性能造成这么大的影响？ 在消除了这些异常之后，代码运行速度与以前相比大幅提升。这让我们产生一种猜测，就是在代码里面使用异常会带来显著的性能开销。因为异常是错误情况处理的重要组成部分，摒弃是不太可能的,所以我们需要衡量异常处理对于性能影响，我们可以通过一个实验看看异常处理的对于性能的影响。 Java在处理异常时会 取操作栈中引用的异常对象 建立一个异常对象，是建立一个普通Object耗时的约20倍（实际上差距会比这个数字更大一些，因为循环也占用了时间，追求精确的读者可以再测一下空循环的耗时然后在对比前减掉这部分），而抛出、接住一个异常对象，所花费时间大约是建立异常对象的4倍。 下面的数据3个为一组，分别是 异常不作处理花费的时间，异常只抛出花费的时间，异常打印到日志花费的时间用时 0.062 s用时 0.094 s用时 0.782 s 用时 0.046 s用时 0.094 s用时 0.75 s 用时 0.047 s用时 0.078 s用时 0.673 s 可以看到 异常不做处理花费的时间最少，异常抛出花费的时间是不处理异常的2倍左右，把异常用slf4j+logback打印到日志的时间是不处理异常的10倍左右。 处理异常花费时间原因是 java处理异常时，会把栈中涉及到的所有堆栈信息取出。然后打印到日志，特别耗费 CPU 内存 IO (待详细测试)。 catch 中不做任何事情1234567891011121314151617public class ExceptionTest &#123; public static void main(String[] args) &#123; doExTest(); doExTest(); &#125; private static void doExTest() &#123; long start = System.nanoTime(); for (int i=0; i&lt;100000; ++i) &#123; try &#123; throw new RuntimeException("" + Math.random()); &#125; catch (Exception e) &#123; &#125; &#125; System.out.println("time: " + (System.nanoTime() - start)); &#125;&#125; 12time: 365218274time: 224583244 第一次 doExTest 只是起到预热的作用，我们以第二次 doExTest 的时间为准。10 万次请求，平均每次请求耗时 2245 纳秒，也就是 0.002 毫秒，速度还是很快的。 catch 中输出异常到日志1234567891011121314151617181920public class ExceptionTest &#123; private static final Logger logger = LoggerFactory.getLogger(ExceptionTest.class); public static void main(String[] args) &#123; doExTest(); doExTest(); &#125; private static void doExTest() &#123; long start = System.nanoTime(); for (int i=0; i&lt;100000; ++i) &#123; try &#123; throw new RuntimeException("" + Math.random()); &#125; catch (Exception e) &#123; logger.error("fuck", e); &#125; &#125; System.out.println("time: " + (System.nanoTime() - start)); &#125;&#125; 12time: 13454674590time: 9891780450 10 万次请求，平均每次请求耗时 98917 纳秒，大约 0.1 毫秒，比“不输出异常”的时候，慢了 50 倍。输出日志如此耗费性能，那么 logger.error 这一句做了什么事儿呢？ 根据过滤规则，判断是否要输出日志获取异常堆栈拼接日志字符串，输出日志到文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132import org.junit.Test;/** * @author: weikeqin.cn@gmail.com * @create: 2018-08-13 20:35 **/public class ExceptionTest &#123; public ExceptionTest() &#123; &#125; /** * @return * @throws Exception */ public boolean testEx1() throws Exception &#123; boolean ret = true; try &#123; ret = testEx2(); &#125; catch (Exception e) &#123; System.out.println(&quot;testEx1, catch exception&quot;); ret = false; throw e; &#125; finally &#123; System.out.println(&quot;testEx1, finally; return value=&quot; + ret); return ret; &#125; &#125; /** * @return * @throws Exception */ public boolean testEx2() throws Exception &#123; boolean ret = true; try &#123; ret = testEx3(); if (!ret) &#123; return false; &#125; System.out.println(&quot;testEx2, at the end of try&quot;); return ret; &#125; catch (Exception e) &#123; System.out.println(&quot;testEx2, catch exception&quot;); ret = false; throw e; &#125; finally &#123; System.out.println(&quot;testEx2, finally; return value=&quot; + ret); return ret; &#125; &#125; /** * @return * @throws Exception */ public boolean testEx3() throws Exception &#123; boolean ret = true; try &#123; int b = 12; int c; for (int i = 2; i &gt;= -2; i--) &#123; c = b / i; System.out.println(&quot;i=&quot; + i); &#125; return true; &#125; catch (Exception e) &#123; System.out.println(&quot;testEx3, catch exception&quot;); ret = false; throw e; &#125; finally &#123; System.out.println(&quot;testEx3, finally; return value=&quot; + ret); return ret; &#125; &#125; /** * 测试异常 */ @Test public void testException() &#123; try &#123; testEx1(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * @param i * @param level */ public void doTest(int i, int level) &#123; if(level &lt; maxLevel)&#123; try &#123; doTest(i, ++level); &#125; catch (Exception e) &#123; // e.printStackTrace(); throw new RuntimeException(&quot;UUUPS&quot;, e); &#125; &#125;else&#123; if(i &gt; 1)&#123; throw new RuntimeException(&quot;Ups&quot;.substring(0, 3)); &#125; &#125; &#125; public long maxLevel = 20; /** * @param args */ public static void main(String[] args) &#123; ExceptionTest test = new ExceptionTest(); long start = System.currentTimeMillis(); int count = 10000; for (int i = 0; i &lt; count; i++) &#123; try &#123; test.doTest(2, 0); &#125; catch (Exception e) &#123; //e.printStackTrace(); &#125; &#125; long diff = System.currentTimeMillis() - start; System.out.println(String.format(&quot;Average time for invocation: %s&quot;, (1.0 * diff / count))); &#125;&#125; 12345678910111213141516171819202122232425262728293031public class Test &#123; public static void main ( String[] args ) &#123; outerTry( args ); innerTry( args ); &#125; private static void outerTry ( String[] args ) &#123; try &#123; for ( String arg : args ) &#123; tryMethod(); &#125; &#125;catch ( Exception e )&#123; catchMethod(); &#125; &#125; private static void innerTry ( String[] args ) &#123; for ( String arg : args ) &#123; try&#123; tryMethod(); &#125;catch ( Exception e )&#123; catchMethod(); &#125; &#125; &#125; private static void tryMethod () &#123; &#125; private static void catchMethod () &#123; &#125;&#125; 更新中.. References[1] Java 进阶：异常影响性能吗？[2] 【译】JAVA 异常对于性能的影响[3] java 异常－性能及实现机制分析[4] 系统性能调优(6)—-Java异常处理性能优化[5] Java Exception性能问题]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM内存区域]]></title>
    <url>%2F2018%2F08%2F12%2Fjvm-memory-area%2F</url>
    <content type="text"><![CDATA[JVM的内存划分中，有部分区域是线程私有的，有部分是属于整个JVM进程；有些区域会抛出OOM异常，有些则不会，了解JVM的内存区域划分以及特征，是定位线上内存问题的基础。那么JVM内存区域是怎么划分的呢？ Java虚拟机运行时数据区 (仅参考模型) Java虚拟机运行时数据区-大概比例 Java虚拟机在执行Java程序的过程中会把它管理的内存划分为若干个不同的数据区域，这些区域都有各自的用途，以及创建和销毁的时间，有的随着虚拟机进程启动而存在，有的区域则依赖于用户线程的启动和结束而建立和销毁。 根据《Java 虚拟机规范》的规定，Java 虚拟机所管理的内存一共分为 Program Counter Register（程序计数器）、VM Stack（虚拟机栈）、Native Method Stack（本地方法栈）、Heap（堆）、Method Area（方法区）五个区域。 1. Java内存区域1.1 程序计数器(Program Counter Register) 首先是程序计数器(Program Counter Register)，简称PC，在JVM规范中，每个线程都有自己的程序计数器。 这是一块比较小的内存空间，存储当前线程正在执行的Java方法的JVM指令(或操作码)地址，即字节码的行号。如果当前方法是 native 方法，那么程序计数器的值为 undefined。 该内存区域是唯一一个在Java虚拟机规范中没有规定任何OOM情况的内存区域。 1.2 Java虚拟机栈(Java Virtal Machine Stack) 第二，Java虚拟机栈(Java Virtal Machine Stack)，同样也是属于线程私有区域，该区域存储着局部变量表，编译时期可知的各种基本类型数据、对象引用、方法出口等信息。 java栈总是和线程关联在一起，每当创建一个线程时，JVM就会为这个线程创建一个对应的java栈。 在这个java栈中又会包含多个栈帧，每运行一个方法就创建一个栈帧，用于存储局部变量表、操作栈、方法返回值等。每一个方法从调用直至执行完成的过程，就对应一个栈帧在java栈中入栈到出栈的过程。 每次方法调用时，一个新的栈帧创建并压栈到栈顶。当方法正常返回或抛出未捕获的异常时，栈帧就会出栈。除了栈帧的压栈和出栈，栈不能被直接操作。所以可以在堆上分配栈帧，并且不需要连续内存。 栈可以是动态分配也可以固定大小。 如果线程请求一个超过允许范围的空间，就会抛出一个StackOverflowError。 如果线程需要一个新的栈帧，但是没有足够的内存可以分配，就会抛出一个 OutOfMemoryError。 1.3 本地方法栈(Native Method Stack) 第三，本地方法栈(Native Method Stack)与虚拟机栈类似，本地方法栈是在调用本地方法时使用的栈，每个线程都有一个本地方法栈。 1.4 堆(Heap) 第四，堆(Heap),几乎所有创建的Java对象实例，都是被直接分配到堆上的。这块是GC的主要区域。 堆被所有的线程所共享，在堆上的区域，会被垃圾回收器做进一步划分，例如新生代、老年代的划分。 Java虚拟机在启动的时候，可以使用 Xms Xmx 之类的参数指定堆区域的大小。 1.5 方法区(Method Area) 第五，方法区(Method Area)。方法区与堆一样，也是所有的线程所共享。 存储被虚拟机加载的元(Meta)数据，包括类信息、构造函数、常量池、静态变量、即时编译器编译后的代码等数据。 这里需要注意的是运行时常量池也在方法区中。 根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。由于早期HotSpot JVM的实现，将CG分代收集拓展到了方法区，因此很多人会将方法区称为永久代。Oracle JDK8中已永久代移除永久代，同时增加了元数据区(Metaspace)。 虽然JVM规范把方法区描述为堆的一个逻辑部分，但它却有个别名non-heap(非堆)，所以大家不要搞混淆了。方法区还包含一个运行时常量池。 运行时常量池(Run-Time Constant Pool)，这是方法区的一部分，受到方法区内存的限制，当常量池无法再申请到内存时，会抛出OutOfMemoryError异常。 常量池主要存放两大类常量： (1)字面量(Literal)，如文本字符串、final常量值 (2)符号引用，存放了与编译相关的一些常量，因为Java不像C++那样有连接的过程，因此字段方法这些符号引用在运行期就需要进行转换，以便得到真正的内存入口地址。 class文件中的常量池，也称为静态常量池，JVM虚拟机完成类装载操作后，会把静态常量池加载到内存中，存放在运行时常量池。 1.6 直接内存(Direct Memory) 第七，直接内存(Direct Memory)，直接内存并不属于Java规范规定的属于Java虚拟机运行时数据区的一部分。 Java的NIO可以使用Native方法直接在java堆外分配内存，使用DirectByteBuffer对象作为这个堆外内存的引用。 1.7 Java SE 7 规范的典型的 JVM 核心内部组件 2. OOM可能发生在哪些区域上？ 内存区域 是否线程私有 是否可能发生OOM 程序计数器 是 否 虚拟机栈 是 是 本地方法栈 是 是 方法区 否 是 直接内存 否 是 堆 否 是 根据javadoc的描述，OOM是指JVM的内存不够用了，同时垃圾收集器也无法提供更多的内存。从描述中可以看出，在JVM抛出OutOfMemoryError之前，垃圾收集器一般会出马先尝试回收内存。 从上面分析的Java数据区来看，除了程序计数器不会发生OOM外，哪些区域会发生OOM的情况呢？ 2.1 堆内存 第一，堆内存。堆内存不足是最常见的发送OOM的原因之一，如果在堆中没有内存完成对象实例的分配，并且堆无法再扩展时，将抛出OutOfMemoryError异常，抛出的错误信息是“java.lang.OutOfMemoryError:Java heap space”。当前主流的JVM可以通过-Xmx和-Xms来控制堆内存的大小，发生堆上OOM的可能是存在内存泄露，也可能是堆大小分配不合理。 2.2 Java虚拟机栈和本地方法栈 第二，Java虚拟机栈和本地方法栈，这两个区域的区别不过是虚拟机栈为虚拟机执行Java方法服务，而本地方法栈则为虚拟机使用到的Native方法服务，在内存分配异常上是相同的。在JVM规范中，对Java虚拟机栈规定了两种异常：1.如果线程请求的栈大于所分配的栈大小，则抛出StackOverFlowError错误，比如进行了一个不会停止的递归调用；2. 如果虚拟机栈是可以动态拓展的，拓展时无法申请到足够的内存，则抛出OutOfMemoryError错误。 2.3 直接内存 第三，直接内存。直接内存虽然不是虚拟机运行时数据区的一部分，但既然是内存，就会受到物理内存的限制。在JDK1.4中引入的NIO使用Native函数库在堆外内存上直接分配内存，但直接内存不足时，也会导致OOM。 2.4 方法区 第四，方法区。随着Metaspace元数据区的引入，方法区的OOM错误信息也变成了“java.lang.OutOfMemoryError:Metaspace”。对于旧版本的Oracle JDK，由于永久代的大小有限，而JVM对永久代的垃圾回收并不积极，如果往永久代不断写入数据，例如String.Intern()的调用，在永久代占用太多空间导致内存不足，也会出现OOM的问题，对应的错误信息为“java.lang.OutOfMemoryError:PermGen space” 3. 堆内存结构是怎么样的？ 站在垃圾收集器的角度来看，可以把内存分为新生代与老年代。内存的分配规则取决于当前使用的是哪种垃圾收集器的组合，以及内存相关的参数配置。 往大的方向说，对象优先分配在新生代的Eden区域，而大对象直接进入老年代。 可以借助一些工具来了解JVM的内存内容，具体到特定的内存区域，应该用什么工具去定位呢？ 图形化工具。图形化工具的优点是直观，连接到Java进程后，可以显示堆内存、堆外内存的使用情况，类似的工具有JConsole,jvisualvm 等。 命令行工具。这类工具可以在运行时进行查询，包括jstat，jmap等，可以对堆内存、方法区等进行查看。定位线上问题时也多会使用这些工具。jmap也可以生成堆转储文件(Heap Dump)文件，如果是在linux上，可以将堆转储文件拉到本地来，使用Eclipse MAT进行分析，也可以使用jhap进行分析。 3.1 新生代(Young Generation)的Eden区域 第一, 新生代的Eden区域，对象优先分配在该区域，同时JVM可以为每个线程分配一个私有的缓存区域，称为TLAB(Thread Local Allocation Buffer)，避免多线程同时分配内存时需要使用加锁等机制而影响分配速度。TLAB在堆上分配，位于Eden中。TLAB的结构如下： 1234567891011121314// ThreadLocalAllocBuffer: a descriptor for thread-local storage used by// the threads for allocation.// It is thread-private at any time, but maybe multiplexed over// time across multiple threads. The park()/unpark() pair is// used to make it avaiable for such multiplexing.class ThreadLocalAllocBuffer: public CHeapObj&lt;mtThread&gt; &#123; friend class VMStructs;private: HeapWord* _start; // address of TLAB HeapWord* _top; // address after last allocation HeapWord* _pf_top; // allocation prefetch watermark HeapWord* _end; // allocation end (excluding alignment_reserve) size_t _desired_size; // desired size (including alignment_reserve) size_t _refill_waste_limit; // hold onto tlab if free() is larger than this 从本质上来说，TLAB的管理是依靠三个指针：start、end、top。start与end标记了Eden中被该TLAB管理的区域，该区域不会被其他线程分配内存所使用，top是分配指针，开始时指向start的位置，随着内存分配的进行，慢慢向end靠近，当撞上end时触发TLAB refill。 因此内存中Eden的结构大体为： 3.2 新生代(Young Generation)的Survivor区域 第二、新生代的Survivor区域。当Eden区域内存不足时会触发Minor GC，也称为新生代GC，在Minor GC存活下来的对象，会被复制到Survivor区域中。 Survivor区的作用在于避免过早触发Full GC。如果没有Survivor，Eden区每进行一次Minor GC都把对象直接送到老年代，老年代很快便会内存不足引发Full GC。 新生代中有两个Survivor区，两个Survivor的作用在于提高性能，避免内存碎片的出现。在任何时候，总有一个Survivor是empty的，在发生Minor GC时，会将Eden及另一个的Survivor的存活对象拷贝到该empty Survivor中，从而避免内存碎片的产生。 新生代的内存结构大体为： 3.3 老年代 (Old Generation) 第三、老年代。老年代放置长生命周期的对象，通常是从Survivor区域拷贝过来的对象，不过当对象过大的时候，无法在新生代中用连续内存的存放，那么这个大对象就会被直接分配在老年代上。一般来说，普通的对象都是分配在TLAB上，较大的对象，直接分配在Eden区上的其他内存区域，而过大的对象，直接分配在老年代上。 3.4 永久代 第四、永久代。如前面所说，在早起的Hotspot JVM中有老年代的概念，老年代用于存储Java类的元数据、常量池、Intern字符串等。 在JDK8之后，就将老年代移除，而引入元数据区的概念。 3.5 Vritual空间 第五、Vritual空间。前面说过，可以使用Xms与Xmx来指定堆的最小与最大空间。如果Xms小于Xmx，堆的大小不会直接扩展到上限，而是留着一部分等待内存需求不断增长时，再分配给新生代。Vritual空间便是这部分保留的内存区域。 3.6 总结 综上所述，可以画出Java堆内的内存结构大体为： 4. 常用配置-Xms1024m 设置初始的最小堆大小1024m -Xmx4096m 设置最大的堆大小为4096m -Xmn1536m 设置新生代大小为1536m。 持久代一般固定大小为64m，所以增大新生代后，将会减小年老代大小。 此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。 -Xss128k 设置每个线程的堆栈大小。 JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。更具应用的线程所需内存大小进行调整。 在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。 -XX:NewSize=1024 设置 新生代 Yong Generation的初始值大小 -XX:MaxNewSize=1024m 设置 新生代 Yong Generation的最大值大小 -XX:NewRatio=4 设置新生代(包括Eden和两个Survivor区)与年老代的比值(除去持久代)。默认为2，设置为4，则新生代与年老代所占比值为1：4，新生代占整个堆栈的1/5 老年代过大的时候，Full GC的时间会很长；老年代过小，则很容易触发Full GC，Full GC频率过高，这就是这个参数会造成的影响。 -XX:SurvivorRatio=4 设置新生代中Eden区与Survivor区的大小比值。 设置为4，则两个Survivor区与一个Eden区的比值为2:4，一个Survivor区占整个新生代的1/6 -XX:MaxPermSize=16m 设置持久代大小为16m。 -XX:MaxTenuringThreshold=0 设置垃圾最大年龄。 如果设置为0的话，则新生代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则新生代对象会在Survivor区进行多次复制，这样可以增加对象再新生代的存活时间，增加在新生代即被回收的概论。 4.1 JVM设置Young Gen参数优先级 JVM设置Young Gen参数优先级 (1) 最高优先级： -XX:NewSize=1024m和-XX:MaxNewSize=1024m (2) 次高优先级： -Xmn1024m (默认等效效果是：-XX:NewSize==-XX:MaxNewSize==1024m) (3) 最低优先级：-XX:NewRatio=2 5. 常见配置汇总5.1 堆设置123456-Xms:初始堆大小-Xmx:最大堆大小-XX:NewSize=n:设置年轻代大小-XX:NewRatio=n:设置年轻代和年老代的比值。如:为3，表示年轻代与年老代比值为1：3，年轻代占整个年轻代年老代和的1/4-XX:SurvivorRatio=n:年轻代中Eden区与两个Survivor区的比值。注意Survivor区有两个。如：3，表示Eden：Survivor=3：2，一个Survivor区占整个年轻代的1/5-XX:MaxPermSize=n:设置持久代大小 5.2 收集器设置1234-XX:+UseSerialGC:设置串行收集器-XX:+UseParallelGC:设置并行收集器-XX:+UseParalledlOldGC:设置并行年老代收集器-XX:+UseConcMarkSweepGC:设置并发收集器 5.3 垃圾回收统计信息1234-XX:+PrintGC-XX:+PrintGCDetails-XX:+PrintGCTimeStamps-Xloggc:filename 5.4 并行收集器设置123-XX:ParallelGCThreads=n:设置并行收集器收集时使用的CPU数。并行收集线程数。-XX:MaxGCPauseMillis=n:设置并行收集最大暂停时间-XX:GCTimeRatio=n:设置垃圾回收时间占程序运行时间的百分比。公式为1/(1+n) 5.5 并发收集器设置12-XX:+CMSIncrementalMode:设置为增量模式。适用于单CPU情况。-XX:ParallelGCThreads=n:设置并发收集器年轻代收集方式为并行收集时，使用的CPU数。并行收集线程数 6. 调优总结6.1 年轻代大小选择 响应时间优先的应用：尽可能设大，直到接近系统的最低响应时间限制(根据实际情况选择)。在此种情况下，年轻代收集发生的频率也是最小的。同时，减少到达年老代的对象。 吞吐量优先的应用：尽可能的设置大，可能到达Gbit的程度。因为对响应时间没有要求，垃圾收集可以并行进行，一般适合8CPU以上的应用。 6.2 年老代大小选择 响应时间优先的应用：年老代使用并发收集器，所以其大小需要小心设置，一般要考虑并发会话率和会话持续时间等一些参数。如果堆设置小了，可以会造成内存碎片、高回收频率以及应用暂停而使用传统的标记清除方式；如果堆大了，则需要较长的收集时间。最优化的方案，一般需要参考以下数据获得： 并发垃圾收集信息 持久代并发收集次数 传统GC信息 花在年轻代和年老代回收上的时间比例 减少年轻代和年老代花费的时间，一般会提高应用的效率 吞吐量优先的应用：一般吞吐量优先的应用都有一个很大的年轻代和一个较小的年老代。原因是，这样可以尽可能回收掉大部分短期对象，减少中期的对象，而年老代尽存放长期存活对象。 6.3 较小堆引起的碎片问题 因为年老代的并发收集器使用标记、清除算法，所以不会对堆进行压缩。当收集器回收时，他会把相邻的空间进行合并，这样可以分配给较大的对象。但是，当堆空间较小时，运行一段时间以后，就会出现“碎片”，如果并发收集器找不到足够的空间，那么并发收集器将会停止，然后使用传统的标记、清除方式进行回收。如果出现“碎片”，可能需要进行如下配置：12-XX:+UseCMSCompactAtFullCollection：使用并发收集器时，开启对年老代的压缩。-XX:CMSFullGCsBeforeCompaction=0：上面配置开启的情况下，这里设置多少次Full GC后，对年老代进行压缩 References[1] 关于JVM内存的N个问题[2] JVM内幕：Java虚拟机详解[3] JVM结构、GC工作机制详解[4] Java虚拟机详解—-JVM常见问题总结[5] Java中JVM原理详解[6] JVM 的 工作原理，层次结构 以及 GC工作原理[7] JVM理解其实并不难！[8] 全面理解Java内存模型[9] 全面理解Java内存模型(JMM)及volatile关键字[10] JVM内存模型[11] JVMM调优总结 -Xms -Xmx -Xmn -Xss[12] JVM设置Young Gen大小]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OrientDB 下载 安装]]></title>
    <url>%2F2018%2F08%2F11%2Forientdb-download-install%2F</url>
    <content type="text"><![CDATA[下载解压orientdb压缩包 通过 server.sh 或者 server.bat 启动orieintdb 访问 http://localhost:2480 查看orientdb 1 下载解压 orientdb 所有版本 https://orientdb.com/releases/ https://orientdb.com/downloads/ https://orientdb.com/download-2/ https://orientdb.org/download orientdb-3.0.8.zip https://orientdb3.s3.us-east-2.amazonaws.com/releases/3.0.8/orientdb-3.0.8.zip orientdb-3.0.8.tar.gz https://orientdb3.s3.us-east-2.amazonaws.com/releases/3.0.8/orientdb-3.0.8.tar.gz agent-3.0.8.jar https://orientdb3.s3.us-east-2.amazonaws.com/releases/3.0.8/agent-3.0.8.jar 无论是zip包还是tar.gz包，解压完进入bin目录，执行 ./server.sh (linux) 或 server.bat(windows) 即可运行OrientDB数据库。 第一次使用需要设置密码，设置完密码启动后在浏览器输入 http://127.0.0.1:2480 访问 图形化界面。 readme.md 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465 . .` ` , `:. `,` ,:` .,. :,, .,, ,,, . .,.::::: ```` ::::::::: ::::::::: ,` .::,,,,::.,,,,,,`;; .: :::::::::: ::: ::: `,. ::,,,,,,,:.,,.` ` .: ::: ::: ::: ::: ,,:,:,,,,,,,,::. ` ` `` .: ::: ::: ::: ::: ,,:.,,,,,,,,,: `::, ,, ::,::` : :,::` :::: ::: ::: ::: ::: ,:,,,,,,,,,,::,: ,, :. : :: : .: ::: ::: ::::::: :,,,,,,,,,,:,:: ,, : : : : .: ::: ::: ::::::::: ` :,,,,,,,,,,:,::, ,, .:::::::: : : .: ::: ::: ::: ::: `,...,,:,,,,,,,,,: .:,. ,, ,, : : .: ::: ::: ::: ::: .,,,,::,,,,,,,: `: , ,, : ` : : .: ::: ::: ::: ::: ...,::,,,,::.. `: .,, :, : : : .: ::::::::::: ::: ::: ,::::,,,. `: ,, ::::: : : .: ::::::::: :::::::::: ,,:` `,,. ,,, .,` MULTI MODEL ,,. `, GRAPH DOCUMENT DATABASE `` `. `` COMMUNITY EDITION ` www.orientdb.org********************************************************************************* QUICK START---------------1) Start the server by executing “server.sh” (or “server.bat” if you’re using Windows) under the “bin” directory. On most of OS you can just double click on it.2) Type in the terminal the password you want to assign to the “root” user. This is needed only the first time.2) To Open Studio Web Tool, open a browser and point it to the URL: http://localhost:24803) You can also use the console, it’s in “bin” directory. Launch “console.sh” (or “console.bat” if you’re using Windows) REQUIREMENTS---------------Before to download, compile and install the last version of OrientDB pleaseassure to have Java installed. OrientDB needs Oracle Java JDK version 8 or majorto run the Server. JDK different by Oracle, like OpenJDK, could not work on someconfiguration. We noticed also Oracle JDK is generally faster than OpenJDK.To download Java go to: http://www.java.com/en/download/Note: Please assure to download the JDK and not JRE. INFORMATION---------------For more information visit the official website: http://www.orientdb.org.Remember OrientDB is an Open Source project released with the Apache v2 license,so it&apos;s always FREE for any purpose. If you&apos;re interested to Enterprise tools,professional support, training or consultancy contact: info@orientdb.com.Enjoy the Graphs,OrientDB(www.orientdb.com) 123456789101112131415161718192021222324252627D:\ProfessionalSoftWare\orientdb\orientdb-3.0.4\binλ dir 驱动器 D 中的卷是 disk-D 卷的序列号是 B0F3-626E D:\ProfessionalSoftWare\orientdb\orientdb-3.0.4\bin 的目录2018-07-12 10:09 &lt;DIR&gt; .2018-07-12 10:09 &lt;DIR&gt; ..2018-07-12 09:33 4,377 backup.sh2018-07-12 09:33 1,757 console.bat2018-07-12 09:33 1,733 console.sh2018-07-12 09:33 3,670 dserver.bat2018-07-12 09:33 4,561 dserver.sh2018-07-12 09:33 1,720 oetl.bat2018-07-12 09:33 2,083 oetl.sh2018-07-12 09:33 268 orientdb.service2018-07-12 09:33 1,770 orientdb.sh2018-07-12 09:33 394 orientdb.upstart2018-07-12 09:38 1,641 oteleporter.bat2018-07-12 09:38 1,475 oteleporter.sh2018-07-12 09:33 4,102 server.bat2018-07-12 09:33 4,634 server.sh2018-07-12 09:33 1,416 shutdown.bat2018-07-12 09:33 1,886 shutdown.sh 16 个文件 37,487 字节 2 个目录 978,574,606,336 可用字节 (2) 启动123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990D:\ProfessionalSoftWare\orientdb\orientdb-3.0.4\binλ server . .` ` , `:. `,` ,:` .,. :,, .,, ,,, . .,.::::: ```` ::::::::: ::::::::: ,` .::,,,,::.,,,,,,`;; .: :::::::::: ::: ::: `,. ::,,,,,,,:.,,.` ` .: ::: ::: ::: ::: ,,:,:,,,,,,,,::. ` ` `` .: ::: ::: ::: ::: ,,:.,,,,,,,,,: `::, ,, ::,::` : :,::` :::: ::: ::: ::: ::: ,:,,,,,,,,,,::,: ,, :. : :: : .: ::: ::: ::::::: :,,,,,,,,,,:,:: ,, : : : : .: ::: ::: ::::::::: ` :,,,,,,,,,,:,::, ,, .:::::::: : : .: ::: ::: ::: ::: `,...,,:,,,,,,,,,: .:,. ,, ,, : : .: ::: ::: ::: ::: .,,,,::,,,,,,,: `: , ,, : ` : : .: ::: ::: ::: ::: ...,::,,,,::.. `: .,, :, : : : .: ::::::::::: ::: ::: ,::::,,,. `: ,, ::::: : : .: ::::::::: :::::::::: ,,:` `,,. ,,, .,` ,,. `, VELOCE `` `. `` www.orientdb.com `2018-08-11 12:55:01:547 INFO Windows OS is detected, 262144 limit of open files will be set for the disk cache. [ONative]2018-08-11 12:55:01:598 INFO Loading configuration from: D:/ProfessionalSoftWare/orientdb/orientdb-3.0.4/config/orientdb-server-config.xml... [OServerConfigurationLoaderXml]2018-08-11 12:55:01:940 INFO OrientDB Server v3.0.4 - Veloce (build 4578b51f72a55feaa0852bc8ddd52929011d956c, branch 3.0.x) is starting up... [OServer]2018-08-11 12:55:01:977 INFO 8194301952 B/7814 MB/7 GB of physical memory were detected on machine [ONative]2018-08-11 12:55:01:978 INFO Detected memory limit for current process is 8194301952 B/7814 MB/7 GB [ONative]2018-08-11 12:55:01:981 INFO JVM can use maximum 1963MB of heap memory [OMemoryAndLocalPaginatedEnginesInitializer]2018-08-11 12:55:01:983 INFO Because OrientDB is running outside a container 2g of memory will be left unallocated according to the setting &apos;memory.leftToOS&apos; not taking into account heap memory [OMemoryAndLocalPaginatedEnginesInitializer]2018-08-11 12:55:01:987 INFO OrientDB auto-config DISKCACHE=3,803MB (heap=1,963MB os=7,814MB) [orientechnologies]2018-08-11 12:55:02:011 INFO Databases directory: D:\ProfessionalSoftWare\orientdb\orientdb-3.0.4\databases [OServer]2018-08-11 12:55:02:076 INFO Creating the system database &apos;OSystem&apos; for current server [OSystemDatabase]2018-08-11 12:55:03:952 INFO Storage &apos;plocal:D:\ProfessionalSoftWare\orientdb\orientdb-3.0.4\databases/OSystem&apos; is created under OrientDB distribution : 3.0.4 - Veloce (build 4578b51f72a55feaa0852bc8ddd52929011d956c, branch 3.0.x) [OLocalPaginatedStorage]2018-08-11 12:55:12:054 INFO Listening binary connections on 0.0.0.0:2424 (protocol v.37, socket=default) [OServerNetworkListener]2018-08-11 12:55:12:057 INFO Listening http connections on 0.0.0.0:2480 (protocol v.10, socket=default) [OServerNetworkListener]+---------------------------------------------------------------+| WARNING: FIRST RUN CONFIGURATION |+---------------------------------------------------------------+| This is the first time the server is running. Please type a || password of your choice for the &apos;root&apos; user or leave it blank || to auto-generate it. || || To avoid this message set the environment variable or JVM || setting ORIENTDB_ROOT_PASSWORD to the root password to use. |+---------------------------------------------------------------+Root password [BLANK=auto generate it]: *****Please confirm the root password: *****2018-08-11 12:55:27:130 INFO Installing dynamic plugin &apos;orientdb-etl-3.0.4.jar&apos;... [OServerPluginManager]2018-08-11 12:55:27:142 INFO Installing dynamic plugin &apos;orientdb-neo4j-importer-plugin-3.0.4-dist.jar&apos;... [OServerPluginManager]2018-08-11 12:55:27:165 INFO Installing dynamic plugin &apos;orientdb-studio-3.0.4.zip&apos;... [OServerPluginManager]2018-08-11 12:55:27:185 INFO Installing dynamic plugin &apos;orientdb-teleporter-3.0.4.jar&apos;... [OServerPluginManager]2018-08-11 12:55:27:211 INFO ODefaultPasswordAuthenticator is active [ODefaultPasswordAuthenticator]2018-08-11 12:55:27:217 INFO OServerConfigAuthenticator is active [OServerConfigAuthenticator]2018-08-11 12:55:27:219 INFO OSystemUserAuthenticator is active [OSystemUserAuthenticator]2018-08-11 12:55:27:221 INFO [OVariableParser.resolveVariables] Property not found: distributed [orientechnologies]2018-08-11 12:55:27:249 WARNI Authenticated clients can execute any kind of code into the server by using the following allowed languages: [sql] [OServerSideScriptInterpreter]2018-08-11 12:55:27:397 INFO OrientDB Studio available at http://10.0.8.244:2480/studio/index.html [OServer]2018-08-11 12:55:27:398 INFO OrientDB Server is active v3.0.4 - Veloce (build 4578b51f72a55feaa0852bc8ddd52929011d956c, branch 3.0.x). [OServer]2018-08-11 13:00:18:547 INFO Storage &apos;plocal:D:\ProfessionalSoftWare\orientdb\orientdb-3.0.4\databases/demodb&apos; is opened under OrientDB distribution : 3.0.4 - Veloce (build 4578b51f72a55feaa0852bc8ddd52929011d956c, branch 3.0.x) [OLocalPaginatedStorage]2018-08-11 13:40:47:544 INFO Loading configuration from: D:/ProfessionalSoftWare/orientdb/orientdb-3.0.4/config/orientdb-server-config.xml... [OServerConfigurationLoaderXml]2018-08-11 21:13:28:970 WARNI Received signal: SIGINT [OSignalHandler]2018-08-11 21:13:28:978 INFO OrientDB Server is shutting down... [OServer]2018-08-11 21:13:28:981 INFO Shutting down listeners: [OServer]2018-08-11 21:13:28:983 INFO - ONetworkProtocolBinary /0.0.0.0:2424: [OServer]2018-08-11 21:13:28:985 INFO - ONetworkProtocolHttpDb /0.0.0.0:2480: [OServer]2018-08-11 21:13:28:986 INFO Shutting down protocols [OServer]2018-08-11 21:13:28:988 INFO Shutting down plugins: [OServerPluginManager]2018-08-11 21:13:28:989 INFO - studio [OServerPluginManager]2018-08-11 21:13:28:989 INFO - teleporter [OServerPluginManager]2018-08-11 21:13:28:991 INFO - custom-sql-functions-manager [OServerPluginManager]2018-08-11 21:13:28:991 INFO - script-interpreter [OServerPluginManager]2018-08-11 21:13:28:991 INFO - etl [OServerPluginManager]2018-08-11 21:13:28:993 INFO - neo4j-importer [OServerPluginManager]2018-08-11 21:13:28:995 INFO Shutting down databases: [OServer]2018-08-11 21:13:28:998 INFO Orient Engine is shutting down... [Orient]2018-08-11 21:13:29:008 INFO - shutdown storage: OSystem... [OrientDBDistributed]2018-08-11 21:13:29:217 WARNI Received signal: SIGINT [OSignalHandler]2018-08-11 21:13:29:598 INFO - shutdown storage: demodb... [OrientDBDistributed]2018-08-11 21:13:30:082 INFO Clearing byte buffer pool [Orient]2018-08-11 21:13:30:098 INFO OrientDB Engine shutdown complete [Orient]2018-08-11 21:13:30:099 INFO OrientDB Server shutdown complete [OServer]终止批处理操作吗(Y/N)? y 安装完成 (3) 配置配置内存 修改 ${ORIENTDB_HOME}/bin/server.sh 搜 ORIENTDB_OPTS_MEMORY ，大概在102行， 修改 -Xms2G -Xmx2G 为 -Xms16G -Xmx16G References[1] orientdb[2] 如何在CentOS 7中安装和配置OrientDB社区版]]></content>
      <categories>
        <category>orientdb</category>
      </categories>
      <tags>
        <tag>orientdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elasticsearch 集群]]></title>
    <url>%2F2018%2F08%2F03%2Felasticsearch-cluster%2F</url>
    <content type="text"><![CDATA[集群内的原理 介绍 cluster 、 node 、 shard 等常用术语，Elastisearch 的扩容机制， 以及如何处理硬件故障的内容。 ElasticSearch 的主旨是随时可用和按需扩容。 而扩容可以通过购买性能更强大（ 垂直扩容 ，或 纵向扩容 ） 或者数量更多的服务器（ 水平扩容 ，或 横向扩容 ）来实现。 虽然 Elasticsearch 可以获益于更强大的硬件设备，但是垂直扩容是有极限的。 真正的扩容能力是来自于水平扩容–为集群添加更多的节点，并且将负载压力和稳定性分散到这些节点中。 (1) 空集群 一个运行中的 Elasticsearch 实例称为一个 节点，而集群是由一个或者多个拥有相同 cluster.name 配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。 (2) 集群健康 查看集群健康状态 http://localhost:9200/_cluster/health?pretty 123456789101112131415161718$ curl -X GET &quot;localhost:9200/_cluster/health?pretty&quot;&#123; &quot;cluster_name&quot; : &quot;my-application&quot;, &quot;status&quot; : &quot;yellow&quot;, &quot;timed_out&quot; : false, &quot;number_of_nodes&quot; : 1, &quot;number_of_data_nodes&quot; : 1, &quot;active_primary_shards&quot; : 5, &quot;active_shards&quot; : 5, &quot;relocating_shards&quot; : 0, &quot;initializing_shards&quot; : 0, &quot;unassigned_shards&quot; : 5, &quot;delayed_unassigned_shards&quot; : 0, &quot;number_of_pending_tasks&quot; : 0, &quot;number_of_in_flight_fetch&quot; : 0, &quot;task_max_waiting_in_queue_millis&quot; : 0, &quot;active_shards_percent_as_number&quot; : 50.0&#125; status 字段指示着当前集群在总体上是否工作正常。它的三种颜色含义如下： green 所有的主分片和副本分片都正常运行。 yellow 所有的主分片都正常运行，但不是所有的副本分片都正常运行。 red 有主分片没能正常运行。 (3) 添加索引 我们往 Elasticsearch 添加数据时需要用到 索引 —— 保存相关数据的地方。 索引实际上是指向一个或者多个物理 分片 的 逻辑命名空间 。 一个 分片 是一个底层的 工作单元 ，它仅保存了全部数据中的一部分。 在分片内部机制中，我们将详细介绍分片是如何工作的，而现在我们只需知道一个分片是一个 Lucene 的实例，以及它本身就是一个完整的搜索引擎。 我们的文档被存储和索引到分片内，但是应用程序是直接与索引而不是与分片进行交互。 Elasticsearch 是利用分片将数据分发到集群内各处的。分片是数据的容器，文档保存在分片内，分片又被分配到集群内的各个节点里。 当你的集群规模扩大或者缩小时， Elasticsearch 会自动的在各节点中迁移分片，使得数据仍然均匀分布在集群里。 一个分片可以是 主 分片或者 副本 分片。 索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量。 技术上来说，一个主分片最大能够存储 Integer.MAX_VALUE - 128 个文档，但是实际最大值还需要参考你的使用场景：包括你使用的硬件， 文档的大小和复杂程度，索引和查询文档的方式以及你期望的响应时长。 一个副本分片只是一个主分片的拷贝。 副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。 在索引建立的时候就已经确定了主分片数，但是副本分片数可以随时修改。 索引在默认情况下会被分配5个主分片 创建一个blogs索引，设置3个主分片和一份副本(每个主分片拥有一个副本分片) 设置分片数和副本数 12345678910111213$ curl -X PUT &quot;localhost:9200/blogs?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&gt; &#123;&gt; &quot;settings&quot; : &#123;&gt; &quot;number_of_shards&quot; : 3,&gt; &quot;number_of_replicas&quot; : 1&gt; &#125;&gt; &#125;&gt; &apos;&#123; &quot;acknowledged&quot; : true, &quot;shards_acknowledged&quot; : true, &quot;index&quot; : &quot;blogs&quot;&#125; 12345678910WKQ@WKQ-PC MINGW64 /c/ProfessionalSoftWare/ElasticSearch/elasticsearch-5.6.10$ curl -X PUT &quot;localhost:9200/blogs&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&gt; &#123;&gt; &quot;settings&quot; : &#123;&gt; &quot;number_of_shards&quot; : 3,&gt; &quot;number_of_replicas&quot; : 1&gt; &#125;&gt; &#125;&gt; &apos;&#123;&quot;acknowledged&quot;:true,&quot;shards_acknowledged&quot;:true,&quot;index&quot;:&quot;blogs&quot;&#125; (4) 添加故障转移 当集群中只有一个节点在运行时，意味着会有一个单点故障问题——没有冗余。 幸运的是，我们只需再启动一个节点即可防止数据丢失。 为了测试第二个节点启动后的情况，你可以在同一个目录内，完全依照启动第一个节点的方式来启动一个新节点。多个节点可以共享同一个目录。 当你在同一台机器上启动了第二个节点时，只要它和第一个节点有同样的 cluster.name 配置，它就会自动发现集群并加入到其中。 但是在不同机器上启动节点的时候，为了加入到同一集群，你需要配置一个可连接到的单播主机列表。 (4.1) 自己搭建一个3个节点的集群 elasticsearch-5.6.10解压三份，然后把3个的配置文件里 cluster.name 全改成统一的一个名字，这儿我写的是 elasticsearch_production ，然后分别启动3个ES 1234567891011121314151617181920212223242526272829303132λ elasticsearch[2018-07-22T17:38:20,925][INFO ][o.e.n.Node ] [elasticsearch_node_001] initializing ...[2018-07-22T17:38:21,034][INFO ][o.e.e.NodeEnvironment ] [elasticsearch_node_001] using [1] data paths, mounts [[disk-C (C:)]], net usable_space [81.2gb], net total_space [118.7gb], spins? [unknown], types [NTFS][2018-07-22T17:38:21,034][INFO ][o.e.e.NodeEnvironment ] [elasticsearch_node_001] heap size [1.9gb], compressed ordinary object pointers [true][2018-07-22T17:38:21,164][INFO ][o.e.n.Node ] [elasticsearch_node_001] node name [elasticsearch_node_001], node ID [gjy4N2RCQ4mLxp1lxdyZ8w][2018-07-22T17:38:21,164][INFO ][o.e.n.Node ] [elasticsearch_node_001] version[5.6.10], pid[9412], build[b727a60/2018-06-06T15:48:34.860Z], OS[Windows 10/10.0/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_131/25.131-b11][2018-07-22T17:38:21,164][INFO ][o.e.n.Node ] [elasticsearch_node_001] JVM arguments [-Xms2g, -Xmx2g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Djdk.io.permissionsUseCanonicalPath=true, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j.skipJansi=true, -XX:+HeapDumpOnOutOfMemoryError, -Delasticsearch, -Des.path.home=C:\ProfessionalSoftWare\ElasticSearch\elasticsearch-5.6.10][2018-07-22T17:38:22,254][INFO ][o.e.p.PluginsService ] [elasticsearch_node_001] loaded module [aggs-matrix-stats][2018-07-22T17:38:22,256][INFO ][o.e.p.PluginsService ] [elasticsearch_node_001] loaded module [ingest-common][2018-07-22T17:38:22,257][INFO ][o.e.p.PluginsService ] [elasticsearch_node_001] loaded module [lang-expression][2018-07-22T17:38:22,258][INFO ][o.e.p.PluginsService ] [elasticsearch_node_001] loaded module [lang-groovy][2018-07-22T17:38:22,258][INFO ][o.e.p.PluginsService ] [elasticsearch_node_001] loaded module [lang-mustache][2018-07-22T17:38:22,258][INFO ][o.e.p.PluginsService ] [elasticsearch_node_001] loaded module [lang-painless][2018-07-22T17:38:22,259][INFO ][o.e.p.PluginsService ] [elasticsearch_node_001] loaded module [parent-join][2018-07-22T17:38:22,259][INFO ][o.e.p.PluginsService ] [elasticsearch_node_001] loaded module [percolator][2018-07-22T17:38:22,259][INFO ][o.e.p.PluginsService ] [elasticsearch_node_001] loaded module [reindex][2018-07-22T17:38:22,260][INFO ][o.e.p.PluginsService ] [elasticsearch_node_001] loaded module [transport-netty3][2018-07-22T17:38:22,260][INFO ][o.e.p.PluginsService ] [elasticsearch_node_001] loaded module [transport-netty4][2018-07-22T17:38:22,261][INFO ][o.e.p.PluginsService ] [elasticsearch_node_001] no plugins loaded[2018-07-22T17:38:24,294][INFO ][o.e.d.DiscoveryModule ] [elasticsearch_node_001] using discovery type [zen][2018-07-22T17:38:24,844][INFO ][o.e.n.Node ] [elasticsearch_node_001] initialized[2018-07-22T17:38:24,844][INFO ][o.e.n.Node ] [elasticsearch_node_001] starting ...[2018-07-22T17:38:25,488][INFO ][o.e.t.TransportService ] [elasticsearch_node_001] publish_address &#123;127.0.0.1:9300&#125;, bound_addresses &#123;127.0.0.1:9300&#125;, &#123;[::1]:9300&#125;[2018-07-22T17:38:28,621][INFO ][o.e.c.s.ClusterService ] [elasticsearch_node_001] new_master &#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;, reason: zen-disco-elected-as-master ([0] nodes joined)[, ][2018-07-22T17:38:28,887][INFO ][o.e.g.GatewayService ] [elasticsearch_node_001] recovered [2] indices into cluster_state[2018-07-22T17:38:29,246][INFO ][o.e.c.r.a.AllocationService] [elasticsearch_node_001] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[megacorp][4], [megacorp][3], [megacorp][0]] ...]).[2018-07-22T17:38:29,360][INFO ][o.e.h.n.Netty4HttpServerTransport] [elasticsearch_node_001] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;127.0.0.1:9200&#125;, &#123;[::1]:9200&#125;[2018-07-22T17:38:29,360][INFO ][o.e.n.Node ] [elasticsearch_node_001] started[2018-07-22T17:38:56,302][INFO ][o.e.c.s.ClusterService ] [elasticsearch_node_001] added &#123;&#123;elasticsearch_node_002&#125;&#123;3ezLh-3jTB-c0pQXFXnr5Q&#125;&#123;Y4i8X_t7QhWDBoP6MoDuvw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9301&#125;,&#125;, reason: zen-disco-node-join[2018-07-22T17:38:56,505][WARN ][o.e.d.z.ElectMasterService] [elasticsearch_node_001] value for setting &quot;discovery.zen.minimum_master_nodes&quot; is too low. This can result in data loss! Please set it to at least a quorum of master-eligible nodes (current value: [-1], total number of master-eligible nodes used for publishing in this round: [2])[2018-07-22T17:38:58,989][INFO ][o.e.c.r.a.AllocationService] [elasticsearch_node_001] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[megacorp][4]] ...]).[2018-07-22T17:39:43,600][INFO ][o.e.c.s.ClusterService ] [elasticsearch_node_001] added &#123;&#123;elasticsearch_node_003&#125;&#123;upVg0NksTRi8LH8mj9xqpg&#125;&#123;ydGaCWORSlWf09TTjpUBQg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9302&#125;,&#125;, reason: zen-disco-node-join 12345678910111213141516171819202122232425262728C:\ProfessionalSoftWare\ElasticSearch\elasticsearch-5.6.10-node2\binλ elasticsearch[2018-07-22T17:38:47,851][INFO ][o.e.n.Node ] [elasticsearch_node_002] initializing ...[2018-07-22T17:38:47,960][INFO ][o.e.e.NodeEnvironment ] [elasticsearch_node_002] using [1] data paths, mounts [[disk-C (C:)]], net usable_space [78.9gb], net total_space [118.7gb], spins? [unknown], types [NTFS][2018-07-22T17:38:47,960][INFO ][o.e.e.NodeEnvironment ] [elasticsearch_node_002] heap size [1.9gb], compressed ordinary object pointers [true][2018-07-22T17:38:47,960][INFO ][o.e.n.Node ] [elasticsearch_node_002] node name [elasticsearch_node_002], node ID [3ezLh-3jTB-c0pQXFXnr5Q][2018-07-22T17:38:47,960][INFO ][o.e.n.Node ] [elasticsearch_node_002] version[5.6.10], pid[13072], build[b727a60/2018-06-06T15:48:34.860Z], OS[Windows 10/10.0/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_131/25.131-b11][2018-07-22T17:38:47,960][INFO ][o.e.n.Node ] [elasticsearch_node_002] JVM arguments [-Xms2g, -Xmx2g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Djdk.io.permissionsUseCanonicalPath=true, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j.skipJansi=true, -XX:+HeapDumpOnOutOfMemoryError, -Delasticsearch, -Des.path.home=C:\ProfessionalSoftWare\ElasticSearch\elasticsearch-5.6.10-node2][2018-07-22T17:38:49,179][INFO ][o.e.p.PluginsService ] [elasticsearch_node_002] loaded module [aggs-matrix-stats][2018-07-22T17:38:49,179][INFO ][o.e.p.PluginsService ] [elasticsearch_node_002] loaded module [ingest-common][2018-07-22T17:38:49,179][INFO ][o.e.p.PluginsService ] [elasticsearch_node_002] loaded module [lang-expression][2018-07-22T17:38:49,179][INFO ][o.e.p.PluginsService ] [elasticsearch_node_002] loaded module [lang-groovy][2018-07-22T17:38:49,179][INFO ][o.e.p.PluginsService ] [elasticsearch_node_002] loaded module [lang-mustache][2018-07-22T17:38:49,179][INFO ][o.e.p.PluginsService ] [elasticsearch_node_002] loaded module [lang-painless][2018-07-22T17:38:49,179][INFO ][o.e.p.PluginsService ] [elasticsearch_node_002] loaded module [parent-join][2018-07-22T17:38:49,179][INFO ][o.e.p.PluginsService ] [elasticsearch_node_002] loaded module [percolator][2018-07-22T17:38:49,179][INFO ][o.e.p.PluginsService ] [elasticsearch_node_002] loaded module [reindex][2018-07-22T17:38:49,179][INFO ][o.e.p.PluginsService ] [elasticsearch_node_002] loaded module [transport-netty3][2018-07-22T17:38:49,179][INFO ][o.e.p.PluginsService ] [elasticsearch_node_002] loaded module [transport-netty4][2018-07-22T17:38:49,179][INFO ][o.e.p.PluginsService ] [elasticsearch_node_002] no plugins loaded[2018-07-22T17:38:51,724][INFO ][o.e.d.DiscoveryModule ] [elasticsearch_node_002] using discovery type [zen][2018-07-22T17:38:52,224][INFO ][o.e.n.Node ] [elasticsearch_node_002] initialized[2018-07-22T17:38:52,240][INFO ][o.e.n.Node ] [elasticsearch_node_002] starting ...[2018-07-22T17:38:52,929][INFO ][o.e.t.TransportService ] [elasticsearch_node_002] publish_address &#123;127.0.0.1:9301&#125;, bound_addresses &#123;127.0.0.1:9301&#125;, &#123;[::1]:9301&#125;[2018-07-22T17:38:56,365][INFO ][o.e.c.s.ClusterService ] [elasticsearch_node_002] detected_master &#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;, added &#123;&#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;,&#125;, reason: zen-disco-receive(from master [master &#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125; committed version [7]])[2018-07-22T17:38:57,005][INFO ][o.e.h.n.Netty4HttpServerTransport] [elasticsearch_node_002] publish_address &#123;127.0.0.1:9201&#125;, bound_addresses &#123;127.0.0.1:9201&#125;, &#123;[::1]:9201&#125;[2018-07-22T17:38:57,005][INFO ][o.e.n.Node ] [elasticsearch_node_002] started[2018-07-22T17:39:43,600][INFO ][o.e.c.s.ClusterService ] [elasticsearch_node_002] added &#123;&#123;elasticsearch_node_003&#125;&#123;upVg0NksTRi8LH8mj9xqpg&#125;&#123;ydGaCWORSlWf09TTjpUBQg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9302&#125;,&#125;, reason: zen-disco-receive(from master [master &#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125; committed version [16]]) 1234567891011121314151617181920212223242526λ elasticsearch[2018-07-22T17:39:34,786][INFO ][o.e.n.Node ] [elasticsearch_node_003] initializing ...[2018-07-22T17:39:35,020][INFO ][o.e.e.NodeEnvironment ] [elasticsearch_node_003] using [1] data paths, mounts [[disk-C (C:)]], net usable_space [76.8gb], net total_space [118.7gb], spins? [unknown], types [NTFS][2018-07-22T17:39:35,020][INFO ][o.e.e.NodeEnvironment ] [elasticsearch_node_003] heap size [1.9gb], compressed ordinary object pointers [true][2018-07-22T17:39:35,020][INFO ][o.e.n.Node ] [elasticsearch_node_003] node name [elasticsearch_node_003], node ID [upVg0NksTRi8LH8mj9xqpg][2018-07-22T17:39:35,020][INFO ][o.e.n.Node ] [elasticsearch_node_003] version[5.6.10], pid[2316], build[b727a60/2018-06-06T15:48:34.860Z], OS[Windows 10/10.0/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_131/25.131-b11][2018-07-22T17:39:35,020][INFO ][o.e.n.Node ] [elasticsearch_node_003] JVM arguments [-Xms2g, -Xmx2g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Djdk.io.permissionsUseCanonicalPath=true, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j.skipJansi=true, -XX:+HeapDumpOnOutOfMemoryError, -Delasticsearch, -Des.path.home=C:\ProfessionalSoftWare\ElasticSearch\elasticsearch-5.6.10-node3][2018-07-22T17:39:36,389][INFO ][o.e.p.PluginsService ] [elasticsearch_node_003] loaded module [aggs-matrix-stats][2018-07-22T17:39:36,389][INFO ][o.e.p.PluginsService ] [elasticsearch_node_003] loaded module [ingest-common][2018-07-22T17:39:36,389][INFO ][o.e.p.PluginsService ] [elasticsearch_node_003] loaded module [lang-expression][2018-07-22T17:39:36,389][INFO ][o.e.p.PluginsService ] [elasticsearch_node_003] loaded module [lang-groovy][2018-07-22T17:39:36,389][INFO ][o.e.p.PluginsService ] [elasticsearch_node_003] loaded module [lang-mustache][2018-07-22T17:39:36,389][INFO ][o.e.p.PluginsService ] [elasticsearch_node_003] loaded module [lang-painless][2018-07-22T17:39:36,389][INFO ][o.e.p.PluginsService ] [elasticsearch_node_003] loaded module [parent-join][2018-07-22T17:39:36,389][INFO ][o.e.p.PluginsService ] [elasticsearch_node_003] loaded module [percolator][2018-07-22T17:39:36,389][INFO ][o.e.p.PluginsService ] [elasticsearch_node_003] loaded module [reindex][2018-07-22T17:39:36,389][INFO ][o.e.p.PluginsService ] [elasticsearch_node_003] loaded module [transport-netty3][2018-07-22T17:39:36,404][INFO ][o.e.p.PluginsService ] [elasticsearch_node_003] loaded module [transport-netty4][2018-07-22T17:39:36,404][INFO ][o.e.p.PluginsService ] [elasticsearch_node_003] no plugins loaded[2018-07-22T17:39:38,995][INFO ][o.e.d.DiscoveryModule ] [elasticsearch_node_003] using discovery type [zen][2018-07-22T17:39:39,605][INFO ][o.e.n.Node ] [elasticsearch_node_003] initialized[2018-07-22T17:39:39,605][INFO ][o.e.n.Node ] [elasticsearch_node_003] starting ...[2018-07-22T17:39:40,386][INFO ][o.e.t.TransportService ] [elasticsearch_node_003] publish_address &#123;127.0.0.1:9302&#125;, bound_addresses &#123;127.0.0.1:9302&#125;, &#123;[::1]:9302&#125;[2018-07-22T17:39:43,631][INFO ][o.e.c.s.ClusterService ] [elasticsearch_node_003] detected_master &#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;, added &#123;&#123;elasticsearch_node_002&#125;&#123;3ezLh-3jTB-c0pQXFXnr5Q&#125;&#123;Y4i8X_t7QhWDBoP6MoDuvw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9301&#125;,&#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;,&#125;, reason: zen-disco-receive(from master [master &#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125; committed version [16]])[2018-07-22T17:39:44,308][INFO ][o.e.h.n.Netty4HttpServerTransport] [elasticsearch_node_003] publish_address &#123;127.0.0.1:9202&#125;, bound_addresses &#123;127.0.0.1:9202&#125;, &#123;[::1]:9202&#125;[2018-07-22T17:39:44,308][INFO ][o.e.n.Node ] [elasticsearch_node_003] started 12345678910111213141516171819WKQ@WKQ-PC MINGW64 /c/ProfessionalSoftWare/ElasticSearch/elasticsearch-5.6.10$ curl -X GET &quot;localhost:9200/_cluster/health&quot;&#123; &quot;cluster_name&quot;:&quot;elasticsearch_production&quot;, &quot;status&quot;:&quot;green&quot;, &quot;timed_out&quot;:false, &quot;number_of_nodes&quot;:3, &quot;number_of_data_nodes&quot;:3, &quot;active_primary_shards&quot;:8, &quot;active_shards&quot;:16, &quot;relocating_shards&quot;:0, &quot;initializing_shards&quot;:0, &quot;unassigned_shards&quot;:0, &quot;delayed_unassigned_shards&quot;:0, &quot;number_of_pending_tasks&quot;:0, &quot;number_of_in_flight_fetch&quot;:0, &quot;task_max_waiting_in_queue_millis&quot;:0, &quot;active_shards_percent_as_number&quot;:100&#125; (5) 水平扩容 每个节点的硬件资源（CPU, RAM, I/O）将被更少的分片所共享，每个分片的性能将会得到提升。 分片是一个功能完整的搜索引擎，它拥有使用一个节点上的所有资源的能力。 主分片的数目在索引创建时 就已经确定了下来。实际上，这个数目定义了这个索引能够 存储 的最大数据量。（实际大小取决于你的数据、硬件和使用场景。） 但是，读操作——搜索和返回数据——可以同时被主分片 或 副本分片所处理，所以当你拥有越多的副本分片时，也将拥有越高的吞吐量。 (5.1) 设置副本数12345678$ curl -X PUT &quot;localhost:9200/blogs/_settings?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&gt; &#123;&gt; &quot;number_of_replicas&quot; : 2&gt; &#125;&gt; &apos;&#123; &quot;acknowledged&quot; : true&#125; blogs 索引现在拥有9个分片：3个主分片和6个副本分片。 这意味着我们可以将集群扩容到9个节点，每个节点上一个分片。相比原来3个节点时，集群搜索性能可以提升 3 倍。 当然，如果只是在相同节点数目的集群上增加更多的副本分片并不能提高性能，因为每个分片从节点上获得的资源会变少。 你需要增加更多的硬件资源来提升吞吐量。 但是更多的副本分片数提高了数据冗余量：按照上面的节点配置，我们可以在失去2个节点的情况下不丢失任何数据。 (6) 应对故障123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158[2018-07-22T17:59:38,513][INFO ][o.e.c.m.MetaDataUpdateSettingsService] [elasticsearch_node_001] updating number_of_replicas to [2] for indices [blogs][2018-07-22T17:59:39,286][INFO ][o.e.c.r.a.AllocationService] [elasticsearch_node_001] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[blogs][2]] ...]).[2018-07-22T18:05:07,871][INFO ][o.e.n.Node ] [elasticsearch_node_001] stopping ...[2018-07-22T18:05:07,943][INFO ][o.e.n.Node ] [elasticsearch_node_001] stopped[2018-07-22T18:05:08,051][INFO ][o.e.n.Node ] [elasticsearch_node_001] closing ...[2018-07-22T18:05:08,065][INFO ][o.e.n.Node ] [elasticsearch_node_001] closed终止批处理操作吗(Y/N)? y[2018-07-22T18:05:07,887][INFO ][o.e.d.z.ZenDiscovery ] [elasticsearch_node_002] master_left [&#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;], reason [shut_down][2018-07-22T18:05:07,910][WARN ][o.e.d.z.ZenDiscovery ] [elasticsearch_node_002] master left (reason = shut_down), current nodes: nodes: &#123;elasticsearch_node_003&#125;&#123;upVg0NksTRi8LH8mj9xqpg&#125;&#123;ydGaCWORSlWf09TTjpUBQg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9302&#125; &#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;, master &#123;elasticsearch_node_002&#125;&#123;3ezLh-3jTB-c0pQXFXnr5Q&#125;&#123;Y4i8X_t7QhWDBoP6MoDuvw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9301&#125;, local[2018-07-22T18:05:07,911][INFO ][o.e.d.z.ZenDiscovery ] [elasticsearch_node_002] master_left [&#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;], reason [transport disconnected][2018-07-22T18:05:07,925][WARN ][o.e.t.n.Netty4Transport ] [elasticsearch_node_002] write and flush on the network layer failed (channel: [id: 0xee4a8fab, L:0.0.0.0/0.0.0.0:9301 ! R:/127.0.0.1:53349])java.nio.channels.ClosedChannelException: null at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source) ~[?:?][2018-07-22T18:05:08,929][WARN ][o.e.c.NodeConnectionsService] [elasticsearch_node_002] failed to connect to node &#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125; (tried [1] times)org.elasticsearch.transport.ConnectTransportException: [elasticsearch_node_001][127.0.0.1:9300] connect_timeout[30s] at org.elasticsearch.transport.netty4.Netty4Transport.connectToChannels(Netty4Transport.java:363) ~[?:?] at org.elasticsearch.transport.TcpTransport.openConnection(TcpTransport.java:570) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TcpTransport.connectToNode(TcpTransport.java:473) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:342) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:329) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.NodeConnectionsService.validateAndConnectIfNeeded(NodeConnectionsService.java:154) [elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.NodeConnectionsService$1.doRun(NodeConnectionsService.java:107) [elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:674) [elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-5.6.10.jar:5.6.10] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: no further information: 127.0.0.1/127.0.0.1:9300 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:?] at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:?] at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:352) ~[?:?] at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:544) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498) ~[?:?] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) ~[?:?] at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) ~[?:?] ... 1 moreCaused by: java.net.ConnectException: Connection refused: no further information at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:?] at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:?] at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:352) ~[?:?] at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:544) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498) ~[?:?] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) ~[?:?] at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) ~[?:?] ... 1 more[2018-07-22T18:05:10,961][INFO ][o.e.c.s.ClusterService ] [elasticsearch_node_002] new_master &#123;elasticsearch_node_002&#125;&#123;3ezLh-3jTB-c0pQXFXnr5Q&#125;&#123;Y4i8X_t7QhWDBoP6MoDuvw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9301&#125;, reason: zen-disco-elected-as-master ([0] nodes joined)[, ][2018-07-22T18:05:13,003][DEBUG][o.e.a.a.c.n.s.TransportNodesStatsAction] [elasticsearch_node_002] failed to execute on node [gjy4N2RCQ4mLxp1lxdyZ8w]org.elasticsearch.transport.NodeNotConnectedException: [elasticsearch_node_001][127.0.0.1:9300] Node not connected at org.elasticsearch.transport.TcpTransport.getConnection(TcpTransport.java:640) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TcpTransport.getConnection(TcpTransport.java:117) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TransportService.getConnection(TransportService.java:540) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:516) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.start(TransportNodesAction.java:197) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.nodes.TransportNodesAction.doExecute(TransportNodesAction.java:89) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.nodes.TransportNodesAction.doExecute(TransportNodesAction.java:52) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:170) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:142) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:84) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:83) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:72) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:408) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.execute(AbstractClient.java:730) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.nodesStats(AbstractClient.java:826) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.InternalClusterInfoService.updateNodeStats(InternalClusterInfoService.java:256) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.InternalClusterInfoService.refresh(InternalClusterInfoService.java:292) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.InternalClusterInfoService.maybeRefresh(InternalClusterInfoService.java:277) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.InternalClusterInfoService.lambda$onMaster$0(InternalClusterInfoService.java:137) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:575) [elasticsearch-5.6.10.jar:5.6.10] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131][2018-07-22T18:05:13,015][DEBUG][o.e.a.a.i.s.TransportIndicesStatsAction] [elasticsearch_node_002] failed to execute [indices:monitor/stats] on node [gjy4N2RCQ4mLxp1lxdyZ8w]org.elasticsearch.transport.NodeNotConnectedException: [elasticsearch_node_001][127.0.0.1:9300] Node not connected at org.elasticsearch.transport.TcpTransport.getConnection(TcpTransport.java:640) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TcpTransport.getConnection(TcpTransport.java:117) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TransportService.getConnection(TransportService.java:540) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:503) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction.sendNodeRequest(TransportBroadcastByNodeAction.java:322) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction.start(TransportBroadcastByNodeAction.java:311) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction.doExecute(TransportBroadcastByNodeAction.java:234) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction.doExecute(TransportBroadcastByNodeAction.java:79) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:170) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:142) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:84) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:83) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:72) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:408) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.support.AbstractClient$IndicesAdmin.execute(AbstractClient.java:1256) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.support.AbstractClient$IndicesAdmin.stats(AbstractClient.java:1577) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.InternalClusterInfoService.updateIndicesStats(InternalClusterInfoService.java:270) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.InternalClusterInfoService.refresh(InternalClusterInfoService.java:321) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.InternalClusterInfoService.maybeRefresh(InternalClusterInfoService.java:277) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.InternalClusterInfoService.lambda$onMaster$0(InternalClusterInfoService.java:137) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:575) [elasticsearch-5.6.10.jar:5.6.10] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131][2018-07-22T18:05:13,057][INFO ][o.e.c.r.a.AllocationService] [elasticsearch_node_002] Cluster health status changed from [GREEN] to [YELLOW] (reason: []).[2018-07-22T18:05:13,058][INFO ][o.e.c.s.ClusterService ] [elasticsearch_node_002] removed &#123;&#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;,&#125;, reason: zen-disco-node-failed(&#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;), reason(transport disconnected)[2018-07-22T18:05:14,096][INFO ][o.e.c.r.DelayedAllocationService] [elasticsearch_node_002] scheduling reroute for delayed shards in [58.8s] (6 delayed shards)[2018-07-22T18:05:07,882][INFO ][o.e.d.z.ZenDiscovery ] [elasticsearch_node_003] master_left [&#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;], reason [shut_down][2018-07-22T18:05:07,891][WARN ][o.e.d.z.ZenDiscovery ] [elasticsearch_node_003] master left (reason = shut_down), current nodes: nodes: &#123;elasticsearch_node_002&#125;&#123;3ezLh-3jTB-c0pQXFXnr5Q&#125;&#123;Y4i8X_t7QhWDBoP6MoDuvw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9301&#125; &#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;, master &#123;elasticsearch_node_003&#125;&#123;upVg0NksTRi8LH8mj9xqpg&#125;&#123;ydGaCWORSlWf09TTjpUBQg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9302&#125;, local[2018-07-22T18:05:07,904][INFO ][o.e.d.z.ZenDiscovery ] [elasticsearch_node_003] master_left [&#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;], reason [transport disconnected][2018-07-22T18:05:08,917][WARN ][o.e.c.NodeConnectionsService] [elasticsearch_node_003] failed to connect to node &#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125; (tried [1] times)org.elasticsearch.transport.ConnectTransportException: [elasticsearch_node_001][127.0.0.1:9300] connect_timeout[30s] at org.elasticsearch.transport.netty4.Netty4Transport.connectToChannels(Netty4Transport.java:363) ~[?:?] at org.elasticsearch.transport.TcpTransport.openConnection(TcpTransport.java:570) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TcpTransport.connectToNode(TcpTransport.java:473) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:342) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:329) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.NodeConnectionsService.validateAndConnectIfNeeded(NodeConnectionsService.java:154) [elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.NodeConnectionsService$1.doRun(NodeConnectionsService.java:107) [elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:674) [elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-5.6.10.jar:5.6.10] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: no further information: 127.0.0.1/127.0.0.1:9300 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:?] at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:?] at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:352) ~[?:?] at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:544) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498) ~[?:?] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) ~[?:?] at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) ~[?:?] ... 1 moreCaused by: java.net.ConnectException: Connection refused: no further information at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:?] at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:?] at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:352) ~[?:?] at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:544) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498) ~[?:?] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) ~[?:?] at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) ~[?:?] ... 1 more[2018-07-22T18:05:11,977][INFO ][o.e.c.s.ClusterService ] [elasticsearch_node_003] detected_master &#123;elasticsearch_node_002&#125;&#123;3ezLh-3jTB-c0pQXFXnr5Q&#125;&#123;Y4i8X_t7QhWDBoP6MoDuvw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9301&#125;, reason: zen-disco-receive(from master [master &#123;elasticsearch_node_002&#125;&#123;3ezLh-3jTB-c0pQXFXnr5Q&#125;&#123;Y4i8X_t7QhWDBoP6MoDuvw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9301&#125; committed version [27]])[2018-07-22T18:05:13,066][INFO ][o.e.c.s.ClusterService ] [elasticsearch_node_003] removed &#123;&#123;elasticsearch_node_001&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;81yv2TghSQSrqvbPWszDTg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;,&#125;, reason: zen-disco-receive(from master [master &#123;elasticsearch_node_002&#125;&#123;3ezLh-3jTB-c0pQXFXnr5Q&#125;&#123;Y4i8X_t7QhWDBoP6MoDuvw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9301&#125; committed version [28]]) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107[2018-07-22T18:11:53,988][INFO ][o.e.n.Node ] [elasticsearch_node_002] stopping ...[2018-07-22T18:11:54,051][INFO ][o.e.n.Node ] [elasticsearch_node_002] stopped[2018-07-22T18:11:54,051][INFO ][o.e.n.Node ] [elasticsearch_node_002] closing ...[2018-07-22T18:11:54,059][INFO ][o.e.n.Node ] [elasticsearch_node_002] closed终止批处理操作吗(Y/N)? y[2018-07-22T18:11:53,998][INFO ][o.e.d.z.ZenDiscovery ] [elasticsearch_node_003] master_left [&#123;elasticsearch_node_002&#125;&#123;3ezLh-3jTB-c0pQXFXnr5Q&#125;&#123;Y4i8X_t7QhWDBoP6MoDuvw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9301&#125;], reason [shut_down][2018-07-22T18:11:54,002][WARN ][o.e.d.z.ZenDiscovery ] [elasticsearch_node_003] master left (reason = shut_down), current nodes: nodes: &#123;elasticsearch_node_002&#125;&#123;3ezLh-3jTB-c0pQXFXnr5Q&#125;&#123;Y4i8X_t7QhWDBoP6MoDuvw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9301&#125;, master &#123;elasticsearch_node_003&#125;&#123;upVg0NksTRi8LH8mj9xqpg&#125;&#123;ydGaCWORSlWf09TTjpUBQg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9302&#125;, local[2018-07-22T18:11:57,059][INFO ][o.e.c.s.ClusterService ] [elasticsearch_node_003] new_master &#123;elasticsearch_node_003&#125;&#123;upVg0NksTRi8LH8mj9xqpg&#125;&#123;ydGaCWORSlWf09TTjpUBQg&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9302&#125;, reason: zen-disco-elected-as-master ([0] nodes joined)[, ][2018-07-22T18:11:58,069][WARN ][o.e.c.NodeConnectionsService] [elasticsearch_node_003] failed to connect to node &#123;elasticsearch_node_002&#125;&#123;3ezLh-3jTB-c0pQXFXnr5Q&#125;&#123;Y4i8X_t7QhWDBoP6MoDuvw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9301&#125; (tried [1] times)org.elasticsearch.transport.ConnectTransportException: [elasticsearch_node_002][127.0.0.1:9301] connect_timeout[30s] at org.elasticsearch.transport.netty4.Netty4Transport.connectToChannels(Netty4Transport.java:363) ~[?:?] at org.elasticsearch.transport.TcpTransport.openConnection(TcpTransport.java:570) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TcpTransport.connectToNode(TcpTransport.java:473) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:342) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:329) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.NodeConnectionsService.validateAndConnectIfNeeded(NodeConnectionsService.java:154) [elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.NodeConnectionsService$1.doRun(NodeConnectionsService.java:107) [elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:674) [elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-5.6.10.jar:5.6.10] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131]Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: no further information: 127.0.0.1/127.0.0.1:9301 at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:?] at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:?] at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:352) ~[?:?] at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:544) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498) ~[?:?] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) ~[?:?] at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) ~[?:?] ... 1 moreCaused by: java.net.ConnectException: Connection refused: no further information at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:?] at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717) ~[?:?] at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:352) ~[?:?] at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:340) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:632) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:544) ~[?:?] at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498) ~[?:?] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) ~[?:?] at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) ~[?:?] ... 1 more[2018-07-22T18:11:58,100][DEBUG][o.e.a.a.c.n.s.TransportNodesStatsAction] [elasticsearch_node_003] failed to execute on node [3ezLh-3jTB-c0pQXFXnr5Q]org.elasticsearch.transport.NodeNotConnectedException: [elasticsearch_node_002][127.0.0.1:9301] Node not connected at org.elasticsearch.transport.TcpTransport.getConnection(TcpTransport.java:640) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TcpTransport.getConnection(TcpTransport.java:117) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TransportService.getConnection(TransportService.java:540) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:516) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.nodes.TransportNodesAction$AsyncAction.start(TransportNodesAction.java:197) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.nodes.TransportNodesAction.doExecute(TransportNodesAction.java:89) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.nodes.TransportNodesAction.doExecute(TransportNodesAction.java:52) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:170) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:142) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:84) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:83) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:72) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:408) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.execute(AbstractClient.java:730) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.support.AbstractClient$ClusterAdmin.nodesStats(AbstractClient.java:826) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.InternalClusterInfoService.updateNodeStats(InternalClusterInfoService.java:256) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.InternalClusterInfoService.refresh(InternalClusterInfoService.java:292) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.InternalClusterInfoService.maybeRefresh(InternalClusterInfoService.java:277) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.InternalClusterInfoService.lambda$onMaster$0(InternalClusterInfoService.java:137) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:575) [elasticsearch-5.6.10.jar:5.6.10] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131][2018-07-22T18:11:58,155][DEBUG][o.e.a.a.i.s.TransportIndicesStatsAction] [elasticsearch_node_003] failed to execute [indices:monitor/stats] on node [3ezLh-3jTB-c0pQXFXnr5Q]org.elasticsearch.transport.NodeNotConnectedException: [elasticsearch_node_002][127.0.0.1:9301] Node not connected at org.elasticsearch.transport.TcpTransport.getConnection(TcpTransport.java:640) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TcpTransport.getConnection(TcpTransport.java:117) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TransportService.getConnection(TransportService.java:540) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:503) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction.sendNodeRequest(TransportBroadcastByNodeAction.java:322) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction$AsyncAction.start(TransportBroadcastByNodeAction.java:311) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction.doExecute(TransportBroadcastByNodeAction.java:234) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.broadcast.node.TransportBroadcastByNodeAction.doExecute(TransportBroadcastByNodeAction.java:79) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:170) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:142) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:84) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:83) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:72) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:408) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.support.AbstractClient$IndicesAdmin.execute(AbstractClient.java:1256) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.client.support.AbstractClient$IndicesAdmin.stats(AbstractClient.java:1577) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.InternalClusterInfoService.updateIndicesStats(InternalClusterInfoService.java:270) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.InternalClusterInfoService.refresh(InternalClusterInfoService.java:321) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.InternalClusterInfoService.maybeRefresh(InternalClusterInfoService.java:277) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.cluster.InternalClusterInfoService.lambda$onMaster$0(InternalClusterInfoService.java:137) ~[elasticsearch-5.6.10.jar:5.6.10] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:575) [elasticsearch-5.6.10.jar:5.6.10] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_131] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_131] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_131][2018-07-22T18:11:58,170][INFO ][o.e.c.s.ClusterService ] [elasticsearch_node_003] removed &#123;&#123;elasticsearch_node_002&#125;&#123;3ezLh-3jTB-c0pQXFXnr5Q&#125;&#123;Y4i8X_t7QhWDBoP6MoDuvw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9301&#125;,&#125;, reason: zen-disco-node-failed(&#123;elasticsearch_node_002&#125;&#123;3ezLh-3jTB-c0pQXFXnr5Q&#125;&#123;Y4i8X_t7QhWDBoP6MoDuvw&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9301&#125;), reason(transport disconnected)[2018-07-22T18:11:58,220][INFO ][o.e.c.r.DelayedAllocationService] [elasticsearch_node_003] scheduling reroute for delayed shards in [59.8s] (8 delayed shards)[2018-07-22T18:12:02,864][INFO ][o.e.n.Node ] [elasticsearch_node_003] stopping ...[2018-07-22T18:12:02,893][INFO ][o.e.n.Node ] [elasticsearch_node_003] stopped[2018-07-22T18:12:02,894][INFO ][o.e.n.Node ] [elasticsearch_node_003] closing ...[2018-07-22T18:12:02,902][INFO ][o.e.n.Node ] [elasticsearch_node_003] closed终止批处理操作吗(Y/N)? y References[1] 空集群[2] 集群健康[3] 添加索引[4] 添加故障转移[5] 水平扩容[6] 应对故障]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[neo4j]]></title>
    <url>%2F2018%2F08%2F03%2Fneo4j-practice%2F</url>
    <content type="text"><![CDATA[查看正在执行的cypher语句1、查看自己当前正在运行的所有查询。语法为： CALL dbms.listQueries() 找到正在运行的语句的id 调用 CALL dbms.killQueries([‘query-1’,‘query-2’]) 根据id杀掉 语句执行的进程 Neo4j备份 Neo4j备份的内容是某一时刻的完整数据库 CALL apoc.export.csv.all(&#39;/data/neo4j/export/graph_all.csv&#39;, null); 全量export Neo4j数据导出 CALL apoc.export.csv.all(&#39;/data/neo4j/export/graph_all.csv&#39;, null); 全量export CALL apoc.export.csv.query(&quot;MATCH (n:Test) RETURN n&quot;, &quot;/data/neo4j/export/graph_one_query.csv&quot;, {}); 某条查询导出 运行 Cypher 语句时间盒（也可以用于防止死锁的发生）1234567call apoc.cypher.runTimeboxed( &quot; match (n),(m) match p=shortestPath((n)-[*]-(m)) return p &quot;, null, 1000)yield valuereturn value.p 启动后动态修改配置 CALL dbms.setConfigValue(&#39;dbms.logs.query.enabled&#39;, &#39;true&#39;) 修改密码 :server change-password 修改密码 CALL dbms.changePassword(&quot;newpassword&quot;) (旧版本)修改密码 查询孤立节点 match (n) where not (n)--() return id(n); 修改属性// Person对象有个sex属性，因为业务需要想改成gender属性 match (p:Person {name:&#39;张三&#39;}) set p.gender = p.sex remove p.sex return p References[1] neo4j-apoc-procedures/#_export_to_csv]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 基础 面试题]]></title>
    <url>%2F2018%2F07%2F13%2Fjava-basic-interview-questions%2F</url>
    <content type="text"><![CDATA[转自 Java 最常见的 200+ 面试题：面试必备 一、Java 基础 1.JDK 和 JRE 有什么区别？ 2.== 和 equals 的区别是什么？ 3.两个对象的 hashCode()相同，则 equals()也一定为 true，对吗？ 4.final 在 java 中有什么作用？ 5.java 中的 Math.round(-1.5) 等于多少？ 6.String 属于基础的数据类型吗？ 7.java 中操作字符串都有哪些类？它们之间有什么区别？ 8.String str=”i”与 String str=new String(“i”)一样吗？ 9.如何将字符串反转？ 10.String 类的常用方法都有那些？ 11.抽象类必须要有抽象方法吗？ 12.普通类和抽象类有哪些区别？ 13.抽象类能使用 final 修饰吗？ 14.接口和抽象类有什么区别？ 15.java 中 IO 流分为几种？ 16.BIO、NIO、AIO 有什么区别？ 17.Files的常用方法都有哪些？ 二、容器 18.java 容器都有哪些？ 19.Collection 和 Collections 有什么区别？ 20.List、Set、Map 之间的区别是什么？ 21.HashMap 和 Hashtable 有什么区别？ 22.如何决定使用 HashMap 还是 TreeMap？ 23.说一下 HashMap 的实现原理？ 24.说一下 HashSet 的实现原理？ 25.ArrayList 和 LinkedList 的区别是什么？ 26.如何实现数组和 List 之间的转换？ 27.ArrayList 和 Vector 的区别是什么？ 28.Array 和 ArrayList 有何区别？ 29.在 Queue 中 poll()和 remove()有什么区别？ 30.哪些集合类是线程安全的？ 31.迭代器 Iterator 是什么？ 32.Iterator 怎么使用？有什么特点？ 33.Iterator 和 ListIterator 有什么区别？ 34.怎么确保一个集合不能被修改？ 三、多线程 35.并行和并发有什么区别？ 36.线程和进程的区别？ 37.守护线程是什么？ 38.创建线程有哪几种方式？ 39.说一下 runnable 和 callable 有什么区别？ 40.线程有哪些状态？ 41.sleep() 和 wait() 有什么区别？ 42.notify()和 notifyAll()有什么区别？ 43.线程的 run()和 start()有什么区别？ 44.创建线程池有哪几种方式？ 45.线程池都有哪些状态？ 46.线程池中 submit()和 execute()方法有什么区别？ 47.在 java 程序中怎么保证多线程的运行安全？ 48.多线程锁的升级原理是什么？ 49.什么是死锁？ 50.怎么防止死锁？ 51.ThreadLocal 是什么？有哪些使用场景？ 52.说一下 synchronized 底层实现原理？ 53.synchronized 和 volatile 的区别是什么？ 54.synchronized 和 Lock 有什么区别？ 55.synchronized 和 ReentrantLock 区别是什么？ 56.说一下 atomic 的原理？ 四、反射 57.什么是反射？ 58.什么是 java 序列化？什么情况下需要序列化？ 59.动态代理是什么？有哪些应用？ 60.怎么实现动态代理？ 五、对象拷贝 61.为什么要使用克隆？ 62.如何实现对象克隆？ 63.深拷贝和浅拷贝区别是什么？ 六、Java Web 64.jsp 和 servlet 有什么区别？ 65.jsp 有哪些内置对象？作用分别是什么？ 66.说一下 jsp 的 4 种作用域？ 67.session 和 cookie 有什么区别？ 68.说一下 session 的工作原理？ 69.如果客户端禁止 cookie 能实现 session 还能用吗？ 70.spring mvc 和 struts 的区别是什么？ 71.如何避免 sql 注入？ 72.什么是 XSS 攻击，如何避免？ 73.什么是 CSRF 攻击，如何避免？ 七、异常 74.throw 和 throws 的区别？ 75.final、finally、finalize 有什么区别？ 76.try-catch-finally 中哪个部分可以省略？ 77.try-catch-finally 中，如果 catch 中 return 了，finally 还会执行吗？ 78.常见的异常类有哪些？ 八、网络 79.http 响应码 301 和 302 代表的是什么？有什么区别？ 80.forward 和 redirect 的区别？ 81.简述 tcp 和 udp的区别？ 82.tcp 为什么要三次握手，两次不行吗？为什么？ 83.说一下 tcp 粘包是怎么产生的？ 84.OSI 的七层模型都有哪些？ 85.get 和 post 请求有哪些区别？ 86.如何实现跨域？ 87.说一下 JSONP 实现原理？ 九、设计模式 88.说一下你熟悉的设计模式？ 89.简单工厂和抽象工厂有什么区别？ 十、Spring/Spring MVC 90.为什么要使用 spring？ 91.解释一下什么是 aop？ 92.解释一下什么是 ioc？ 93.spring 有哪些主要模块？ 94.spring 常用的注入方式有哪些？ 95.spring 中的 bean 是线程安全的吗？ 96.spring 支持几种 bean 的作用域？ 97.spring 自动装配 bean 有哪些方式？ 98.spring 事务实现方式有哪些？ 99.说一下 spring 的事务隔离？ 100.说一下 spring mvc 运行流程？ 101.spring mvc 有哪些组件？ 102.@RequestMapping 的作用是什么？ 103.@Autowired 的作用是什么？ 十一、Spring Boot/Spring Cloud 104.什么是 spring boot？ 105.为什么要用 spring boot？ 106.spring boot 核心配置文件是什么？ 107.spring boot 配置文件有哪几种类型？它们有什么区别？ 108.spring boot 有哪些方式可以实现热部署？ 109.jpa 和 hibernate 有什么区别？ 110.什么是 spring cloud？ 111.spring cloud 断路器的作用是什么？ 112.spring cloud 的核心组件有哪些？ 十二、Hibernate 113.为什么要使用 hibernate？ 114.什么是 ORM 框架？ 115.hibernate 中如何在控制台查看打印的 sql 语句？ 116.hibernate 有几种查询方式？ 117.hibernate 实体类可以被定义为 final 吗？ 118.在 hibernate 中使用 Integer 和 int 做映射有什么区别？ 119.hibernate 是如何工作的？ 120.get()和 load()的区别？ 121.说一下 hibernate 的缓存机制？ 122.hibernate 对象有哪些状态？ 123.在 hibernate 中 getCurrentSession 和 openSession 的区别是什么？ 124.hibernate 实体类必须要有无参构造函数吗？为什么？ 十三、Mybatis 125.mybatis 中 #{}和 ${}的区别是什么？ 126.mybatis 有几种分页方式？ 127.RowBounds 是一次性查询全部结果吗？为什么？ 128.mybatis 逻辑分页和物理分页的区别是什么？ 129.mybatis 是否支持延迟加载？延迟加载的原理是什么？ 130.说一下 mybatis 的一级缓存和二级缓存？ 131.mybatis 和 hibernate 的区别有哪些？ 132.mybatis 有哪些执行器（Executor）？ 133.mybatis 分页插件的实现原理是什么？ 134.mybatis 如何编写一个自定义插件？ 十四、RabbitMQ 135.rabbitmq 的使用场景有哪些？ 136.rabbitmq 有哪些重要的角色？ 137.rabbitmq 有哪些重要的组件？ 138.rabbitmq 中 vhost 的作用是什么？ 139.rabbitmq 的消息是怎么发送的？ 140.rabbitmq 怎么保证消息的稳定性？ 141.rabbitmq 怎么避免消息丢失？ 142.要保证消息持久化成功的条件有哪些？ 143.rabbitmq 持久化有什么缺点？ 144.rabbitmq 有几种广播类型？ 145.rabbitmq 怎么实现延迟消息队列？ 146.rabbitmq 集群有什么用？ 147.rabbitmq 节点的类型有哪些？ 148.rabbitmq 集群搭建需要注意哪些问题？ 149.rabbitmq 每个节点是其他节点的完整拷贝吗？为什么？ 150.rabbitmq 集群中唯一一个磁盘节点崩溃了会发生什么情况？ 151.rabbitmq 对集群节点停止顺序有要求吗？ 十五、Kafka 152.kafka 可以脱离 zookeeper 单独使用吗？为什么？ 153.kafka 有几种数据保留的策略？ 154.kafka 同时设置了 7 天和 10G 清除数据，到第五天的时候消息达到了 10G，这个时候 kafka 将如何处理？ 155.什么情况会导致 kafka 运行变慢？ 156.使用 kafka 集群需要注意什么？ 十六、Zookeeper 157.zookeeper 是什么？ 158.zookeeper 都有哪些功能？ 159.zookeeper 有几种部署模式？ 160.zookeeper 怎么保证主从节点的状态同步？ 161.集群中为什么要有主节点？ 162.集群中有 3 台服务器，其中一个节点宕机，这个时候 zookeeper 还可以使用吗？ 163.说一下 zookeeper 的通知机制？ 十七、MySql 164.数据库的三范式是什么？ 165.一张自增表里面总共有 7 条数据，删除了最后 2 条数据，重启 mysql 数据库，又插入了一条数据，此时 id 是几？ 166.如何获取当前数据库版本？ 167.说一下 ACID 是什么？ 168.char 和 varchar 的区别是什么？ 169.float 和 double 的区别是什么？ 170.mysql 的内连接、左连接、右连接有什么区别？ 171.mysql 索引是怎么实现的？ 172.怎么验证 mysql 的索引是否满足需求？ 173.说一下数据库的事务隔离？ 174.说一下 mysql 常用的引擎？ 175.说一下 mysql 的行锁和表锁？ 176.说一下乐观锁和悲观锁？ 177.mysql 问题排查都有哪些手段？ 178.如何做 mysql 的性能优化？ 十八、Redis 179.redis 是什么？都有哪些使用场景？ 180.redis 有哪些功能？ 181.redis 和 memecache 有什么区别？ 182.redis 为什么是单线程的？ 183.什么是缓存穿透？怎么解决？ 184.redis 支持的数据类型有哪些？ 185.redis 支持的 java 客户端都有哪些？ 186.jedis 和 redisson 有哪些区别？ 187.怎么保证缓存和数据库数据的一致性？ 188.redis 持久化有几种方式？ 189.redis 怎么实现分布式锁？ 190.redis 分布式锁有什么缺陷？ 191.redis 如何做内存优化？ 192.redis 淘汰策略有哪些？ 193.redis 常见的性能问题有哪些？该如何解决？ 十九、JVM 194.说一下 jvm 的主要组成部分？及其作用？ 195.说一下 jvm 运行时数据区？ 196.说一下堆栈的区别？ 197.队列和栈是什么？有什么区别？ 198.什么是双亲委派模型？ 199.说一下类加载的执行过程？ 200.怎么判断对象是否可以被回收？ 201.java 中都有哪些引用类型？ 202.说一下 jvm 有哪些垃圾回收算法？ 203.说一下 jvm 有哪些垃圾回收器？ 204.详细介绍一下 CMS 垃圾回收器？ 205.新生代垃圾回收器和老生代垃圾回收器都有哪些？有什么区别？ 206.简述分代垃圾回收器是怎么工作的？ 207.说一下 jvm 调优的工具？ 208.常用的 jvm 调优的参数都有哪些？ References[1] Java 最常见的 200+ 面试题：面试必备]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[万能 CURD 接口]]></title>
    <url>%2F2018%2F07%2F12%2Funiversal-curd-interface%2F</url>
    <content type="text"><![CDATA[在实际的使用时，经常遇到这么一个问题，我要从某种数据库中查询出数据，但是不告诉你字段/属性。 乍一看，这个怎么可能，不告诉我字段/属性，要把数据查出来，后来在github上看见一段代码，读了这段代码后恍然大悟。还是 too young to simple 原理是通过JDBC的getMetaData()方法来获取表结构，里面用到了反射 (进行了处理，高并发下还是不建议使用) 比较重要的一点是查询的时候数据库有数据字典，可以根据数据字典获取所有字段/属性 实测，MySQL 可用 代码如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import java.sql.Connection;import java.sql.SQLException;import java.util.Iterator;import java.util.List;import java.util.Map;/** * 通用接口&lt;br&gt; * * @author weikeqin.cn@gmail.com * @version V1.0 * @date 2017-07-11 15:12 */public interface Executor &#123; /** * 查询接口 * * @param sql 查询语句 * @param params 占位符对应的参数 * @param conn 连接 * @return Iterator&lt;LinkedHashMap&lt;String, Object&gt;&gt; * @throws SQLException */ Iterator&lt;Map&lt;String, Object&gt;&gt; query(String sql, Map&lt;Integer, Object&gt; params, Connection conn) throws SQLException; /** * (单条)增删改接口 * * @param sql 增删改语句 * @param params 占位符对应的参数 * @param conn 连接 * @return 操作的条数 * @throws SQLException */ int operat(String sql, Map&lt;Integer, Object&gt; params, Connection conn) throws SQLException; /** * 批量操作&lt;br&gt; * 大于等于0是成功处理的 * * @param sql 增删改语句 * @param list 占位符对应的参数 * @param conn 连接 * @return * @throws SQLException * @author : weikeqin.cn@gmail.com * @date : 2017-09-08 14:47:15 */ int[] betchOperat(String sql, List&lt;Map&lt;Integer, Object&gt;&gt; list, Connection conn) throws SQLException;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176import java.sql.*;import java.util.*;/** * @author weikeqin.cn@gmail.com * @version V1.0 * @date 2017-07-11 15:31 */public class ExecutorImpl implements Executor &#123; /** * @param sql 查询语句 * @param params 占位符 参数 * @param conn 连接 * @return */ @Override public Iterator&lt;Map&lt;String, Object&gt;&gt; query(String sql, Map&lt;Integer, Object&gt; params, Connection conn) throws SQLException &#123; final PreparedStatement statement = conn.prepareStatement(sql); // 设置参数 setParameters(statement, params); // 执行查询并获得结果 final ResultSet result = statement.executeQuery(); // 封装返回 return new Iterator&lt;Map&lt;String, Object&gt;&gt;() &#123; boolean hasNext = result.next(); // 所有字段 public List&lt;String&gt; columns; // 字段个数 public int columnsCount; /** * * * @return */ @Override public boolean hasNext() &#123; return hasNext; &#125; /** * 获得所有字段&lt;br&gt; * 第一次会查询出所有字段，第二 第三次 直接用columns * * @return * @throws SQLException */ private List&lt;String&gt; getColumns() throws SQLException &#123; if (columns != null) &#123; return columns; &#125; ResultSetMetaData metaData = result.getMetaData(); // 查询出的字段 int count = metaData.getColumnCount(); List&lt;String&gt; cols = new ArrayList&lt;&gt;(count); for (int i = 1; i &lt;= count; i++) &#123; cols.add(metaData.getColumnName(i)); &#125; columnsCount = cols.size(); return columns = cols; &#125; /** * * @return */ @Override public Map&lt;String, Object&gt; next() &#123; try &#123; if (hasNext) &#123; // Map&lt;String, Object&gt; map = new LinkedHashMap&lt;&gt;(columnsCount); for (String col : getColumns()) &#123; map.put(col, result.getObject(col)); &#125; hasNext = result.next(); if (!hasNext) &#123; result.close(); statement.close(); &#125; return map; &#125; else &#123; throw new NoSuchElementException(); &#125; &#125; catch (SQLException e) &#123; throw new RuntimeException(e); &#125; &#125; /** * */ @Override public void remove() &#123; &#125; &#125;; &#125; /** * 增删改操作 * * @param sql 增删改语句 * @param params 占位符参数 * @param conn 连接 * @return * @throws SQLException */ @Override public int operat(String sql, Map&lt;Integer, Object&gt; params, Connection conn) throws SQLException &#123; PreparedStatement statement = conn.prepareStatement(sql); setParameters(statement, params); int count = statement.executeUpdate(); statement.close(); return count; &#125; /** * 大于等于0是成功处理的 * * @param sql * @param list * @param conn * @author : weikeqin.cn@gmail.com * @date : 2017-09-08 14:48:50 */ @Override public int[] betchOperat(String sql, List&lt;Map&lt;Integer, Object&gt;&gt; list, Connection conn) throws SQLException &#123; conn.setAutoCommit(false); PreparedStatement statement = conn.prepareStatement(sql); int size = list.size(); for (int i = 0; i &lt; size; i++) &#123; // 一条语句里的多个占位符参数 Map&lt;Integer, Object&gt; map = list.get(i); setParameters(statement, map); statement.addBatch(); &#125; int[] array = statement.executeBatch(); conn.commit(); statement.close(); return array; &#125; /** * 赋值 给占位符赋值 * * @param statement * @param params * @throws SQLException * @author weikeqin.cn@gmail.com */ private void setParameters(PreparedStatement statement, Map&lt;Integer, Object&gt; params) throws SQLException &#123; // 校验 if (params == null || params.isEmpty()) &#123; return; &#125; for (Map.Entry&lt;Integer, Object&gt; entry : params.entrySet()) &#123; statement.setObject(entry.getKey(), entry.getValue()); &#125; &#125; // end method&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode 笔记]]></title>
    <url>%2F2018%2F07%2F08%2Fsoftware-engineer-knowledge%2F</url>
    <content type="text"><![CDATA[前一段时间面试的时候接面试官会问很多算法，解决他们公司实际的业务问题，然后会引导你找到比较好，时间复杂度或者空间复杂度比较小的解。这些算法中，有些知道，有些不知道，然后在面试过程中了解到一些算法的实际应用场景。感觉学到的东西用到实际生活了，特别开心。 在解决实际问题的时候发现，转化问题的能力很重要，数据结构很重要。希望可以不断累积，同时把学过的分享给大家，欢迎大家指正，交流。 最大连续子序列和问题 给定K个整数的序列{ N1, N2, …, NK }，其任意连续子序列可表示为{ Ni, Ni+1, …, Nj }，其中 1 &lt;= i &lt;= j &lt;= K。 最大连续子序列是所有连续子序中元素和最大的一个。求最大连续子序列之和 例如给定序列{-2, 11, -4, 13, -5, -2}，其最大连续子序列为{11, -4, 13}，最大和为20。 思路 暴力破解 尝试列举出所有的子序列，然后找出其中最大连续子序列 时间复杂度 O(n^2) 分治法 分而治之 时间复杂度 O(nlogn) 动态规划 根据子序列之和判断当前子序列是否加下一个元素，是否是最大子序列 时间复杂度 O(n) 参考文章 最大连续子序列和 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163import org.junit.Test;/** * 最大连续子序列和 * 动态规划 * * &lt;pre&gt; * 给定K个整数的序列&#123; N1, N2, …, NK &#125;，其任意连续子序列可表示为&#123; Ni, Ni+1, …, Nj &#125;，其中 1 &lt;= i &lt;= j &lt;= K。 * 最大连续子序列是所有连续子序中元素和最大的一个。 * * 例如给定序列&#123;-2, 11, -4, 13, -5, -2&#125;，其最大连续子序列为&#123;11, -4, 13&#125;，最大和为20。 * * References: * 最大连续子序列和 https://blog.csdn.net/sgbfblog/article/details/8032464 * * &lt;/pre&gt; * * @author: weikeqin.cn@gmail.com * @date: 2018-07-08 9:54 */public class MaxSequence &#123; /** * &lt;pre&gt; * O(N^2) 解法 * 暴力破解 * * 因为最大连续子序列和只可能从数组0到n-1中某个位置开始， * 我们可以遍历0到n-1个位置，计算由这个位置开始的所有连续子序列和中的最大值。 * 最终求出最大值即可。 * * 更详细的讲， * 就是计算从位置0开始的最大连续子序列和，从位置1开始的最大连续子序列和 ... 从位置n-1开始的最大连续子序列和， * 最后比较所有这些连续子序列和中的最大值就是答案。 * * &lt;/pre&gt; * * @param arr * @param len * @return */ int maxsequence(int[] arr, int len) &#123; //初始化最大值为第一个元素 int max = arr[0]; for (int i = 0; i &lt; len; i++) &#123; //sum必须清零 int sum = 0; //从位置i开始计算从i开始的最大连续子序列和的大小，如果大于max，则更新max。 for (int j = i; j &lt; len; j++) &#123; sum += arr[j]; if (sum &gt; max) &#123; max = sum; &#125; &#125; &#125; return max; &#125; /** * 解法2 — O(nlgn)解法 * * &lt;pre&gt; * 该问题还可以通过分治法来求解， * 最大连续子序列和要么出现在数组左半部分，要么出现在数组右半部分，要么横跨左右两半部分。 * 因此求出这三种情况下的最大值就可以得到最大连续子序列和。 * &lt;/pre&gt; * * @param arr * @param l * @param u * @return */ int maxsequence2(int[] arr, int l, int u) &#123; if (l &gt; u) &#123; return 0; &#125; if (l == u) &#123; return arr[l]; &#125; int m = (l + u) / 2; /**求横跨左右的最大连续子序列左半部分*/ int lmax = arr[m], lsum = 0; for (int i = m; i &gt;= l; i--) &#123; lsum += arr[i]; if (lsum &gt; lmax) &#123; lmax = lsum; &#125; &#125; /**求横跨左右的最大连续子序列右半部分*/ int rmax = arr[m + 1], rsum = 0; for (int i = m + 1; i &lt;= u; i++) &#123; rsum += arr[i]; if (rsum &gt; rmax) &#123; rmax = rsum; &#125; &#125; //返回三者最大值 return max3(lmax + rmax, maxsequence2(arr, l, m), maxsequence2(arr, m + 1, u)); &#125; /** * 求三个数最大值 */ int max3(int i, int j, int k) &#123; if (i &gt;= j &amp;&amp; i &gt;= k) &#123; return i; &#125; return max3(j, k, i); &#125; /** * 解法3 — O(n)解法 * * &lt;pre&gt; * 还有一种更好的解法，只需要O(n)的时间。 * 因为最大 连续子序列和只可能是以位置0～n-1中某个位置结尾。 * 设 阀值为x * 当遍历到第i个元素时，判断在它前面的连续子序列和是否大于阀值x， * 如果大于阀值x，则以位置i结尾的最大连续子序列和为元素i和前门的连续子序列和相加； * 否则，则以位置i结尾的最大连续子序列和为元素i。 * * 这儿为了方便，用阀值=0，其实阀值是多少都可以，对最后结果没影响 * &lt;/pre&gt; * * @param arr * @param len * @return */ public int maxsequence3(int[] arr, int len) &#123; int maxsum, maxhere; //初始化最大和为a[0] maxsum = maxhere = arr[0]; for (int i = 1; i &lt; len; i++) &#123; if (maxhere &lt;= 0) &#123; //如果前面位置最大连续子序列和小于等于0，则以当前位置i结尾的最大连续子序列和为a[i] maxhere = arr[i]; &#125; else &#123; //如果前面位置最大连续子序列和大于0，则以当前位置i结尾的最大连续子序列和为它们两者之和 maxhere += arr[i]; &#125; // 如果 arr[n] &gt; arr[m]+arr[m+1] +...+arr[n-1]，(m&lt;n) 更新最大连续子序列和，否则不更新 // 如果 arr[m]+arr[m+1] +...+arr[n-1]+arr[n] &gt; arr[m]+arr[m+1] +...+arr[n-1]，(m&lt;n) 更新最大连续子序列和，否则不更新 if (maxhere &gt; maxsum) &#123; //更新最大连续子序列和 maxsum = maxhere; &#125; &#125; return maxsum; &#125; @Test public void testMaxSwquence3()&#123; //int[] arr = &#123;-2, 11, -4, 13, -5, -2&#125;; int[] arr = &#123;-2, -11, -4, -13, -5, -2&#125;; int result = maxsequence3(arr, arr.length); System.out.println(result); //log.info(result); &#125;&#125; 合并多个有序链表问题思路 维持一个最小堆，利用堆的性质每次从堆中取出一个最小值然后放入数组/链表中。 优先队列 递归,每次将数组二分，求出二分后每一块的ListNode,然后合并这两块。 图问题问题 世界杯期间，甲乙丙丁戊己庚辛几个人各买了几个队的彩票 甲买了 A、B、C 三队的彩票 乙买了 B、D队的彩票 丙买了 C、E队的彩票 丁买了 F队的彩票 戊买了 D、F队的彩票 己买了 M、N队的彩票 庚买了 M队的彩票 辛买了 O队的彩票 假如买了相同队的可以分到一组， (答案是 一共有三组，甲、乙、丙、丁、戊 可以分到一组，己、庚可以分到一组，辛分到一组) 请计算一共有多少组，每组分别有谁？ 思路 List&lt;Map&lt;队, 人&gt;&gt; 图 海量文本去重问题 有几十亿 上百亿的文本数据(可以认为是新闻)，现在来了一篇新闻，怎么判断库里是否已经有这篇新闻，是否有特别相似的新闻。 思路 SimHash 我知道谷歌的网页去重用的是SimHash，处理的是非结构化数据，想出的第一个方案就是SimHash TFIDF 面试官提示我用TFIDF 基于文本相似度和微博频道特征的博文排重方法 References[1] 最大连续子序列和[2] 合并多个有序链表[3] LeetCode:Merge k Sorted Lists[4] LeetCode Merge k Sorted Lists 合并k个有序链表]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tf-idf]]></title>
    <url>%2F2018%2F06%2F04%2Ftf-idf%2F</url>
    <content type="text"><![CDATA[tf-idf的具体实现，代码稍后上传到github 在进行文本特征选择的时候，有很多种方法，tf-idf是比较简单常用的一种。 tf-idf选择出的是有区分度的词，对选特征词有很大的帮助。 TF-IDF（term frequency–inverse document frequency）是一种用于资讯检索与资讯探勘的常用加权技术。 TFIDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。 假如一篇文件的总词语数是100个，而词语”蜜蜂”出现了3次，那么”蜜蜂”一词在该文件中的词频就是3/100=0.03。 一个计算文件频率 (DF) 的方法是测定有多少份文件出现过”蜜蜂”一词，然后除以文件集里包含的文件总数。 所以，如果”蜜蜂”一词在2份文件出现过，而文件总数是8份的话，其逆向文件频率就是 lg(8/2)=2。最后的TF-IDF的分数为 0.03 * 2 = 0.06。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161package com.xxxx.xxxx.xxxx.featureselection;import com.xxxx.xxxx.xxxx.util.SegUtil;import com.xxxx.xxxx.odk.common.collection.MapUtil;import com.xxxx.xxxx.odk.common.file.CharsetUtil;import com.xxxx.xxxx.odk.common.file.FileUtil;import com.xxxx.xxxx.odk.common.string.StringUtil;import lombok.extern.slf4j.Slf4j;import java.io.*;import java.util.*;/** * @date: 2018-06-04 15:37 */@Slf4jpublic class TfIdf &#123; /** * * @param filePaths 所有文本的绝对路径 * @return */ public List&lt;Map.Entry&lt;String, Double&gt;&gt; getFeature(List&lt;String&gt; filePaths)&#123; return MapUtil.sortByValue(getTfIdf(filePaths)); &#125; /** * 获取所有词语的tf-idf值 * * @param docs */ public Map&lt;String, Double&gt; getTfIdf(List&lt;String&gt; filePaths) &#123; // &lt;文件路径, &lt;词语, 词频&gt;&gt; Map&lt;String, Map&lt;String, Integer&gt;&gt; allFilesTd = tfAllFiles(filePaths); // 猜测的词语大小，用于初始化map，避免内存拷贝 int guessWordCount = allFilesTd.size() &lt;&lt; 3; // &lt;词, 词频&gt; Map&lt;String, Integer&gt; wordAndFrequeryMap = new HashMap&lt;&gt;(guessWordCount); // &lt;词, 出现该词的文件个数&gt; Map&lt;String, Integer&gt; wordAndFileCountMap = new HashMap&lt;&gt;(guessWordCount); guessWordCount = 0; // 循环文件 for (Map.Entry&lt;String, Map&lt;String, Integer&gt;&gt; bigEntry : allFilesTd.entrySet()) &#123; // 词 词频 Map&lt;String, Integer&gt; wordAndCountMap = bigEntry.getValue(); for (Map.Entry&lt;String, Integer&gt; entry : wordAndCountMap.entrySet()) &#123; String word = entry.getKey(); Integer wordFrequery = entry.getValue(); /** 处理 &lt;词, 词频&gt; map */ if (wordAndFrequeryMap.containsKey(word)) &#123; wordAndFrequeryMap.put(word, wordAndFrequeryMap.get(word) + wordFrequery); &#125; else &#123; wordAndFrequeryMap.put(word, wordFrequery); &#125; /** 处理 &lt;词, 出现该词的文件个数&gt; map */ if (wordAndFileCountMap.containsKey(word)) &#123; wordAndFileCountMap.put(word, wordAndFileCountMap.get(word) + 1); &#125; else &#123; wordAndFileCountMap.put(word, 1); &#125; &#125; // end for &#125; // end for // 所有词的个数 int allWordCount = wordAndFrequeryMap.size(); // 文档总数 int allDocCount = filePaths.size(); // 指定大小，避免内存拷贝 // wordCount / 0.75 = wordCount / 4 * 3 = wordCount &gt;&gt; 2 / 3 Map&lt;String, Double&gt; wordTfidfMap = new HashMap&lt;&gt;(wordAndFrequeryMap.size() &gt;&gt; 2 * 3); /** 计算TF-IDF */ for (Map.Entry&lt;String, Integer&gt; entry : wordAndFileCountMap.entrySet()) &#123; String word = entry.getKey(); // 为了避免除数为0 ，对于分母为0的+1，其他的不作处理(避免一个词在所有文档里出现)，出现该词语的文件个数 + 1 int wordfileCount = entry.getValue() == 0 ? 1 : entry.getValue(); // tf * idf = (词频 / 词语总数) * ( log(文档总数/出现该词语的文件个数) ) wordTfidfMap.put(word, 1.0 * wordAndFrequeryMap.get(word) / allWordCount * Math.log(allDocCount / wordfileCount)); &#125; if(log.isDebugEnabled())&#123; /** 打印词和词频 */ List&lt;Map.Entry&lt;String, Integer&gt;&gt; wordAndFrequeryList = MapUtil.sortByValue(wordAndFrequeryMap); StringBuilder sbwf = new StringBuilder(); for(Map.Entry&lt;String, Integer&gt; entry : wordAndFrequeryList)&#123; sbwf.append(entry.getKey()); sbwf.append(" : "); sbwf.append(entry.getValue()); sbwf.append("\r\n"); &#125; wordAndFrequeryList.clear(); wordAndFrequeryList = null; log.debug("打印词和词频集合\r\n&#123;&#125;", sbwf); /** 打印词和包含该词的文档数 */ List&lt;Map.Entry&lt;String, Integer&gt;&gt; wordAndFileCountList = MapUtil.sortByValue(wordAndFileCountMap); StringBuilder sbwc = new StringBuilder(); for(Map.Entry&lt;String, Integer&gt; entry : wordAndFileCountList)&#123; sbwc.append(entry.getKey()); sbwc.append(" : "); sbwc.append(entry.getValue()); sbwc.append("\r\n"); &#125; wordAndFileCountMap.clear(); wordAndFileCountMap = null; log.debug("打印 词 和 包含该词的文档数 集合\r\n&#123;&#125;", sbwc); &#125; allDocCount = 0; allDocCount = 0; wordAndFileCountMap.clear(); wordAndFileCountMap = null; wordAndFrequeryMap.clear(); wordAndFrequeryMap = null; return wordTfidfMap; &#125; /** * 统计所有文件 * * @param filePaths * @return &lt;文件路径, &lt;词语, 词频&gt;&gt; */ public Map&lt;String, Map&lt;String, Integer&gt;&gt; tfAllFiles(List&lt;String&gt; filePaths) &#123; Map&lt;String, Map&lt;String, Integer&gt;&gt; allTf = new HashMap&lt;&gt;(1 &lt;&lt; 20); for (String filePath : filePaths) &#123; // &lt;文件路径, &lt;词语, 词频&gt;&gt; allTf.put(filePath, tfOneFile(filePath)); &#125; return allTf; &#125; /** * 统计一个文件里的词和词频&lt;br&gt; * 读文件 分词 统计 * * @param filePath * @return &lt;词, 词频&gt; */ public Map&lt;String, Integer&gt; tfOneFile(String filePath) &#123; String str = null; try &#123; str = FileUtil.readFile2String(filePath); &#125; catch (IOException e) &#123; log.error("读文件出错 &#123;&#125;", e.toString()); &#125; return SegUtil.segAndFilter(str); &#125;&#125; References[1] 《数学之美》 吴军[2] 中文文本分类中的特征选择研究 [3] 基于TFIDF的文本特征选择方法 [4] 改进的χ^2统计文本特征选择方法 [5] TF-IDF理解及其Java实现]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>feature-selection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用排序算法比较]]></title>
    <url>%2F2018%2F06%2F01%2Fsort-algorithm-comparison%2F</url>
    <content type="text"><![CDATA[在比较排序算法时，得先找一个标准比较，一般都使用时间复杂度和空间复杂度来衡量。 排序算法 根据排序过程中使用的存储器不同，排序算法分为 内部排序 和 外部排序 两大类。 内部排序指的是待排序记录存放在计算机随机存储器中进行的排序过程 外部排序指的在排序过程中尚需对外村进行访问的排序过程 123456789101112131415161718192021222324252627282930313233343536373839404142/** * 交换数组元素 * * @param array * @param i * @param j */public static void swap(int[] array, int i, int j) &#123; int tmp = array[i]; array[i] = array[j]; array[j] = tmp;&#125;/** * 交换数组元素 * 用了一个巧妙的方法，不需要临时变量 * 不足之处，就是当数组中的arr[a]+arr[b]的值大于int的最大值时，计算结果会溢出。导致arr[a]和arr[b]的值错误。 * @param arr * @param a * @param b */public static void swap(int[] arr, int a, int b)&#123; arr[a] = arr[a] + arr[b]; arr[b] = arr[a] - arr[b]; arr[a] = arr[a] - arr[b];&#125;/** * 交换数组元素 * * @param arr * @param a * @param b */public void swap(int[] arr, int a, int b) &#123; arr[a] ^= arr[b]; arr[b] ^= arr[a]; arr[a] ^= arr[b];&#125; 一、 冒泡排序(Bubble Sort) 冒泡排序( Bubble Sort )是一种交换排序，基本思想是：两两比较相邻记录的关键字，如果反序则交换，直到没有反序的记录为止。 在最好的情况下，也就是数列本身是排好序的，需要进行 n - 1 次比较；在最坏的情况下，也就是数列本身是逆序的，需要进行 n(n-1)/2 次比较。因此冒泡排序总的时间复杂度是 O(n^2)。 最坏时间复杂度 O(n^{2}) 最优时间复杂度 O(n) 平均时间复杂度 O(n^{2}) 空间复杂度 总共O(n)，需要辅助空间O(1) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165/** * 冒泡排序&lt;br&gt; * 这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到 数列 的顶端，故名。&lt;br&gt; * &lt;p&gt; * 冒泡排序最好的时间复杂度为O(n)，最坏时间复杂度为O(n^2)&lt;br&gt; * 综上，因此冒泡排序总的平均时间复杂度为 O(n^2) * * References * https://www.cnblogs.com/melon-h/archive/2012/09/20/2694941.html * * @author: weikeqin.cn@gmail.com */public class BubbleSort &#123; /** * 冒泡排序 * * &lt;pre&gt; * * 在数列里把小的往上浮就相当于把小的往(数组)前放 * * 基本思想：在要排序的一组数中，对当前还未排好序的范围内的全部数，自上而下对相邻的两个数依次进行比较和调整， 让较大的数往上浮，较小的往下。 * 即：每当两相邻的数比较后发现它们的排序与排序要求相反时，就将它们互换。 * * 在这个方法里(未改进)，冒泡排序最好的时间复杂度为O(n^2) 冒泡排序的最坏时间复杂度为O(n^2) * * 原理：假如有一个长度为n的数组，要循环n-1次 每次循环把最大值换到第一个位置 * * 例子： 28276130 长度n=9;要循环8次 * 第一次循环完 变成 2 7 2 7 6 1 3 0 8 * 第二次循环完 变成 2 2 7 6 1 3 0 7 8 * 第三次循环完 变成 2 2 6 1 3 0 7 7 8 * 第四次循环完 变成 2 2 1 3 0 6 7 7 8 * 第五次循环完 变成 2 1 2 0 3 6 7 7 8 * 第六次循环完 变成 1 2 0 2 3 6 7 7 8 * 第七次循环完 变成 1 0 2 2 3 6 7 7 8 * 第八次循环完 变成 0 1 2 2 3 6 7 7 8 * &lt;/pre&gt; * * @param array */ public static void bubbleSort(int[] array) &#123; int temp = 0; //int count = 0; int arrayLength = array.length; // 比多少趟 int x = arrayLength - 1; for (int i = 0; i &lt; x; i++) &#123; // 一趟比多少次 int y = arrayLength - i - 1; for (int j = 0; j &lt; y; j++) &#123; // 相邻元素比较，小的往前放，大的往后放 if (array[j] &gt; array[j + 1]) &#123; swap(array, j, j+1); &#125; &#125; // end for print(array); &#125; // end for &#125; // end method /** * 交换数组元素 * * @param array * @param big * @param small */ public static void swap(int[] array, int big, int small) &#123; int tmp = 0; tmp = array[big]; array[big] = array[small]; array[small] = tmp; &#125; /** * 改进版的冒泡排序&lt;br&gt; * * 改进版的冒泡排序最好的时间复杂度为O(n) 冒泡排序的最坏时间复杂度为O(n^2) * * @param array */ public static void bubbleSortImprove(int[] array) &#123; int temp = 0; int arrayLength = array.length; // 比多少趟 int x = arrayLength - 1; boolean didSwap = false; for (int i = 0; i &lt; x; i++) &#123; // 一趟比多少次 int y = arrayLength - i - 1; for (int j = 0; j &lt; y; j++) &#123; // 相邻元素比较，小的往前放，大的往后放 if (array[j] &gt; array[j + 1]) &#123; swap(array, j, j+1); didSwap = true; &#125; &#125; // end for //print(array); if(didSwap == false)&#123; return; &#125; &#125; // end for &#125; // end method /** * * @param array */ private static void print(int[] array) &#123; StringBuilder sb = new StringBuilder(); int length = array.length; for (int i = 0; i &lt; length; i++) &#123; sb.append(array[i]); sb.append(" "); &#125; System.out.println(sb.toString()); &#125; /** * * @param args * @author weikeqin.cn@gmail.com * @date 2018/6/2 15:35 */ public static void main(String[] args) &#123; int[] a = null; a = new int[]&#123;2, 7, 8, 2, 7, 6, 1, 3, 0&#125;; a = new int[]&#123;1, 2, 3, 4, 5, 6, 7, 8, 9&#125;; a = new int[]&#123;9, 8, 7, 6, 5, 4, 3, 2, 1&#125;;// int count = 100000;// a = new int[count];// for(int i = 0; i &lt; count; i++)&#123;// // 倒序// //a[i] = count - i;// // 正序// a[i] = i;// &#125; System.out.println("---------------排序前--------------------"); print(a); System.out.println("--------------开始排序--------------------"); long start = System.currentTimeMillis(); // 冒泡排序 bubbleSortImprove(a); long end = System.currentTimeMillis(); System.out.println("共花费" + 1.0 * (end - start) / 1000 + "s"); System.out.println("---------------排序后---------------------"); print(a); &#125;&#125; 二、直接插入排序 (Straight Insertion Sort) 直接插入排序(Straight Insertion Sort)的基本思想是：把n个待排序的元素看成为一个有序表和一个无序表。开始时有序表中只包含1个元素，无序表中包含有n-1个元素，排序过程中每次从无序表中取出第一个元素，将它插入到有序表中的适当位置，使之成为新的有序表，重复n-1次可完成排序过程。 插入排序的时间复杂度为 O(n^2)，比冒泡法和选择排序的性能要更好一些，是稳定的排序方法，适用于数量较少的排序。 最坏时间复杂度 O(n^{2}) 最优时间复杂度 O(n) 平均时间复杂度 O(n^{2}) 空间复杂度 总共 O(n)，需要辅助空间 O(1) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169/** * 直接插入排序 &lt;br&gt; * * References * https://www.cnblogs.com/chengxiao/p/6103002.html * https://www.cnblogs.com/skywang12345/p/3596881.html * * @author: weikeqin.cn@gmail.com */public class StraightInsertionSort &#123; /** * 直接插入排序&lt;br&gt; * * &lt;pre&gt; * 基本思想： 把n个待排序的元素看成为一个有序表和一个无序表。 * 开始时有序表中只包含1个元素，无序表中包含有n-1个元素， * 排序过程中每次从无序表中取出第一个元素，将它插入到有序表中的适当位置， * 使之成为新的有序表，重复n-1次可完成排序过程。 * * 数据 2 7 8 2 7 6 1 3 0 * 假设有序表里有一个元素，是数组的第一个元素，其他的元素在无序表里，逐个插入 * 插入时如果相等，要插入的元素放到当前元素的后面 * * 第一次插入 2 7 * 第二次插入 2 7 8 * 第三次插入 2 2 7 8 * 第四次插入 2 2 7 7 8 * 第五次插入 2 2 6 7 7 8 * 第六次插入 1 2 2 6 7 7 8 * 第七次插入 1 2 2 3 6 7 7 8 * 第八次插入 0 1 2 2 3 6 7 7 8 * * 最好情况要比较n-1次，最差情况要比较 n(n-1)/2 * 时间复杂度是 O(n^2) * * &lt;/pre&gt; * * @param array * @author weikeqin.cn@gmail.com */ public static int[] starightInsertionSort(int[] array) &#123; int length = array.length; for (int i = 1; i &lt; length; i++) &#123; int j = i; // 把 选择数组下标 和 插入数据时元素后移 合二为一 while (j &gt; 0 &amp;&amp; array[j] &lt; array[j - 1]) &#123; swap(array, j, j - 1); j--; &#125; &#125; return array; &#125; /** * 直接插入排序&lt;br&gt; * 第一次自己实现的 * * @param array * @author weikeqin.cn@gmail.com */ public static int[] starightInsertionSortMy(int[] array) &#123; int length = array.length; // 要插入的位置 int index = 0; int tmp = 0; // 以数组第一个元素作为有序表，其他元素作为无序表 // 最小的放前面，大的放后面，如果等于，等于的全部放到后面 for (int i = 1; i &lt; length; i++) &#123; if (array[i] &lt; array[i - 1]) &#123; System.out.println("第" + i + "个元素" + array[i] + "小于第" + (i - 1) + "个元素" + array[i - 1] + " 准备插入。"); // 寻找插入位置，从小到大找 if (array[i] &lt; array[0]) &#123; index = 0; &#125; else &#123; for (int j = 0; j &lt; i; j++) &#123; if (array[i] &gt;= array[j] &amp;&amp; array[i] &lt; array[j + 1]) &#123; // 要插入到j+1个位置 index = j + 1; &#125; &#125; &#125; System.out.println("第" + i + "个元素" + array[i] + "准备插入到第" + index + "个位置。"); System.out.println("第" + index + "个元素到第" + (i - 1) + "个元素全部往右移。"); tmp = array[i]; // 插入有序表对应位置，index位置及以后的全部后移 for (int k = i - 1; k &gt;= index; k--) &#123; System.out.println("第" + k + "个元素" + array[k] + "移动到到第" + (k + 1) + "个位置。"); array[k + 1] = array[k]; &#125; array[index] = tmp; &#125; else &#123; // 如果 array[i] &gt;= array[i-1]，array[i]插入到array[i-1]后面，也就是位置不变 System.out.println("第" + i + "个元素" + array[i] + "大于第" + (i - 1) + "个元素" + array[i - 1] + " 不用插入。"); &#125; // 打印当前数组 print(array); &#125; return array; &#125; /** * 交换数组元素 * * @param array * @param a * @param b */ public static void swapUseCpu(int[] array, int a, int b) &#123; array[a] = array[a] + array[b]; array[b] = array[a] - array[b]; array[a] = array[a] - array[b]; &#125; /** * 交换数组元素 * * @param array * @param a * @param b */ public static void swap(int[] array, int a, int b) &#123; int tmp = 0; tmp = array[a]; array[a] = array[b]; array[b] = tmp; &#125; /** * @param array */ private static void print(int[] array) &#123; StringBuilder sb = new StringBuilder(); int length = array.length; for (int i = 0; i &lt; length; i++) &#123; sb.append(array[i]); sb.append(" "); &#125; System.out.println(sb.toString()); &#125; /** * @param args */ public static void main(String[] args) &#123; int[] a = null; a = new int[]&#123;2, 7, 8, 2, 7, 6, 1, 3, 0&#125;; //a = new int[]&#123;1, 2, 3, 4, 5, 6, 7, 8, 9&#125;; //a = new int[]&#123;9, 8, 7, 6, 5, 4, 3, 2, 1&#125;; System.out.println("---------------排序前--------------------"); print(a); System.out.println("--------------开始排序--------------------"); a = starightInsertionSort(a); System.out.println("---------------排序后---------------------"); print(a); &#125;&#125; 三、简单选择排序(Selection sort) 选择排序的基本思想是：每一趟在n-i+1 (i=1,2,3…,n-1)个记录中选择关键字最小的记录 作为有序序列中第i个记录。 简单选择排序：通过n-i次关键字之间的比较，从n-i+1 个记录中选择关键字最小的记录，并和第i(1&lt;=i&lt;=n)个记录交换之。 尽管时间复杂度与冒泡排序同为O(n^2),但简单选择排序的性能要略优于冒泡排序。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118/** * 简单选择排序 * * https://www.cnblogs.com/chengxiao/p/6103002.html * * @author: weikeqin.cn@gmail.com */public class SelectionSort &#123; /** * 简单选择排序&lt;br&gt; * * &lt;pre&gt; * 基本思想: 为每一趟从待排序的数据元素中选择最小（或最大）的一个元素作为首元素， * 直到所有元素排完为止。 * * 原始数据 2 7 8 2 7 6 1 3 0 * 第一次交换 0 7 8 2 7 6 1 3 2 * 第二次交换 0 1 8 2 7 6 7 3 2 * 第三次交换 0 1 2 8 7 6 7 3 2 * 第四次交换 0 1 2 2 7 6 7 3 8 * 第五次交换 0 1 2 2 3 6 7 7 8 * 第六次交换 0 1 2 2 3 6 7 7 8 * 第七次交换 0 1 2 2 3 6 7 7 8 * 第八次交换 0 1 2 2 3 6 7 7 8 * 第九次交换 0 1 2 2 3 6 7 7 8 * * 无论数据如何排列，所需比较次数都为 n(n-1)/2 * 时间复杂度是O(n^2) * 简单选择排序是不稳定排序。 * * &lt;/pre&gt; * * @author weikeqin.cn@gmail.com */ public static int[] selectionSort(int[] array)&#123; int length = array.length; int tmp = 0; for(int i = 0; i &lt; length; i++)&#123; // int index = selectMinKey(array, i, length); System.out.println("数组下标："+index + " 对应元素："+array[index]); // 交换值 if(i != index)&#123; tmp = array[i]; array[i] = array[index]; array[index] = tmp; &#125; print(array); &#125; return array; &#125; /** * 选出数组给定范围最小值对应的数组下标 * * @param array * @param begin * @param length */ public static int selectMinKey(int[] array, int begin, int length) &#123; int minKey = 0; int index = 0; boolean first = true; for(int j = begin; j &lt; length; j++)&#123; if(first)&#123; minKey = array[j]; index = j; first = false; &#125; if(array[j] &lt; minKey)&#123; minKey = array[j]; index = j; &#125; &#125; return index; &#125; /** * @param array */ private static void print(int[] array) &#123; StringBuilder sb = new StringBuilder(); int length = array.length; for (int i = 0; i &lt; length; i++) &#123; sb.append(array[i]); sb.append(" "); &#125; System.out.println(sb.toString()); &#125; /** * @param args */ public static void main(String[] args) &#123; int[] a = null; a = new int[]&#123;2, 7, 8, 2, 7, 6, 1, 3, 0&#125;; //a = new int[]&#123;1, 2, 3, 4, 5, 6, 7, 8, 9&#125;; //a = new int[]&#123;9, 8, 7, 6, 5, 4, 3, 2, 1&#125;; System.out.println("---------------排序前--------------------"); print(a); System.out.println("--------------开始排序--------------------"); a = selectionSort(a); System.out.println("---------------排序后---------------------"); print(a); &#125;&#125; 四、希尔排序(Shell Sort) 希尔排序也是一种插入排序，它是简单插入排序经过改进之后的一个更高效的版本，也称为缩小增量排序，同时该算法是冲破O(n^2)的第一批算法之一。 基本思想：先将整个待排元素序列分割成若干子序列(由相隔某个”增量”的元素组成的)分别进行直接插入排序，然后依次缩减增量再进行排序，待整个序列中的元素基本有序(增量足够小)时，再对全体元素进行一次直接插入排序(增量为1)。 简单插入排序很循规蹈矩，不管数组分布是怎么样的，依然一步一步的对元素进行比较，移动，插入，比如[5,4,3,2,1,0]这种倒序序列，数组末端的0要回到首位置很是费劲，比较和移动元素均需n-1次。而希尔排序在数组中采用跳跃式分组的策略，通过某个增量将数组元素划分为若干组，然后分组进行插入排序，随后逐步缩小增量，继续按组进行插入排序操作，直至增量为1。希尔排序通过这种策略使得整个数组在初始阶段达到从宏观上看基本有序，小的基本在前，大的基本在后。然后缩小增量，到增量为1时，其实多数情况下只需微调即可，不会涉及过多的数据移动。 当增量序列为 dlta[k] = (2^(t-k+1)) - 1 时，其时间复杂度为O(n^(3/2))，其中t为排序趟数， 1 &lt;= k &lt;= t &lt; log2^(n+1) 当n在特定范围内，希尔排序所需的比较和移动次数约为n^1.3，当n -&gt; 无穷， 可减少到n(log2^n)^(2^[2]) 。 最优时间复杂度‎: ‎O(n) 最坏时间复杂度‎: ‎根据步长序列的不同而不同。 平均时间复杂度‎: ‎根据步长序列的不同而不同。 空间复杂度‎: ‎O(n) 增量(步长)序列 最坏情况下复杂度 n/2^i O(n^2) 2^k - 1 O(n^(3/2)) 2^i * 3^j O(nlog2^n) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220/** * 希尔排序 * &lt;p&gt; * References * https://zh.wikipedia.org/wiki/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F * http://www.cnblogs.com/chengxiao/p/6104371.html * http://bubkoo.com/2014/01/15/sort-algorithm/shell-sort/ * https://blog.csdn.net/morewindows/article/details/6668714 * * @author: weikeqin.cn@gmail.com */public class ShellSort &#123; /** * 希尔排序 针对有序序列在插入时采用交换法 * &lt;pre&gt; * * 基本思想：先将整个待排元素序列分割成若干子序列(由相隔某个"增量"的元素组成的)分别进行直接插入排序， * 然后依次缩减增量再进行排序，待整个序列中的元素基本有序(增量足够小)时， * 再对全体元素进行一次直接插入排序(增量为1)。 * * 原始数据 2 7 8 2 7 6 1 3 0 * 第一趟排序结果 0 6 1 2 2 7 8 3 7 * 第二趟排序结果 0 1 2 2 3 6 7 7 8 * * 在希尔排序时，先对数据进行分组，然后对每组的数据做插入排序，然后合并多组的数据 * 在代码实现时，对每组做插入排序要稍微转一下弯 * * 希尔排序时增量(步长)选择是一个难题 * 当增量序列为 dlta[k] = (2^(t-k+1)) - 1 时，其时间复杂度为O(n^(3/2))，其中t为排序趟数， 1 &lt;= k &lt;= t &lt; log2^(n+1) * 当n在特定范围内，希尔排序所需的比较和移动次数约为n^1.3，当n -&gt; 无穷， 可减少到n(log2^n)^(2^[2]) 。 * * &lt;/pre&gt; * * @param arr * @author weikeqin.cn@gmail.com */ public static int[] shellSort(int[] arr) &#123; int gap = 1, i, j, len = arr.length; int temp; while (gap &lt; len / 3) &#123; // &lt;O(n^(3/2)) by Knuth,1973&gt;: 1, 4, 13, 40, 121, ... gap = gap * 3 + 1; &#125; // 循环趟 for (; gap &gt; 0; gap /= 3) &#123; // 插入排序 for (i = gap; i &lt; len; i++) &#123; temp = arr[i]; // 同一组循环比较，目的是把最小的放到该组最前面 for (j = i - gap; j &gt;= 0 &amp;&amp; arr[j] &gt; temp; j -= gap) &#123; arr[j + gap] = arr[j]; &#125; arr[j + gap] = temp; &#125; // end for print(arr); &#125; // end for return arr; &#125; /** * @author weikeqin.cn@gmail.com */ public static int[] shellSort2(int[] arr) &#123; int i, j, gap, len = arr.length; for (gap = len / 2; gap &gt; 0; gap /= 2) &#123; for (i = gap; i &lt; len; i++) &#123; for (j = i - gap; j &gt;= 0 &amp;&amp; arr[j] &gt; arr[j + gap]; j -= gap) &#123; swap(arr, arr[j], arr[j + gap]); &#125; &#125; &#125; return arr; &#125; /** * @author weikeqin.cn@gmail.com */ public static int[] shellSort3(int[] arr) &#123; int length = arr.length; for (int gap = length / 2; gap &gt; 0; gap /= 2) &#123; System.out.println("gap:" + gap); // 每一趟相当于插入排序，把每组的小的往前放 // 这个i从gap开始的目的是为了避免后面数组越界异常 // i从0开始也可以，但是试了就会发现，从0 ~ (gap-1) 全部数组越界，还要跳过，相当于i从gap开始 for (int i = gap; i &lt; length; i++) &#123; System.out.println("i:" + i); int j = i; // 可以把上面的for循环改成从0开始，发现 0 ~ (gap-1) 全部跳过 if (j - gap &lt; 0) &#123; System.out.println("比较 第" + j + "个元素 与 第" + (j - gap) + "个元素 数组越界， 跳过"); continue; &#125; System.out.println("比较 第" + j + "个元素 与 第" + (j - gap) + "个元素。"); while (j - gap &gt;= 0 &amp;&amp; arr[j] &lt; arr[j - gap]) &#123; System.out.println("交换 第" + j + "个元素:" + arr[j] + " 与 第" + (j - gap) + "个元素:" + arr[j - gap]); swap(arr, j, j - gap); // 同一组迭代比较，目的是把最小的放到该组最前面 j -= gap; print(arr); &#125; &#125; System.out.println("----一趟排序完，打印数组----"); print(arr); &#125; return arr; &#125; /** * 希尔排序 针对有序序列在插入时采用移动法。 * * @param arr */ public static int[] shellSort4(int[] arr) &#123; int length = arr.length; //增量gap，并逐步缩小增量 for (int gap = length / 2; gap &gt; 0; gap /= 2) &#123; //从第gap个元素，逐个对其所在组进行直接插入排序操作 for (int i = gap; i &lt; length; i++) &#123; int j = i; // 同一组的整体处理 int temp = arr[j]; if (arr[j] &lt; arr[j - gap]) &#123; while (j - gap &gt;= 0 &amp;&amp; temp &lt; arr[j - gap]) &#123; //移动法 arr[j] = arr[j - gap]; j -= gap; &#125; arr[j] = temp; &#125; &#125; // end for &#125; // end for return arr; &#125; /** * 交换数组元素 * * @param arr * @param a * @param b */ public static void swap(int[] arr, int a, int b) &#123; int tmp = 0; tmp = arr[a]; arr[a] = arr[b]; arr[b] = tmp; &#125; /** * 交换数组元素 * * @param arr * @param a * @param b */ public static void swapUseCpu(int[] arr, int a, int b) &#123; arr[a] = arr[a] + arr[b]; arr[b] = arr[a] - arr[b]; arr[a] = arr[a] - arr[b]; &#125; /** * @param arr */ private static void print(int[] arr) &#123; StringBuilder sb = new StringBuilder(); int length = arr.length; for (int i = 0; i &lt; length; i++) &#123; sb.append(arr[i]); sb.append(" "); &#125; System.out.println(sb.toString()); &#125; /** * @param args */ public static void main(String[] args) &#123; int[] a = null; a = new int[]&#123;2, 7, 8, 2, 7, 6, 1, 3, 0&#125;; //a = new int[]&#123;1, 2, 3, 4, 5, 6, 7, 8, 9&#125;; //a = new int[]&#123;9, 8, 7, 6, 5, 4, 3, 2, 1&#125;; //a = new int[]&#123;49, 38, 65, 97, 76, 13, 27, 49, 55, 4&#125;; //a = new int[]&#123;8, 9, 1, 7, 2, 3, 5, 4, 6, 0&#125;; //a = new int[]&#123;49, 38, 65, 97, 26, 13, 27, 49, 55, 4&#125;; System.out.println("---------------排序前--------------------"); print(a); System.out.println("--------------开始排序--------------------"); a = shellSort(a); System.out.println("---------------排序后---------------------"); print(a); &#125;&#125; 五、归并排序(Merge sort) 归并排序（英语：Merge sort，或mergesort），是创建在归并操作上的一种有效的排序算法，效率为 O(nlogn)（大O符号）。1945年由约翰·冯·诺伊曼首次提出。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用，且各层分治递归可以同时进行。 归并排序(Merge sort) 是一种分治算法，是建立在归并操作上的一种有效的排序算法。常用的 2 路归并排序假设初始序列有 n 个记录，可以看成是 n 个长度为 1 的子序列，进行两两归并，可以得到 n / 2 个长度为 2 或 1 的子序列；再两两归并，**，直到得到一个长度为 n 的有序序列为止。 归并排序的时间复杂度是 O(nlogn)，是一种效率高且稳定的算法。 （1）稳定性 归并排序是一种稳定的排序。（2）存储结构要求 可用顺序存储结构。也易于在链表上实现。（3）时间复杂度 对长度为n的文件，需进行 趟二路归并，每趟归并的时间为O(n)，故其时间复杂度无论是在最好情况下还是在最坏情况下均是O(nlgn)。（4）空间复杂度 需要一个辅助向量来暂存两有序子文件归并的结果，故其辅助空间复杂度为O(n) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103import org.junit.Test;import java.util.Arrays;/** * @author: weikeqin.cn@gmail.com * @date: 2018-06-03 19:52 */public class MergeSort &#123; /** * 归并排序 迭代法 * * @author weikeqin.cn@gmail.com */ public static int[] mergeSortBottomUp(int[] arr) &#123; int len = arr.length; int[] result = new int[len]; int block, start; // 原版代码的迭代次数少了一次，没有考虑到奇数列数组的情况 for (block = 1; block &lt; len; block *= 2) &#123; for (start = 0; start &lt; len; start += 2 * block) &#123; int low = start; int mid = (start + block) &lt; len ? (start + block) : len; int high = (start + 2 * block) &lt; len ? (start + 2 * block) : len; //两个块的起始下标及结束下标 int start1 = low, end1 = mid; int start2 = mid, end2 = high; //开始对两个block进行归并排序 while (start1 &lt; end1 &amp;&amp; start2 &lt; end2) &#123; result[low++] = arr[start1] &lt; arr[start2] ? arr[start1++] : arr[start2++]; &#125; while (start1 &lt; end1) &#123; result[low++] = arr[start1++]; &#125; while (start2 &lt; end2) &#123; result[low++] = arr[start2++]; &#125; &#125; int[] temp = arr; arr = result; result = temp; &#125; result = arr; return result; &#125; /** * * @param arr 要排序的数组 * @param result 保存排序结果的数组 * @param start 开始位置 * @param end 结束位置 * * @author weikeqin.cn@gmail.com * @date 2018-06-03 21:00 */ static void merge_sort_recursive(int[] arr, int[] result, int start, int end) &#123; if (start &gt;= end) &#123; return; &#125; int len = end - start, mid = (len &gt;&gt; 1) + start; int start1 = start, end1 = mid; int start2 = mid + 1, end2 = end; merge_sort_recursive(arr, result, start1, end1); merge_sort_recursive(arr, result, start2, end2); int k = start; while (start1 &lt;= end1 &amp;&amp; start2 &lt;= end2) &#123; result[k++] = arr[start1] &lt; arr[start2] ? arr[start1++] : arr[start2++]; &#125; while (start1 &lt;= end1) &#123; result[k++] = arr[start1++]; &#125; while (start2 &lt;= end2) &#123; result[k++] = arr[start2++]; &#125; for (k = start; k &lt;= end; k++) &#123; arr[k] = result[k]; &#125; &#125; /** * 归并排序 递归版 * * @param arr * @author weikeqin.cn@gmail.com * @date 2018-06-03 20:41 */ public static void merge_sort(int[] arr) &#123; int len = arr.length; int[] result = new int[len]; merge_sort_recursive(arr, result, 0, len - 1); &#125;&#125; 堆排序 (Heap sort) 堆是具有下列性质的完全二叉树： 每个节点的值都大于或等于其左右孩子节点的值，称为大顶堆； 每个节点的值都小于或等于其左右孩子节点的值，称为小顶堆。 堆排序(Heap sort)是指利用堆这种数据结构所设计的一种排序算法。基本思想是把待排序的序列构造成一个大顶堆，此时序列的最大值就是队顶元素，把该元素放在最后，然后对剩下的 n - 1 个元素继续构造大顶堆，直到排序完成。堆排序的时间复杂度为 O(nlogn)，由于要构造堆，因此不适用于序列个数较少的情况。堆排序的实现需要解决的两个关键问题：(1) 将一个无序序列构成一个堆。(2) 输出堆顶元素后，调整剩余元素成为一个新堆。 复杂度分析 堆排序的运行时间主要耗费在初始构建堆和在重建堆时反复筛选上。在构建对的过程中，因为我们是完全二叉树从最下层最右边的非终端节点开始构建，将它与其孩子进行比较和若有必要的互换，对每个非终端节点来说，其实最多进行两次比较和互换操作，因此整个构建堆的时间复杂度为O(n)。 在正式排序时，第i次取堆顶记录重建堆需要用O(logi)的时间（完全二叉树的某个节点到根节点的距离为(log2(n))+1 ），并且需要取n-1次堆顶记录，因此，重建堆的时间复杂度为O(nlogn)。 所以总体来说，堆排序的时间复杂度为O(nlogn)，由于堆排序对原始记录的状态并不敏感，因此它无论是最好、最坏和平均时间复杂度均为O(nlogn)。这在性能上显然要远远好过于冒泡、简单选择、直接插入的时间复杂度了。 空间复杂度上，它只有一个用来交换的暂存单元，也非常的不错。不过由于记录的比较与交换是跳跃式进行的，因此堆排序也是一种不稳定的排序方法。 另外，由于出事构建堆所需要的比较次数比较多，因此，他并不适合待排序序列个数较少的情况。 初始化堆：将数列a[1…n]构造成最大堆。 交换数据：将a[1]和a[n]交换，使a[n]是a[1…n]中的最大值；然后将a[1…n-1]重新调整为最大堆。 接着，将a[1]和a[n-1]交换，使a[n-1]是a[1…n-1]中的最大值；然后将a[1…n-2]重新调整为最大值。 依次类推，直到整个数列都是有序的。 快速排序快速排序（Quicksort）是对冒泡排序的一种改进。基本思想是通过一趟排序将待排记录分割成独立的两部分，其中一部分的记录都比另一部分小，然后再分别对这两个部分进行快速排序，最终实现整个序列的排序。 快速排序的时间复杂度为 O(nlogn)，是一种不稳定的排序算法； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139import java.util.Arrays;/** * 快速排序&lt;br&gt; * 快速排序是对冒泡排序的一种改进 * * &lt;pre&gt; * 基本思想：选择一个基准元素，通常选择第一个元素或者最后一个元素， * 通过一趟扫描， 将待排序列分成两部分，一部分比基准元素小，一部分大于等于基准元素, * 此时基准元素在其排好序后的正确位置，然后再用同样的方法递归地排序划分的两部分。 * &lt;/pre&gt; * * @author weikeqin.cn@gmail.com */public class QuickSort &#123; /** * &lt;pre&gt; * * 基本思想：选择一个基准元素，通常选择第一个元素或者最后一个元素， * 通过一趟扫描， 将待排序列分成两部分，一部分比基准元素小，一部分大于等于基准元素, * 此时基准元素在其排好序后的正确位置，然后再用同样的方法递归地排序划分的两部分。 * * &lt;/pre&gt; * * @param arr * @param start * @param end */ public static void quickSort(int[] arr, int start, int end) &#123; if(end &lt;= start)&#123; return ; &#125; int i = start; int j = end; // 数组的第一个作为中轴/基准元素 int pivot = arr[i]; while (i &lt; j) &#123; // 比中轴大的在高端(右边) // 从右向左找小于x的数来填arr[i] while (i &lt; j &amp;&amp; arr[j] &gt;= pivot) &#123; j--; &#125; if(i &lt; j)&#123; // 比中轴/基准元素小的，移到低端/左边 //将arr[j]填到arr[i]中，arr[j]就形成了一个新的坑 arr[i] = arr[j]; &#125; // 比中轴元素小的在低端(左边) // 从左向右找大于或等于x的数来填arr[j] while (i &lt; j &amp;&amp; arr[i] &lt; pivot) &#123; i++; &#125; if(i &lt; j)&#123; // 比中轴大的，跳出while循环，移到高端/右边 //将arr[i]填到arr[j]中，arr[i]就形成了一个新的坑 arr[j] = arr[i]; &#125; &#125; // 中轴记录到尾 // 一次快速排序完，将pivot填到i这个坑中。 arr[i] = pivot; quickSort(arr, start, i - 1); quickSort(arr, i+1, end); &#125; /** * * @param arr * @param head * @param tail */ public static void qSort(int[] arr, int head, int tail) &#123; if (head &gt;= tail || arr == null || arr.length &lt;= 1) &#123; return; &#125; int i = head, j = tail, pivot = arr[(head + tail) / 2]; while (i &lt;= j) &#123; while (arr[i] &lt; pivot) &#123; ++i; &#125; while (arr[j] &gt; pivot) &#123; --j; &#125; if (i &lt; j) &#123; int t = arr[i]; arr[i] = arr[j]; arr[j] = t; ++i; --j; &#125; else if (i == j) &#123; ++i; &#125; &#125; qSort(arr, head, j); qSort(arr, i, tail); &#125; /** * @param arr */ private static void print(int[] arr) &#123; System.out.println(Arrays.toString(arr)); &#125; /** * * @param args */ public static void main(String[] args) &#123; int[] arr1 = &#123;2, 7, 8, 2, 7, 6, 1, 3, 0&#125;; int[] arr = &#123;49, 38, 65, 97, 76, 13, 27, 49&#125;; int[] a3 = &#123;49, 38, 65, 97, 76, 13, 27, 49, 78, 34, 12, 64, 5, 4, 62, 99, 98, 54, 56, 17, 18, 23, 34, 15, 35, 25, 53, 51&#125;; arr = new int[]&#123;1, 4, 8, 2, 55, 3, 4, 8, 6, 4, 0, 11, 34, 90, 23, 54, 77, 9, 2, 9, 4, 10&#125;; System.out.print("--------------排序前---------------------"); print(arr); // 快速排序 quickSort(arr, 0, arr.length - 1); //quickSortMy(arr, 0, arr.length - 1); System.out.print("--------------排序后---------------------"); print(arr); &#125;&#125; 八：计数排序计数排序(Counting sort)是一种稳定的排序算法。计数排序使用一个额外的数组C，其中第i个元素是待排序数组A中值等于i的元素的个数。然后根据数组C来将A中的元素排到正确的位置。 算法的步骤如下： 找出待排序的数组中最大和最小的元素统计数组中每个值为i的元素出现的次数，存入数组C的第i项对所有的计数累加（从C中的位置为1的元素开始，每一项和前一项相加）反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1 由于用来计数的数组C的长度取决于待排序数组中数据的范围（等于待排序数组的最大值与最小值的差加上1），这使得计数排序对于数据范围很大的数组，需要大量时间和内存。 九、桶排序桶排序 (Bucket sort)或所谓的箱排序，是一个排序算法，工作的原理是将数组分到有限数量的桶子里。每个桶子再个别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排序） 桶排序以下列程序进行： 设置一个定量的数组当作空桶子。寻访串行，并且把项目一个一个放到对应的桶子去。（hash）对每个不是空的桶子进行排序。从不是空的桶子里把项目再放回原来的串行中。 十、基数排序基数排序（英语：Radix sort）是一种非比较型整数排序算法，其原理是将整数按位数切割成不同的数字，然后按每个位数分别比较。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序也不是只能使用于整数。 它是这样实现的：将所有待比较数值（正整数）统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排序完成以后, 数列就变成一个有序序列。 基数排序的方式可以采用LSD（Least significant digital）或MSD（Most significant digital），LSD的排序方式由键值的最右边开始，而MSD则相反，由键值的最左边开始。 鸡尾酒排序鸡尾酒排序是冒泡排序的一种变形。先找到最小的数字，放在第一位，再找到最大的数字放在最后一位。然后再找到第二小的数字放到第二位，再找到第二大的数字放到倒数第二位。以此类推，直到完成排序。 鸡尾酒排序的时间复杂度为 O(n^2)。 梳排序梳排序和希尔排序很类似。希尔排序是在直接插入排序的基础上做的优化，而梳排序是在冒泡排序的基础上做的优化，也就是将相距某个增量 d 的记录组成一个子序列，通过冒泡排序使得这个子序列基本有序，然后减少增量继续排序。 梳排序的时间复杂度是 O(nlogn)。 比较标准时间复杂度 简单的说，就是运行一个程序需要多长时间。 时间频度 一个算法执行所耗费的时间。 一个算法中语句执行次数多，它花费时间就多。一个算法中的语句执行次数称为语句频度或时间频度。记为T(n)。 时间复杂度 在刚才提到的时间频度中，n称为问题的规模，当n不断变化时，时间频度T(n)也会不断变化。为此，我们引入时间复杂度概念。 一般情况下，算法中基本操作重复执行的次数是问题规模n的某个函数，用T(n)表示，若有某个辅助函数f(n),使得当n趋近于无穷大时，T(n)/f(n)的极限值为不等于零的常数，则称f(n)是T(n)的同数量级函数。记作T(n)=Ｏ(f(n)),称Ｏ(f(n)) 为算法的渐进时间复杂度，简称时间复杂度。 在各种不同算法中，若算法中语句执行次数为一个常数，则时间复杂度为O(1);另外，在时间频度不相同时，时间复杂度有可能相同，如T(n)=n2+3n+4与T(n)=4n2+2n+1它们的频度不同，但时间复杂度相同，都为O(n2)。 按数量级递增排列，常见的时间复杂度有：常数阶O(1),对数阶O(log2n),线性阶O(n), 线性对数阶O(nlog2n),平方阶O(n2)，立方阶O(n3),…， k次方阶O(nk),指数阶O(2n)。 空间复杂度 简单地说，就是运行一个程序需要多少空间(可能是内存，可能是外存)，是10MB，100MB，还是1G。 一个程序的空间复杂度是指运行完一个程序所需内存的大小。 利用程序的空间复杂度，可以对程序的运行所需要的内存多少有个预先估计。 一个程序执行时除了需要存储空间和存储本身所使用的指令、常数、变量和输入数据外，还需要一些对数据进行操作的工作单元和存储一些为现实计算所需信息的辅助空间： 1 固定部分。这部分空间的大小与输入/输出的数据的个数多少、数值无关。主要包括指令空间（即代码空间）、数据空间（常量、简单变量）等所占的空间。这部分属于静态空间。 2 可变空间，这部分空间的主要包括动态分配的空间，以及递归栈所需的空间等。这部分的空间大小与算法有关。 一个算法所需的存储空间用f(n)表示。S(n)=O(f(n)) 其中n为问题的规模，S(n)表示空间复杂度。 References[1] 《数据结构》(C语言版) 严蔚敏 吴伟民[2] 算法的时间复杂度和空间复杂度-总结[3] 算法的时间复杂度和空间复杂度[4] 时间复杂度和空间复杂度详解[5] 十种排序算法总结（冒泡、插入、选择、希尔、归并、堆、快速，计数，桶，基数）[6] Data Structure Visualizations[7] 九种排序算法的可视化及比较[8] 冒泡排序[9] 冒泡排序最佳情况的时间复杂度，为什么是O(n)[10] 直接插入排序[11] 插入排序[12] 图解排序算法(一)之3种简单排序(选择，冒泡，直接插入)[13] 希尔排序[14] 图解排序算法(二)之希尔排序[15] 白话经典算法系列之三 希尔排序的实现[16] 归并排序[17] 白话经典算法系列之五 归并排序的实现[18] “深入理解”—归并排序算法[19] 白话经典算法系列之六 快速排序 快速搞定[20] 快速排序[21] 常见排序算法 - 堆排序 (Heap Sort)[22] 排序算法 堆排序原理及Java实现[23] 堆排序介绍[24] Java排序算法(五)：堆排序]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[canal 笔记]]></title>
    <url>%2F2018%2F05%2F16%2Fcanal-notes%2F</url>
    <content type="text"><![CDATA[使用canal前需要准备以下几个内容 安装配置MySQL 1.1 安装 mysql， 1.2 配置 mysql binlog使用ROW模式 1.3 在MySQL添加对应的canal用户 1.4 检查canal用户生效 下载canal并配置2.1 下载canal2.2 配置 canal2.3 启动canal (需要JDK&gt;=1.6.25) (1) 配置MySQL(1.1) 安装MySQL 参考 https://dev.mysql.com/doc/refman/5.7/en/installing.html (1.2) 修改MySQL配置文件 canal的原理是基于mysql binlog技术，所以需要开启mysql的binlog写入功能，并且配置binlog模式为row. 1234[mysqld] log-bin=mysql-bin # 开启 binlogbinlog-format=ROW # 选择 ROW 模式server_id=1 # 配置 MySQL replaction ，不能和 canal 的 slaveId 重复 (1.3) MySQL添加canal用户并授权 canal的原理是模拟自己为mysql slave，所以需要mysql slave的相关权限 12345CREATE USER canal IDENTIFIED BY 'canal'; GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'canal'@'%'; FLUSH PRIVILEGES; (1.4) 校验用户对应权限 show master status ; 如果正常显示binlog，则没问题，如果提示 Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation ，则没有对应 REPLICATION CLIENT 权限 show slave status ; 如果提示 Access denied; you need (at least one of) the REPLICATION SLAVE privilege(s) for this operation ，则没有对应 REPLICATION SLAVE 权限 (2) 下载并启动canal 执行 ./bin/startup.sh 即可启动 (2.1) 下载canal 到 https://github.com/alibaba/canal/releases 选择合适的版本 下载 wget https://github.com/alibaba/canal/releases/download/canal-1.1.14/canal.deployer-1.1.14.tar.gz (2.2) 修改配置 修改 conf/example/instance.properties 以下只列出比较重要的配置 12345678910111213## mysql serverId 不能重复canal.instance.mysql.slaveId = 1234#position info，需要改成自己的数据库信息canal.instance.master.address = 127.0.0.1:3306 #username/password，需要改成自己的数据库信息canal.instance.dbUsername = canal canal.instance.dbPassword = canalcanal.instance.defaultDatabaseName = database_wkqcanal.instance.connectionCharset = UTF-8#table regex 需要监控的表 通过,分隔 也可以使用正则 .*\\..*canal.instance.filter.regex = table_wkq,table_2,table_3,# table black regexcanal.instance.filter.black.regex = (2.3) 启动canal 通过 sh bin/startup.sh 或者 ./bin/startup.sh 启动 启动后通过 jps -l 命令 可以看到 com.alibaba.otter.canal.deployer.CanalLauncher canal启动时canal.log canal.deployer-1.0.24/logs/canal/canal.log 123456789102018-07-23 20:27:46.449 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server.2018-07-23 20:27:46.625 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[10.0.62.130:11111]2018-07-23 20:27:47.576 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ......2018-07-23 20:27:47.721 [canal-instance-scan-0] INFO c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify start xxx1 successful.2018-07-23 20:27:47.802 [canal-instance-scan-0] INFO c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify start xxx2 successful.2018-07-23 20:27:47.862 [canal-instance-scan-0] INFO c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify start xxx3 successful.2018-07-23 20:27:47.921 [canal-instance-scan-0] INFO c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify start xxx4 successful.2018-07-23 20:27:47.987 [canal-instance-scan-0] INFO c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify start xxx5 successful.2018-07-23 20:27:48.044 [canal-instance-scan-0] INFO c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify start xxx6 successful.2018-07-23 20:27:48.094 [canal-instance-scan-0] INFO c.a.o.canal.deployer.monitor.SpringInstanceConfigMonitor - auto notify start xxx7 successful. (2.2) canal正常启动时instance对应的日志 canal.deployer-1.0.24/logs/example/example.log 1234562018-07-23 20:27:47.429 [canal-instance-scan-0] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties]2018-07-23 20:27:47.436 [canal-instance-scan-0] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [xxx/instance.properties]2018-07-23 20:27:47.444 [canal-instance-scan-0] WARN org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration, e.g. on the BeanWrapper/BeanFactory!2018-07-23 20:27:47.451 [canal-instance-scan-0] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-xxx 2018-07-23 20:27:47.453 [canal-instance-scan-0] INFO c.a.otter.canal.instance.core.AbstractCanalInstance - start successful....2018-07-23 20:27:47.666 [destination = xxx , address = /127.0.0.1:3306 , EventParser] WARN c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status 停止canal sh stop.sh 或 ./bin/stop.sh 1232018-07-23 21:45:08.241 [Thread-5] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## stop the canal server2018-07-23 21:45:08.296 [Thread-5] INFO com.alibaba.otter.canal.deployer.CanalController - ## stop the canal server[10.0.62.130:11111]2018-07-23 21:45:08.296 [Thread-5] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## canal server is down. (4) 程序中使用 以下代码仅作为示例 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba.otter&lt;/groupId&gt; &lt;artifactId&gt;canal.client&lt;/artifactId&gt; &lt;version&gt;1.1.4&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156import com.alibaba.otter.canal.client.CanalConnector;import com.alibaba.otter.canal.client.CanalConnectors;import com.alibaba.otter.canal.protocol.CanalEntry;import com.alibaba.otter.canal.protocol.Message;import lombok.extern.slf4j.Slf4j;import java.net.InetSocketAddress;import java.util.HashMap;import java.util.List;import java.util.Map;/** * CanalTest * * @author: weikeqin.cn@gmail.com * @date: 2020-05-30 08:26 **/@Slf4jpublic class CanalTest &#123; /** * @param args */ public static void main(String args[]) &#123; String canalHost = "127.0.0.1"; int canalPort = 11111; String destination = "example"; InetSocketAddress address = new InetSocketAddress(canalHost, canalPort); // 创建链接 CanalConnector connector = CanalConnectors.newSingleConnector(address, destination, "", ""); // connector = CanalConnectors.newClusterConnector(addresses, destination, "", ""); int batchSize = 1000; int emptyCount = 0; try &#123; // 链接对应的canal server connector.connect(); // 客户端订阅，不提交客户端filter，以服务端的filter为准 connector.subscribe(); // 回滚到未进行ack的地方，下次fetch的时候，可以从最后一个没有 ack 的地方开始拿 connector.rollback(); int totalEmptyCount = 12000000; // 退出条件 一般是 while true while (emptyCount &lt; totalEmptyCount) &#123; // 获取指定数量的数据 Message message = connector.getWithoutAck(batchSize); long batchId = message.getId(); int size = message.getEntries().size(); if (batchId == -1 || size == 0) &#123; emptyCount++; log.info("empty count : " + emptyCount); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; log.info("", e); &#125; &#125; else &#123; emptyCount = 0; log.info("message[batchId=&#123;&#125;,size=&#123;&#125;] ", batchId, size); // 消费 consumeMsg(message.getEntries()); &#125; // 提交确认 connector.ack(batchId); // 处理失败, 回滚数据 // connector.rollback(batchId); &#125; log.info("empty too many times, exit"); &#125; finally &#123; // 释放链接 connector.disconnect(); &#125; &#125; /** * 消费消息 * * @param entries */ private static void consumeMsg(List&lt;CanalEntry.Entry&gt; entries) &#123; // 这里只打印 printEntry(entries); // TODO 其它操作 &#125; /** * @param entrys */ private static void printEntry(List&lt;CanalEntry.Entry&gt; entrys) &#123; for (CanalEntry.Entry entry : entrys) &#123; if (entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONBEGIN || entry.getEntryType() == CanalEntry.EntryType.TRANSACTIONEND) &#123; continue; &#125; CanalEntry.RowChange rowChage = null; try &#123; rowChage = CanalEntry.RowChange.parseFrom(entry.getStoreValue()); &#125; catch (Exception e) &#123; throw new RuntimeException("ERROR ## parser of eromanga-event has an error , data:" + entry.toString(), e); &#125; CanalEntry.EventType eventType = rowChage.getEventType(); log.info(String.format("================ binlog[%s:%s] , name[%s,%s] , eventType : %s", entry.getHeader().getLogfileName(), entry.getHeader().getLogfileOffset(), entry.getHeader().getSchemaName(), entry.getHeader().getTableName(), eventType) ); for (CanalEntry.RowData rowData : rowChage.getRowDatasList()) &#123; if (eventType == CanalEntry.EventType.DELETE) &#123; printColumn(rowData.getBeforeColumnsList()); &#125; else if (eventType == CanalEntry.EventType.INSERT) &#123; printColumn(rowData.getAfterColumnsList()); &#125; else &#123; log.info("------- before"); printColumn(rowData.getBeforeColumnsList()); log.info("------- after"); printColumn(rowData.getAfterColumnsList()); &#125; &#125; &#125; &#125; /** * @param columns */ private static void printColumn(List&lt;CanalEntry.Column&gt; columns) &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); for (CanalEntry.Column column : columns) &#123; map.put(column.getName(), column.getValue()); //log.info(column.getName() + " : " + column.getValue() + " update=" + column.getUpdated()); &#125; log.info("&#123;&#125;", map); &#125;&#125; (5) canal复制原理 复制如何工作，整体上来说，复制有3个步骤： (1) master将改变记录到二进制日志(binary log)中（这些记录叫做二进制日志事件，binary log events）； (2) slave将master的binary log events复制到它的中继日志(relay log)中； (3) slave读取中继日志中的事件，将其重放到备库数据之上。 下图描述了复制的过程： (6) 遇到的问题(6.1) Error When doing Client Authentication:ErrorPacket1234567Caused by: java.io.IOException: connect /127.0.0.1:3306 failure:java.io.IOException: Error When doing Client Authentication:ErrorPacket [errorNumber=1045, fieldCount=-1, message=Access denied for user &apos;canal&apos;@&apos;localhost&apos; (using password: YES), sqlState=28000, sqlStateMarker=#] at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.negotiate(MysqlConnector.java:208) at com.alibaba.otter.canal.parse.driver.mysql.MysqlConnector.connect(MysqlConnector.java:71) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlConnection.connect(MysqlConnection.java:56) at com.alibaba.otter.canal.parse.inbound.mysql.MysqlEventParser.preDump(MysqlEventParser.java:86) at com.alibaba.otter.canal.parse.inbound.AbstractEventParser$3.run(AbstractEventParser.java:157) at java.lang.Thread.run(Thread.java:748) 原因 用户名密码不正确 (6.2) Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation1234ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:xx[com.alibaba.otter.canal.parse.exception.CanalParseException: command : &apos;show master status&apos; has an error!Caused by: java.io.IOException: ErrorPacket [errorNumber=1227, fieldCount=-1, message=Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation, sqlState=42000, sqlStateMarker=#] with command: show master status 用canal账户登录后发现可以查看对应数据库对应表的数据，但是 show master status 提示 Access denied; you need (at least one of) the SUPER, REPLICATION CLIENT privilege(s) for this operation 1、instance.properties配置文件里配置的用户没有REPLICATION权限 2、canal instance.properties 配置错误 3、配置文件里用户名密码不正确 4、MySQL对应用户不存在 5、MySQL配置不对 给canal用户对应的replication权限 grant replication client on *.* to &#39;canal&#39;@&#39;%&#39;; flush privileges (6.3) Access denied; you need (at least one of) the REPLICATION SLAVE privilege(s) for this operation12[destination = xxx , address = /127.0.0.1:3306 , EventParser] ERROR com.alibaba.otter.canal.common.alarm.LogAlarmHandler - destination:xx[java.io.IOException: Received error packet: errno = 1227, sqlstate = 42000 errmsg = Access denied; you need (at least one of) the REPLICATION SLAVE privilege(s) for this operation at com.alibaba.otter.canal.parse.inbound.mysql.dbsync.DirectLogFetcher.fetch(DirectLogFetcher.java:95) Access denied 没权限 需要给对应账户授权 REPLICATION SLAVE 常用于建立复制时所需要用到的用户权限，也就是slave server必须被master server授权具有该权限的用户，才能通过该用户复制。 并且”SHOW SLAVE HOSTS”这条命令和REPLICATION SLAVE权限有关，否则执行时会报错： REPLICATION CLIENT 不可用于建立复制，有该权限时，只是多了可以使用如”SHOW SLAVE STATUS”、”SHOW MASTER STATUS”等命令。 在5.6.6版本以后，也可以使用”SHOW BINARY LOGS”。 GRANT REPLICATION SLAVE ON *.* TO &#39;canal&#39;@&#39;%&#39; flush privileges (6.4) canal用了UseConcMarkSweepGC不能用JDK141234567Java HotSpot(TM) 64-Bit Server VM warning: Ignoring option PermSize; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Ignoring option MaxPermSize; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: Ignoring option UseConcMarkSweepGC; support was removed in 14.0Java HotSpot(TM) 64-Bit Server VM warning: Ignoring option CMSParallelRemarkEnabled; support was removed in 14.0Unrecognized VM option &apos;UseCMSCompactAtFullCollection&apos;Error: Could not create the Java Virtual Machine.Error: A fatal exception has occurred. Program will exit. (6.5) com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection refused123Exception in thread &quot;main&quot; com.alibaba.otter.canal.protocol.exception.CanalClientException: java.net.ConnectException: Connection refused at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.doConnect(SimpleCanalConnector.java:198) at com.alibaba.otter.canal.client.impl.SimpleCanalConnector.connect(SimpleCanalConnector.java:115) canal没启动 或者 canal挂了 配置被删了，检查对应 destination 和 instance.properties instance.properties 没配置 (7) canal-admin后台管理 canal-admin设计上是为canal提供整体配置管理、节点运维等面向运维的功能，提供相对友好的WebUI操作界面，方便更多用户快速和安全的操作 简单来说，canal-admin是一个后台维护系统，简化了配置canal的工作，提高了效率，终于不用到服务器上一个一个配了 访问地址 http://127.0.0.1:8089/ (7.1) canal-admin的核心模型主要有 instance，对应canal-server里的instance，一个最小的订阅mysql的队列 server，对应canal-server，一个server里可以包含多个instance 集群，对应一组canal-server，组合在一起面向高可用HA的运维 References[1] canal/wiki[2] canal-AdminGuide[3] ClientExample[4] Canal-Admin-QuickStart[5] Canal-Admin-Guide[6] canal配置使用[7] Mysql 普通账户授权replication client后登录失败问题[8] REPLICATION SLAVE 与 REPLICATION CLIENT 权限[9] 对replication slave,replication client的一点说明[10] MySQL 5.6 Reference Manual – 6.2.1 Privileges Provided by MySQL[11] SimpleCanalClientTest[12] ClusterCanalClientTest]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot error]]></title>
    <url>%2F2018%2F05%2F16%2Fspringboot-error-note%2F</url>
    <content type="text"><![CDATA[(1) maven.plugins:maven-compiler-plugin:3.8.0:compile failed missing: org/objectweb/asm/ClassVisitor 在使用 springboot 2.1.3 后，开始出现这个问题 123[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:compile (default-compile) on project model: Execution default-compile of goal org.apache.maven.plugins:maven-compiler-plugin:3.8.0:compile failed: Unable to load the mojo &apos;compile&apos; in the plugin &apos;org.apache.maven.plugins:maven-compiler-plugin:3.8.0&apos;. A required class ismissing: org/objectweb/asm/ClassVisitor 123456[INFO] --- maven-compiler-plugin:3.8.0:compile (default-compile) @ ---[WARNING] Error injecting: org.apache.maven.plugin.compiler.CompilerMojocom.google.inject.ProvisionException: Unable to provision, see the following errors:1) Error injecting constructor, java.lang.NoClassDefFoundError: org/objectweb/asm/ClassVisitor at org.codehaus.plexus.languages.java.jpms.LocationManager.&lt;init&gt;(Unknown Source) maven-compiler-plugin 版本问题，指定版本 方法一，在pluginManagement指定 123456789101112131415&lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.7.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt;&lt;/build&gt; 方法二，在每个模块里手动指定1234567891011&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;classifier&gt;exec&lt;/classifier&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; (2) java.lang.IllegalStateException: Ambiguous mapping. Cannot map ‘basicErrorController’ method1234567891011121314151617Error starting ApplicationContext. To display the conditions report re-run your application with &apos;debug&apos; enabled.2019-06-16 18:42:26.409 ERROR 16924 --- [ restartedMain] o.s.boot.SpringApplication : Application run failedorg.springframework.beans.factory.BeanCreationException: Error creating bean with name &apos;requestMappingHandlerMapping&apos; defined in class path resource [org/springframework/boot/autoconfigure/web/servlet/WebMvcAutoConfiguration$EnableWebMvcConfiguration.class]: Invocation of init method failed; nested exception is java.lang.IllegalStateException: Ambiguous mapping. Cannot map &apos;basicErrorController&apos; method public org.springframework.http.ResponseEntity&lt;java.util.Map&lt;java.lang.String, java.lang.Object&gt;&gt; org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.error(javax.servlet.http.HttpServletRequest)to &#123; /error&#125;: There is already &apos;indexController&apos; bean methodpublic java.lang.String edu.controller.IndexController.error() mapped. at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1762) ~[spring-beans-5.1.5.RELEASE.jar:5.1.5.RELEASE] ... at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49) [spring-boot-devtools-2.1.3.RELEASE.jar:2.1.3.RELEASE]Caused by: java.lang.IllegalStateException: Ambiguous mapping. Cannot map &apos;basicErrorController&apos; method public org.springframework.http.ResponseEntity&lt;java.util.Map&lt;java.lang.String, java.lang.Object&gt;&gt; org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.error(javax.servlet.http.HttpServletRequest)to &#123; /error&#125;: There is already &apos;indexController&apos; bean methodpublic java.lang.String edu.controller.IndexController.error() mapped. at org.springframework.web.servlet.handler.AbstractHandlerMethodMapping$MappingRegistry.assertUniqueMethodMapping(AbstractHandlerMethodMapping.java:618) ~[spring-webmvc-5.1.5.RELEASE.jar:5.1.5.RELEASE] ... ... 21 common frames omitted 少jar包，少 asm-commons-3.3.jar asm-3.3.jar asm-tree-3.3.jar 这几个jar包，对应pom如下。 可能是版本问题，换个低版本就不报错了。 forceupdate 更新jar包， 或者删除完整的.m2本地存储库(笨办法) 12345678910111213141516171819202122232425262728293031&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;!-- https://mvnrepository.com/artifact/asm/asm --&gt;&lt;dependency&gt; &lt;groupId&gt;asm&lt;/groupId&gt; &lt;artifactId&gt;asm&lt;/artifactId&gt; &lt;version&gt;3.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/asm/asm-commons --&gt;&lt;dependency&gt; &lt;groupId&gt;asm&lt;/groupId&gt; &lt;artifactId&gt;asm-commons&lt;/artifactId&gt; &lt;version&gt;3.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/asm/asm-tree --&gt;&lt;dependency&gt; &lt;groupId&gt;asm&lt;/groupId&gt; &lt;artifactId&gt;asm-tree&lt;/artifactId&gt; &lt;version&gt;3.3.1&lt;/version&gt;&lt;/dependency&gt; (3) This application has no explicit mapping for /error, so you are seeing this as a fallback.1234This application has no explicit mapping for /error, so you are seeing this as a fallback.There was an unexpected error (type=Not Found, status=404).No message available Controller里是不是忘记加 @Conterller 注解了 路径不对 (4) java.net.SocketException: Permission denied123456789101112131415162019-06-17 16:41:37.898 [main] ERROR [o.s.b.diagnostics.LoggingFailureAnalysisReporter] [42] - ***************************APPLICATION FAILED TO START***************************Description:The Tomcat connector configured to listen on port 80 failed to start. The port may already be in use or the connector may be misconfigured.Action:Verify the connector&apos;s configuration, identify and stop any process that&apos;s listening on port 80, or configure this application to listen on another port.2019-06-17 16:41:37.899 [main] INFO [o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext] [989] - Closing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@27ddd392: startup date [Mon Jun 17 16:41:32 CST 2019]; root of context hierarchy2019-06-17 16:41:37.900 [main] INFO [o.s.jmx.export.annotation.AnnotationMBeanExporter] [452] - Unregistering JMX-exposed beans on shutdown 1、端口被占用 2、没有对应权限 (5) Circular placeholder reference ‘’ in property definitions123Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [com.alibaba.druid.pool.DruidDataSource]: Factory method &apos;dataSource&apos; threw exception; nested exception is java.lang.IllegalArgumentException: Circular placeholder reference &apos;jdbc_url&apos; in property definitionsCaused by: java.lang.IllegalArgumentException: Circular placeholder reference &apos;jdbc_url&apos; in property definitions application.properties 里和 application-dev.properties 里的变量名一样，提示Circular placeholder reference ‘’ in property definitions 升级到 springboot v2.1.3.RELEASE 后出现这个问题 有两个办法 1、springboot 新版推荐使用 @jdbc_url@ 替代 ${jdbc_url} 修改前 jdbc_url=${jdbc_url} 修改后 jdbc_url=@jdbc_url@ 2、变量用不一样的名字 修改前 jdbc_url=${jdbc_url} 修改后 jdbc_url=${url} 启动报错(Circular placeholder reference) 使用SpringBoot1.4.0的一个坑 Maven resource filtering not working - because of spring boot dependency duplicate (6) Failed to create assembly:Error creating assembly archive asm: Problem creating zip:Execution exception1Failed to execute goalorg.apache.maven.plugins:maven-assembly-plugin:2.5.5:single(make-assembly) on project web: Failed to create assembly:Error creating assembly archive asm: Problem creating zip:Execution exception (and the archive is probably corrupt but Icould not delete it): Java heap space -&gt; [Help 1] 原因：maven编译时内存溢出导致 Java heap space 解决办法：1、使用自己安装的maven2、配置IDEA里Maven的配置 Settings -&gt; Build,Execution,Deployment -&gt; Build Tools -&gt; Maven -&gt; Importing VM options for importer -Xmx2048m3、JDK版本设置高一点 Problem creating zip: Execution exce ption (and the archive is probably corrupt but I could not delete it): Java heap space (7) Field in required a bean of type ‘’ that could not be found.123456789Field in required a bean of type &apos;&apos; that could not be found.The injection point has the following annotations: - @org.springframework.beans.factory.annotation.Autowired(required=true)Action:Consider defining a bean of type &apos;com.jdcar.wx.dao.dataserver.CarBrandSeriesDao&apos; in your configuration. 查看 scanBasePackge 配置是否正确，springboot默认只扫描配置包下的 查看 dao层 配置是否正确 查看 数据库 配置是否正确 查看 是否加了 @Autowired springboot 启动报错：required a bean of type ‘’ that could not be found (8) Class path contains multiple SLF4J bindings.1234567891011SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/export/App/lib/log4j-slf4j-impl-2.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/export/App/lib/slf4j-log4j12-1.7.24.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/home/agent/jmonitor/jmonitor/../lib/jmonitor-core.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [ch.qos.logback.classic.util.ContextSelectorStaticBinder]SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/export/App/lib/log4j-slf4j-impl-2.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/export/App/lib/slf4j-log4j12-1.7.24.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory] 用maven找到对应jar，exclusion 对应依赖 1234&lt;exclusion&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt;&lt;/exclusion&gt; (9) log4j:WARN No appenders could be found for logger (org.springframework.data.repository.config.RepositoryConfigurationDelegate).123log4j:WARN No appenders could be found for logger (org.springframework.data.repository.config.RepositoryConfigurationDelegate).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. (10) log4j:WARN No appenders could be found for logger (com.alibaba.druid.pool.DruidDataSource).123log4j:WARN No appenders could be found for logger (com.alibaba.druid.pool.DruidDataSource).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. http://logging.apache.org/log4j/1.2/faq.html#noconfig (11) An attempt was made to call the method javax.servlet.ServletContext.getClassLoader()Ljava/lang/ClassLoader; but it does not exist. Its class, javax.servlet.ServletContext, is available from the following locations12345678910111213141516171819202122Error starting ApplicationContext. To display the conditions report re-run your application with &apos;debug&apos; enabled.2020-02-21 20:51:18.989 ERROR[main]o.s.b.d.LoggingFailureAnalysisReporter.report|***************************APPLICATION FAILED TO START***************************Description:An attempt was made to call the method javax.servlet.ServletContext.getClassLoader()Ljava/lang/ClassLoader; but it does not exist. Its class, javax.servlet.ServletContext, is available from the following locations: jar:file:/export/App/lib/servlet-api-2.5-6.1.14.jar!/javax/servlet/ServletContext.class jar:file:/export/App/lib/javax.servlet-api-4.0.1.jar!/javax/servlet/ServletContext.classIt was loaded from the following location: file:/export/App/lib/servlet-api-2.5-6.1.14.jarAction:Correct the classpath of your application so that it contains a single, compatible version of javax.servlet.ServletContext 记一次jar包冲突导致项目启动失败的处理过程【java.lang.NoSuchMethodError:javax.servlet.ServletContext】 有2个jar包有 javax/servlet/ServletContext.class 类，去掉一个低版本的。 1234&lt;exclusion&gt; &lt;artifactId&gt;servlet-api-2.5&lt;/artifactId&gt; &lt;groupId&gt;org.mortbay.jetty&lt;/groupId&gt;&lt;/exclusion&gt; (11) java.lang.ClassNotFoundException: javax.servlet.annotation.WebServlet1234562020-02-21 21:59:26.787 ERROR [main] org.springframework.boot.SpringApplication.reportFailure | Application run failed org.springframework.beans.factory.BeanCreationException: Error creating bean with name &apos;servletComponentRegisteringPostProcessor&apos;: Bean instantiation via constructor failed; nested exception is java.lang.NoClassDefFoundError: javax/servlet/annotation/WebServletCaused by: java.lang.NoClassDefFoundError: javax/servlet/annotation/WebServletCaused by: java.lang.ClassNotFoundException: javax.servlet.annotation.WebServlet 添加jar包 1234&lt;dependency&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt;&lt;/dependency&gt; (12) A bean with that name has already been defined in file123456789101112132020-03-11 14:13:38.753 ERROR [main] org.springframework.boot.diagnostics.LoggingFailureAnalysisReporter.report | ***************************APPLICATION FAILED TO START***************************Description:The bean &apos;xxxImpl&apos;, defined in class path resource [spring-xxx-provider.xml], could not be registered. A bean with that name has already been defined in file [xxxImpl.class] and overriding is disabled.Action:Consider renaming one of the beans or enabling overriding by setting spring.main.allow-bean-definition-overriding=true 在 application.properties 文件里配置 spring.main.allow-bean-definition-overriding=true 后解决 Springboot高版本启动报错。需增加配置信息 References[1] 启动报错(Circular placeholder reference)[2] 使用SpringBoot1.4.0的一个坑[3] Maven resource filtering not working - because of spring boot dependency duplicate[4] Problem creating zip: Execution exce ption (and the archive is probably corrupt but I could not delete it): Java heap space[5] springboot 启动报错：required a bean of type ‘’ that could not be found[6] Springboot高版本启动报错。需增加配置信息]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 笔记]]></title>
    <url>%2F2018%2F05%2F10%2Felasticsearch-notes%2F</url>
    <content type="text"><![CDATA[Elasticsearch 是一个分布式的开源搜索和分析引擎，适用于所有类型的数据，包括文本、数字、地理空间、结构化和非结构化数据。 Elasticsearch 是一个实时的分布式搜索分析引擎，它被用作全文检索、结构化搜索、分析以及这三个功能的组合。 Elasticsearch 的用途是什么？ Elasticsearch 在速度和可扩展性方面都表现出色，而且还能够索引多种类型的内容，这意味着其可用于多种用例： 应用程序搜索 网站搜索 企业搜索 日志处理和分析 基础设施指标和容器监测 应用程序性能监测 地理空间数据分析和可视化 安全分析 业务分析 基础入门下载安装 ElasticSearch下载完解压就可以使用，很方便。 官网下载地址 https://www.elastic.co/downloads/ https://www.elastic.co/downloads/elasticsearch https://www.elastic.co/cn/downloads/elasticsearch https://www.elastic.co/downloads/past-releases/elasticsearch-5-6-3 https://www.elastic.co/downloads/past-releases/elasticsearch-6-6-2 https://www.elastic.co/downloads/past-releases#elasticsearch https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.5.1-linux-x86_64.tar.gz https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.5.1-darwin-x86_64.tar.gz https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.5.1-windows-x86_64.zip Linux下启动ElasticSearch ./bin/elasticsearch 是启动es ./bin/elasticsearch -d 是启动es守护进程，后台运行 使用 kill 命令关闭es sh elasticsearch -d 重启es 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273[wkq@VM_77_25_centos elasticsearch-6.6.2]$ ./bin/elasticsearch -d[wkq@VM_77_25_centos logs]$ pwd/home/wkq/software/elasticsearch-6.6.2/logs[wkq@VM_77_25_centos logs]$[wkq@VM_77_25_centos logs]$ lltotal 84-rw-rw-r-- 1 wkq wkq 63972 Jan 19 16:31 gc.log.0.current-rw-rw-r-- 1 wkq wkq 14615 Jan 19 16:31 my-application.log-rw-rw-r-- 1 wkq wkq 0 Jan 19 16:26 my-application_access.log-rw-rw-r-- 1 wkq wkq 0 Jan 19 16:26 my-application_audit.log-rw-rw-r-- 1 wkq wkq 0 Jan 19 16:26 my-application_deprecation.log-rw-rw-r-- 1 wkq wkq 0 Jan 19 16:26 my-application_index_indexing_slowlog.log-rw-rw-r-- 1 wkq wkq 0 Jan 19 16:26 my-application_index_search_slowlog.log[wkq@VM_77_25_centos logs]$ tail -1000 my-application.log[2020-01-19T16:30:45,880][INFO ][o.e.e.NodeEnvironment ] [node-1] using [1] data paths, mounts [[/ (rootfs)]], net usable_space [32.2gb], net total_space [49gb], types [rootfs][2020-01-19T16:30:45,883][INFO ][o.e.e.NodeEnvironment ] [node-1] heap size [1015.6mb], compressed ordinary object pointers [true][2020-01-19T16:30:45,885][INFO ][o.e.n.Node ] [node-1] node name [node-1], node ID [urmXtplyRmyt_LKCTC6_3w][2020-01-19T16:30:45,885][INFO ][o.e.n.Node ] [node-1] version[6.6.2], pid[16976], build[default/tar/3bd3e59/2019-03-06T15:16:26.864148Z], OS[Linux/3.10.0-957.21.3.el7.x86_64/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_172/25.172-b11][2020-01-19T16:30:45,886][INFO ][o.e.n.Node ] [node-1] JVM arguments [-Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Djava.io.tmpdir=/tmp/elasticsearch-7194610470468096547, -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -XX:+PrintGCDetails, -XX:+PrintGCDateStamps, -XX:+PrintTenuringDistribution, -XX:+PrintGCApplicationStoppedTime, -Xloggc:logs/gc.log, -XX:+UseGCLogFileRotation, -XX:NumberOfGCLogFiles=32, -XX:GCLogFileSize=64m, -Des.path.home=/home/wkq/software/elasticsearch-6.6.2, -Des.path.conf=/home/wkq/software/elasticsearch-6.6.2/config, -Des.distribution.flavor=default, -Des.distribution.type=tar][2020-01-19T16:30:51,895][INFO ][o.e.p.PluginsService ] [node-1] loaded module [aggs-matrix-stats][2020-01-19T16:30:51,895][INFO ][o.e.p.PluginsService ] [node-1] loaded module [analysis-common][2020-01-19T16:30:51,896][INFO ][o.e.p.PluginsService ] [node-1] loaded module [ingest-common][2020-01-19T16:30:51,896][INFO ][o.e.p.PluginsService ] [node-1] loaded module [lang-expression][2020-01-19T16:30:51,896][INFO ][o.e.p.PluginsService ] [node-1] loaded module [lang-mustache][2020-01-19T16:30:51,896][INFO ][o.e.p.PluginsService ] [node-1] loaded module [lang-painless][2020-01-19T16:30:51,896][INFO ][o.e.p.PluginsService ] [node-1] loaded module [mapper-extras][2020-01-19T16:30:51,896][INFO ][o.e.p.PluginsService ] [node-1] loaded module [parent-join][2020-01-19T16:30:51,896][INFO ][o.e.p.PluginsService ] [node-1] loaded module [percolator][2020-01-19T16:30:51,896][INFO ][o.e.p.PluginsService ] [node-1] loaded module [rank-eval][2020-01-19T16:30:51,896][INFO ][o.e.p.PluginsService ] [node-1] loaded module [reindex][2020-01-19T16:30:51,896][INFO ][o.e.p.PluginsService ] [node-1] loaded module [repository-url][2020-01-19T16:30:51,896][INFO ][o.e.p.PluginsService ] [node-1] loaded module [transport-netty4][2020-01-19T16:30:51,896][INFO ][o.e.p.PluginsService ] [node-1] loaded module [tribe][2020-01-19T16:30:51,896][INFO ][o.e.p.PluginsService ] [node-1] loaded module [x-pack-ccr][2020-01-19T16:30:51,896][INFO ][o.e.p.PluginsService ] [node-1] loaded module [x-pack-core][2020-01-19T16:30:51,897][INFO ][o.e.p.PluginsService ] [node-1] loaded module [x-pack-deprecation][2020-01-19T16:30:51,897][INFO ][o.e.p.PluginsService ] [node-1] loaded module [x-pack-graph][2020-01-19T16:30:51,897][INFO ][o.e.p.PluginsService ] [node-1] loaded module [x-pack-ilm][2020-01-19T16:30:51,897][INFO ][o.e.p.PluginsService ] [node-1] loaded module [x-pack-logstash][2020-01-19T16:30:51,897][INFO ][o.e.p.PluginsService ] [node-1] loaded module [x-pack-ml][2020-01-19T16:30:51,897][INFO ][o.e.p.PluginsService ] [node-1] loaded module [x-pack-monitoring][2020-01-19T16:30:51,897][INFO ][o.e.p.PluginsService ] [node-1] loaded module [x-pack-rollup][2020-01-19T16:30:51,897][INFO ][o.e.p.PluginsService ] [node-1] loaded module [x-pack-security][2020-01-19T16:30:51,897][INFO ][o.e.p.PluginsService ] [node-1] loaded module [x-pack-sql][2020-01-19T16:30:51,897][INFO ][o.e.p.PluginsService ] [node-1] loaded module [x-pack-upgrade][2020-01-19T16:30:51,897][INFO ][o.e.p.PluginsService ] [node-1] loaded module [x-pack-watcher][2020-01-19T16:30:51,898][INFO ][o.e.p.PluginsService ] [node-1] no plugins loaded[2020-01-19T16:31:02,727][INFO ][o.e.x.s.a.s.FileRolesStore] [node-1] parsed [0] roles from file [/home/wkq/software/elasticsearch-6.6.2/config/roles.yml][2020-01-19T16:31:04,226][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [node-1] [controller/17005] [Main.cc@109] controller (64 bit): Version 6.6.2 (Build 62531230b275d3) Copyright (c) 2019 Elasticsearch BV[2020-01-19T16:31:05,572][DEBUG][o.e.a.ActionModule ] [node-1] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security[2020-01-19T16:31:06,168][INFO ][o.e.d.DiscoveryModule ] [node-1] using discovery type [zen] and host providers [settings][2020-01-19T16:31:08,407][INFO ][o.e.n.Node ] [node-1] initialized[2020-01-19T16:31:08,407][INFO ][o.e.n.Node ] [node-1] starting ...[2020-01-19T16:31:08,870][INFO ][o.e.t.TransportService ] [node-1] publish_address &#123;127.0.0.1:9300&#125;, bound_addresses &#123;127.0.0.1:9300&#125;[2020-01-19T16:31:09,010][WARN ][o.e.b.BootstrapChecks ] [node-1] max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144][2020-01-19T16:31:12,176][INFO ][o.e.c.s.MasterService ] [node-1] zen-disco-elected-as-master ([0] nodes joined), reason: new_master &#123;node-1&#125;&#123;urmXtplyRmyt_LKCTC6_3w&#125;&#123;ZuGIzWmFQMmIFb8P4DIXMA&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;ml.machine_memory=1927528448, xpack.installed=true, ml.max_open_jobs=20, ml.enabled=true&#125;[2020-01-19T16:31:12,182][INFO ][o.e.c.s.ClusterApplierService] [node-1] new_master &#123;node-1&#125;&#123;urmXtplyRmyt_LKCTC6_3w&#125;&#123;ZuGIzWmFQMmIFb8P4DIXMA&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;ml.machine_memory=1927528448, xpack.installed=true, ml.max_open_jobs=20, ml.enabled=true&#125;, reason: apply cluster state (from master [master &#123;node-1&#125;&#123;urmXtplyRmyt_LKCTC6_3w&#125;&#123;ZuGIzWmFQMmIFb8P4DIXMA&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;ml.machine_memory=1927528448, xpack.installed=true, ml.max_open_jobs=20, ml.enabled=true&#125; committed version [1] source [zen-disco-elected-as-master ([0] nodes joined)]])[2020-01-19T16:31:12,330][INFO ][o.e.h.n.Netty4HttpServerTransport] [node-1] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;127.0.0.1:9200&#125;[2020-01-19T16:31:12,331][INFO ][o.e.n.Node ] [node-1] started[2020-01-19T16:31:12,974][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [node-1] Failed to clear cache for realms [[]][2020-01-19T16:31:13,083][INFO ][o.e.l.LicenseService ] [node-1] license [d8539dc2-6d7e-434b-90fc-dd78a104b531] mode [basic] - valid[2020-01-19T16:31:13,122][INFO ][o.e.g.GatewayService ] [node-1] recovered [0] indices into cluster_state[2020-01-19T16:39:47,664][INFO ][o.e.n.Node ] [node-1] stopping ...[2020-01-19T16:39:47,682][INFO ][o.e.x.w.WatcherService ] [node-1] stopping watch service, reason [shutdown initiated][2020-01-19T16:39:47,894][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [node-1] [controller/17005] [Main.cc@148] Ml controller exiting[2020-01-19T16:39:47,896][INFO ][o.e.x.m.p.NativeController] [node-1] Native controller process has stopped - no new native processes can be started[2020-01-19T16:39:47,907][INFO ][o.e.n.Node ] [node-1] stopped[2020-01-19T16:39:47,907][INFO ][o.e.n.Node ] [node-1] closing ...[2020-01-19T16:39:47,922][INFO ][o.e.n.Node ] [node-1] closed[wkq@VM_77_25_centos logs]$ 测试 Elasticsearch 是否启动成功 123456789101112131415161718[wkq@VM_77_25_centos ~]$ curl &apos;http://localhost:9200/?pretty&apos;&#123; &quot;name&quot; : &quot;node-1&quot;, &quot;cluster_name&quot; : &quot;my-application&quot;, &quot;cluster_uuid&quot; : &quot;NsxYKhI1Qw63MzaPKl34dA&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;6.6.2&quot;, &quot;build_flavor&quot; : &quot;default&quot;, &quot;build_type&quot; : &quot;tar&quot;, &quot;build_hash&quot; : &quot;3bd3e59&quot;, &quot;build_date&quot; : &quot;2019-03-06T15:16:26.864148Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;7.6.0&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; Windows下启动ElasticSearch12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758C:\ProfessionalSoftWare\ElasticSearch\elasticsearch-5.6.10λ cd bin\C:\ProfessionalSoftWare\ElasticSearch\elasticsearch-5.6.10\binλ dir 驱动器 C 中的卷是 disk-C 卷的序列号是 D6F0-5A56 C:\ProfessionalSoftWare\ElasticSearch\elasticsearch-5.6.10\bin 的目录2018-06-06 15:50 &lt;DIR&gt; .2018-06-06 15:50 &lt;DIR&gt; ..2018-06-06 15:45 8,075 elasticsearch2018-06-06 15:45 2,550 elasticsearch-keystore2018-06-06 15:45 744 elasticsearch-keystore.bat2018-06-06 15:45 2,540 elasticsearch-plugin2018-06-06 15:45 732 elasticsearch-plugin.bat2018-06-06 15:45 104,448 elasticsearch-service-mgr.exe2018-06-06 15:45 103,936 elasticsearch-service-x64.exe2018-06-06 15:45 80,896 elasticsearch-service-x86.exe2018-06-06 15:45 11,263 elasticsearch-service.bat2018-06-06 15:45 223 elasticsearch-systemd-pre-exec2018-06-06 15:45 2,514 elasticsearch-translog2018-06-06 15:45 1,436 elasticsearch-translog.bat2018-06-06 15:45 3,344 elasticsearch.bat2018-06-06 15:45 1,023 elasticsearch.in.bat2018-06-06 15:45 367 elasticsearch.in.sh 15 个文件 324,091 字节 2 个目录 88,870,948,864 可用字节C:\ProfessionalSoftWare\ElasticSearch\elasticsearch-5.6.10\binλ elasticsearch[2018-07-21T16:24:38,626][INFO ][o.e.n.Node ] [] initializing ...[2018-07-21T16:24:38,858][INFO ][o.e.e.NodeEnvironment ] [gjy4N2R] using [1] data paths, mounts [[disk-C (C:)]], net usable_space [82.7gb], net total_space [118.7gb], spins? [unknown], types [NTFS][2018-07-21T16:24:38,858][INFO ][o.e.e.NodeEnvironment ] [gjy4N2R] heap size [1.9gb], compressed ordinary object pointers [true][2018-07-21T16:24:38,862][INFO ][o.e.n.Node ] node name [gjy4N2R] derived from node ID [gjy4N2RCQ4mLxp1lxdyZ8w]; set [node.name] to override[2018-07-21T16:24:38,863][INFO ][o.e.n.Node ] version[5.6.10], pid[5412], build[b727a60/2018-06-06T15:48:34.860Z], OS[Windows 10/10.0/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_131/25.131-b11][2018-07-21T16:24:38,863][INFO ][o.e.n.Node ] JVM arguments [-Xms2g, -Xmx2g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Djdk.io.permissionsUseCanonicalPath=true, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j.skipJansi=true, -XX:+HeapDumpOnOutOfMemoryError, -Delasticsearch, -Des.path.home=C:\ProfessionalSoftWare\ElasticSearch\elasticsearch-5.6.10][2018-07-21T16:24:40,396][INFO ][o.e.p.PluginsService ] [gjy4N2R] loaded module [aggs-matrix-stats][2018-07-21T16:24:40,396][INFO ][o.e.p.PluginsService ] [gjy4N2R] loaded module [ingest-common][2018-07-21T16:24:40,397][INFO ][o.e.p.PluginsService ] [gjy4N2R] loaded module [lang-expression][2018-07-21T16:24:40,397][INFO ][o.e.p.PluginsService ] [gjy4N2R] loaded module [lang-groovy][2018-07-21T16:24:40,398][INFO ][o.e.p.PluginsService ] [gjy4N2R] loaded module [lang-mustache][2018-07-21T16:24:40,398][INFO ][o.e.p.PluginsService ] [gjy4N2R] loaded module [lang-painless][2018-07-21T16:24:40,398][INFO ][o.e.p.PluginsService ] [gjy4N2R] loaded module [parent-join][2018-07-21T16:24:40,399][INFO ][o.e.p.PluginsService ] [gjy4N2R] loaded module [percolator][2018-07-21T16:24:40,399][INFO ][o.e.p.PluginsService ] [gjy4N2R] loaded module [reindex][2018-07-21T16:24:40,399][INFO ][o.e.p.PluginsService ] [gjy4N2R] loaded module [transport-netty3][2018-07-21T16:24:40,400][INFO ][o.e.p.PluginsService ] [gjy4N2R] loaded module [transport-netty4][2018-07-21T16:24:40,401][INFO ][o.e.p.PluginsService ] [gjy4N2R] no plugins loaded[2018-07-21T16:24:42,923][INFO ][o.e.d.DiscoveryModule ] [gjy4N2R] using discovery type [zen][2018-07-21T16:24:43,538][INFO ][o.e.n.Node ] initialized[2018-07-21T16:24:43,538][INFO ][o.e.n.Node ] [gjy4N2R] starting ...[2018-07-21T16:24:44,237][INFO ][o.e.t.TransportService ] [gjy4N2R] publish_address &#123;127.0.0.1:9300&#125;, bound_addresses &#123;127.0.0.1:9300&#125;, &#123;[::1]:9300&#125;[2018-07-21T16:24:47,301][INFO ][o.e.c.s.ClusterService ] [gjy4N2R] new_master &#123;gjy4N2R&#125;&#123;gjy4N2RCQ4mLxp1lxdyZ8w&#125;&#123;8uMlHHkOS6aXWlfjDMz1uQ&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;, reason: zen-disco-elected-as-master ([0] nodes joined)[, ][2018-07-21T16:24:47,349][INFO ][o.e.g.GatewayService ] [gjy4N2R] recovered [0] indices into cluster_state[2018-07-21T16:24:47,723][INFO ][o.e.h.n.Netty4HttpServerTransport] [gjy4N2R] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;127.0.0.1:9200&#125;, &#123;[::1]:9200&#125;[2018-07-21T16:24:47,723][INFO ][o.e.n.Node ] [gjy4N2R] started 测试 Elasticsearch 是否启动成功，可以打开另一个终端，执行操作： curl &#39;http://localhost:9200/?pretty&#39; TIP：如果你是在 Windows 上面运行 Elasticsearch，你可以从 http://curl.haxx.se/download.html 中下载 cURL cURL 给你提供了一种将请求提交到 Elasticsearch 的便捷方式，并且安装 cURL 之后，你可以通过复制与粘贴去尝试许多例子。 123456789101112131415$ curl &apos;http://localhost:9200/?pretty&apos;&#123; &quot;name&quot; : &quot;gjy4N2R&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;a3A44fSNQFOKf2bv_ybVjw&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;5.6.10&quot;, &quot;build_hash&quot; : &quot;b727a60&quot;, &quot;build_date&quot; : &quot;2018-06-06T15:48:34.860Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.6.1&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; ElasticSearch 和 MySQL 结构对照 ES MySQLNode/Cluster ClusterIndex DatabaseType tableDocument row (一行)field field (一列) 123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; &quot;settings&quot;:&#123; &quot;index&quot;:&#123; &quot;number_of_shards&quot;:&quot;1&quot;, &quot;number_of_replicas&quot;:&quot;1&quot; &#125; &#125;, &quot;mappings&quot;:&#123; &quot;documents&quot;:&#123; &quot;properties&quot;:&#123; &quot;fact&quot;:&#123; &quot;search_analyzer&quot;:&quot;query_ansj&quot;, &quot;analyzer&quot;:&quot;index_ansj&quot;, &quot;type&quot;:&quot;text&quot; &#125;, &quot;criminals&quot;:&#123; &quot;search_analyzer&quot;:&quot;query_ansj&quot;, &quot;analyzer&quot;:&quot;index_ansj&quot;, &quot;type&quot;:&quot;text&quot; &#125;, &quot;punishOfMoney&quot;:&#123; &quot;type&quot;:&quot;integer&quot; &#125;, &quot;accusation&quot;:&#123; &quot;search_analyzer&quot;:&quot;query_ansj&quot;, &quot;analyzer&quot;:&quot;index_ansj&quot;, &quot;type&quot;:&quot;text&quot; &#125;, &quot;relevantArticles&quot;:&#123; &quot;type&quot;:&quot;integer&quot; &#125;, &quot;deathPenalty&quot;:&#123; &quot;type&quot;:&quot;boolean&quot; &#125;, &quot;lifeImprisonment&quot;:&#123; &quot;type&quot;:&quot;boolean&quot; &#125;, &quot;imprisonment&quot;:&#123; &quot;type&quot;:&quot;integer&quot; &#125; &#125; &#125; &#125;&#125; ES head 谷歌插件 References[1] Elasticsearch: 权威指南中文版[2] 全文搜索引擎 Elasticsearch 入门教程[3] Elastic Search快速上手（2）：将数据存入ES[4] ElasticSearch Office doc[5] Elasticsearch 5.6 官方英文文档[6] Elasticsearch 英文社区[7] Elasticsearch 中文社区[8] ElasticSearch入门[9] elasticsearch[10] Elasticearch索引mapping写入、查看、修改（head、kopf插件）[11] getting-started[12] getting-started[13] mapping-intro[14] Elasticsearch在windows上安装好了之后怎么使用？[15] what-is/elasticsearch[16] ElasticSearch教程]]></content>
      <categories>
        <category>elasticsearch</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式 常见面试题]]></title>
    <url>%2F2018%2F04%2F20%2Fdistributed-question%2F</url>
    <content type="text"><![CDATA[什么是CAP定理？说说CAP理论和BASE理论？什么是最终一致性？最终一致性实现方式？什么是一致性Hash？讲讲分布式事务？如何实现分布式锁？如何实现分布式 Session?如何保证消息的一致性?负载均衡的理解？正向代理和反向代理？CDN实现原理？怎么提升系统的QPS和吞吐？Dubbo的底层实现原理和机制？描述一个服务从发布到被消费的详细过程？分布式系统怎么做服务治理？消息中间件如何解决消息丢失问题？Dubbo的服务请求失败怎么处理？对分布式事务的理解？如何实现负载均衡,有哪些算法可以实现?Zookeeper的用途,选举的原理是什么?讲讲数据的垂直拆分水平拆分？zookeeper原理和适用场景？zookeeper watch机制？redis/zk节点宕机如何处理？分布式集群下如何做到唯一序列号？用过哪些MQ,怎么用的,和其他mq比较有什么优缺点,MQ的连接是线程安全的吗？MQ系统的数据如何保证不丢失？列举出能想到的数据库分库分表策略？]]></content>
      <categories>
        <category>distributed</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>interview</tag>
        <tag>distributed</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程 常见面试题]]></title>
    <url>%2F2018%2F04%2F20%2Fnetwork-question%2F</url>
    <content type="text"><![CDATA[TCP建立连接和断开连接的过程？HTTP协议的交互流程，HTTP和HTTPS的差异，SSL的交互流程？TCP的滑动窗口协议有什么用？HTTP协议都有哪些方法？Socket交互的基本流程？讲讲tcp协议（建连过程，慢启动，滑动窗口，七层模型）？webservice协议（wsdl/soap格式，与restt办议的区别）？说说Netty线程模型，什么是零拷贝？TCP三次握手、四次挥手？DNS解析过程？TCP如何保证数据的可靠传输的？]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis 常见面试题]]></title>
    <url>%2F2018%2F04%2F20%2Fredis-question%2F</url>
    <content type="text"><![CDATA[redis数据结构有哪些？Redis缓存穿透，缓存雪崩？如何使用Redis来实现分布式锁？Redis的并发竞争问题如何解决？Redis持久化的几种方式，优缺点是什么，怎么实现的？Redis的缓存失效策略？Redis集群，高可用，原理？Redis缓存分片？Redis的数据淘汰策略？redis队列应用场景？分布式使用场景（储存session）？]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>interview</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql 常见面试题]]></title>
    <url>%2F2018%2F04%2F20%2Fmysql-question%2F</url>
    <content type="text"><![CDATA[MySQL 有哪些存储引擎啊？都有什么区别？Float、Decimal 存储金额的区别？Datetime、Timestamp 存储时间的区别？Char、Varchar、Varbinary 存储字符的区别？对比一下B+树索引和 Hash索引？MySQL索引类型有？如何管理 MySQL索引？对Explain参数及重要参数的理解？索引利弊是什么及索引分类？聚簇索引和非聚簇索引的区别？B+tree 如何进行优化？索引遵循哪些原则？索引与锁有什么关系？还有什么其他的索引类型，各自索引有哪些优缺点？谈谈对Innodb事务的理解？说说数据库事务特点及潜在问题？什么是MySQL隔离级别？有多少种事务失效的场景，如何解决？一致性非锁定读和一致性锁定读是什么？Innodb如何解决幻读？讲讲Innodb行锁？死锁及监控是什么？自增长与锁 ，锁的算法，锁问题，锁升级是什么？乐观锁的线程如何做失败补偿？高并发场景（领红包）如何防止死锁，保证数据一致性？谈谈MySQL的锁并发？查询优化的基本思路是什么？说说MySQL读写分离、分库分表？表结构对性能有什么影响?浅谈索引优化？说说Sql优化的几点原则？MySQL表设计及规范？说说MySQL几种存储引擎应用场景？MySQL常用优化方式有哪些？MySQL常用监控？MySQL瓶颈分析？]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring 常见面试题]]></title>
    <url>%2F2018%2F04%2F20%2Fspring-question%2F</url>
    <content type="text"><![CDATA[为什么需要代理模式？讲讲静态代理模式的优点及其瓶颈？对Java 接口代理模式的实现原理的理解？如何使用 Java 反射实现动态代理？Java 接口代理模式的指定增强？谈谈对Cglib 类增强动态代理的实现？怎么理解面向切面编程的切面？讲解OOP与AOP的简单对比？讲解JDK 动态代理和 CGLIB 代理原理以及区别？讲解Spring 框架中基于 Schema 的 AOP 实现原理？讲解Spring 框架中如何基于 AOP 实现的事务管理？谈谈对控制反转的设计思想的理解？怎么理解 Spring IOC 容器？Spring IOC 怎么管理 Bean 之间的依赖关系，怎么避免循环依赖？对Spring IOC 容器的依赖注入的理解？说说对Spring IOC 的单例模式和高级特性？BeanFactory 和 FactoryBean 有什么区别？BeanFactory 和 ApplicationContext 又有什么不同？Spring 在 Bean 创建过程中是如何解决循环依赖的？谈谈Spring Bean 创建过程中的设计模式？]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>spring</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm 常见面试题]]></title>
    <url>%2F2018%2F04%2F20%2Fjvm-question%2F</url>
    <content type="text"><![CDATA[(1) Java 内存分配？ 1) Java虚拟机在执行Java的过程中会把管理的内存划分为若干个不同的数据区域。这些区域有各自的用途，以及创建和销毁的时间，有的区域随着虚拟机进程的启动而存在，而有的区域则依赖线程的启动和结束而创建和销毁。 程序计数器、java虚拟机栈、本地方法栈、堆、方法区、常量池 Java内存区域划分、内存分配原理 2) Java 内存分配全面浅析 (2) Java 堆的结构是什么样子的？ 站在垃圾收集器的角度来看，可以把内存分为新生代与老年代。 什么是堆中的永久代（Perm Gen space）?说说各个区域的作用？ 程序计数器 java虚拟机栈 本地方法栈 堆 方法区 常量池 Java 中会存在内存泄漏吗，简述一下？ 在 Java 中，内存泄漏就是存在一些被分配的对象，这些对象有下面两个特点，首先，这些对象是可达的，即在有向图中，存在通路可以与其相连（也就是说仍存在该内存对象的引用）；其次，这些对象是无用的，即程序以后不会再使用这些对象。如果对象满足这两个条件，这些对象就可以判定为 Java 中的内存泄漏，这些对象不会被 GC 所回收，然而它却占用内存。 Java 类加载过程？描述一下 JVM 加载 Class 文件的原理机制?什么是类加载器？类加载器有哪些？什么是tomcat类加载机制？类加载器双亲委派模型机制？什么是GC? 为什么要有 GC？简述一下Java 垃圾回收机制？如何判断一个对象是否存活？垃圾回收的优点和原理，并考虑 2 种回收机制？垃圾回收器的基本原理是什么？垃圾回收器可以马上回收内存吗？有什么办法主动通知虚拟机进行垃圾回收？深拷贝和浅拷贝？System.gc() 和 Runtime.gc() 会做些什么？什么是分布式垃圾回收（DGC）？它是如何工作的？串行（serial）收集器和吞吐量（throughput）收集器的区别是什么？在 Java 中，对象什么时候可以被垃圾回收？简述Minor GC 和 Major GC？Java 中垃圾收集的方法有哪些？讲讲你理解的性能评价及测试指标？常用的性能优化方式有哪些？说说分布式缓存和一致性哈希？同步与异步？阻塞与非阻塞？什么是GC调优？常见异步的手段有哪些？内存模型以及分区，每个区放什么一个对象从创建到销毁都是怎么在这些部分里存活和转移的内存的哪些部分会参与GC的回收，回收策略是什么Java的内存模型是怎么设计的，为什么要这么设计结合内存模型的设计谈谈volatile关键字的作用 深入Java虚拟机的内存结构Java底层字节码分析类加载机制详细讲解个内存区域垃圾回收机制深入讲解垃圾收集器JDK调优工具 1、什么是Java虚拟机？为什么Java被称作是“平台无关的编程语言”？ 2、Java代码是怎么运行的？ 3、Java虚拟机是如何加载Java类的? 4、JVM运行内存的分类 5、如何监控和诊断JVM堆内和堆外内存使用？ 6、Java四引用是什么？ 7、如何理解JVM内置的编译或GC日志？ 8、JVM的永久代中会发生垃圾回收么？ 9、Java中的两种异常类型是什么？他们有什么区别？ 10、JVM是如何实现同步的？ 11、Java内存模型是什么？ 12、即时编译器有哪些优化？ 13、在什么情况下重复读写操作会被优化？ 14、什么样的垃圾才被回收？ 15、什么时候会导致垃圾回收？ 16、如何利用JFR和JMC监控Java程序？ 17、如何利用Unsafe API 绕开 JVM的控制？ 18、如何利用字节码注入为已有代码加料？]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>interview</tag>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发 常见面试题]]></title>
    <url>%2F2018%2F04%2F20%2Fconcurrent-question%2F</url>
    <content type="text"><![CDATA[什么是多线程并发和并行？什么是线程安全问题？什么是共享变量的内存可见性问题？什么是Java中原子性操作？什么是Java中的CAS操作,AtomicLong实现原理？什么是Java指令重排序？Java中Synchronized关键字的内存语义是什么？Java中Volatile关键字的内存语义是什么？什么是伪共享,为何会出现，以及如何避免？什么是可重入锁、乐观锁、悲观锁、公平锁、非公平锁、独占锁、共享锁？讲讲ThreadLocal 的实现原理？ThreadLocal 作为变量的线程隔离方式，其内部是如何做的？说说InheritableThreadLocal 的实现原理？InheritableThreadLocal 是如何弥补 ThreadLocal 不支持继承的特性？CyclicBarrier内部的实现与 CountDownLatch 有何不同？随机数生成器 Random 类如何使用 CAS 算法保证多线程下新种子的唯一性？ThreadLocalRandom 是如何利用 ThreadLocal 的原理来解决 Random 的局限性？Spring 框架中如何使用 ThreadLocal 实现 request scope 作用域 Bean？并发包中锁的实现底层（对AQS的理解）？讲讲独占锁 ReentrantLock 原理？谈谈读写锁 ReentrantReadWriteLock 原理？StampedLock 锁原理的理解？谈下对基于链表的非阻塞无界队列 ConcurrentLinkedQueue 原理的理解？ConcurrentLinkedQueue 内部是如何使用 CAS 非阻塞算法来保证多线程下入队出队操作的线程安全？基于链表的阻塞队列 LinkedBlockingQueue 原理。阻塞队列LinkedBlockingQueue 内部是如何使用两个独占锁 ReentrantLock 以及对应的条件变量保证多线程先入队出队操作的线程安全？分析下JUC 中倒数计数器 CountDownLatch 的使用与原理？CountDownLatch 与线程的 Join 方法区别是什么？讲讲对JUC 中回环屏障 CyclicBarrier 的使用？CyclicBarrier内部的实现与 CountDownLatch 有何不同？Semaphore 的内部实现是怎样的？并发组件CopyOnWriteArrayList 是如何通过写时拷贝实现并发安全的 List？]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>concurrent</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j APOC 使用]]></title>
    <url>%2F2018%2F04%2F17%2Fneo4j-apoc-use%2F</url>
    <content type="text"><![CDATA[(1) 下载配置(1.1) 下载对应版本的apoc jar包 从github apoc各个版本下载地址下载对应版本的apoc jar包，并放到 $NEO4J_HOME/plugs 目录下 wget https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases/download/3.3.0.2/apoc-3.3.0.2-all.jar (1.2) 配置 neo4j.conf 配置文件在 neo4j.conf 最后一行添加12dbms.security.procedures.unrestricted=apoc.*apoc.import.file.enabled=true 12345678neo4j-sh (?)$ return apoc.version();+----------------+| apoc.version() |+----------------+| &quot;3.3.0.2&quot; |+----------------+1 row29 ms 查看apoc版本 return apoc.version(); call apoc.help(&#39;apoc&#39;); call dbms.procedures call dbms.functions() (2) 导入json apoc.load.json(2.1) 官方文档12345678910111213neo4j-sh (?)$ CALL apoc.help(&quot;apoc.load.json&quot;);+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| type | name | text | signature | roles | writes |+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| &quot;procedure&quot; | &quot;apoc.load.json&quot; | &quot;apoc.load.json(&apos;url&apos;,path, config) YIELD value - import JSON as stream of values if the JSON was an array or a single value if it was a map&quot; | &quot;apoc.load.json(url :: STRING?, path = :: STRING?, config = &#123;&#125; :: MAP?) :: (value :: MAP?)&quot; | &lt;null&gt; | &lt;null&gt; || &quot;procedure&quot; | &quot;apoc.load.jsonArray&quot; | &quot;apoc.load.jsonArray(&apos;url&apos;) YIELD value - load array from JSON URL (e.g. web-api) to import JSON as stream of values&quot; | &quot;apoc.load.jsonArray(url :: STRING?, path = :: STRING?) :: (value :: ANY?)&quot; | &lt;null&gt; | &lt;null&gt; || &quot;procedure&quot; | &quot;apoc.load.jsonParams&quot; | &quot;apoc.load.jsonParams(&apos;url&apos;,&#123;header:value&#125;,payload, config) YIELD value - load from JSON URL (e.g. web-api) while sending headers / payload to import JSON as stream of values if the JSON was an array or a single value if it was a map&quot; | &quot;apoc.load.jsonParams(url :: STRING?, headers :: MAP?, payload :: STRING?, path = :: STRING?, config = &#123;&#125; :: MAP?) :: (value :: MAP?)&quot; | &lt;null&gt; | &lt;null&gt; |+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+3 rows42 ms (2.2) 官方示例1234WITH &apos;https://raw.githubusercontent.com/neo4j-contrib/neo4j-apoc-procedures/3.3.0.2/src/test/resources/person.json&apos; AS urlCALL apoc.load.json(url) YIELD value as personMERGE (p:Person &#123;name:person.name&#125;) ON CREATE SET p.age = person.age, p.children = size(person.children); 1234&#123;&quot;name&quot;:&quot;Michael&quot;, &quot;age&quot;: 41, &quot;children&quot;: [&quot;Selina&quot;,&quot;Rana&quot;,&quot;Selma&quot;]&#125; (3) 导入csv文件 apoc.load.csv(3.1) 官方文档1234567891011neo4j-sh (?)$ CALL apoc.help(&quot;apoc.load.csv&quot;);+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| type | name | text | signature | roles | writes |+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| &quot;procedure&quot; | &quot;apoc.load.csv&quot; | &quot;apoc.load.csv(&apos;url&apos;,&#123;config&#125;) YIELD lineNo, list, map - load CSV fom URL as stream of values, config contains any of: &#123;skip:1,limit:5,header:false,sep:&apos;TAB&apos;,ignore:[&apos;tmp&apos;],nullValues:[&apos;na&apos;],arraySep:&apos;;&apos;,mapping:&#123;years:&#123;type:&apos;int&apos;,arraySep:&apos;-&apos;,array:false,name:&apos;age&apos;,ignore:false&#125;&#125;&quot; | &quot;apoc.load.csv(url :: STRING?, config = &#123;&#125; :: MAP?) :: (lineNo :: INTEGER?, list :: LIST? OF ANY?, strings :: LIST? OF STRING?, map :: MAP?, stringMap :: MAP?)&quot; | &lt;null&gt; | &lt;null&gt; |+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row50 ms (3.2) 用法apoc.load.csv(&#39;url&#39;,{config}) YIELD lineNo, list, map - load CSV fom URL as stream of values, config contains any of: {skip:1,limit:5,header:false,sep:&#39;TAB&#39;,ignore:[&#39;tmp&#39;],arraySep:&#39;;&#39;,mapping:{years:{type:&#39;int&#39;,arraySep:&#39;-&#39;,array:false,name:&#39;age&#39;,ignore:false}} 返回行号 call apoc.load.csv(&#39;file:/data/stale/data01/neo4j/apoc_test_data.csv&#39;, {limit:5, header:true, sep:&#39;|&#39;}) yield lineNo return lineNo; 返回map，header必须是true call apoc.load.csv(&#39;file:/data/stale/data01/neo4j/apoc_test_data.csv&#39;, {limit:5, header:true, sep:&#39;|&#39;}) yield map return map; 返回行号+map call apoc.load.csv(&#39;file:/data/stale/data01/neo4j/apoc_test_data.csv&#39;, {skip:0, limit:5, header:true, sep:&#39;,&#39;}) yield lineNo,map return lineNo,map; call apoc.load.csv(&#39;file:/data/stale/data01/neo4j/apoc_test_data.csv&#39;, {limit:5, header:true, sep:&#39;|&#39;}) yield map return map.uuid; (4) 动态创建节点 apoc.create.node CALL apoc.create.node([&#39;Label&#39;], {key:value,…​}) create node with dynamic labels 创建节点时动态传入参数 官方作者在stackoverflow说，cypther里不能动态传Label，这是由Cypther语言决定的，只能通过动态拼接字符串在程序里实现。 但是APOC里有apoc.create.node方法，可以动态传Label，应该是用的动态拼接字符串，源码还没看。 123456789uuid,name,Label2a3e275d9abc4c45913d8e7e619db87a,&quot;张忆耕&quot;,Laebl1db6ee76baff64db5956b6a5deb80acbf,&quot;傅某评&quot;,Laebl28d2f4a74e7e7429390b3389d64d77637,&quot;王苏维&quot;,Laebl34d0a5c3fa89a49e89f81a152f2aa259c,&quot;蓝波&quot;,Laebl42f811c5341b84b70840acbdd451e2490,&quot;范为华&quot;,Laebl5311f7441fe4b4b7c8d5dc56d7590c1c3,&quot;黄日波&quot;,Laebl1b4d04f00887c49308e86afd0f0baf641,&quot;徐国康&quot;,Laebl3b717f94e2f3f4d25bb3dcd8a8efbc667,&quot;王心迪&quot;,Laebl6 不加headers load csv from &quot;file:/data/stale/data01/neo4j/node_uuid_10w.csv&quot; as line with line CALL apoc.create.node([line[2]], {uuid:line[0], name:line[1]}) YIELD node return labels(node), node limit 5 ; 123456789101112neo4j-sh (?)$ load csv from &quot;file:/data/stale/data01/neo4j/node_uuid_10w.csv&quot; as line with line CALL apoc.create.node([line[2]], &#123;uuid:line[0], name:line[1]&#125;) YIELD node return labels(node), node limit 5 ;+-----------------------------------------------------------------------------------+| labels(node) | node |+-----------------------------------------------------------------------------------+| [&quot;Label&quot;] | Node[18004017]&#123;name:&quot;name&quot;,uuid:&quot;uuid&quot;&#125; || [&quot;WB_AJ&quot;] | Node[18004018]&#123;name:&quot;张忆耕&quot;,uuid:&quot;2a3e275d9abc4c45913d8e7e619db87a&quot;&#125; || [&quot;WP_SJH&quot;] | Node[18004019]&#123;name:&quot;傅某评&quot;,uuid:&quot;db6ee76baff64db5956b6a5deb80acbf&quot;&#125; || [&quot;LG_JG&quot;] | Node[18004020]&#123;name:&quot;王苏维&quot;,uuid:&quot;8d2f4a74e7e7429390b3389d64d77637&quot;&#125; || [&quot;JTSGXX&quot;] | Node[18004021]&#123;name:&quot;蓝波&quot;,uuid:&quot;4d0a5c3fa89a49e89f81a152f2aa259c&quot;&#125; |+-----------------------------------------------------------------------------------+5 rows84 ms 加headers load csv with headers from &quot;file:/data/stale/data01/neo4j/node_uuid_10w.csv&quot; as line with line CALL apoc.create.node([line.Label], {uuid:line.uuid, name:line.name}) YIELD node return labels(node), node limit 5 ; 123456789101112neo4j-sh (?)$ load csv with headers from &quot;file:/data/stale/data01/neo4j/node_uuid_10w.csv&quot; as line with line CALL apoc.create.node([line.Label], &#123;uuid:line.uuid, name:line.name&#125;) YIELD node return labels(node), node limit 5 ;+-----------------------------------------------------------------------------------+| labels(node) | node |+-----------------------------------------------------------------------------------+| [&quot;WB_AJ&quot;] | Node[18004022]&#123;name:&quot;张忆耕&quot;,uuid:&quot;2a3e275d9abc4c45913d8e7e619db87a&quot;&#125; || [&quot;WP_SJH&quot;] | Node[18004023]&#123;name:&quot;傅某评&quot;,uuid:&quot;db6ee76baff64db5956b6a5deb80acbf&quot;&#125; || [&quot;LG_JG&quot;] | Node[18004024]&#123;name:&quot;王苏维&quot;,uuid:&quot;8d2f4a74e7e7429390b3389d64d77637&quot;&#125; || [&quot;JTSGXX&quot;] | Node[18004025]&#123;name:&quot;蓝波&quot;,uuid:&quot;4d0a5c3fa89a49e89f81a152f2aa259c&quot;&#125; || [&quot;SWXX&quot;] | Node[18004026]&#123;name:&quot;范为华&quot;,uuid:&quot;2f811c5341b84b70840acbdd451e2490&quot;&#125; |+-----------------------------------------------------------------------------------+5 rows75 ms (5) 动态创建关系 apoc.create.relationship(5.1) 官方文档123456789neo4j-sh (?)$ CALL apoc.help(&quot;apoc.create.relationship&quot;);+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| type | name | text | signature | roles | writes |+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| &quot;procedure&quot; | &quot;apoc.create.relationship&quot; | &quot;apoc.create.relationship(person1,&apos;KNOWS&apos;,&#123;key:value,...&#125;, person2) create relationship with dynamic rel-type&quot; | &quot;apoc.create.relationship(from :: NODE?, relType :: STRING?, props :: MAP?, to :: NODE?) :: (rel :: RELATIONSHIP?)&quot; | &lt;null&gt; | &lt;null&gt; |+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+1 row61 ms CALL apoc.create.relationship(person1,&#39;KNOWS&#39;,{key:value,…​}, person2) create relationship with dynamic rel-type (5.2) 示例12345678using periodic commit 10000//call apoc.load.csv(&apos;file:/data/stale/data01/neo4j/apoc_test_data.csv&apos;, &#123;limit:5, header:true, sep:&apos;|&apos;&#125;) yield map return mapload csv with headers from &apos;file:/data/stale/data01/neo4j/relathionship_uuid_10w.csv&apos; as line fieldterminator &apos;,&apos;match (p1:Person &#123;uuid: line[0]&#125;)match (p2:Person &#123;uuid: line[1]&#125;)WITH p1, p2, lineCALL apoc.create.relationship(p1, line[2], &#123;name: line[3]&#125;, p2) YIELD relRETURN rel 1234merge (n1:Test &#123;name:&apos;zhangsan&apos;&#125;) merge (n2:Test &#123;name:&apos;lisi&apos;&#125;)with n1, n2call apoc.create.relationship(n1, &apos;R1&apos;, &#123;&#125;, n2) YIELD relreturn id(rel), type(rel), rel ; 1234567using periodic commit 10000load csv from &apos;file:/data/stale/data01/neo4j/relathionship_uuid_10w.csv&apos; as line fieldterminator &apos;,&apos;merge (n1:Test &#123;uuid: line[0]&#125;)merge (n2:Test &#123;uuid: line[1]&#125;)with n1, n2, lineCALL apoc.create.relationship(n1, line[2], &#123;&#125;, n2) YIELD relreturn id(rel), type(rel), rel ; 1234567using periodic commit 10000load csv with headers from &apos;file:/data/stale/data01/neo4j/relathionship_uuid_10w.csv&apos; as line fieldterminator &apos;,&apos;merge (n1:Test &#123;uuid: line.uuid1&#125;)merge (n2:Test &#123;uuid: line.uuid2&#125;)with n1, n2, lineCALL apoc.create.relationship(n1, line.type, &#123;&#125;, n2) YIELD relreturn id(rel), type(rel), rel ; 123456789101112131415161718create (n:Test &#123;name:&apos;zhangsan&apos;&#125;) return n;create constraint on (n:Test) assert n.name is unique;merge (n:Test &#123;name:&apos;zhangsan&apos;&#125;)-[r:Friend]-&gt;(m:Test &#123;name:&apos;lisi&apos;&#125;);&#123; &quot;signature&quot;: 127, &quot;fields&quot;: [ &#123; &quot;code&quot;: &quot;Neo.ClientError.Schema.ConstraintValidationFailed&quot;, &quot;message&quot;: &quot;Node 1000020 already exists with label Test and property \&quot;name\&quot;=[zhangsan]&quot; &#125; ], &quot;timings&quot;: &#123; &quot;type&quot;: &quot;client&quot; &#125;&#125; (6) 热启动 apoc.warmup.run12345678neo4j-sh (?)$ CALL apoc.warmup.run();+--------------------------------------------------------------------------------------------------------------------------+| pageSize | nodesPerPage | nodesTotal | nodePages | nodesTime | relsPerPage | relsTotal | relPages | relsTime | totalTime |+--------------------------------------------------------------------------------------------------------------------------+| 8192 | 546 | 119510256 | 234717 | 9 | 240 | 131001022 | 587505 | 21 | 30 |+--------------------------------------------------------------------------------------------------------------------------+1 row30337 ms (7) 从RDBMS导入数据 apoc.load.jdbc(7.1) 官方文档1234567891011neo4j-sh (?)$ CALL apoc.help(&quot;apoc.load.jdbc&quot;);+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| type | name | text | signature | roles | writes |+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| &quot;procedure&quot; | &quot;apoc.load.jdbc&quot; | &quot;apoc.load.jdbc(&apos;key or url&apos;,&apos;table or kernelTransaction&apos;) YIELD row - load from relational database, from a full table or a sql kernelTransaction&quot; | &quot;apoc.load.jdbc(jdbc :: STRING?, tableOrSql :: STRING?, params = [] :: LIST? OF ANY?) :: (row :: MAP?)&quot; | &lt;null&gt; | &lt;null&gt; || &quot;procedure&quot; | &quot;apoc.load.jdbcParams&quot; | &quot;deprecated - please use: apoc.load.jdbc(&apos;key or url&apos;,&apos;kernelTransaction&apos;,[params]) YIELD row - load from relational database, from a sql kernelTransaction with parameters&quot; | &quot;apoc.load.jdbcParams(jdbc :: STRING?, sql :: STRING?, params :: LIST? OF ANY?) :: (row :: MAP?)&quot; | &lt;null&gt; | &lt;null&gt; || &quot;procedure&quot; | &quot;apoc.load.jdbcUpdate&quot; | &quot;apoc.load.jdbcUpdate(&apos;key or url&apos;,&apos;kernelTransaction&apos;,[params]) YIELD row - update relational database, from a SQL kernelTransaction with optional parameters&quot; | &quot;apoc.load.jdbcUpdate(jdbc :: STRING?, query :: STRING?, params = [] :: LIST? OF ANY?) :: (row :: MAP?)&quot; | &lt;null&gt; | &lt;null&gt; |+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+3 rows180 ms (7.2) 使用 CALL apoc.load.driver(&quot;com.mysql.jdbc.Driver&quot;); (7.3.1) 查看数据个数123with &quot;jdbc:mysql://localhost:3306/dataserver?user=admin&amp;password=admin&quot; as urlCALL apoc.load.jdbc(url, &quot;city&quot;) YIELD rowRETURN count(*); (7.3.2) 查看数据样例12345// 固定参数with &quot;jdbc:mysql://localhost:3306/dataserver?user=admin&amp;password=admin&quot; as url, &quot;select id, name from city where level = 1&quot; as sqlCALL apoc.load.jdbc(url, sql, []) YIELD rowRETURN row LIMIT 10; (7.3.3) 动态传入参数12345// 动态传参数with &quot;jdbc:mysql://localhost:3306/dataserver?user=admin&amp;password=admin&quot; as url, &quot;select id, name from city where level = ? and is_deleted = ? &quot; as sqlCALL apoc.load.jdbcParams(url, sql, [1, &apos;N&apos;]) YIELD rowRETURN row LIMIT 10; (7.3.4) 分批处理12345CALL apoc.periodic.iterate( &apos;call apoc.load.jdbc(&quot;jdbc:mysql://localhost:3306/dataserver?user=admin&amp;password=admin&quot;, &quot; select id, name, level, pid from city where level = 6 and is_deleted = \&apos;N\&apos; &quot;, [])&apos;, &apos;MATCH (s:Test &#123;id: toInteger(row.pid)&#125;) MERGE (s)-[r:Connect]-&gt;(n:Group &#123;id: toInteger(row.id)&#125;) set n += row &apos;, &#123;batchSize:10000, parallel:true&#125;) (8) 遇到的错误(1) There is no procedure with the name apoc.version registeredThere is no procedure with the nameapoc.versionregistered for this database instance. Please ensure you&#39;ve spelled the procedure name correctly and that the procedure is properly deployed. 检查是否下载对应的apoc jar包，并放到plugs目录下 在neo4j.conf里添加 dbms.security.procedures.unrestricted=apoc.* apoc.import.file.enabled=true 的配置 重启Neo4j数据库 (2) Failed to invoke procedure apoc.load.driver: Caused by: java.lang.RuntimeException: Could not load driver class com.mysql.jdbc.Driver com.mysql.jdbc.Driver1234neo4j-sh (?)$ CALL apoc.load.driver(&quot;com.mysql.jdbc.Driver&quot;);1 msWARNING: Failed to invoke procedure `apoc.load.driver`: Caused by: java.lang.RuntimeException: Could not load driver class com.mysql.jdbc.Driver com.mysql.jdbc.Driver 少 mysql jar包 (3) Invalid input &#39;y&#39;: expected &#39;r/R&#39; or &#39;a/A&#39;12345neo4j-sh (?)$ with &quot;jdbc:mysql://localhost:3306/test?user=root&amp;password=root&quot; as url cypher CALL apoc.load.jdbc(url,&quot;test_table&quot;) YIELD row RETURN count(*);5 msWARNING: Invalid input &apos;y&apos;: expected &apos;r/R&apos; or &apos;a/A&apos; (line 1, column 73 (offset: 72))&quot;with &quot;jdbc:mysql://localhost:3306/test?user=root&amp;password=root&quot; as url cypher CALL apoc.load.jdbc(url,&quot;test_table&quot;) YIELD row RETURN count(*)&quot; 原因： 缺少MySQL jar包 下载MySQL jar包放到lib或plugs目录下，重启Neo4j数据库就好了。 (4) Neo.ClientError.Statement.SyntaxError: Unknown function ‘apoc.version’ (line 1, column 8 (offset: 7))123Neo.ClientError.Statement.SyntaxError: Unknown function &apos;apoc.version&apos; (line 1, column 8 (offset: 7)) &quot;return apoc.version();&quot; ^ neo4j.conf 里配置有问题 (5) Neo.ClientError.Procedure.ProcedureNotFound: There is no procedure with the name apoc.help registered for this database instance.1Neo.ClientError.Procedure.ProcedureNotFound: There is no procedure with the name `apoc.help` registered for this database instance. Please ensure you&apos;ve spelled the procedure name correctly and that the procedure is properly deployed. neo4j.conf 里配置有问题 References[1] neo4j-apoc-procedures github地址[2] neo4j-apoc 官方文档[3] cypher-dynamic-label-at-query cypher语句中动态传Label参数[4] Cannot run a query looking for a dynamic label[5] how-to-use-apoc-load-csv-in-conjunction-with-apoc-create-node[6] Neo4j 导入动态类型关系[7] how-do-i-fix-this-neo4j-apoc-query-that-creates-nodes-froma-csv-file]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[帆布指纹]]></title>
    <url>%2F2018%2F03%2F02%2Fcanvas-fingerprinting%2F</url>
    <content type="text"><![CDATA[一般情况下，网站或者广告联盟都会非常想要一种技术方式可以在网络上精确定位到每一个个体，这样可以通过收集这些个体的数据，通过分析后更加精准的去推送广告（精准化营销）或其他有针对性的一些活动。 Cookie技术是非常受欢迎的一种。当用户访问一个网站时，网站可以在用户当前的浏览器Cookie中永久植入一个含有唯一标示符（UUID）的信息，并通过这个信息将用户所有行为（浏览了哪些页面？搜索了哪些关键字？对什么感兴趣？点了哪些按钮？用了哪些功能？看了哪些商品？把哪些放入了购物车等等）关联起来。 但是Cookie可以伪装，在浏览器隐私窗口，不同用户，甚至是爬虫时无法区分。 这个时候，提出一种 “帆布指纹识别”技术 ，这种技术可以避免以上的缺点。 123456789101112131415161718192021222324252627282930313233343536373839404142function bin2hex (s) &#123; // From: http://phpjs.org/functions // + original by: Kevin van Zonneveld (http://kevin.vanzonneveld.net) // + bugfixed by: Onno Marsman // + bugfixed by: Linuxworld // + improved by: ntoniazzi (http://phpjs.org/functions/bin2hex:361#comment_177616) // * example 1: bin2hex('Kev'); // * returns 1: '4b6576' // * example 2: bin2hex(String.fromCharCode(0x00)); // * returns 2: '00' var i, l, o = "", n; s += ""; for (i = 0, l = s.length; i &lt; l; i++) &#123; n = s.charCodeAt(i).toString(16) o += n.length &lt; 2 ? "0" + n : n; &#125; return o;&#125;var canvas = document.createElement('canvas');var ctx = canvas.getContext('2d');var txt = 'http://security.tencent.com/';ctx.textBaseline = "top";ctx.font = "14px 'Arial'";ctx.textBaseline = "tencent";ctx.fillStyle = "#f60";ctx.fillRect(125,1,62,20);ctx.fillStyle = "#069";ctx.fillText(txt, 2, 15);ctx.fillStyle = "rgba(102, 204, 0, 0.7)";ctx.fillText(txt, 4, 17);var b64 = canvas.toDataURL().replace("data:image/png;base64,","");var bin = atob(b64);var crc = bin2hex(bin.slice(-16,-12));console.log(crc); 以上代码在chrome浏览器里运行会得到一个结果 3c5c9044 ，在火狐会得到 7e7f4d86，在IE会得到 5ec31ae3只要用户用同一浏览器，产生的 帆布指纹就不会变。这样就可以达到反爬虫的效果。缺点是 公司批量购买的电脑，批量安装系统，这些电脑的结果可能会一样。 References[1] 取代cookie的网站追踪技术：”帆布指纹识别”初探[2] 帆布指纹识别（CANVAS FINGERPRINTING）[3] fingerprintJS[4] browser-uniqueness.pdf[5] Token 认证的来龙去脉[6] token认证]]></content>
      <categories>
        <category>crawler</category>
      </categories>
      <tags>
        <tag>crawler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-log]]></title>
    <url>%2F2018%2F03%2F01%2Fpython-log%2F</url>
    <content type="text"><![CDATA[Python日志 python确实挺简单，用起来确实挺方便，但是python的日志我真想说一句头疼 只输出到文件12345678910111213141516import logging# 改成对应的日志路径，推荐使用绝对路径logging.basicConfig(level=logging.INFO, filename=&apos;log.log&apos;, filemode=&apos;a&apos;, format=&apos;%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s&apos;) def main(): #print(&quot;hello world&quot;) logging.info(&quot;hello world&quot;)if __name__ == &apos;__main__&apos;: main() References[1] http://python.jobbole.com/81666/[] https://www.jianshu.com/p/4f6ff84cb3e9[] https://my.oschina.net/leejun2005/blog/126713 python 日志模块 logging 详解[] http://logbook.readthedocs.io/en/stable/[] https://github.com/getlogbook/logbook[] https://github.com/metachris/logzero[] https://www.jianshu.com/p/d615bf01e37b python logging日志模块以及多进程日志[] http://blog.csdn.net/liuchunming033/article/details/39080457 Python中的logging模块就这么用[] https://www.jianshu.com/p/72fb43127c85 Python日志教程]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>log</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-error-notes]]></title>
    <url>%2F2018%2F02%2F06%2Fpython-error-notes%2F</url>
    <content type="text"><![CDATA[(1) command ‘C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\x86_amd64\cl.exe’ failed with exit status 4 把cl.exe所在目录添加到环境变量里 C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\bin 安装VS 2015 visual-cpp-build-tools vs2015离线安装包 离线安装包-国内 安装大概需要80分钟左右 (2) UnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0xb6 in position 30: invalid start byte123456789101112131415161718192021222324Installing collected packages: Twisted, Scrapy Running setup.py install for Twisted ... errorException:Traceback (most recent call last): File &quot;c:\professionsofware\python\python36\lib\site-packages\pip\compat\__init__.py&quot;, line 73, in console_to_str return s.decode(sys.__stdout__.encoding)UnicodeDecodeError: &apos;utf-8&apos; codec can&apos;t decode byte 0xb6 in position 30: invalid start byteDuring handling of the above exception, another exception occurred:Traceback (most recent call last): File &quot;c:\professionsofware\python\python36\lib\site-packages\pip\basecommand.py&quot;, line 215, in main status = self.run(options, args) File &quot;c:\professionsofware\python\python36\lib\site-packages\pip\commands\install.py&quot;, line 342, in run prefix=options.prefix_path, File &quot;c:\professionsofware\python\python36\lib\site-packages\pip\req\req_set.py&quot;, line 784, in install **kwargs File &quot;c:\professionsofware\python\python36\lib\site-packages\pip\req\req_install.py&quot;, line 878, in install spinner=spinner, File &quot;c:\professionsofware\python\python36\lib\site-packages\pip\utils\__init__.py&quot;, line 676, in call_subprocess line = console_to_str(proc.stdout.readline()) File &quot;c:\professionsofware\python\python36\lib\site-packages\pip\compat\__init__.py&quot;, line 75, in console_to_str return s.decode(&apos;utf_8&apos;)UnicodeDecodeError: &apos;utf-8&apos; codec can&apos;t decode byte 0xb6 in position 30: invalid start byte 编码问题 换个编码试试就可以了 (3) ModuleNotFoundError: No module named ‘win32api’缺少模块，安装对应的模块就可以了1pip install pypiwin32 (4) UnicodeEncodeError: ‘gbk’ codec can’t encode character ‘\u2022’ in position 4030: illegal multibyte sequence编码问题 在写文件时使用 open(filename, mode=’a’, encoding=’utf-8’) 强制使用UTF-8编码 在代码编码，输出，字符串全部使用UTF-81open(filename, mode=&apos;a&apos;, encoding=&apos;utf-8&apos;) (5) an integer is required when open()’ing a file as utf-8 输入的参数少了，或者加上key 1open(filename, mode=&apos;rb&apos;, encoding=None, errors=&apos;strict&apos;, buffering=1) (6) json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)1json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1) 问题原因：用单引号包装对应字段，按照规范应该是双引号包装字段 方法一： 把单引号替换成双引号 str.replace(&quot;&#39;&quot;, &quot;\&quot;&quot;)) 方法二：使用 demjson 包 pip install demjson json_obj = demjson(json_string) Python解析json之ValueError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1) (7) RequestsDependencyWarning: urllib3 (1.24.3) or chardet (3.0.4) doesn’t match a supported version!123/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 /Users/weikeqin1/WorkSpaces/python/python-study/src/car/car_deal_new_data.py/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.24.3) or chardet (3.0.4) doesn&apos;t match a supported version! RequestsDependencyWarning) 解决办法 pip install --upgrade urllib3 pip install --upgrade requests References[1] python-pip-on-windows-command-cl-exe-failed[2] WindowsCompilers[3] importerror-no-module-named-win32api[4] Python解析json之ValueError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)[5] requests请求出现RequestsDependencyWarning异常]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[httpclient 笔记]]></title>
    <url>%2F2018%2F01%2F19%2Fhttpclient-notes%2F</url>
    <content type="text"><![CDATA[调试时想要打印出httpClient日志httpClient 4.5.x12345678Context Logging 上下文日志 org.apache.http.impl.client Wire Logging 传送和接收的数据日志 org.apache.http.wire HTTP header Logging 请求头和响应头日志 org.apache.http.headers 参考httpcomponents-client-4.5.x logging.html httpClient 3.x12345678System.setProperty(&quot;org.apache.commons.logging.Log&quot;, &quot;org.apache.commons.logging.impl.SimpleLog&quot;);System.setProperty(&quot;org.apache.commons.logging.simplelog.showdatetime&quot;, &quot;true&quot;);// write loggingSystem.setProperty(&quot;org.apache.commons.logging.simplelog.log.httpclient.wire&quot;, &quot;debug&quot;);// header loggingSystem.setProperty(&quot;org.apache.commons.logging.simplelog.log.httpclient.wire.header&quot;, &quot;debug&quot;);// content loggingSystem.setProperty(&quot;org.apache.commons.logging.simplelog.log.org.apache.commons.httpclient&quot;, &quot;debug&quot;); 参考httpclient-3.x logging.xml 使用slf4j+logback1234&lt;logger name=&quot;org.apache&quot; level=&quot;DEBUG&quot; /&gt;&lt;logger name=&quot;org.apache.http.client&quot; level=&quot;DEBUG&quot; /&gt;&lt;logger name=&quot;org.apache.http.wire&quot; level=&quot;DEBUG&quot; /&gt;&lt;logger name=&quot;org.apache.commons.httpclient&quot; level=&quot;DEBUG&quot; /&gt; 整个logback.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration scan="true" scanPeriod="60 seconds" debug="false"&gt;&lt;!-- scan="true" scanPeriod="60 seconds" debug="false" --&gt;&lt;!-- scan：当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true。 --&gt;&lt;!-- scanPeriod：设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒当scan为true时，此属性生效。默认的时间间隔为1分钟。 --&gt;&lt;!-- debug：当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。 --&gt; &lt;!--定义日志文件的存储地址 勿在 LogBack 的配置中使用相对路径--&gt; &lt;property name="LOG_HOME" value="./logs"/&gt; &lt;property name="LOG_LEVEL" value="DEBUG" /&gt; &lt;property name="appName" value="crawler"&gt;&lt;/property&gt; &lt;!-- 控制台输出 --&gt; &lt;appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level [%logger&#123;50&#125;] [%line] - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 按照每天生成日志文件 --&gt; &lt;appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!--&lt;file&gt;$&#123;LOG_HOME&#125;/$&#123;appName&#125;log&lt;/file&gt;--&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy"&gt; &lt;!--日志文件输出的文件名 .%i --&gt; &lt;FileNamePattern&gt;$&#123;LOG_HOME&#125;/$&#123;appName&#125;.%d&#123;yyyy-MM-dd&#125;_%i.log&lt;/FileNamePattern&gt; &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;totalSizeCap&gt;600MB&lt;/totalSizeCap&gt; &lt;/rollingPolicy&gt; &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level [%logger&#123;50&#125;] [%line] - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 日志输出级别 --&gt; &lt;root level="$&#123;LOG_LEVEL&#125;"&gt; &lt;appender-ref ref="STDOUT" /&gt; &lt;appender-ref ref="FILE" /&gt; &lt;/root&gt; &lt;logger name="org.apache" level="DEBUG" /&gt; &lt;logger name="org.apache.http.client" level="DEBUG" /&gt; &lt;logger name="org.apache.http.wire" level="DEBUG" /&gt; &lt;logger name="org.apache.commons.httpclient" level="DEBUG" /&gt;&lt;/configuration&gt; 使用log4j123456# httpclient 4.5.xlog4j.logger.org.apache.http.client=DEBUG# httpclient 3.xlog4j.logger.org.apache.commons.httpclient=DEBUGlog4j.logger.httpclient.wire.header=DEBUGlog4j.logger.httpclient.wire=DEBUG 整个log4j.properties1234567891011log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%5p [%c] %m%n# httpclient 4.5.xlog4j.logger.org.apache.http.client=DEBUG# httpclient 3.xlog4j.logger.org.apache.commons.httpclient=DEBUGlog4j.logger.httpclient.wire.header=DEBUGlog4j.logger.httpclient.wire=DEBUG 常见的Content-Type1234Content-Type : application/x-www-form-urlencoded;charset=utf-8Content-Type : multipart/form-data; boundary=----WebKitFormBoundaryrGKCBY7qhFd3TrwAContent-Type : application/json; charset=UTF-8Content-Type : text/xml References[1] office document[2] source code[3] 浅谈HttpClient[4] Http请求连接池 - HttpClient 的 PoolingHttpClientConnectionManager[5] 高并发场景下的httpClient优化使用[6] 搜索系统17：HttpClient的网络连接是否被复用了[7] httpclient 多线程高并发Get请求[8] HttpClient模拟get，post请求并发送请求参数（json等）[9] 常用的几种 Content-Type[10] HTTP Content-Type常用一览表[11] 关闭HttpClient控制台输出语句[12] Spring boot 配置HttpClient 请求日志[13] httpclient-3.x logging document[14] httpcomponents-client-4.5.x logging document]]></content>
      <categories>
        <category>crawler</category>
      </categories>
      <tags>
        <tag>crawler</tag>
        <tag>httpclient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反爬虫 笔记]]></title>
    <url>%2F2018%2F01%2F13%2Fanti-web-crawler%2F</url>
    <content type="text"><![CDATA[对于一张网页，我们往往希望它是结构良好，内容清晰的，这样搜索引擎才能准确地认知它。 而反过来，又有一些情景，我们不希望内容能被轻易获取，比方说电商网站的交易额，教育网站的题目等。因为这些内容，往往是一个产品的生命线，必须做到有效地保护。这就是爬虫与反爬虫这一话题的由来。 但是世界上没有一个网站，能做到完美地反爬虫。 如果页面希望能在用户面前正常展示，同时又不给爬虫机会，就必须要做到识别真人与机器人。因此工程师们做了各种尝试，这些策略大多采用于后端，也是目前比较常规单有效的手段。 反爬手段 User-Agent检测 Referer检测 IP限制频次 账号及Cookie验证 验证码 混淆或者加密js 各类型网站 爬虫难度根据我爬取过的网站，爬虫难度从易到难依次是： 1. 新闻类网站 比如腾讯新闻、新浪新闻、网易新闻、和讯网、凤凰网等等 公开的信息，可以直接爬取。 很多都没有反爬策略，即使有，反爬策略也比较简单，User-Agent检测，IP限制频次，可能有一些有ajax网页，对新手来说可能难一点 2. 微博类网站 比如 新浪微博 半公开的信息，不登录可以爬取少量信息，登陆后可以爬取大量信息 有一些反爬策略，反爬策略稍微复杂，User-Agent检测、Referer检测、Cookie验证、IP限制频次。需要伪装User-Agent、Referer、Cookie、要使用代理 3. 猫眼类网站 比如 猫眼 能爬取，但是爬取到的信息要经过处理 要用到图像处理 4. 携程类网站 比如 携程、去哪儿、裁判文书 公开的信息，不登录可以爬取。如果获取少量信息不难，但是要爬取数量上万，有难度。 反爬策略较难，User-Agent检测、Referer检测、Cookie验证、加密js、IP限制频次、参数动态变化、返回的结果还需要处理(字符拼接、图像识别) 裁判文书的反爬策略是，1.首先用post请求(太可气了) 2.guid 随机生成 3. number 每次变化 4. 混淆加密js 5. 图形验证码 6.IP限制 7.漫长的等待(文书呀，你们是技术不行还是故意要拖延时间，就查一页，你让我等这么久) 8.抽风的结果 即使你所有参数都对，偶尔也给你来个查不到内容 携程的js动态变化 5. 工商类网站 比如天眼查 算是公开的信息 反爬策略特别难，User-Agent检测、Referer检测、登录及Cookie验证、IP限制频次、混淆加密js(不定期更新)、参数动态变化、图形验证码 天眼查 1. User-Agent检测 Referer检测 IP限制频次 这些对天眼查来说简直是小儿科 2.要查询必须登录 2. 混淆加密js 3.参数动态变化 4.不同页面反爬策略不一样 5.对页数进行限制(你只能爬少量数据) 6. 头疼的特别难破的验证码 7. 最难的是，即使你把多有反爬策略都分析出来，你没办法爬到天眼查的全量数据 即使我爬了天眼查的部分数据，但是，论爬虫，我最服天眼查。 PS：天眼查的一条数据可以卖好几块钱，终于知道天眼查的反爬策略为什么这么难了吧。去招聘网站搜一搜天眼查的招聘信息。一个反爬虫工程师 15k-30k，一个月才3W，平摊到每条数据上，连一里都不到。有钱就是任性呀。 2017-06改过一个版，全部使用https，而且反爬策略严了 然后2017年后半年可能又改了几次 而爬虫是可以无限逼近于真人的，比如： chrome headless或phantomjs来模拟浏览器环境tesseract识别验证码代理IP淘宝就能买到所以我们说，100%的反爬虫策略？不存在的。更多的是体力活，是个难易程度的问题。 反爬手段举例自动封/解封ipdiscuz论坛，每天有很多注册机注册的用户，然后发垃圾广告帖子。虽然使用了一些插件但没有效果。分析访问日志，发现有几个ip访问量特别大，所以想到可以写个shell脚本，通过分析访问日志，把访问量大的ip直接封掉。但是这个脚本很有可能误伤，所以还需要考虑到自动解封这些ip。 思路：1 可以每分钟分析1次访问日志，设定一个阈值，把访问量大的ip用iptables封掉80端口2 每20分钟检测一次已经被封ip的请求数据包数量，设定阈值，把没有请求的或者请求量很小的解封 123456789101112131415161718192021222324252627282930313233343536373839404142#! /bin/bash## To block the ip of bad requesting.## Writen by aming 2017-11-18.log="/data/logs/www.xxx.com.log"tmpdir="/tmp/badip"#白名单ip，不应该被封goodip="27.133.28.101"[ -d $tmpdir ] || mkdir -p $tmpdirt=`date -d "-1 min" +%Y:%H:%M`#截取一分钟以前的日志grep "$t:" $log &gt; $tmpdir/last_min.log#把一分钟内日志条数大于120的标记为不正常的请求awk '&#123;print $1&#125;' $tmpdir/last_min.log |sort -n |uniq -c |sort -n |tail |awk '$1&gt;120 &#123;print $2&#125;'|grep -v "$good_ip"&gt; $tmpdir/bad.ipd3=`date +%M`#每隔20分钟解封一次ipif [ $d3 -eq "20" ] || [ $d3 -eq "40" ] || [ $d3 -eq "00" ]; then /sbin/iptables -nvL INPUT|grep 'DROP' |awk '$1&lt;10 &#123;print $8&#125;'&gt;$tmpdir/good.ip if [ -s $tmpdir/good.ip ]; then for ip in `cat $tmpdir/good.ip` do /sbin/iptables -D INPUT -p tcp --dport 80 -s $ip -j DROP d4=`date +%Y%m%d-%H:%M` echo "$d4 $ip unblock" &gt;&gt;$tmpdir/unblock.ip done fi #解封后，再把iptables的计数器清零 /sbin/iptables -Z INPUTfiif [ -s $tmpdir/bad.ip ] ; then for ip in `cat $tmpdir/bad.ip` do /sbin/iptables -A INPUT -p tcp --dport 80 -s $ip -j DROP d4=`date +%Y%m%d-%H:%M` echo "$d4 $ip block" &gt;&gt;$tmpdir/block.ip donefi References[1] https://segmentfault.com/a/1190000012293292 如果有人问你爬虫抓取技术的门道，请叫他来看这篇文章[2] http://imweb.io/topic/595b7161d6ca6b4f0ac71f05?from=timeline[3] https://mp.weixin.qq.com/s/0itKQmk9Z6OPiRqvUkSfUA[4] https://mp.weixin.qq.com/s/LSSY17QgpA9iwMKAX1As_Q[5] http://litten.me/2017/07/09/prevent-spiders/ 反击爬虫，前端工程师的脑洞可以有多大？]]></content>
      <categories>
        <category>crawler</category>
      </categories>
      <tags>
        <tag>crawler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j-auto-increment-id]]></title>
    <url>%2F2017%2F12%2F25%2Fneo4j-auto-increment-id%2F</url>
    <content type="text"><![CDATA[对于neo4j自增id，没有好的办法 只能用neo4j提供的自增id 可以设置一个属性用uuid来标识。(记得给这个属性建索引) neo4j-uuid可以直接用neo4j其中一个作者写的插件https://github.com/graphaware/neo4j-uuid Reference:[1] https://stackoverflow.com/questions/29434020/auto-increment-property-in-neo4j[2] https://github.com/graphaware/neo4j-uuid[3] http://grokbase.com/t/gg/neo4j/12akdzasym/auto-increment-ids-for-nodes[4] https://gist.github.com/jacquibo/8012859[5] https://gist.github.com/jacquibo/8012859[6] http://grokbase.com/t/gg/neo4j/12ay3p71jr/auto-increment[7] http://forum.spring.io/forum/spring-projects/data/nosql/121977-neo4j-auto-increment-id-for-node[8] https://groups.google.com/forum/#!topic/neo4j/_6m-Yu-6q-8]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络 OSI TCP/IP 笔记]]></title>
    <url>%2F2017%2F12%2F12%2Fnetwork-osi-tip-ip%2F</url>
    <content type="text"><![CDATA[OSI（Open System Interconnect），即开放式系统互联。 一般都叫OSI参考模型，是ISO（国际标准化组织）组织在1985年研究的网络互连模型。为了更好的使网络应用更为普及，推荐使用这个规范来控制网络。 OSI七层网络模型OSI定义了网络互连的七层框架（物理层、数据链路层、网络层、传输层、会话层、表示层、应用层），即ISO开放互连系统参考模型。 OSI七层网络模型 OSI各层功能 应用层（Application） 为应用程序提供服务，文件传输、电子邮件、文件服务、虚拟终端 表示层（Presentation） 数据格式转换、数据加密 会话层（Session） 建立、管理和维护会话 传输层（Transport） 建立、管理和维护端到端的连接 网络层（Network） IP选址及路由选择 数据链路层（Data Link） 提供介质访问和链路管理，传输有地址的帧以及错误检测功能 物理层（Physical） 物理层、以二进制数据形式在屋里媒体上传输数据 OSI各层的功能网络通信其实和现实中寄信的通信方式相似 以书信的方式进行通信为例。 1、物理层：提供为建立、维护和拆除物理链路所需要的机械的、电气的、功能的和规程的特性；有关的物理链路上传输非结构的位流以及故障检测指示（信件的运输工具，比如火车、汽车） 2、数据链路层：在网络层实体间提供数据发送和接收的功能和过程；提供数据链路的流控（相当于货物核对单，表明里面有些什么东西，接受的时候确认一下是否正确（CRC检验））。 3、网络层：控制分组传送系统的操作、路由选择、拥护控制、网络互连等功能，它的作用是将具体的物理传送对高层透明（相当于邮政局或快递公司地址（IP地址），能正确到达对方） 4、传输层：提供建立、维护和拆除传送连接的功能；选择网络层提供最合适的服务；在系统之间提供可靠的透明的数据传送，提供端到端的错误恢复和流量控制（相当于信封（TCP协议是挂号信，是可靠的；UDP协议是平信，尽力送到对方，不保证一点送到对方）） 5、会话层：提供两进程之间建立、维护和结束会话连接的功能；提供交互会话的管理功能，如三种数据流方向的控制，即一路交互、两路交替和两路同时会话模式（相当于邮票,优质邮票寄一封信，相当与一个会话） 6、表示层：代表应用进程协商数据表示；完成数据转换、格式化和文本压缩（你用普通话还是用方言？或者是英语？） 7、应用层：提供OSI用户服务，例如事务处理程序、文件传送协议和网络管理等（信件的内容） OSI封装过程 描述一下封装过程 1，你应该有需要表达的内容，信的内容（应用层）2，你需要有一种合适的表达语言，中文、英文（表示层）3，你要把信纸装进一个信封，贴上一张邮票（一封信就是一个会话）4，你要选择什么方式寄信（挂号信或平信，TCP或UDP）5，选择一个快递公司或邮政局，告诉地址，邮政局根据地址选择运输方式（根据IP地址选择路由）6，邮政局对货物进行再包装，写上装箱单，供接收地的邮政局核对（货物总是先送到对方邮政局，对方邮政局的地址就相当于MAC地址，装箱单就相当于CRC校验码）7，货物通过具体的运输根据（汽车、汽车、飞机等) OSI网络体系结构各层协议一、应用层：TELNET、FTP、TFTP、SMTP、SNMP、HTTP、BOOTP、DHCP、DNS 等 二、表示层： 文本：ASCII，EBCDIC 图形：TIFF，JPEG，GIF，PICT 声音：MIDI，MPEG，QUICKTIME 三、会话层： 四、传输层：TCP、UDP、SPX 五、网络层：IP、IPX、ICMP、RIP、OSPF(Open Shortest Path First开放式最短路径优先) 六、数据链路层：SDLC、HDLC、PPP、STP（Spanning Tree Protocol）、帧中继 七、物理层：EIA/TIA RS-232、EIA/TIA RS-449、V.35、RJ-45 OSI七层网络模型 TCP/IP四层概念模型 应用层（Application） 应用层 表示层（Presentation） 会话层（Session） 传输层（Transport） 传输层 网络层（Network） 网络层 数据链路层（Data Link） 数据链路层 物理层（Physical） OSI模型和TCP-IP模型的区别OSI七层和TCP/IP四层的关系 OSI引入了服务、接口、协议、分层的概念 TCP/IP借鉴了OSI的这些概念建立TCP/IP模型。 OSI先有模型，后有协议，先有标准，后进行实践； TCP/IP则相反，先有协议和应用再提出了模型，且是参照的OSI模型。 OSI是一种理论下的模型 TCP/IP已被广泛使用，成为网络互联事实上的标准。 TCP/IP他是一个协议簇；而OSI（开放系统互联）则是一个模型，且TCP/IP的开发时间在OSI之前。 TCP/IP是由一些交互性的模块做成的分层次的协议，其中每个模块提供特定的功能；OSi则指定了哪个功能是属于哪一层的。 TCP/IP是五层结构，而OSI是七层结构。OSI的最高三层在TCP中用应用层表示。 TCP：transmission control protocol 传输控制协议UDP：user data protocol 用户数据报协议 服务器常用端口 端口 服务 说明 21 FTP FTP 服务器所开放的端口，用于上传、下载 22 SSH 22 端口就是 ssh 端口，用于通过命令行模式远程连接 Linux 系统服务器 25 SMTP SMTP 服务器所开放的端口，用于发送邮件 80 HTTP 用于网站服务例如 IIS、Apache、Nginx 等提供对外访问 110 POP3 110 端口是为 POP3（邮件协议 3）服务开放的 137/138/139 NETBIOS 其中 137、138 是 UDP 端口，当通过网上邻居传输文件时用这个端口。而 139 端口：通过这个端口进入的连接试图获得 NetBIOS/SMB 服务。这个协议被用于 windows 文件和打印机共享和 SAMBA 143 IMAP 143 端口主要是用于“Internet Message AccessProtocol”v2（Internet 消息访问协议，简称 IMAP），和 POP3 一样，是用于电子邮件的接收的协议 443 HTTPS 网页浏览端口，能提供加密和通过安全端口传输的另一种 HTTP 1433 SQL Server 1433 端口，是 SQL Server 默认的端口，SQL Server 服务使用两个端口：TCP-1433、UDP-1434。其中 1433 用于供 SQL Server 对外提供服务，1434 用于向请求者返回 SQL Server 使用了哪个 TCP/IP 端口 3306 MySQL 3306 端口，是 MySQL 数据库的默认端口，用于 MySQL 对外提供服务 3389 Windows Server Remote Desktop Services 3389 端口是 Windows 远程桌面的服务端口，可以通过这个端口，用 “远程桌面” 等连接工具来连接到远程的服务器 8080 代理端口 8080 端口同 80 端口，是被用于 WWW 代理服务的，可以实现网页浏览，经常在访问某个网站或使用代理服务器的时候，会加上 “:8080” 端口号。另外 Apache Tomcat web server 安装后，默认的服务端口就是 8080 References[1] 一文读懂OSI七层模型与TCP/IP四层的区别/联系[2] OSI模型的通俗理解以及和TCP-IP模型的区别]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络笔记]]></title>
    <url>%2F2017%2F12%2F12%2Fnetwork-note%2F</url>
    <content type="text"><![CDATA[首先提出一个问题：从输入URL到页面加载发生了什么？ 仔细思考这个问题，发现确实很深，这个过程涉及到的东西很多。这个问题的回答真的能够很好的考验一个人对知识了解的水平。而且从不同角度/专业看，回答的结果也会不一样。 从网络的角度回答，简略回答如下：1.DNS域名解析；2.建立TCP连接；3.发送HTTP请求；4.服务器处理请求；5.返回响应结果；6.浏览器解析HTML，布局渲染；7.关闭TCP连接； 计算机网络概述ISO与TCP/IP 需要了解的知识 ISO TCP IP TCP/IP UDP 在网络体系结构中，包含了众多的网络协议，这篇文章主要围绕 HTTP 协议（HTTP/1.1版本）展开。 互联网的关键技术就是TCP/IP协议。两台计算机之间的通信是通过TCP/IP协议在因特网上进行的。实际上这个是两个协议： TCP : Transmission Control Protocol 传输控制协议IP： Internet Protocol 网际协议。 IP：计算机之间的通信 IP协议是计算机用来相互识别的通信的一种机制，每台计算机都有一个IP.用来在internet上标识这台计算机。 IP 负责在因特网上发送和接收数据包。通过 IP，消息（或者其他数据）被分割为小的独立的包，并通过因特网在计算机之间传送。IP 负责将每个包路由至它的目的地。 IP协议仅仅是允许计算机相互发消息，但它并不检查消息是否以发送的次序到达而且没有损坏（只检查关键的头数据）。为了提供消息检验功能，直接在IP协议上设计了传输控制协议TCP. TCP : 应用程序之间的通信 TCP确保数据包以正确的次序到达，并且尝试确认数据包的内容没有改变。TCP在IP地址之上引端口（port），它允许计算机通过网络提供各种服务。一些端口号为不同的服务保留，而且这些端口号是众所周知。 服务或者守护进程：在提供服务的机器上，有程序监听特定端口上的通信流。例如大多数电子邮件通信流出现在端口25上，用于wwww的HTTP通信流出现在80端口上。 当应用程序希望通过 TCP 与另一个应用程序通信时，它会发送一个通信请求。这个请求必须被送到一个确切的地址。在双方“握手”之后，TCP 将在两个应用程序之间建立一个全双工 (full-duplex) 的通信，占用两个计算机之间整个的通信线路。TCP 用于从应用程序到网络的数据传输控制。TCP 负责在数据传送之前将它们分割为 IP 包，然后在它们到达的时候将它们重组。 TCP/IP 就是TCP 和 IP 两个协议在一起协同工作，有上下层次的关系。 TCP 负责应用软件（比如你的浏览器）和网络软件之间的通信。IP 负责计算机之间的通信。TCP 负责将数据分割并装入 IP 包，IP 负责将包发送至接受者，传输过程要经IP路由器负责根据通信量、网络中的错误或者其他参数来进行正确地寻址，然后在它们到达的时候重新组合它们。 HTTP协议（HyperText Transfer Protocol，超文本传输协议）是用于从WWW服务器传输超文本到本地浏览器的传输协议。它可以使浏览器更加高效，使网络传输减少。它不仅保证计算机正确快速地传输超文本文档，还确定传输文档中的哪一部分，以及哪部分内容首先显示(如文本先于图形)等。 HTTP是客户端浏览器或其他程序与Web服务器之间的应用层通信协议。在Internet上的Web服务器上存放的都是超文本信息，客户机需要通过HTTP协议传输所要访问的超文本信息。HTTP包含命令和传输信息，不仅可用于Web访问，也可以用于其他因特网/内联网应用系统之间的通信，从而实现各类应用资源超媒体访问的集成。 我们在浏览器的地址栏里输入的网站地址叫做URL (Uniform Resource Locator，统一资源定位符)。就像每家每户都有一个门牌地址一样，每个网页也都有一个Internet地址。当你在浏览器的地址框中输入一个URL或是单击一个超级链接时，URL就确定了要浏览的地址。浏览器通过超文本传输协议(HTTP)，将Web服务器上站点的网页代码提取出来，并翻译成对应的网页。 HTTP 工作过程 就像刚才问的 从输入URL到页面加载发生了什么，其中工作的过程有以下几部分： 一次HTTP操作称为一个事务，其工作整个过程如下： 1、地址解析， 如用客户端浏览器请求这个页面：http://www.baidu.com/index.html 从中分解出协议名、主机名、端口、对象路径等部分，对于我们的这个地址，解析得到的结果如下： 协议名：http 主机名：www.baidu.com 被解析成 220.181.38.150 端口：80 对象路径：/index.html 在这一步，需要域名系统DNS解析域名www.baidu.com,得主机的IP地址。 2、封装HTTP请求数据包 把以上部分结合本机自己的信息，封装成一个HTTP请求数据包 3、封装成TCP包，建立TCP连接（TCP的三次握手） 在HTTP工作开始之前，客户机（Web浏览器）首先要通过网络与服务器建立连接，该连接是通过TCP来完成的，该协议与IP协议共同构建Internet，即著名的TCP/IP协议族，因此Internet又被称作是TCP/IP网络。HTTP是比TCP更高层次的应用层协议，根据规则，只有低层协议建立之后才能，才能进行更层协议的连接，因此，首先要建立TCP连接，一般TCP连接的端口号是80。 4、客户机发送请求命令 建立连接后，客户机发送一个请求给服务器，请求方式的格式为：统一资源标识符（URL）、协议版本号，后边是MIME信息包括请求修饰符、客户机信息和可内容。 5、服务器响应 服务器接到请求后，给予相应的响应信息，其格式为一个状态行，包括信息的协议版本号、一个成功或错误的代码，后边是MIME信息包括服务器信息、实体信息和可能的内容。 实体消息是服务器向浏览器发送头信息后，它会发送一个空白行来表示头信息的发送到此为结束，接着，它就以Content-Type应答头信息所描述的格式发送用户所请求的实际数据 6、服务器关闭TCP连接 一般情况下，一旦Web服务器向浏览器发送了请求数据，它就要关闭TCP连接，然后如果浏览器或者服务器在其头信息加入了这行代码 Connection:keep-alive TCP连接在发送后将仍然保持打开状态，于是，浏览器可以继续通过相同的连接发送请求。保持连接节省了为每个请求建立新连接所需的时间，还节约了网络带宽。 HTTP 协议基础HTTP是一个无状态的协议 HTTP是一个无状态的协议。无状态是指客户机（Web浏览器）和服务器之间不需要建立持久的连接，这意味着当一个客户端向服务器端发出请求，然后服务器返回响应(response)，连接就被关闭了，在服务器端不保留连接的有关信息.HTTP遵循请求(Request)/应答(Response)模型。客户机（浏览器）向服务器发送请求，服务器处理请求并返回适当的应答。所有HTTP连接都被构造成一套请求和应答。 这么做为了更快地处理大量事务，确保协议的可伸缩性 使用 Cookie 的状态管理 Cookie 技术通过在请求和响应报文中写入 Cookie 信息来控制客户端的状态。Cookie 会根据从服务器端发送的响应报文内的一个叫做 Set-Cookie 的首部字段信息，通知客户端保存Cookie。当下次客户端再往该服务器发送请求时，客户端会自动在请求报文中加入 Cookie 值后发送出去。服务器端发现客户端发送过来的 Cookie 后，会去检查究竟是从哪一个客户端发来的连接请求，然后对比服务器上的记录，最后得到之前的状态信息。 请求 URI 定位资源 HTTP 协议使用 URI 定位互联网上的资源。正是因为 URI 的特定功能，在互联网上任意位置的资源都能访问到。 告知服务器意图的 HTTP 方法（HTTP/1.1）持久连接 HTTP 协议的初始版本中，每进行一个 HTTP 通信都要断开一次 TCP 连接。比如使用浏览器浏览一个包含多张图片的 HTML 页面时，在发送请求访问 HTML 页面资源的同时，也会请求该 HTML 页面里包含的其他资源。因此，每次的请求都会造成无畏的 TCP 连接建立和断开，增加通信量的开销。 为了解决上述 TCP 连接的问题，HTTP/1.1 和部分 HTTP/1.0 想出了持久连接的方法。其特点是，只要任意一端没有明确提出断开连接，则保持 TCP 连接状态。旨在建立一次 TCP 连接后进行多次请求和响应的交互。在 HTTP/1.1 中，所有的连接默认都是持久连接。 谷歌浏览器默认最多打开6个TCP连接 管线化 持久连接使得多数请求以管线化方式发送成为可能。以前发送请求后需等待并接收到响应，才能发送下一个请求。管线化技术出现后，不用等待亦可发送下一个请求。这样就能做到同时并行发送多个请求，而不需要一个接一个地等待响应了。比如，当请求一个包含多张图片的 HTML 页面时，与挨个连接相比，用持久连接可以让请求更快结束。而管线化技术要比持久连接速度更快。请求数越多，时间差就越明显。 HTTP 协议报文结构HTTP报文 用于 HTTP 协议交互的信息被称为 HTTP 报文。 HTTP报文是面向文本的，报文中的每一个字段都是一些ASCII码串，各个字段的长度是不确定的。 HTTP有两类报文：请求报文和响应报文。 HTTP请求报文格式 HTTP请求报文主要由请求行、请求头部、请求正文3部分组成。 1234＜request-line＞＜headers＞＜blank line＞[＜request-body＞] 请求行 请求航由3部分组成，请求方法、URL以及协议版本，之间由空格分隔。 请求方法包括GET、HEAD、PUT、POST、TRACE、OPTIONS、DELETE以及扩展方法，当然并不是所有的服务器都实现了所有的方法，部分方法即便支持，处于安全性的考虑也是不可用的 协议版本的格式为：HTTP/主版本号.次版本号，常用的有HTTP/1.0和HTTP/1.1 请求头部 请求头部为请求报文添加了一些附加信息，由“键/值”对组成，每行一对，名和值之间使用冒号分隔。 12345678910Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3Accept-Encoding: gzip, deflate, brAccept-Language: zh-CN,zh;q=0.9,zh-TW;q=0.8Cache-Control: no-cacheConnection: keep-aliveCookie: BAIDUID=xx:FG=1; PSTM=1562221918; BIDUPSID=xxx; BD_UPN=xxx; BDUSS=xxx; H_PS_PSSID=xxx; BDORZ=xx; delPer=0; BD_CK_SAM=1; locale=zh; BD_HOME=1; BDRCVFR[S4-dAuiWMmn]=xx; PSINO=7; BDRCVFR[feWj1Vr5u3D]=xx; pgv_pvi=xx; pgv_si=xx; yjs_js_security_passport=xx; COOKIE_SESSION=xx; H_PS_645EC=xx; BDSFRCVID=xx; H_BDCLCKID_SF=xx; sugstore=1Host: www.baidu.comPragma: no-cacheUpgrade-Insecure-Requests: 1User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36 key value Host 接受请求的服务器地址，可以是IP:端口号，也可以是域名 User-Agent 发送请求的应用程序名称 Connection 指定与连接相关的属性，如Connection:Keep-Alive Accept-Charset 通知服务端可以发送的编码格式 Accept-Encoding 通知服务端可以发送的数据压缩格式 Accept-Language 通知服务端可以发送的语言 请求头部的最后会有一个空行，表示请求头部结束，接下来为请求正文，这一行非常重要，必不可少。 请求正文 可选部分，比如GET请求就没有请求正文 HTTP响应报文格式 HTTP响应报文主要由状态行、响应头部、响应正文3部分组成。 1234＜status-line＞＜headers＞＜blank line＞[＜response-body＞] 状态行 状态行由3部分组成，分别为：协议版本、状态码、状态码描述，之间由空格分隔 HTTP/1.1 200 OK 协议版本：HTTP/1.1 状态码：200 状态码描述：OK 协议版本状态码 状态代码为3位数字，200~299的状态码表示成功，300~399的状态码指资源重定向，400~499的状态码指客户端请求出错，500~599的状态码指服务端出错（HTTP/1.1向协议中引入了信息性状态码，范围为100~199） 1XX：信息提示。表示请求已被服务器接受，但需要继续处理，范围为100~101。2XX：请求成功。服务器成功处理了请求。范围为200~206。3XX：客户端重定向。重定向状态码用于告诉客户端浏览器，它们访问的资源已被移动，并告诉客户端新的资源位置。客户端收到重定向会重新对新资源发起请求。范围为300~305。4XX：客户端信息错误。客户端可能发送了服务器无法处理的东西，比如请求的格式错误，或者请求了一个不存在的资源。范围为400~415。5XX：服务器出错。客户端发送了有效的请求，但是服务器自身出现错误，比如Web程序运行出错。范围是500~505。 状态码描述HTTP响应头部 响应头 说明 Server 服务器应用程序软件的名称和版本 Content-Type 响应正文的类型（是图片还是二进制字符串） Content-Length 响应正文长度 Content-Charset 响应正文使用的编码 Content-Encoding 响应正文使用的数据压缩格式 Content-Language 响应正文使用的语言 响应示例：12345678910111213141516HTTP/1.1 200 OKBdpagetype: 2Bdqid: xxxCache-Control: privateConnection: Keep-AliveContent-Encoding: gzipContent-Type: text/html;charset=utf-8Date: Sat, 10 Aug 2019 08:00:56 GMTExpires: Sat, 10 Aug 2019 08:00:56 GMTServer: BWS/1.1Set-Cookie: BDSVRTM=269; path=/Set-Cookie: BD_HOME=1; path=/Set-Cookie: H_PS_PSSID=xxx; path=/; domain=.baidu.comStrict-Transport-Security: max-age=172800Transfer-Encoding: chunkedX-Ua-Compatible: IE=Edge,chrome=1 响应正文 对应的响应正文 12&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;&lt;meta http-equiv=X-UA-Compatible content=IE=Edge&gt;&lt;meta content=always name=referrer&gt;&lt;link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css&gt;&lt;title&gt;百度一下，你就知道&lt;/title&gt;&lt;/head&gt; &lt;body link=#0000cc&gt; &lt;div id=wrapper&gt; &lt;div id=head&gt; &lt;div class=head_wrapper&gt; &lt;div class=s_form&gt; &lt;div class=s_form_wrapper&gt; &lt;div id=lg&gt; &lt;img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129&gt; &lt;/div&gt; &lt;form id=form name=f action=//www.baidu.com/s class=fm&gt; &lt;input type=hidden name=bdorz_come value=1&gt; &lt;input type=hidden name=ie value=utf-8&gt; &lt;input type=hidden name=f value=8&gt; &lt;input type=hidden name=rsv_bp value=1&gt; &lt;input type=hidden name=rsv_idx value=1&gt; &lt;input type=hidden name=tn value=baidu&gt;&lt;span class="bg s_ipt_wr"&gt;&lt;input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus&gt;&lt;/span&gt;&lt;span class="bg s_btn_wr"&gt;&lt;input type=submit id=su value=百度一下 class="bg s_btn"&gt;&lt;/span&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=u1&gt; &lt;a href=http://news.baidu.com name=tj_trnews class=mnav&gt;新闻&lt;/a&gt; &lt;a href=http://www.hao123.com name=tj_trhao123 class=mnav&gt;hao123&lt;/a&gt; &lt;a href=http://map.baidu.com name=tj_trmap class=mnav&gt;地图&lt;/a&gt; &lt;a href=http://v.baidu.com name=tj_trvideo class=mnav&gt;视频&lt;/a&gt; &lt;a href=http://tieba.baidu.com name=tj_trtieba class=mnav&gt;贴吧&lt;/a&gt; &lt;noscript&gt; &lt;a href=http://www.baidu.com/bdorz/login.gif?login&amp;amp;tpl=mn&amp;amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb&gt;登录&lt;/a&gt; &lt;/noscript&gt; &lt;script&gt;document.write('&lt;a href="http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u='+ encodeURIComponent(window.location.href+ (window.location.search === "" ? "?" : "&amp;")+ "bdorz_come=1")+ '" name="tj_login" class="lb"&gt;登录&lt;/a&gt;');&lt;/script&gt; &lt;a href=//www.baidu.com/more/ name=tj_briicon class=bri style="display: block;"&gt;更多产品&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=ftCon&gt; &lt;div id=ftConw&gt; &lt;p id=lh&gt; &lt;a href=http://home.baidu.com&gt;关于百度&lt;/a&gt; &lt;a href=http://ir.baidu.com&gt;About Baidu&lt;/a&gt; &lt;/p&gt; &lt;p id=cp&gt;&amp;copy;2017&amp;nbsp;Baidu&amp;nbsp;&lt;a href=http://www.baidu.com/duty/&gt;使用百度前必读&lt;/a&gt;&amp;nbsp; &lt;a href=http://jianyi.baidu.com/ class=cp-feedback&gt;意见反馈&lt;/a&gt;&amp;nbsp;京ICP证030173号&amp;nbsp; &lt;img src=//www.baidu.com/img/gs.gif&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; HTTP请求GET和POST的区别1.GET提交，请求的数据会附在URL之后（就是把数据放置在HTTP协议头＜request-line＞中），以?分割URL和传输数据，多个参数用&amp;连接;例如：login.action?name=hyddd&amp;password=idontknow&amp;verify=%E4%BD%A0 %E5%A5%BD。如果数据是英文字母/数字，原样发送，如果是空格，转换为+，如果是中文/其他字符，则直接把字符串用BASE64加密，得出如： %E4%BD%A0%E5%A5%BD，其中％XX中的XX为该符号以16进制表示的ASCII。 POST提交：把提交的数据放置在是HTTP包的包体＜request-body＞中。上文示例中红色字体标明的就是实际的传输数据 因此，GET提交的数据会在地址栏中显示出来，而POST提交，地址栏不会改变 2.传输数据的大小： 首先声明,HTTP协议没有对传输的数据大小进行限制，HTTP协议规范也没有对URL长度进行限制。 而在实际开发中存在的限制主要有： GET:特定浏览器和服务器对URL长度有限制，例如IE对URL长度的限制是2083字节(2K+35)。对于其他浏览器，如Netscape、FireFox等，理论上没有长度限制，其限制取决于操作系统的支持。 因此对于GET提交时，传输数据就会受到URL长度的限制。 POST:由于不是通过URL传值，理论上数据不受限。但实际各个WEB服务器会规定对post提交数据大小进行限制，Apache、IIS6都有各自的配置。 3.安全性： POST的安全性要比GET的安全性高。注意：这里所说的安全性和上面GET提到的“安全”不是同个概念。上面“安全”的含义仅仅是不作数据修改，而这里安全的含义是真正的Security的含义，比如：通过GET提交数据，用户名和密码将明文出现在URL上，因为(1)登录页面有可能被浏览器缓存， (2)其他人查看浏览器的历史纪录，那么别人就可以拿到你的账号和密码了 References[1] 一篇文章带你熟悉 TCP/IP 协议（网络协议篇二）[2] 一篇文章带你详解 HTTP 协议（网络协议篇一）[3] Http 和 Socket 到底是哪门子亲戚[4] 你必须知道的HTTP基本概念[5] 你必须知道的HTTP基本概念[6] 一文读懂OSI七层模型与TCP/IP四层的区别/联系[7] 前端经典面试题: 从输入URL到页面加载发生了什么？[8] 从输入 URL 到页面加载完成的过程中都发生了什么事情？[9] OSI模型的通俗理解以及和TCP-IP模型的区别[10] HTTP详解(1)-工作原理[11] HTTP请求报文和HTTP响应报文[12] HTTP请求、响应报文格式]]></content>
      <categories>
        <category>network</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[httpclient 错误 笔记]]></title>
    <url>%2F2017%2F12%2F11%2Fhttpclient-error-notes%2F</url>
    <content type="text"><![CDATA[No permitted “Access-Control-Allow-Origin” header.12345net.sourceforge.htmlunit.corejs.javascript.WrappedException: Wrapped java.lang.RuntimeException: No permitted &quot;Access-Control-Allow-Origin&quot; header. at net.sourceforge.htmlunit.corejs.javascript.Context.throwAsScriptRuntimeEx(Context.java:2052) ~[htmlunit-core-js-2.23.jar:na] Caused by: java.lang.RuntimeException: No permitted &quot;Access-Control-Allow-Origin&quot; header. ... 9 common frames omitted java.security.ProviderException: java.security.KeyException123Caused by: java.security.ProviderException: java.security.KeyException Caused by: java.security.KeyException 我的JDK环境是sun的1.8.0，操作系统CentOS 6.5 运行如下linux命令后程序可以完美运行，BUG解决：yum upgrade nss java.net.SocketException: Permission denied: connect12java.net.SocketException: Permission denied: connect at java.net.DualStackPlainSocketImpl.waitForConnect(Native Method) ~[na:1.8.0_111] 被墙了 connect timed out1234org.apache.http.conn.ConnectTimeoutException: Connect to zh.wikipedia.org:443 [zh.wikipedia.org/208.80.154.224] failed: connect timed out at org.apache.http.impl.conn.HttpClientConnectionOperator.connect(HttpClientConnectionOperator.java:132) ~[httpclient-4.3.5.jar:4.3.5]Caused by: java.net.SocketTimeoutException: connect timed out 2个可能原因1.被墙了2.网断了 java.net.UnknownHostException1java.net.UnknownHostException: zh.wikipedia.org UnknownHost 可能被墙了 Content-Length header already present12345org.apache.http.client.ClientProtocolException: null at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:186) ~[httpclient-4.5.2.jar:4.5.2]Caused by: org.apache.http.ProtocolException: Content-Length header already present at org.apache.http.protocol.RequestContent.process(RequestContent.java:97) ~[httpcore-4.4.6.jar:4.4.6] 查看httpclient中RequestContent类的process方法，看到当body非空时，会自动加上Content-Length请求头及其对应值，不需要自己手动加上它。 参考 解决HttpClient：org.apache.http.ProtocolException异常 Read timed out12java.net.SocketTimeoutException: Read timed out at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_111] 超时 javax.net.ssl.SSLException: Unrecognized SSL message, plaintext connection12345678910111213141516171819javax.net.ssl.SSLException: Unrecognized SSL message, plaintext connection? at sun.security.ssl.InputRecord.handleUnknownRecord(InputRecord.java:710) at sun.security.ssl.InputRecord.read(InputRecord.java:527) at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:983) at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1385) at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1413) at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1397) at org.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:396) at org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:355) at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142) at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:373) at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:389) at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:237) at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:185) at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89) at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:111) at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83) at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:108) 遇到这个问题，可能有两种情况： 对方使用的https的链接，但其实用的是http的服务，遇到这种情况url用https，但是代理用http，然后就可以解决问题了。 对方服务器是HTTP Server，不是HTTPS Server，所以没必要用HTTPS，用HTTP就可以。虽然url显示的是https，但是你可以用http请求到。 强制jvm使用IPv4堆栈来解决错误 -Djava.net.preferIPv4Stack=true -Dhttps.protocols=”TLSv1” -Djsse.enableSNIExtension=false I got the same error. it was because I was accessing the https port using http.. The issue solved when I changed http to https. I think this is due to the connection established with the client machine is not secure.It is due to the fact that you are talking to an HTTP server, not an HTTPS server. Probably you didn’t use the correct port number for HTTPS. References[1] http://www.javaranger.com/archives/674 解决HttpClient：org.apache.http.ProtocolException异常[2] https://mp.weixin.qq.com/s/_Ya-tfBLukOAspg7L90gFA##[3] http://www.data5u.com/api/create.html#javacode_2 创建API接口_接口文档Java示例代码无忧代理IP[4] http://www.data5u.com/dayip-1-10.html 每日最新免费代理IP分享-无忧代理IP[5] https://stackoverflow.com/questions/6532273/unrecognized-ssl-message-plaintext-connection-exception[6] http://www.javaranger.com/archives/674]]></content>
      <categories>
        <category>crawler</category>
      </categories>
      <tags>
        <tag>crawler</tag>
        <tag>httpclient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取 裁判文书]]></title>
    <url>%2F2017%2F12%2F06%2Fcrawler-wenshu%2F</url>
    <content type="text"><![CDATA[裁判文书爬取只写了思路，有部分内容没公开。这篇文章只提供思路，不提供实现。仅供参考。 如果经济允许，可以直接购买裁判文书网的文书，一年10万元，和爬虫工程师开销和代理费用来说，10万元很划算了。 如果做实验使用，可以邮件或者加qq群在群里要，数量在百万以下一般会直接给。也免得增加文书网服务器的压力。 文书网已经放弃使用瑞数，现在使用的是免费的sojson加密 2019-04-09 文书网停服更新 2019-01-31 疯狂封ip 2018-08-07 增加7道爬虫防御，增加反爬难度，文书列表页返回的docId加密 2018-01-31 修改获取vl5x的js 还是eval函数，原理没变，换了换字符 2017-06-23 更新反爬策略 增加反爬难度 用的瑞数的产品 每一次请求翻页都要过一次验证码，加密一次密钥，带密钥访问下一个页面 谨以此文献给在我生命中留下回忆的非结构化团队。谢谢领导给了我充足的时间研究裁判文书网。 (1) 2019-06版 爬取裁判文书 首先找获取数据的页面。 浏览器 F12 很快就可以找到返回数据的list页面 http://wenshu.court.gov.cn/List/ListContent ，不过返回的数据是加密的。 用的是Post请求，请求参数有 Param Index Page Order Direction vl5x number guid ，cookie里的参数有 _gscu_2116842793 Hm_lvt_d2caefee2de09b8a6ea438d74fd98db2 _gscbrs_2116842793 ASP.NET_SessionId vjkl5 Hm_lpvt_d2caefee2de09b8a6ea438d74fd98db2 _gscs_2116842793，用于反爬参数的有3个，vl5x、number、guid 。 ctrl + F 搜索 guid number vl5x 发现 guid 是通过js生成。 number 第一页默认是 wens 后面的是数字 vl5x 通过混淆加密的js获得。在获取时要另获取vjkl5参数才能计算出vl5x data: { &quot;Param&quot;: listparam, &quot;Index&quot;: index, &quot;Page&quot;: page, &quot;Order&quot;: order, &quot;Direction&quot;: direction, &quot;vl5x&quot;: getKey(), &quot;number&quot;: yzm1, &quot;guid&quot;: guid1 }, Lawyee.CPWSW.JsTree.js Lawyee.CPWSW.List.js Lawyee.CPWSW.ListExtend.js pako.min.js (1.1) guid Lawyee.CPWSW.List.js 123456var createGuid = function () &#123; return (((1 + Math.random()) * 0x10000) | 0).toString(16).substring(1);&#125; var guid1 = createGuid() + createGuid() + "-" + createGuid() + "-" + createGuid() + createGuid() + "-" + createGuid() + createGuid() + createGuid(); (1.2) number number 默认为 wens Lawyee.CPWSW.List.js 看到这段代码，一阵心痛，看来文书网的程序员们很无奈，大家爬的时候注意一点，相互体谅一下，程序员何苦为难程序员。 1234567891011121314151617181920212223242526272829var url = window.location.href;var nyzm = url.indexOf("&amp;number");var subyzm = url.substring(nyzm + 1);var n1yzm = subyzm.indexOf("&amp;");var yzm1 = subyzm.substr(7, 4);var nyzm = url.indexOf("&amp;guid");var subguid = url.substring(nyzm + 1);var n1guid = subguid.indexOf("&amp;");var guid1 = subguid.substr(5, 35);if (yzm != undefined &amp;&amp; yzm != "undefined") &#123; yzm1 = yzm;&#125; if (guid != undefined &amp;&amp; guid != "undefined") &#123; guid1 = guid;&#125;var yzm1; //dzf 20180805 验证码没什么作用，屏蔽// $.ajax(&#123;// url: "/ValiCode/GetCode", type: "POST", async: false,// data: &#123; "guid": guid1 &#125;,// success: function (data) &#123;// yzm1 = data;// &#125;// &#125;); data: &#123; "Param": listparam, "Index": index, "Page": page, "Order": order, "Direction": direction, "vl5x": getKey(), "number": yzm1, "guid": guid1 &#125;, (1.3) vl5x Lawyee.CPWSW.ListExtend.js 还是老样子，eval函数，有换了个字符，不过看整个js又增加了点复杂度(对爬取的人复杂) 12345678910111213141516171819202122232425262728293031323334353637383940414243var _fxxx = function (p, a, c, k, e, d) &#123; e = function (c) &#123; return (c &lt; a ? "" : e(parseInt(c / a))) + ((c = c % a) &gt; 35 ? String.fromCharCode(c + 29) : c.toString(36)) &#125;; if (!"".replace(/^/, String)) &#123; while (c--) &#123; d[e(c)] = k[c] || e(c) &#125; k = [function (e) &#123; return d[e] &#125;]; e = function () &#123; return "\\w+" &#125;; c = 1 &#125; while (c--) &#123; if (k[c]) &#123; p = p.replace(new RegExp("\\b" + e(c) + "\\b", "g"), k[c]) &#125; &#125; return p&#125;;function de(str, count, strReplace) &#123; var arrReplace = strReplace.split("|"); for (var i = 0; i &lt; count; i++) &#123; str = str.replace(new RegExp("\\&#123;" + i + "\\&#125;", "g"), arrReplace[i]) &#125; return str&#125;function getKey() &#123; eval(de('eval(_fxxx(\'e n(7)&#123;9 d=0;j(9 i=0;i&lt;7.k;i++)&#123;d+=(7.g(i)&lt;&lt;(i%m))&#125;f d&#125;e p(7)&#123;9 d=0;j(9 i=0;i&lt;7.k;i++)&#123;d+=(7.g(i)&lt;&lt;(i%m))+i&#125;f d&#125;e E(7,o)&#123;9 d=0;j(9 i=0;i&lt;7.k;i++)&#123;d+=(7.g(i)&lt;&lt;(i%m))+(i*o)&#125;f d&#125;e x(7,o)&#123;9 d=0;j(9 i=0;i&lt;7.k;i++)&#123;d+=(7.g(i)&lt;&lt;(i%m))+(i+o-7.g(i))&#125;f d&#125;e z(7)&#123;9 7=7.8(5,5*5)+7.8((5+1)*(5+1),3);9 a=7.8(5)+7.8(-4);9 b=7.8(4)+a.8(-6);f h(7).8(4,l)&#125;e w(7)&#123;9 7=7.8(5,5*5)+"5"+7.8(1,2)+"1"+7.8((5+1)*(5+1),3);9 a=7.8(5)+7.8(4);9 b=7.8(t)+a.8(-6);9 c=7.8(4)+a.8(6);f h(c).8(4,l)&#125;e A(7)&#123;9 7=7.8(5,5*5)+"r"+7.8(1,2)+7.8((5+1)*(5+1),3);9 a=n(7.8(5))+7.8(4);9 b=n(7.8(5))+7.8(4);9 c=7.8(4)+b.8(5);f h(c).8(1,l)&#125;e y(7)&#123;9 7=7.8(5,5*5)+"r"+7.8(1,2)+7.8((5+1)*(5+1),3);9 a=p(7.8(5))+7.8(4);9 b=7.8(4)+a.8(5);9 c=n(7.8(5))+7.8(4);f h(b).8(3,l)&#125;e B(7)&#123;9 7=7.8(5,5*5)+"2"+7.8(1,2)+7.8((5+1)*(5+1),3);9 d=0;j(9 i=0;i&lt;7.8(1).k;i++)&#123;d+=(7.g(i)&lt;&lt;(i%m))&#125;9 s=d+7.8(4);9 d=0;9 a=7.8(5);j(9 i=0;i&lt;a.k;i++)&#123;d+=(a.g(i)&lt;&lt;(i%m))+i&#125;a=d+""+7.8(4);9 b=h(7.8(1))+n(a.8(5));f h(b).8(3,l)&#125;e v(7)&#123;9 q=u C();9 7=q.F(7.8(5,5*5)+7.8(1,2)+"1")+7.8((5+1)*(5+1),3);9 a=p(7.8(4,D))+7.8(-4);9 b=h(7.8(4))+a.8(2);9 a=7.8(3);9 c=n(7.8(5))+7.8(4);9 s=d+7.8(4);9 d=0;j(9 i=0;i&lt;a.k;i++)&#123;d+=(a.g(i)&lt;&lt;(i%t))+i&#125;a=d+""+7.8(4);f h(7).8(4,l)&#125;\', 42, 42, \'|||||||str|substr|var||||long|&#123;0&#125;|return|charCodeAt|hex_md5||for|length|24|16|strToLong|step|strToLongEn|base|15|aa|12|new|&#123;1&#125;5|&#123;1&#125;1|strToLongEn3|&#123;1&#125;3|&#123;1&#125;0|&#123;1&#125;2|&#123;1&#125;4|Base64|10|strToLongEn2|encode\'.split(\'|\'), 0, &#123;&#125;))', 4, "function|makeKey_|(k(0)+|(c(0)+")); eval(_fxxx('o B(8)&#123;d j=p q();d 8=8.9(5,5*5)+8.9((5+1)*(5+1),3);d a=j.s(8.9(4,G))+8.9(2);d b=8.9(6)+a.9(2);d c=x(8.9(5))+8.9(4);d r=e+8.9(4);d e=0;d a=8.9(5);h(d i=0;i&lt;a.g;i++)&#123;e+=(a.f(i)&lt;&lt;(i%k))+i&#125;a=e+""+8.9(4);n l(b).9(2,m)&#125;o F(8)&#123;d j=p q();d 8=j.s(8.9(5,5*4)+"E"+8.9(1,2))+8.9((5+1)*(5+1),3);d e=0;h(d i=0;i&lt;8.9(1).g;i++)&#123;e+=(8.f(i)&lt;&lt;(i%k+5))+3+5&#125;d r=e+8.9(4);d e=0;d a=8.9(5);h(d i=0;i&lt;a.g;i++)&#123;e+=(a.f(i)&lt;&lt;(i%k))&#125;a=e+""+8.9(4);d b=l(8.9(1))+x(a.9(5));n l(b).9(3,m)&#125;o H(8)&#123;d j=p q();d 8=j.s(8.9(5,5*5-1)+"5"+"-"+"5")+8.9(1,2)+8.9((5+1)*(5+1),3);d e=0;h(d i=0;i&lt;8.9(1).g;i++)&#123;e+=(8.f(i)&lt;&lt;(i%k))&#125;d r=e+8.9(4);d e=0;d a=8.9(5);h(d i=0;i&lt;a.g;i++)&#123;e+=(a.f(i)&lt;&lt;(i%k))&#125;a=e+""+8.9(4);d b=l(8.9(1))+K(a.9(5));n l(b).9(4,m)&#125;o J(8)&#123;d 8=8.9(5,5*5)+"5"+8.9(1,2)+"1"+8.9((5+1)*(5+1),3);d a=8.9(5)+8.9(4);d b=8.9(I)+a.9(-6);d c=t(8.9(4))+a.9(6);n l(c).9(4,m)&#125;o w(8)&#123;d j=p q();d 8=j.s(8.9(5,5*5-1)+"5")+8.9(1,2)+8.9((5+1)*(5+1),3);d e=0;h(d i=0;i&lt;8.9(1).g;i++)&#123;e+=(8.f(i)&lt;&lt;(i%k))&#125;d r=e+8.9(4);d e=0;d a=8.9(5);h(d i=0;i&lt;a.g;i++)&#123;e+=(a.f(i)&lt;&lt;(i%k))&#125;a=e+""+8.9(4);d b=l(8.9(1))+t(a.9(5));n l(b).9(4,m)&#125;o D(8)&#123;d j=p q();d 8=8.9(5,5*5-1)+"2"+8.9(1,2)+8.9((5+1)*(5+1),3);d e=0;h(d i=0;i&lt;8.9(1).g;i++)&#123;e+=(8.f(i)&lt;&lt;(i%k))&#125;d r=e+8.9(4);d e=0;d a=8.9(5);h(d i=0;i&lt;a.g;i++)&#123;e+=(a.f(i)&lt;&lt;(i%k))&#125;a=e+""+8.9(2);d b=8.9(1)+t(a.9(5));n l(b).9(2,m)&#125;o y(8)&#123;d j=p q();d 8=8.9(5,5*5-1)+8.9((5+1)*(5+1),3)+"2"+8.9(1,2);d e=0;h(d i=0;i&lt;8.9(1).g;i++)&#123;e+=(8.f(i)&lt;&lt;(i%k))&#125;d r=e+8.9(4);d e=0;d a=8.9(5);h(d i=0;i&lt;a.g;i++)&#123;e+=(a.f(i)&lt;&lt;(i%k))&#125;a=e+""+8.9(2);d b=8.9(1)+t(8.9(5));n l(b).9(1,m)&#125;o z(8)&#123;d j=p q();d 8=8.9(5,5*5-1)+"2"+8.9(1,2);d e=0;h(d i=0;i&lt;8.9(1).g;i++)&#123;e+=(8.f(i)&lt;&lt;(i%k))&#125;d r=e+8.9(4);d e=0;d a=8.9(5);h(d i=0;i&lt;a.g;i++)&#123;e+=(a.f(i)&lt;&lt;(i%k))&#125;a=e+""+8.9(2);d b=j.s(8.9(1)+t(8.9(5)));n l(b).9(1,m)&#125;o C(8)&#123;d j=p q();d 8=8.9(5,5*5-1)+"2"+8.9(1,2);d e=0;h(d i=0;i&lt;8.9(1).g;i++)&#123;e+=(8.f(i)&lt;&lt;(i%k))&#125;d r=e+8.9(4);d e=0;d a=8.9(5);h(d i=0;i&lt;a.g;i++)&#123;e+=(a.f(i)&lt;&lt;(i%k))&#125;a=e+""+8.9(2);d b=j.s(8.9(1)+8.9(5)+8.9(1,3));n t(b).9(1,m)&#125;o A(8)&#123;d j=p q();d 8=8.9(5,5*5-1)+"2"+8.9(1,2);d e=0;h(d i=0;i&lt;8.9(1).g;i++)&#123;e+=(8.f(i)&lt;&lt;(i%k))&#125;d r=e+8.9(4);d e=0;d a=8.9(5);h(d i=0;i&lt;a.g;i++)&#123;e+=(a.f(i)&lt;&lt;(i%k))&#125;a=e+""+8.9(2);d b=j.s(a.9(1)+8.9(5)+8.9(2,3));n t(b).9(1,m)&#125;o N(8)&#123;d j=p q();d 8=8.9(5,5*5-1)+"2"+8.9(1,2)+"-"+"5";d e=0;h(d i=0;i&lt;8.9(1).g;i++)&#123;e+=(8.f(i)&lt;&lt;(i%u))&#125;d r=e+8.9(4);d e=0;d a=8.9(5);h(d i=0;i&lt;a.g;i++)&#123;e+=(a.f(i)&lt;&lt;(i%k))+i&#125;a=e+""+8.9(2);d b=j.s(a.9(1))+v(8.9(5),5)+8.9(2,3);n l(b).9(2,m)&#125;o L(8)&#123;d j=p q();d 8=8.9(5,5*5-1)+"7"+8.9(1,2)+"-"+"5";d e=0;h(d i=0;i&lt;8.9(1).g;i++)&#123;e+=(8.f(i)&lt;&lt;(i%u))&#125;d r=e+8.9(4);d e=0;d a=8.9(5);h(d i=0;i&lt;a.g;i++)&#123;e+=(a.f(i)&lt;&lt;(i%k))+i&#125;a=e+""+8.9(2);d b=j.s(a.9(1))+v(8.9(5),5+1)+8.9(2+5,3);n l(b).9(0,m)&#125;o R(8)&#123;d j=p q();d 8=8.9(5,5*5-1)+"7"+8.9(1,2)+"5"+8.9(2+5,3);d e=0;h(d i=0;i&lt;8.9(1).g;i++)&#123;e+=(8.f(i)&lt;&lt;(i%u))&#125;d r=e+8.9(4);d e=0;d a=8.9(5);h(d i=0;i&lt;a.g;i++)&#123;e+=(a.f(i)&lt;&lt;(i%k))+i&#125;a=e+""+8.9(2);d b=a.9(1)+v(8.9(5),5+1)+8.9(2+5,3);n l(b).9(0,m)&#125;o P(8)&#123;d j=p q();d 8=8.9(5,5*5-1)+"7"+8.9(5,2)+"5"+8.9(2+5,3);d e=0;h(d i=0;i&lt;8.9(1).g;i++)&#123;e+=(8.f(i)&lt;&lt;(i%u))&#125;d r=e+8.9(4);d e=0;d a=8.9(5);h(d i=0;i&lt;a.g;i++)&#123;e+=(a.f(i)&lt;&lt;(i%k))+i&#125;a=e+""+8.9(2);d b=a.9(1)+O(8.9(5),5-1)+8.9(2+5,3);n l(b).9(0,m)&#125;o M(8)&#123;n l(w(8)+Q(8)).9(1,m)&#125;', 54, 54, "||||||||str|substr||||var|long|charCodeAt|length|for||base|16|hex_md5|24|return|function|new|Base64|aa|encode|hex_sha1|11|strToLongEn2|makeKey_10|strToLong|makeKey_12|makeKey_13|makeKey_15|makeKey_6|makeKey_14|makeKey_11|55|makeKey_7|10|makeKey_8|12|makeKey_9|strToLongEn|makeKey_17|makeKey_20|makeKey_16|strToLongEn3|makeKey_19|makeKey_5|makeKey_18".split("|"), 0, &#123;&#125;)); eval(_fxxx("6 3f(0)&#123;7 5(1v(0)+g(0)).8(2,24)&#125;6 1w(0)&#123;7 5(k(0)+b(0)).8(3,24)&#125;6 1x(0)&#123;7 5(i(0)+9(0)).8(4,24)&#125;6 1s(0)&#123;7 5(j(0)+a(0)).8(1,24)&#125;6 1t(0)&#123;7 5(h(0)+c(0)).8(2,24)&#125;6 1u(0)&#123;7 5(f(0)+m(0)).8(3,24)&#125;6 1y(0)&#123;7 5(e(0)+g(0)).8(4,24)&#125;6 1C(0)&#123;7 5(d(0)+l(0)).8(1,24)&#125;6 1D(0)&#123;7 5(b(0)+g(0)).8(2,24)&#125;6 1E(0)&#123;7 5(9(0)+l(0)).8(3,24)&#125;6 1z(0)&#123;7 5(a(0)+n(0)).8(4,24)&#125;6 1A(0)&#123;7 5(c(0)+k(0)).8(3,24)&#125;6 1B(0)&#123;7 5(m(0)+i(0)).8(4,24)&#125;6 1i(0)&#123;7 5(g(0)+j(0)).8(1,24)&#125;6 1j(0)&#123;7 5(l(0)+h(0)).8(2,24)&#125;6 1k(0)&#123;7 5(n(0)+f(0)).8(3,24)&#125;6 1f(0)&#123;7 5(1g(0)+e(0)).8(1,24)&#125;6 1h(0)&#123;7 5(o(0)+d(0)).8(2,24)&#125;6 1l(0)&#123;7 5(k(0)+b(0)).8(3,24)&#125;6 1p(0)&#123;7 5(i(0)+9(0)).8(4,24)&#125;6 1q(0)&#123;7 5(j(0)+a(0)).8(3,24)&#125;6 1r(0)&#123;7 5(h(0)+c(0)).8(4,24)&#125;6 1m(0)&#123;7 5(f(0)+m(0)).8(1,24)&#125;6 1n(0)&#123;7 5(e(0)+g(0)).8(2,24)&#125;6 1o(0)&#123;7 5(d(0)+l(0)).8(3,24)&#125;6 1V(0)&#123;7 5(b(0)+e(0)).8(4,24)&#125;6 1W(0)&#123;7 5(9(0)+d(0)).8(1,24)&#125;6 1X(0)&#123;7 5(a(0)+b(0)).8(2,24)&#125;6 1S(0)&#123;7 5(c(0)+9(0)).8(3,24)&#125;6 1T(0)&#123;7 5(m(0)+a(0)).8(4,24)&#125;6 1U(0)&#123;7 5(g(0)+c(0)).8(1,24)&#125;6 1Y(0)&#123;7 5(l(0)+k(0)).8(2,24)&#125;6 22(0)&#123;7 5(o(0)+i(0)).8(3,24)&#125;6 23(0)&#123;7 5(k(0)+j(0)).8(4,24)&#125;6 25(0)&#123;7 5(i(0)+h(0)).8(3,24)&#125;6 1Z(0)&#123;7 5(j(0)+f(0)).8(4,24)&#125;6 20(0)&#123;7 5(h(0)+e(0)).8(1,24)&#125;6 21(0)&#123;7 5(f(0)+d(0)).8(2,24)&#125;6 1I(0)&#123;7 5(e(0)+b(0)).8(3,24)&#125;6 1J(0)&#123;7 5(d(0)+9(0)).8(1,24)&#125;6 1K(0)&#123;7 5(b(0)+a(0)).8(2,24)&#125;6 1F(0)&#123;7 5(9(0)+c(0)).8(3,24)&#125;6 1G(0)&#123;7 5(a(0)+b(0)).8(4,24)&#125;6 1H(0)&#123;7 5(c(0)+9(0)).8(3,24)&#125;6 1L(0)&#123;7 5(k(0)+a(0)).8(1,24)&#125;6 1P(0)&#123;7 5(i(0)+c(0)).8(2,24)&#125;6 1Q(0)&#123;7 5(j(0)+m(0)).8(3,24)&#125;6 1R(0)&#123;7 5(h(0)+g(0)).8(4,24)&#125;6 1M(0)&#123;7 5(f(0)+l(0)).8(1,24)&#125;6 1N(0)&#123;7 5(e(0)+9(0)).8(2,24)&#125;6 1O(0)&#123;7 5(d(0)+a(0)).8(3,24)&#125;6 1e(0)&#123;7 5(b(0)+c(0)).8(4,24)&#125;6 M(0)&#123;7 5(9(0)+e(0)).8(1,24)&#125;6 D(0)&#123;7 5(a(0)+d(0)).8(2,24)&#125;6 E(0)&#123;7 5(k(0)+b(0)).8(3,24)&#125;6 F(0)&#123;7 5(i(0)+9(0)).8(4,24)&#125;6 C(0)&#123;7 5(j(0)+a(0)).8(3,24)&#125;6 z(0)&#123;7 5(h(0)+c(0)).8(4,24)&#125;6 A(0)&#123;7 5(f(0)+h(0)).8(1,24)&#125;6 B(0)&#123;7 5(e(0)+f(0)).8(2,24)&#125;6 K(0)&#123;7 5(d(0)+e(0)).8(3,24)&#125;6 L(0)&#123;7 5(k(0)+d(0)).8(1,24)&#125;6 J(0)&#123;7 5(i(0)+b(0)).8(4,24)&#125;6 G(0)&#123;7 5(j(0)+9(0)).8(1,24)&#125;6 H(0)&#123;7 5(h(0)+a(0)).8(2,24)&#125;6 I(0)&#123;7 5(f(0)+c(0)).8(3,24)&#125;6 s(0)&#123;7 5(k(0)+k(0)).8(4,24)&#125;6 x(0)&#123;7 5(i(0)+i(0)).8(1,24)&#125;6 u(0)&#123;7 5(j(0)+j(0)).8(2,24)&#125;6 p(0)&#123;7 5(h(0)+h(0)).8(3,24)&#125;6 t(0)&#123;7 5(f(0)+f(0)).8(4,24)&#125;6 w(0)&#123;7 5(e(0)+e(0)).8(3,24)&#125;6 v(0)&#123;7 5(d(0)+d(0)).8(4,24)&#125;6 y(0)&#123;7 5(b(0)+b(0)).8(1,24)&#125;6 q(0)&#123;7 5(9(0)+9(0)).8(2,24)&#125;6 r(0)&#123;7 5(a(0)+a(0)).8(3,24)&#125;6 N(0)&#123;7 5(c(0)+c(0)).8(4,24)&#125;6 14(0)&#123;7 5(m(0)+m(0)).8(3,24)&#125;6 15(0)&#123;7 5(g(0)+g(0)).8(4,24)&#125;6 16(0)&#123;7 5(l(0)+g(0)).8(1,24)&#125;6 11(0)&#123;7 5(f(0)+l(0)).8(2,24)&#125;6 12(0)&#123;7 5(e(0)+d(0)).8(1,24)&#125;6 13(0)&#123;7 5(d(0)+b(0)).8(2,24)&#125;6 17(0)&#123;7 5(b(0)+9(0)).8(3,24)&#125;6 1b(0)&#123;7 5(9(0)+9(0)).8(4,24)&#125;6 1c(0)&#123;7 5(a(0)+a(0)).8(1,24)&#125;6 1d(0)&#123;7 5(k(0)+k(0)).8(2,24)&#125;6 18(0)&#123;7 5(i(0)+i(0)).8(3,24)&#125;6 19(0)&#123;7 5(j(0)+j(0)).8(4,24)&#125;6 1a(0)&#123;7 5(h(0)+h(0)).8(1,24)&#125;6 R(0)&#123;7 5(f(0)+f(0)).8(2,24)&#125;6 S(0)&#123;7 5(e(0)+e(0)).8(3,24)&#125;6 T(0)&#123;7 5(d(0)+d(0)).8(4,24)&#125;6 O(0)&#123;7 5(b(0)+b(0)).8(3,24)&#125;6 P(0)&#123;7 5(9(0)+9(0)).8(4,24)&#125;6 Q(0)&#123;7 5(a(0)+a(0)).8(1,24)&#125;6 U(0)&#123;7 5(c(0)+c(0)).8(2,24)&#125;6 Y(0)&#123;7 5(m(0)+i(0)).8(3,24)&#125;6 Z(0)&#123;7 5(g(0)+j(0)).8(1,24)&#125;6 10(0)&#123;7 5(b(0)+h(0)).8(1,24)&#125;6 V(0)&#123;7 5(9(0)+f(0)).8(2,24)&#125;6 W(0)&#123;7 5(a(0)+e(0)).8(3,24)&#125;6 X(0)&#123;7 5(c(0)+d(0)).8(4,24)&#125;6 26(0)&#123;7 5(m(0)+b(0)).8(1,24)&#125;6 3c(0)&#123;7 5(g(0)+9(0)).8(2,24)&#125;6 3d(0)&#123;7 5(l(0)+a(0)).8(3,24)&#125;6 3e(0)&#123;7 5(g(0)+c(0)).8(4,24)&#125;6 39(0)&#123;7 5(l(0)+m(0)).8(1,24)&#125;6 3a(0)&#123;7 5(n(0)+g(0)).8(2,24)&#125;6 3b(0)&#123;7 5(k(0)+l(0)).8(3,24)&#125;6 3i(0)&#123;7 5(i(0)+f(0)).8(4,24)&#125;6 3j(0)&#123;7 5(j(0)+e(0)).8(3,24)&#125;6 3k(0)&#123;7 5(h(0)+d(0)).8(4,24)&#125;6 2E(0)&#123;7 5(f(0)+b(0)).8(1,24)&#125;6 3g(0)&#123;7 5(e(0)+9(0)).8(2,24)&#125;6 3h(0)&#123;7 5(d(0)+a(0)).8(1,24)&#125;6 38(0)&#123;7 5(b(0)+k(0)).8(2,24)&#125;6 2Z(0)&#123;7 5(9(0)+i(0)).8(3,24)&#125;6 30(0)&#123;7 5(a(0)+j(0)).8(4,24)&#125;6 31(0)&#123;7 5(c(0)+h(0)).8(1,24)&#125;6 2W(0)&#123;7 5(m(0)+f(0)).8(2,24)&#125;6 2X(0)&#123;7 5(g(0)+e(0)).8(3,24)&#125;6 2Y(0)&#123;7 5(l(0)+d(0)).8(4,24)&#125;6 35(0)&#123;7 5(e(0)+b(0)).8(1,24)&#125;6 36(0)&#123;7 5(d(0)+9(0)).8(2,24)&#125;6 37(0)&#123;7 5(b(0)+a(0)).8(3,24)&#125;6 32(0)&#123;7 5(9(0)+c(0)).8(4,24)&#125;6 33(0)&#123;7 5(a(0)+m(0)).8(3,24)&#125;6 34(0)&#123;7 5(c(0)+g(0)).8(4,24)&#125;6 3D(0)&#123;7 5(k(0)+b(0)).8(1,24)&#125;6 3B(0)&#123;7 5(i(0)+9(0)).8(2,24)&#125;6 3C(0)&#123;7 5(j(0)+a(0)).8(3,24)&#125;6 3y(0)&#123;7 5(h(0)+c(0)).8(1,24)&#125;6 3z(0)&#123;7 5(f(0)+m(0)).8(1,24)&#125;6 3A(0)&#123;7 5(e(0)+g(0)).8(2,24)&#125;6 3I(0)&#123;7 5(d(0)+l(0)).8(3,24)&#125;6 3J(0)&#123;7 5(b(0)+g(0)).8(4,24)&#125;6 3H(0)&#123;7 5(9(0)+l(0)).8(1,24)&#125;6 3E(0)&#123;7 5(a(0)+n(0)).8(2,24)&#125;6 3F(0)&#123;7 5(c(0)+k(0)).8(3,24)&#125;6 3G(0)&#123;7 5(b(0)+i(0)).8(4,24)&#125;6 3x(0)&#123;7 5(9(0)+j(0)).8(1,24)&#125;6 3o(0)&#123;7 5(a(0)+h(0)).8(2,24)&#125;6 3p(0)&#123;7 5(c(0)+f(0)).8(3,24)&#125;6 3q(0)&#123;7 5(m(0)+e(0)).8(4,24)&#125;6 3l(0)&#123;7 5(g(0)+d(0)).8(3,24)&#125;6 3m(0)&#123;7 5(l(0)+b(0)).8(4,24)&#125;6 3n(0)&#123;7 5(9(0)+9(0)).8(1,24)&#125;6 3u(0)&#123;7 5(a(0)+a(0)).8(2,24)&#125;6 3v(0)&#123;7 5(c(0)+c(0)).8(3,24)&#125;6 3w(0)&#123;7 5(e(0)+m(0)).8(1,24)&#125;6 3r(0)&#123;7 5(d(0)+g(0)).8(2,24)&#125;6 3s(0)&#123;7 5(b(0)+l(0)).8(3,24)&#125;6 3t(0)&#123;7 5(9(0)+e(0)).8(4,24)&#125;6 2V(0)&#123;7 5(a(0)+d(0)).8(1,24)&#125;6 2n(0)&#123;7 5(c(0)+b(0)).8(2,24)&#125;6 2o(0)&#123;7 5(h(0)+9(0)).8(3,24)&#125;6 2p(0)&#123;7 5(f(0)+a(0)).8(4,24)&#125;6 2k(0)&#123;7 5(e(0)+c(0)).8(1,24)&#125;6 2l(0)&#123;7 5(d(0)+k(0)).8(3,24)&#125;6 2m(0)&#123;7 5(b(0)+i(0)).8(1,24)&#125;6 2t(0)&#123;7 5(9(0)+j(0)).8(2,24)&#125;6 2u(0)&#123;7 5(a(0)+h(0)).8(3,24)&#125;6 2v(0)&#123;7 5(c(0)+f(0)).8(4,24)&#125;6 2q(0)&#123;7 5(k(0)+e(0)).8(3,24)&#125;6 2r(0)&#123;7 5(i(0)+d(0)).8(4,24)&#125;6 2s(0)&#123;7 5(j(0)+b(0)).8(4,24)&#125;6 2j(0)&#123;7 5(h(0)+9(0)).8(1,24)&#125;6 2a(0)&#123;7 5(f(0)+a(0)).8(2,24)&#125;6 2b(0)&#123;7 5(e(0)+c(0)).8(3,24)&#125;6 2c(0)&#123;7 5(d(0)+b(0)).8(4,24)&#125;6 27(0)&#123;7 5(b(0)+9(0)).8(1,24)&#125;6 28(0)&#123;7 5(9(0)+a(0)).8(2,24)&#125;6 29(0)&#123;7 5(a(0)+c(0)).8(3,24)&#125;6 2g(0)&#123;7 5(c(0)+k(0)).8(4,24)&#125;6 2h(0)&#123;7 5(m(0)+i(0)).8(3,24)&#125;6 2i(0)&#123;7 5(g(0)+j(0)).8(4,24)&#125;6 2d(0)&#123;7 5(g(0)+h(0)).8(1,24)&#125;6 2e(0)&#123;7 5(l(0)+a(0)).8(2,24)&#125;6 2f(0)&#123;7 5(d(0)+b(0)).8(2,24)&#125;6 2M(0)&#123;7 5(b(0)+9(0)).8(3,24)&#125;6 2N(0)&#123;7 5(9(0)+a(0)).8(1,24)&#125;6 2O(0)&#123;7 5(a(0)+c(0)).8(2,24)&#125;6 2J(0)&#123;7 5(c(0)+m(0)).8(3,24)&#125;6 2K(0)&#123;7 5(k(0)+g(0)).8(4,24)&#125;6 2L(0)&#123;7 5(i(0)+l(0)).8(1,24)&#125;6 2S(0)&#123;7 5(j(0)+e(0)).8(2,24)&#125;6 2T(0)&#123;7 5(h(0)+d(0)).8(3,24)&#125;6 2U(0)&#123;7 5(f(0)+b(0)).8(4,24)&#125;6 2P(0)&#123;7 5(e(0)+9(0)).8(1,24)&#125;6 2Q(0)&#123;7 5(d(0)+a(0)).8(3,24)&#125;6 2R(0)&#123;7 5(b(0)+c(0)).8(1,24)&#125;6 2I(0)&#123;7 5(9(0)+k(0)).8(2,24)&#125;6 2z(0)&#123;7 5(a(0)+i(0)).8(3,24)&#125;6 2A(0)&#123;7 5(c(0)+j(0)).8(4,24)&#125;6 2B(0)&#123;7 5(b(0)+h(0)).8(3,24)&#125;6 2w(0)&#123;7 5(9(0)+f(0)).8(4,24)&#125;6 2x(0)&#123;7 5(a(0)+e(0)).8(4,24)&#125;6 2y(0)&#123;7 5(c(0)+d(0)).8(1,24)&#125;6 2F(0)&#123;7 5(m(0)+b(0)).8(2,24)&#125;6 2G(0)&#123;7 5(g(0)+9(0)).8(3,24)&#125;6 2H(0)&#123;7 5(l(0)+a(0)).8(4,24)&#125;6 2C(0)&#123;7 5(9(0)+c(0)).8(1,24)&#125;6 2D(0)&#123;7 5(a(0)+m(0)).8(2,24)&#125;", 62, 232, "str|||||hex_md5|function|return|substr|makeKey_0|makeKey_1|makeKey_19|makeKey_4|makeKey_18|makeKey_17|makeKey_10|makeKey_3|makeKey_9|makeKey_15|makeKey_16|makeKey_14|makeKey_7|makeKey_5|makeKey_8|makeKey_12|makeKey_90|makeKey_95|makeKey_96|makeKey_87|makeKey_91|makeKey_89|makeKey_93|makeKey_92|makeKey_88|makeKey_94|makeKey_78|makeKey_79|makeKey_80|makeKey_77|makeKey_74|makeKey_75|makeKey_76|makeKey_84|makeKey_85|makeKey_86|makeKey_83|makeKey_81|makeKey_82|makeKey_73|makeKey_97|makeKey_114|makeKey_115|makeKey_116|makeKey_111|makeKey_112|makeKey_113|makeKey_117|makeKey_121|makeKey_122|makeKey_123|makeKey_118|makeKey_119|makeKey_120|makeKey_101|makeKey_102|makeKey_103|makeKey_98|makeKey_99|makeKey_100|makeKey_104|makeKey_108|makeKey_109|makeKey_110|makeKey_105|makeKey_106|makeKey_107|makeKey_72|makeKey_37|makeKey_6|makeKey_38|makeKey_34|makeKey_35|makeKey_36|makeKey_39|makeKey_43|makeKey_44|makeKey_45|makeKey_40|makeKey_41|makeKey_42|makeKey_24|makeKey_25|makeKey_26|makeKey_11|makeKey_22|makeKey_23|makeKey_27|makeKey_31|makeKey_32|makeKey_33|makeKey_28|makeKey_29|makeKey_30|makeKey_62|makeKey_63|makeKey_64|makeKey_59|makeKey_60|makeKey_61|makeKey_65|makeKey_69|makeKey_70|makeKey_71|makeKey_66|makeKey_67|makeKey_68|makeKey_49|makeKey_50|makeKey_51|makeKey_46|makeKey_47|makeKey_48|makeKey_52|makeKey_56|makeKey_57|makeKey_58|makeKey_53|makeKey_54||makeKey_55|makeKey_124|makeKey_192|makeKey_193|makeKey_194|makeKey_189|makeKey_190|makeKey_191|makeKey_198|makeKey_199|makeKey_200|makeKey_195|makeKey_196|makeKey_197|makeKey_188|makeKey_179|makeKey_180|makeKey_181|makeKey_176|makeKey_177|makeKey_178|makeKey_185|makeKey_186|makeKey_187|makeKey_182|makeKey_183|makeKey_184|makeKey_217|makeKey_218|makeKey_219|makeKey_214|makeKey_215|makeKey_216|makeKey_223|makeKey_224|makeKey_134|makeKey_220|makeKey_221|makeKey_222|makeKey_213|makeKey_204|makeKey_205|makeKey_206|makeKey_201|makeKey_202|makeKey_203|makeKey_210|makeKey_211|makeKey_212|makeKey_207|makeKey_208|makeKey_209|makeKey_175|makeKey_141|makeKey_142|makeKey_143|makeKey_138|makeKey_139|makeKey_140|makeKey_147|makeKey_148|makeKey_149|makeKey_144|makeKey_145|makeKey_146|makeKey_137|makeKey_128|makeKey_129|makeKey_130|makeKey_125|makeKey_126|makeKey_127|makeKey_21|makeKey_135|makeKey_136|makeKey_131|makeKey_132|makeKey_133|makeKey_166|makeKey_167|makeKey_168|makeKey_163|makeKey_164|makeKey_165|makeKey_172|makeKey_173|makeKey_174|makeKey_169|makeKey_170|makeKey_171|makeKey_162|makeKey_153|makeKey_154|makeKey_155|makeKey_151|makeKey_152|makeKey_150|makeKey_159|makeKey_160|makeKey_161|makeKey_158|makeKey_156|makeKey_157".split("|"), 0, &#123;&#125;)); eval(_fxxx("5 y(0)&#123;7 6(d(0)+m(0)).9(3,8)&#125;5 z(0)&#123;7 6(e(0)+l(0)).9(4,8)&#125;5 w(0)&#123;7 6(f(0)+e(0)).9(2,8)&#125;5 x(0)&#123;7 6(a(0)+f(0)).9(3,8)&#125;5 C(0)&#123;7 6(b(0)+a(0)).9(1,8)&#125;5 D(0)&#123;7 6(c(0)+b(0)).9(2,8)&#125;5 A(0)&#123;7 6(d(0)+c(0)).9(3,8)&#125;5 B(0)&#123;7 6(h(0)+d(0)).9(4,8)&#125;5 v(0)&#123;7 6(i(0)+j(0)).9(1,8)&#125;5 p(0)&#123;7 6(e(0)+g(0)).9(2,8)&#125;5 q(0)&#123;7 6(f(0)+k(0)).9(3,8)&#125;5 n(0)&#123;7 6(a(0)+h(0)).9(4,8)&#125;5 o(0)&#123;7 6(b(0)+i(0)).9(1,8)&#125;5 t(0)&#123;7 6(c(0)+e(0)).9(3,8)&#125;5 u(0)&#123;7 6(d(0)+a(0)).9(1,8)&#125;5 r(0)&#123;7 6(j(0)+b(0)).9(2,8)&#125;5 s(0)&#123;7 6(g(0)+c(0)).9(3,8)&#125;5 N(0)&#123;7 6(k(0)+d(0)).9(4,8)&#125;5 O(0)&#123;7 6(h(0)+M(0)).9(3,8)&#125;5 Q(0)&#123;7 6(i(0)+m(0)).9(4,8)&#125;5 G(0)&#123;7 6(e(0)+l(0)).9(4,8)&#125;5 F(0)&#123;7 6(f(0)+e(0)).9(2,8)&#125;5 H(0)&#123;7 6(a(0)+f(0)).9(3,8)&#125;5 K(0)&#123;7 6(b(0)+a(0)).9(1,8)&#125;5 J(0)&#123;7 6(c(0)+b(0)).9(2,8)&#125;5 I(0)&#123;7 6(d(0)+c(0)).9(3,8)&#125;5 E(0)&#123;7 6(a(0)+d(0)).9(4,8)&#125;5 L(0)&#123;7 6(b(0)+j(0)).9(1,8)&#125;5 P(0)&#123;7 6(c(0)+g(0)).9(2,8)&#125;", 53, 53, "str|||||function|hex_md5|return|24|substr|makeKey_19|makeKey_0|makeKey_1|makeKey_4|makeKey_17|makeKey_18|makeKey_15|makeKey_9|makeKey_10|makeKey_14|makeKey_16|makeKey_7|makeKey_3|makeKey_236|makeKey_237|makeKey_234|makeKey_235|makeKey_240|makeKey_241|makeKey_238|makeKey_239|makeKey_233|makeKey_227|makeKey_228|makeKey_225|makeKey_226|makeKey_231|makeKey_232|makeKey_229|makeKey_230|makeKey_251|makeKey_246|makeKey_245|makeKey_247|makeKey_250|makeKey_249|makeKey_248|makeKey_252|makeKey_5|makeKey_242|makeKey_243|makeKey_253|makeKey_244".split("|"), 0, &#123;&#125;)); eval(_fxxx("7 p(0)&#123;6 5(a(0)+a(0)).8(3,9)&#125;7 G(0)&#123;6 5(n(0)+i(0)).8(4,9)&#125;7 E(0)&#123;6 5(l(0)+j(0)).8(1,9)&#125;7 I(0)&#123;6 5(m(0)+h(0)).8(3,9)&#125;7 z(0)&#123;6 5(c(0)+g(0)).8(1,9)&#125;7 C(0)&#123;6 5(b(0)+k(0)).8(2,9)&#125;7 B(0)&#123;6 5(a(0)+f(0)).8(3,9)&#125;7 D(0)&#123;6 5(f(0)+e(0)).8(4,9)&#125;7 y(0)&#123;6 5(e(0)+d(0)).8(3,9)&#125;7 A(0)&#123;6 5(d(0)+c(0)).8(4,9)&#125;7 H(0)&#123;6 5(c(0)+b(0)).8(4,9)&#125;7 J(0)&#123;6 5(b(0)+a(0)).8(1,9)&#125;7 F(0)&#123;6 5(a(0)+d(0)).8(2,9)&#125;7 x(0)&#123;6 5(g(0)+c(0)).8(3,9)&#125;7 r(0)&#123;6 5(k(0)+b(0)).8(4,9)&#125;7 q(0)&#123;6 5(f(0)+a(0)).8(1,9)&#125;7 o(0)&#123;6 5(e(0)+i(0)).8(2,9)&#125;7 v(0)&#123;6 5(d(0)+j(0)).8(3,9)&#125;7 u(0)&#123;6 5(c(0)+h(0)).8(4,9)&#125;7 s(0)&#123;6 5(b(0)+g(0)).8(3,9)&#125;7 t(0)&#123;6 5(d(0)+b(0)).8(4,9)&#125;7 w(0)&#123;6 5(c(0)+d(0)).8(1,9)&#125;7 K(0)&#123;6 5(b(0)+c(0)).8(2,9)&#125;7 U(0)&#123;6 5(a(0)+b(0)).8(2,9)&#125;7 Y(0)&#123;6 5(n(0)+a(0)).8(3,9)&#125;7 W(0)&#123;6 5(l(0)+n(0)).8(1,9)&#125;7 X(0)&#123;6 5(m(0)+l(0)).8(2,9)&#125;7 V(0)&#123;6 5(f(0)+m(0)).8(3,9)&#125;7 11(0)&#123;6 5(e(0)+f(0)).8(4,9)&#125;7 12(0)&#123;6 5(d(0)+e(0)).8(1,9)&#125;7 Z(0)&#123;6 5(c(0)+d(0)).8(2,9)&#125;7 10(0)&#123;6 5(b(0)+c(0)).8(3,9)&#125;7 N(0)&#123;6 5(a(0)+b(0)).8(4,9)&#125;7 O(0)&#123;6 5(i(0)+a(0)).8(1,9)&#125;7 L(0)&#123;6 5(j(0)+i(0)).8(3,9)&#125;7 M(0)&#123;6 5(h(0)+j(0)).8(1,9)&#125;7 P(0)&#123;6 5(g(0)+h(0)).8(2,9)&#125;7 S(0)&#123;6 5(k(0)+g(0)).8(3,9)&#125;7 T(0)&#123;6 5(f(0)+k(0)).8(4,9)&#125;7 Q(0)&#123;6 5(e(0)+f(0)).8(3,9)&#125;7 R(0)&#123;6 5(e(0)+e(0)).8(4,9)&#125;", 62, 65, "str|||||hex_md5|return|function|substr|24|makeKey_4|makeKey_1|makeKey_0|makeKey_19|makeKey_18|makeKey_17|makeKey_9|makeKey_16|makeKey_14|makeKey_15|makeKey_10|makeKey_3|makeKey_7|makeKey_5|makeKey_270|makeKey_254|makeKey_269|makeKey_268|makeKey_273|makeKey_274|makeKey_272|makeKey_271|makeKey_275|makeKey_267|makeKey_262|makeKey_258|makeKey_263|makeKey_260|makeKey_259|makeKey_261|makeKey_256|makeKey_266|makeKey_255|makeKey_264|makeKey_257|makeKey_265|makeKey_276|makeKey_288|makeKey_289|makeKey_286|makeKey_287|makeKey_290|makeKey_293|makeKey_294|makeKey_291|makeKey_292|makeKey_277|makeKey_281|makeKey_279|makeKey_280|makeKey_278|makeKey_284|makeKey_285|makeKey_282|makeKey_283".split("|"), 0, &#123;&#125;)); eval(de("eval(_fxxx('6 1F(0)&#123;5 7(b(0)+b(0)).8(4,24)&#125;6 W(0)&#123;5 7(9(0)+9(0)).8(1,24)&#125;6 V(0)&#123;5 7(a(0)+a(0)).8(2,24)&#125;6 U(0)&#123;5 7&#123;3&#125;c(0)).8(3,24)&#125;6 X(0)&#123;5 7(h(0)+h(0)).8(4,24)&#125;6 10(0)&#123;5 7(g(0)+g(0)).8(1,24)&#125;6 Z(0)&#123;5 7(f(0)+f(0)).8(2,24)&#125;6 Y(0)&#123;5 7(d(0)+d(0)).8(3,24)&#125;6 P(0)&#123;5 7(e(0)+e(0)).8(4,24)&#125;6 O(0)&#123;5 7(b(0)+b(0)).8(3,24)&#125;6 N(0)&#123;5 7(9(0)+9(0)).8(4,24)&#125;6 Q(0)&#123;5 7(a(0)+a(0)).8(1,24)&#125;6 T(0)&#123;5 7&#123;3&#125;c(0)).8(2,24)&#125;6 S(0)&#123;5 7&#123;2&#125;k(0)).8(2,24)&#125;6 R(0)&#123;5 7(m(0)+m(0)).8(3,24)&#125;6 1a(0)&#123;5 7(l(0)+l(0)).8(1,24)&#125;6 19(0)&#123;5 7(i(0)+i(0)).8(2,24)&#125;6 18(0)&#123;5 7(j(0)+j(0)).8(3,24)&#125;6 1b(0)&#123;5 7(d(0)+d(0)).8(4,24)&#125;6 1e(0)&#123;5 7(b(0)+b(0)).8(1,24)&#125;6 1d(0)&#123;5 7(9(0)+9(0)).8(2,24)&#125;6 1c(0)&#123;5 7(a(0)+a(0)).8(3,24)&#125;6 13(0)&#123;5 7&#123;3&#125;c(0)).8(4,24)&#125;6 12(0)&#123;5 7(h(0)+h(0)).8(1,24)&#125;6 11(0)&#123;5 7(g(0)+g(0)).8(3,24)&#125;6 14(0)&#123;5 7(f(0)+f(0)).8(1,24)&#125;6 17(0)&#123;5 7(d(0)+d(0)).8(2,24)&#125;6 16(0)&#123;5 7(e(0)+e(0)).8(3,24)&#125;6 15(0)&#123;5 7(b(0)+b(0)).8(4,24)&#125;6 M(0)&#123;5 7(9(0)+9(0)).8(3,24)&#125;6 w(0)&#123;5 7(a(0)+a(0)).8(4,24)&#125;6 s(0)&#123;5 7&#123;3&#125;c(0)).8(4,24)&#125;6 o(0)&#123;5 7(b(0)+k(0)).8(1,24)&#125;6 t(0)&#123;5 7(9(0)+m(0)).8(2,24)&#125;6 r(0)&#123;5 7(a(0)+l(0)).8(3,24)&#125;6 v(0)&#123;5 7&#123;3&#125;i(0)).8(4,24)&#125;6 u(0)&#123;5 7(b(0)+j(0)).8(1,24)&#125;6 q(0)&#123;5 7(9(0)+d(0)).8(2,24)&#125;6 n(0)&#123;5 7(a(0)+e(0)).8(3,24)&#125;6 p(0)&#123;5 7&#123;3&#125;e(0)).8(4,24)&#125;6 H(0)&#123;5 7(h(0)+b(0)).8(3,24)&#125;6 G(0)&#123;5 7(g(0)+9(0)).8(4,24)&#125;6 F(0)&#123;5 7(f(0)+a(0)).8(2,24)&#125;6 I(0)&#123;5 7(9(0)+c(0)).8(3,24)&#125;6 L(0)&#123;5 7(a(0)+h(0)).8(1,24)&#125;6 J(0)&#123;5 7&#123;3&#125;g(0)).8(2,24)&#125;6 E(0)&#123;5 7(d(0)+f(0)).8(3,24)&#125;6 z(0)&#123;5 7(e(0)+d(0)).8(4,24)&#125;6 y(0)&#123;5 7(b(0)+e(0)).8(1,24)&#125;6 x(0)&#123;5 7(9(0)+b(0)).8(2,24)&#125;6 A(0)&#123;5 7(a(0)+9(0)).8(3,24)&#125;6 D(0)&#123;5 7&#123;3&#125;a(0)).8(4,24)&#125;6 C(0)&#123;5 7(i(0)+c(0)).8(1,24)&#125;6 B(0)&#123;5 7(j(0)+k(0)).8(3,24)&#125;6 K(0)&#123;5 7(d(0)+m(0)).8(1,24)&#125;6 1f(0)&#123;5 7(e(0)+l(0)).8(2,24)&#125;6 1N(0)&#123;5 7(b(0)+i(0)).8(3,24)&#125;6 1M(0)&#123;5 7(9(0)+j(0)).8(4,24)&#125;6 1L(0)&#123;5 7(a(0)+d(0)).8(3,24)&#125;6 1Q(0)&#123;5 7(e(0)+b(0)).8(4,24)&#125;6 1P(0)&#123;5 7(b(0)+9(0)).8(4,24)&#125;6 1O(0)&#123;5 7(9(0)+a(0)).8(1,24)&#125;6 1H(0)&#123;5 7(a(0)+c(0)).8(2,24)&#125;6 1G(0)&#123;5 7&#123;3&#125;h(0)).8(3,24)&#125;6 1w(0)&#123;5 7(h(0)+g(0)).8(4,24)&#125;6 1K(0)&#123;5 7(g(0)+f(0)).8(2,24)&#125;6 1J(0)&#123;5 7(f(0)+d(0)).8(3,24)&#125;6 1I(0)&#123;5 7(d(0)+e(0)).8(1,24)&#125;6 1R(0)&#123;5 7(e(0)+b(0)).8(2,24)&#125;6 20(0)&#123;5 7(b(0)+9(0)).8(3,24)&#125;6 1Y(0)&#123;5 7(9(0)+a(0)).8(4,24)&#125;6 21(0)&#123;5 7(a(0)+c(0)).8(1,24)&#125;6 1Z(0)&#123;5 7&#123;3&#125;f(0)).8(2,24)&#125;6 23(0)&#123;5 7&#123;2&#125;d(0)).8(3,24)&#125;6 22(0)&#123;5 7(m(0)+e(0)).8(4,24)&#125;6 1U(0)&#123;5 7(l(0)+b(0)).8(1,24)&#125;6 1T(0)&#123;5 7(i(0)+9(0)).8(3,24)&#125;6 1S(0)&#123;5 7(j(0)+a(0)).8(1,24)&#125;6 1X(0)&#123;5 7(d(0)+c(0)).8(2,24)&#125;6 1W(0)&#123;5 7(b(0)+d(0)).8(3,24)&#125;6 1V(0)&#123;5 7(9(0)+e(0)).8(4,24)&#125;6 1o(0)&#123;5 7(a(0)+b(0)).8(3,24)&#125;6 1n(0)&#123;5 7&#123;3&#125;9(0)).8(4,24)&#125;6 1m(0)&#123;5 7(h(0)+a(0)).8(4,24)&#125;6 1r(0)&#123;5 7(g(0)+c(0)).8(1,24)&#125;6 1q(0)&#123;5 7(f(0)+i(0)).8(2,24)&#125;6 1p(0)&#123;5 7(d(0)+j(0)).8(3,24)&#125;6 1i(0)&#123;5 7(e(0)+d(0)).8(4,24)&#125;6 1h(0)&#123;5 7(b(0)+e(0)).8(1,24)&#125;6 1g(0)&#123;5 7(9(0)+b(0)).8(2,24)&#125;6 1l(0)&#123;5 7(a(0)+9(0)).8(3,24)&#125;6 1k(0)&#123;5 7&#123;3&#125;a(0)).8(4,24)&#125;6 1j(0)&#123;5 7(d(0)+a(0)).8(2,24)&#125;6 1s(0)&#123;5 7(e(0)+c(0)).8(3,24)&#125;6 1B(0)&#123;5 7(b(0)+f(0)).8(1,24)&#125;6 1A(0)&#123;5 7(9(0)+d(0)).8(2,24)&#125;6 1z(0)&#123;5 7(a(0)+e(0)).8(3,24)&#125;6 1E(0)&#123;5 7&#123;3&#125;b(0)).8(4,24)&#125;6 1D(0)&#123;5 7(i(0)+9(0)).8(1,24)&#125;6 1C(0)&#123;5 7(j(0)+a(0)).8(2,24)&#125;6 1v(0)&#123;5 7(d(0)+c(0)).8(3,24)&#125;6 1u(0)&#123;5 7(e(0)+d(0)).8(4,24)&#125;6 1t(0)&#123;5 7(b(0)+e(0)).8(1,24)&#125;6 1y(0)&#123;5 7(9(0)+b(0)).8(3,24)&#125;6 1x(0)&#123;5 7(a(0)+9(0)).8(1,24)&#125;', 62, 129, 'str|||||return|&#123;0&#125;|hex_md5|substr|&#123;1&#125;0|&#123;1&#125;1|&#123;1&#125;19|&#123;1&#125;4|&#123;1&#125;17|&#123;1&#125;18|&#123;1&#125;7|&#123;1&#125;3|&#123;1&#125;5|&#123;1&#125;9|&#123;1&#125;10|&#123;1&#125;14|&#123;1&#125;16|&#123;1&#125;15|&#123;1&#125;333|&#123;1&#125;327|&#123;1&#125;334|&#123;1&#125;332|&#123;1&#125;329|&#123;1&#125;326|&#123;1&#125;328|&#123;1&#125;331|&#123;1&#125;330|&#123;1&#125;325|&#123;1&#125;344|&#123;1&#125;343|&#123;1&#125;342|&#123;1&#125;345|&#123;1&#125;348|&#123;1&#125;347|&#123;1&#125;346|&#123;1&#125;341|&#123;1&#125;337|&#123;1&#125;336|&#123;1&#125;335|&#123;1&#125;338|&#123;1&#125;340|&#123;1&#125;349|&#123;1&#125;339|&#123;1&#125;324|&#123;1&#125;305|&#123;1&#125;304|&#123;1&#125;303|&#123;1&#125;306|&#123;1&#125;309|&#123;1&#125;308|&#123;1&#125;307|&#123;1&#125;298|&#123;1&#125;297|&#123;1&#125;296|&#123;1&#125;299|&#123;1&#125;302|&#123;1&#125;301|&#123;1&#125;300|&#123;1&#125;319|&#123;1&#125;318|&#123;1&#125;317|&#123;1&#125;320|&#123;1&#125;323|&#123;1&#125;322|&#123;1&#125;321|&#123;1&#125;312|&#123;1&#125;311|&#123;1&#125;310|&#123;1&#125;313|&#123;1&#125;316|&#123;1&#125;315|&#123;1&#125;314|&#123;1&#125;350|&#123;1&#125;384|&#123;1&#125;383|&#123;1&#125;382|&#123;1&#125;387|&#123;1&#125;386|&#123;1&#125;385|&#123;1&#125;378|&#123;1&#125;377|&#123;1&#125;376|&#123;1&#125;381|&#123;1&#125;380|&#123;1&#125;379|&#123;1&#125;388|&#123;1&#125;397|&#123;1&#125;396|&#123;1&#125;395|&#123;1&#125;359|&#123;1&#125;399|&#123;1&#125;398|&#123;1&#125;391|&#123;1&#125;390|&#123;1&#125;389|&#123;1&#125;394|&#123;1&#125;393|&#123;1&#125;392|&#123;1&#125;295|&#123;1&#125;358|&#123;1&#125;357|&#123;1&#125;362|&#123;1&#125;361|&#123;1&#125;360|&#123;1&#125;353|&#123;1&#125;352|&#123;1&#125;351|&#123;1&#125;356|&#123;1&#125;355|&#123;1&#125;354|&#123;1&#125;363|&#123;1&#125;372|&#123;1&#125;371|&#123;1&#125;370|&#123;1&#125;375|&#123;1&#125;374|&#123;1&#125;373|&#123;1&#125;365|&#123;1&#125;367|&#123;1&#125;364|&#123;1&#125;366|&#123;1&#125;369|&#123;1&#125;368|'.split('|'), 0, &#123;&#125;))", 4, "function|makeKey_|(k(0)+|(c(0)+")); eval(_fxxx("0 2=2f('2e');0 1=[2d,2i,2h,2g,29,28,27,2c,2b,2a,2j,2s,2r,2q,2v,2u,2t,2m,2l,2k,2p,2o,2n,1Q,1P,1O,1T,1S,1R,1K,1J,1I,1N,1M,1L,1U,23,22,21,26,25,24,1X,1W,1V,20,1Z,1Y,2w,34,33,32,37,36,35,2Y,2X,2W,31,30,2Z,38,3h,3g,3f,3k,3j,3i,3b,3a,39,3e,3d,3c,2F,2E,2D,2I,2H,2G,2z,2y,2x,2C,2B,2A,2J,2S,2R,2Q,2V,2U,2T,2M,2L,2K,2P,2O,2N,C,B,A,F,E,D,w,v,u,z,y,x,G,P,O,N,S,R,Q,J,I,H,M,L,K,d,c,b,g,f,e,7,6,5,a,9,8,h,q,p,o,t,s,r,k,j,i,n,m,l,T,1r,1q,1p,1u,1t,1s,1l,1k,1j,1o,1n,1m,1v,1E,1D,1C,1H,1G,1F,1y,1x,1w,1B,1A,1z,12,11,10,15,14,13,W,V,U,Z,Y,X,16,1f,1e,1d,1i,1h,1g,19,18,17,1c,1b,1a,3l,5w,5v,5u,5z,5y,5x,5q,5p,5o,5t,5s,5r,5A,5J,5I,5H,5M,5L,5K,5D,5C,5B,5G,5F,5E,57,56,55,5a,59,58,51,50,4Z,54,53,52,5b,5k,5j,5i,5n,5m,5l,5e,5d,5c,5h,5g,5f,5N,6l,6k,6j,6o,6n,6m,6f,6e,6d,6i,6h,6g,6p,6y,6x,6w,6B,6A,6z,6s,6r,6q,6v,6u,6t,5W,5V,5U,5Z,5Y,5X,5Q,5P,5O,5T,5S,5R,60,69,68,67,6c,6b,6a,63,62,61,66,65,64,3T,3S,3R,3W,3V,3U,3N,3M,3L,3Q,3P,3O,3X,46,45,44,49,48,47,40,3Z,3Y,43,42,41,3u,3t,3s,3x,3w,3v,3o,3n,3m,3r,3q,3p,3y,3H,3G,3F,3K,3J,3I,3B,3A,3z,3E,3D,3C,4a,4I,4H,4G,4L,4K,4J,4C,4B,4A,4F,4E,4D,4M,4V,4U,4T,4Y,4X,4W,4P,4O,4N,4S,4R,4Q,4j,4i,4h,4m,4l,4k,4d,4c,4b,4g,4f,4e,4n,4w,4v,4u,4z,4y,4x,4q,4p,4o];0 3=4t(2)%1.4s;0 4=1[3];0 4r=4(2);", 62, 410, eval(de("'var|arrFun|cookie|funIndex|fun|&#123;0&#125;132|&#123;0&#125;131|&#123;0&#125;130|&#123;0&#125;135|&#123;0&#125;134|&#123;0&#125;133|&#123;0&#125;126|&#123;0&#125;125|&#123;0&#125;124|&#123;0&#125;129|&#123;0&#125;128|&#123;0&#125;127|&#123;0&#125;136|&#123;0&#125;145|&#123;0&#125;144|&#123;0&#125;143|&#123;0&#125;148|&#123;0&#125;147|&#123;0&#125;146|&#123;0&#125;139|&#123;0&#125;138|&#123;0&#125;137|&#123;0&#125;142|&#123;0&#125;141|&#123;0&#125;140|&#123;0&#125;107|&#123;0&#125;106|&#123;0&#125;105|&#123;0&#125;110|&#123;0&#125;109|&#123;0&#125;108|&#123;0&#125;101|&#123;0&#125;100|&#123;0&#125;99|&#123;0&#125;104|&#123;0&#125;103|&#123;0&#125;102|&#123;0&#125;111|&#123;0&#125;120|&#123;0&#125;119|&#123;0&#125;118|&#123;0&#125;123|&#123;0&#125;122|&#123;0&#125;121|&#123;0&#125;114|&#123;0&#125;113|&#123;0&#125;112|&#123;0&#125;117|&#123;0&#125;116|&#123;0&#125;115|&#123;0&#125;149|&#123;0&#125;183|&#123;0&#125;182|&#123;0&#125;181|&#123;0&#125;186|&#123;0&#125;185|&#123;0&#125;184|&#123;0&#125;177|&#123;0&#125;176|&#123;0&#125;175|&#123;0&#125;180|&#123;0&#125;179|&#123;0&#125;178|&#123;0&#125;187|&#123;0&#125;196|&#123;0&#125;195|&#123;0&#125;194|&#123;0&#125;199|&#123;0&#125;198|&#123;0&#125;197|&#123;0&#125;190|&#123;0&#125;189|&#123;0&#125;188|&#123;0&#125;193|&#123;0&#125;192|&#123;0&#125;191|&#123;0&#125;158|&#123;0&#125;157|&#123;0&#125;156|&#123;0&#125;161|&#123;0&#125;160|&#123;0&#125;159|&#123;0&#125;152|&#123;0&#125;151|&#123;0&#125;150|&#123;0&#125;155|&#123;0&#125;154|&#123;0&#125;153|&#123;0&#125;162|&#123;0&#125;171|&#123;0&#125;170|&#123;0&#125;169|&#123;0&#125;174|&#123;0&#125;173|&#123;0&#125;172|&#123;0&#125;165|&#123;0&#125;164|&#123;0&#125;163|&#123;0&#125;168|&#123;0&#125;167|&#123;0&#125;166|&#123;0&#125;31|&#123;0&#125;30|&#123;0&#125;29|&#123;0&#125;34|&#123;0&#125;33|&#123;0&#125;32|&#123;0&#125;25|&#123;0&#125;24|&#123;0&#125;23|&#123;0&#125;28|&#123;0&#125;27|&#123;0&#125;26|&#123;0&#125;35|&#123;0&#125;44|&#123;0&#125;43|&#123;0&#125;42|&#123;0&#125;47|&#123;0&#125;46|&#123;0&#125;45|&#123;0&#125;38|&#123;0&#125;37|&#123;0&#125;36|&#123;0&#125;41|&#123;0&#125;40|&#123;0&#125;39|&#123;0&#125;6|&#123;0&#125;5|&#123;0&#125;4|&#123;0&#125;9|&#123;0&#125;8|&#123;0&#125;7|&#123;0&#125;0|vjkl5|getCookie|&#123;0&#125;3|&#123;0&#125;2|&#123;0&#125;1|&#123;0&#125;10|&#123;0&#125;19|&#123;0&#125;18|&#123;0&#125;17|&#123;0&#125;22|&#123;0&#125;21|&#123;0&#125;20|&#123;0&#125;13|&#123;0&#125;12|&#123;0&#125;11|&#123;0&#125;16|&#123;0&#125;15|&#123;0&#125;14|&#123;0&#125;48|&#123;0&#125;82|&#123;0&#125;81|&#123;0&#125;80|&#123;0&#125;85|&#123;0&#125;84|&#123;0&#125;83|&#123;0&#125;76|&#123;0&#125;75|&#123;0&#125;74|&#123;0&#125;79|&#123;0&#125;78|&#123;0&#125;77|&#123;0&#125;86|&#123;0&#125;95|&#123;0&#125;94|&#123;0&#125;93|&#123;0&#125;98|&#123;0&#125;97|&#123;0&#125;96|&#123;0&#125;89|&#123;0&#125;88|&#123;0&#125;87|&#123;0&#125;92|&#123;0&#125;91|&#123;0&#125;90|&#123;0&#125;57|&#123;0&#125;56|&#123;0&#125;55|&#123;0&#125;60|&#123;0&#125;59|&#123;0&#125;58|&#123;0&#125;51|&#123;0&#125;50|&#123;0&#125;49|&#123;0&#125;54|&#123;0&#125;53|&#123;0&#125;52|&#123;0&#125;61|&#123;0&#125;70|&#123;0&#125;69|&#123;0&#125;68|&#123;0&#125;73|&#123;0&#125;72|&#123;0&#125;71|&#123;0&#125;64|&#123;0&#125;63|&#123;0&#125;62|&#123;0&#125;67|&#123;0&#125;66|&#123;0&#125;65|&#123;0&#125;200|&#123;0&#125;335|&#123;0&#125;334|&#123;0&#125;333|&#123;0&#125;338|&#123;0&#125;337|&#123;0&#125;336|&#123;0&#125;329|&#123;0&#125;328|&#123;0&#125;327|&#123;0&#125;332|&#123;0&#125;331|&#123;0&#125;330|&#123;0&#125;339|&#123;0&#125;348|&#123;0&#125;347|&#123;0&#125;346|&#123;0&#125;351|&#123;0&#125;350|&#123;0&#125;349|&#123;0&#125;342|&#123;0&#125;341|&#123;0&#125;340|&#123;0&#125;345|&#123;0&#125;344|&#123;0&#125;343|&#123;0&#125;310|&#123;0&#125;309|&#123;0&#125;308|&#123;0&#125;313|&#123;0&#125;312|&#123;0&#125;311|&#123;0&#125;304|&#123;0&#125;303|&#123;0&#125;302|&#123;0&#125;307|&#123;0&#125;306|&#123;0&#125;305|&#123;0&#125;314|&#123;0&#125;323|&#123;0&#125;322|&#123;0&#125;321|&#123;0&#125;326|&#123;0&#125;325|&#123;0&#125;324|&#123;0&#125;317|&#123;0&#125;316|&#123;0&#125;315|&#123;0&#125;320|&#123;0&#125;319|&#123;0&#125;318|&#123;0&#125;352|&#123;0&#125;386|&#123;0&#125;385|&#123;0&#125;384|&#123;0&#125;389|&#123;0&#125;388|&#123;0&#125;387|&#123;0&#125;380|&#123;0&#125;379|&#123;0&#125;378|&#123;0&#125;383|&#123;0&#125;382|&#123;0&#125;381|&#123;0&#125;390|&#123;0&#125;399|&#123;0&#125;398|&#123;0&#125;397|result|length|strToLong|&#123;0&#125;393|&#123;0&#125;392|&#123;0&#125;391|&#123;0&#125;396|&#123;0&#125;395|&#123;0&#125;394|&#123;0&#125;361|&#123;0&#125;360|&#123;0&#125;359|&#123;0&#125;364|&#123;0&#125;363|&#123;0&#125;362|&#123;0&#125;355|&#123;0&#125;354|&#123;0&#125;353|&#123;0&#125;358|&#123;0&#125;357|&#123;0&#125;356|&#123;0&#125;365|&#123;0&#125;374|&#123;0&#125;373|&#123;0&#125;372|&#123;0&#125;377|&#123;0&#125;376|&#123;0&#125;375|&#123;0&#125;368|&#123;0&#125;367|&#123;0&#125;366|&#123;0&#125;371|&#123;0&#125;370|&#123;0&#125;369|&#123;0&#125;234|&#123;0&#125;233|&#123;0&#125;232|&#123;0&#125;237|&#123;0&#125;236|&#123;0&#125;235|&#123;0&#125;228|&#123;0&#125;227|&#123;0&#125;226|&#123;0&#125;231|&#123;0&#125;230|&#123;0&#125;229|&#123;0&#125;238|&#123;0&#125;247|&#123;0&#125;246|&#123;0&#125;245|&#123;0&#125;250|&#123;0&#125;249|&#123;0&#125;248|&#123;0&#125;241|&#123;0&#125;240|&#123;0&#125;239|&#123;0&#125;244|&#123;0&#125;243|&#123;0&#125;242|&#123;0&#125;209|&#123;0&#125;208|&#123;0&#125;207|&#123;0&#125;212|&#123;0&#125;211|&#123;0&#125;210|&#123;0&#125;203|&#123;0&#125;202|&#123;0&#125;201|&#123;0&#125;206|&#123;0&#125;205|&#123;0&#125;204|&#123;0&#125;213|&#123;0&#125;222|&#123;0&#125;221|&#123;0&#125;220|&#123;0&#125;225|&#123;0&#125;224|&#123;0&#125;223|&#123;0&#125;216|&#123;0&#125;215|&#123;0&#125;214|&#123;0&#125;219|&#123;0&#125;218|&#123;0&#125;217|&#123;0&#125;251|&#123;0&#125;285|&#123;0&#125;284|&#123;0&#125;283|&#123;0&#125;288|&#123;0&#125;287|&#123;0&#125;286|&#123;0&#125;279|&#123;0&#125;278|&#123;0&#125;277|&#123;0&#125;282|&#123;0&#125;281|&#123;0&#125;280|&#123;0&#125;289|&#123;0&#125;298|&#123;0&#125;297|&#123;0&#125;296|&#123;0&#125;301|&#123;0&#125;300|&#123;0&#125;299|&#123;0&#125;292|&#123;0&#125;291|&#123;0&#125;290|&#123;0&#125;295|&#123;0&#125;294|&#123;0&#125;293|&#123;0&#125;260|&#123;0&#125;259|&#123;0&#125;258|&#123;0&#125;263|&#123;0&#125;262|&#123;0&#125;261|&#123;0&#125;254|&#123;0&#125;253|&#123;0&#125;252|&#123;0&#125;257|&#123;0&#125;256|&#123;0&#125;255|&#123;0&#125;264|&#123;0&#125;273|&#123;0&#125;272|&#123;0&#125;271|&#123;0&#125;276|&#123;0&#125;275|&#123;0&#125;274|&#123;0&#125;267|&#123;0&#125;266|&#123;0&#125;265|&#123;0&#125;270|&#123;0&#125;269|&#123;0&#125;268'", 1, "makeKey_")).split("|"), 0, &#123;&#125;)); return result&#125;; 获取 vl5x 的时候会用到 vjkl5 ，vjkl5 在cookie里 (1.4) docId1&lt;a href="javascript:void(0)" onclick="javascript:Navi(&amp;quot;FcOMwrkBBDEIBMOBwpR4w4VgIhDDucKHdHteO10ywoc2byEFwr3Do0TDnBLCsHQXw7hiw7TDikhuYynDhV9zbGTCk8KdaMKewoQ8NlnCsVdww6RePm/Cmxk+w5A+OsOrw6bCnBZKw6pZc8K3wpLDoMK6CcO9w4B/w7Ezwo5Tw5FEw5fDp8OOwq13YRJ1wpwVwq/CnsOuwqVyw7vCvG/CqywcwqYawofCmAhkA3HCjMObw6pNwrfClCkywrfCtl8HecO+AA==&amp;quot;,&amp;quot;&amp;quot;)" target="_self" style="color:Black; text-decoration:none"&gt;王守仁犯挪用公款罪刑事决定书&lt;/a&gt; 查看html源码会发现，其实是把docId加密了一下，在调用时再解密，使用时调用 Navi 函数解密，总体来说增加了一下反爬难度 总体来说增加了一下反爬难度 Navi函数在 Lawyee.CPWSW.List.js 12345678910111213141516//增加7道爬虫防御 段智峰 20180807function Navi(id, keyword) &#123; var unzipid = unzip(id); try &#123; var realid = com.str.Decrypt(unzipid); if (realid == "") &#123; setTimeout("Navi('" + id + "','" + keyword + "')", 1000); &#125; else &#123; var url = "/content/content?DocID=" + realid + "&amp;KeyWord=" + encodeURI(keyword); openWin(url); &#125; &#125; catch (ex) &#123; setTimeout("Navi('" + id + "','" + keyword + "')", 1000); &#125;&#125; 对应的unzip函数和com.str.Decrypt函数在 pako.min.js 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576function unzip(b64Data) &#123; var strData; if (!window.atob) &#123; &#125; else &#123; &#125; var charData; if (!Array.prototype.map) &#123; &#125; else &#123; &#125; strData = Base64_Zip.btou(RawDeflate.inflate(Base64_Zip.fromBase64(b64Data))); return strData&#125;var com = &#123;&#125;;com.str = &#123; _KEY: "12345678900000001234567890000000", _IV: "abcd134556abcedf", Encrypt: function (str) &#123; var key = CryptoJS.enc.Utf8.parse(this._KEY); var iv = CryptoJS.enc.Utf8.parse(this._IV); var encrypted = ""; var srcs = CryptoJS.enc.Utf8.parse(str); encrypted = CryptoJS.AES.encrypt(srcs, key, &#123;iv: iv, mode: CryptoJS.mode.CBC, padding: CryptoJS.pad.Pkcs7&#125;); return encrypted.ciphertext.toString() &#125;, Decrypt: function (str) &#123; var result = com.str.DecryptInner(str); try &#123; var newstr = com.str.DecryptInner(result); if (newstr != "") &#123; result = newstr &#125; &#125; catch (ex) &#123; var msg = ex &#125; return result &#125;, DecryptInner: function (str) &#123; var key = CryptoJS.enc.Utf8.parse(this._KEY); var iv = CryptoJS.enc.Utf8.parse(this._IV); var encryptedHexStr = CryptoJS.enc.Hex.parse(str); var srcs = CryptoJS.enc.Base64.stringify(encryptedHexStr); var decrypt = CryptoJS.AES.decrypt(srcs, key, &#123;iv: iv, mode: CryptoJS.mode.CBC, padding: CryptoJS.pad.Pkcs7&#125;); var decryptedStr = decrypt.toString(CryptoJS.enc.Utf8); var result = decryptedStr.toString(); try &#123; result = Decrypt(result) &#125; catch (ex) &#123; var msg = ex &#125; return result &#125;&#125;;function iemap(myarray, callback, thisArg) &#123; var T, A, k; if (myarray == null) &#123; throw new TypeError(" this is null or not defined") &#125; var O = Object(myarray); var len = O.length &gt;&gt;&gt; 0; if (typeof callback !== "function") &#123; throw new TypeError(callback + " is not a function") &#125; if (thisArg) &#123; T = thisArg &#125; A = new Array(len); k = 0; while (k &lt; len) &#123; var kValue, mappedValue; if (k in O) &#123; kValue = O[k]; mappedValue = callback.call(T, kValue, k, O); A[k] = mappedValue &#125; k++ &#125; return A&#125;; (1.5) 爬取docId对应文书 拿到docId后 根据docId爬取文书 爬取到结果后解析处理即可拿到内容。 (2) 2018-04版 爬取裁判文书 先自己在浏览器里F12看一看请求，就会发现，想要爬取裁判文书，首先要爬取list页面，如下： list页面很好找，很快就找到了，用的是Post，用于反爬参数的有3个，vl5x、number、guid 。如下 12345678910http://wenshu.court.gov.cn/List/ListContent PostParam:全文检索:租房,文书类型:全部,裁判日期:2017-01-01 TO 2017-12-31Index:2Page:5Order:裁判日期/法院层级Direction:desc/ascvl5x:fd8e5d85dc53f6f4ea89a90enumber:CJBJXSFCguid:6ce7c4a4-c08b-28fbb93a-c94db684e6c2 经过 ctrl + F 搜索分析，发现 guid是通过js生成。 number通过请求获得，每次访问list页面前通过 http://wenshu.court.gov.cn/ValiCode/GetCode 获取。 vl5x通过混淆加密的js获得。在获取时要另获取vjkl5参数才能计算出vl5x (2.1) guid 经过 ctrl + F 搜索 guid ，发现，发现了生成guid的方法： 12345678function createGuid() &#123; return (((1 + Math.random()) * 0x10000) | 0).toString(16).substring(1);&#125;function ref() &#123; var guid = createGuid() + createGuid() + "-" + createGuid() + "-" + createGuid() + createGuid() + "-" + createGuid() + createGuid() + createGuid(); //CreateGuid(); $("#txthidGuid").val(guid); $("#divYzmImg").html("&lt;img alt='点击刷新验证码！' name='validateCode' id='ImgYzm' onclick='ref()' title='点击切换验证码' src='/ValiCode/CreateCode/?guid=" + guid + "' style='cursor: pointer;' /&gt;");&#125; (2.2) number number参数通过访问 http://wenshu.court.gov.cn/ValiCode/GetCode 就可以获得 (2.3) vl5x 经过 ctrl + F 搜索，发现 vl5x 在的js代码里。 Lawyee.CPWSW.List.js Lawyee.CPWSW.JsTree.js 可以看到getKey()的结果就是vl5x 1data: &#123; &quot;Param&quot;: listparam, &quot;Index&quot;: index, &quot;Page&quot;: page, &quot;Order&quot;: order, &quot;Direction&quot;: direction, &quot;vl5x&quot;: getKey(), &quot;number&quot;: yzm1, &quot;guid&quot;: guid1 &#125;, 1data: &#123; &quot;Param&quot;: treeparam, &quot;vl5x&quot;: getKey(), &quot;guid&quot;: guid1, &quot;number&quot;: yzm1 &#125;, (2.4) 寻找getKey()函数 通过不断寻找，发现getKey()函数是一个混淆加密的js，分析出这个js，就知道getKey()函数了，但是这儿我就不公布答案了。 在解析出getKey()函数后，发现还需要vjkl5参数。 (2.5) 获取vjkl5 得到getKey()函数之后，发现还需要vjkl5参数。 vjkl5 和 cookie 有关。 最后再根据vjkl5得到vl5x 把这几个参数全得到了，那么得到参数就可以通过Post请求获取数据了。 (2.6) 爬取结果样例 来两张图片，看看爬到的数据 (3) 可能遇到的问题(3.1) 500 解决问题主要在于表单中的 vl5x 参数与 guid 参数 其中通过 post guid 参数到 http://wenshu.court.gov.cn/ValiCode/GetCode 得到number，在其出现500时。参数 number 为’number’: ‘wens’ vjkl5 : 69aef9fffbc2aec01a418bbfc8d49225f26a8a5f vl5x : 57e0a6c84e242d6bde76830d (3.2) 返回 remind key 找一个浏览器里已经用过的vl5x去post ListContent，返回的是”remind key”，因此这个vl5x值是实时计算生成的。而且每次都会变化。 (4) 手机端(4.1) 2_Request_Response12345678910111213141516171819202122232425POST http://wenshuapp.court.gov.cn/MobileServices/GetLawListData HTTP/1.1Content-Type: application/jsontimespan: 20171206211000nonce: g1lldevid: 70498ab0cxxxxbcd90c1e59dbfdcxxxxsignature: d82e86ebe4dd8ec3512d2c53a814a0c7User-Agent: Dalvik/2.1.0 (Linux; U; Android 7.0; EVA-AL10 Build/HUAWEIEVA-AL10)Host: wenshuapp.court.gov.cnConnection: Keep-AliveAccept-Encoding: gzipContent-Length: 185&#123;&quot;limit&quot;:&quot;20&quot;,&quot;dicval&quot;:&quot;asc&quot;,&quot;reqtoken&quot;:&quot;843a9bdcd4e28da79624549c051c0075&quot;,&quot;skip&quot;:&quot;0&quot;,&quot;dickey&quot;:&quot;/CaseInfo/案/@法院层级&quot;,&quot;app&quot;:&quot;cpws&quot;,&quot;condition&quot;:&quot;/CaseInfo/案/@DocContent=抢劫&quot;&#125;HTTP/1.1 200 OKServer: 360wzwsDate: Wed, 06 Dec 2017 13:09:55 GMTContent-Type: application/json;charset=utf-8;Content-Length: 6682Connection: keep-aliveX-Powered-By-360WZB: wangzhan.360.cnWZWS-RAY: 112-1512594595.15-s1jsm (4.2) 6_Request_Response123456789101112131415161718192021222324POST http://wenshuapp.court.gov.cn/MobileServices/GetLawListData HTTP/1.1Content-Type: application/jsontimespan: 20171206211016nonce: 274ddevid: 70498ab0cxxxxbcd90c1e59dbfdcxxxxsignature: 95e57f2d8e4597ffbd0682291d6b4258User-Agent: Dalvik/2.1.0 (Linux; U; Android 7.0; EVA-AL10 Build/HUAWEIEVA-AL10)Host: wenshuapp.court.gov.cnConnection: Keep-AliveAccept-Encoding: gzipContent-Length: 186&#123;&quot;limit&quot;:&quot;20&quot;,&quot;dicval&quot;:&quot;asc&quot;,&quot;reqtoken&quot;:&quot;843a9bdcd4e28da79624549c051c0075&quot;,&quot;skip&quot;:&quot;20&quot;,&quot;dickey&quot;:&quot;/CaseInfo/案/@法院层级&quot;,&quot;app&quot;:&quot;cpws&quot;,&quot;condition&quot;:&quot;/CaseInfo/案/@DocContent=抢劫&quot;&#125;HTTP/1.1 200 OKServer: 360wzwsDate: Wed, 06 Dec 2017 13:10:21 GMTContent-Type: application/json;charset=utf-8;Content-Length: 6490Connection: keep-aliveX-Powered-By-360WZB: wangzhan.360.cnWZWS-RAY: 112-1512594621.039-s1jsm (4.3) 手机端的主要参数timespan nonce devid signature 这个看Java功底了，我是得到高人相助以后把这几个参数分析出来了。答案就不公布了。 (4.4) 反编译工具apktool 作用：资源文件获取，可以提取出图片文件和布局文件进行使用查看 dex2jar 作用：将apk反编译成java源码（classes.dex转化成jar文件） jd-gui 作用：查看APK中classes.dex转化成出的jar文件，即源码文件 References[1] 用Python抓取新版裁判文书网（附代码，针对初学者修订） [2] 中国裁决文书网爬虫 [3] 文书网vl5x值的计算 [4] 中国裁判文书网 爬虫求助[5] 文书网-破解反爬思路详解 [6] CJOSpider [7] 破解＂中国裁判文书网＂App加密过程 [8] 裁判文书与大数据 [9] Android APK反编译就这么简单 详解（附图） [10] User-Agent 汇总 [11] 中国裁判文书网爬虫[12] 文书网反反爬虫SDK[13] 爬取裁判文书网(一)[14] R语言完成中国裁判文书网最新爬虫[15] 2018-10-09中国裁判文书网爬虫vl5x和DocID分析[16] 裁判文书网APP采集思路-失败案例]]></content>
      <categories>
        <category>crawler</category>
      </categories>
      <tags>
        <tag>crawler</tag>
        <tag>spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[neo4j 误删 数据 恢复]]></title>
    <url>%2F2017%2F12%2F03%2Fneo4j-recover-data%2F</url>
    <content type="text"><![CDATA[neo4j误删数据恢复 今天有一个朋友告诉我他误操作，删了几十G的数据，问我有没有什么办法可以恢复数据。说来也巧了，今天我在测试大数据量下load csv的性能，导入三次删除三次，理论上数据库为空，data目录下的文件应该&lt;1MB，但是我一看，800M，肯定有日志呀。正好碰到他问我这个问题，首先告诉他不要慌，有办法，然后google了一下，是neo4j的其中一个作者(Stefan Armbruster，带飞机帽的那个)在stackoverflow回答的，还好，谢天谢地。 用这个恢复数据https://github.com/neo4j/neo4j/blob/3.0/tools/src/main/java/org/neo4j/tools/applytx/DatabaseRebuildTool.java References[1] https://stackoverflow.com/questions/40152819/how-to-recover-neo4j-3-0-database-from-transaction-log[2] https://github.com/neo4j/neo4j/blob/3.0/tools/src/main/java/org/neo4j/tools/applytx/DatabaseRebuildTool.java[3] https://stackoverflow.com/users/158701/stefan-armbruster]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java]]></title>
    <url>%2F2017%2F11%2F15%2Fjava-interview%2F</url>
    <content type="text"><![CDATA[Java面试题 前200页 转自Java 面试题：百度前200页都在这里了 基本概念 操作系统中 heap 和 stack 的区别 什么是基于注解的切面实现 什么是 对象/关系 映射集成模块 什么是 Java 的反射机制 什么是 ACID BS与CS的联系与区别 Cookie 和 Session的区别 fail-fast 与 fail-safe 机制有什么区别 get 和 post请求的区别 Interface 与 abstract 类的区别 IOC的优点是什么 IO 和 NIO的区别，NIO优点 Java 8 / Java 7 为我们提供了什么新功能 什么是竞态条件？ 举个例子说明。 JRE、JDK、JVM 及 JIT 之间有什么不同 MVC的各个部分都有那些技术来实现?如何实现? RPC 通信和 RMI 区别 什么是 Web Service（Web服务） JSWDL开发包的介绍。JAXP、JAXM的解释。SOAP、UDDI,WSDL解释。 WEB容器主要有哪些功能? 并请列出一些常见的WEB容器名字。 一个”.java”源文件中是否可以包含多个类（不是内部类）？有什么限制 简单说说你了解的类加载器。是否实现过类加载器 解释一下什么叫AOP（面向切面编程） 请简述 Servlet 的生命周期及其相关的方法 请简述一下 Ajax 的原理及实现步骤 简单描述Struts的主要功能 什么是 N 层架构 什么是CORBA？用途是什么 什么是Java虚拟机？为什么Java被称作是“平台无关的编程语言” 什么是正则表达式？用途是什么？哪个包使用正则表达式来实现模式匹配 什么是懒加载（Lazy Loading） 什么是尾递归，为什么需要尾递归 什么是控制反转（Inversion of Control）与依赖注入（Dependency Injection） 关键字 finalize 什么是finalize()方法 finalize()方法什么时候被调用 析构函数(finalization)的目的是什么 final 和 finalize 的区别 final final关键字有哪些用法 final 与 static 关键字可以用于哪里？它们的作用是什么 final, finally, finalize的区别 final、finalize 和 finally 的不同之处？ 能否在运行时向 static final 类型的赋值 使用final关键字修饰一个变量时，是引用不能变，还是引用的对象不能变 一个类被声明为final类型，表示了什么意思 throws, throw, try, catch, finally分别代表什么意义 Java 有几种修饰符？分别用来修饰什么 volatile volatile 修饰符的有过什么实践 volatile 变量是什么？volatile 变量和 atomic 变量有什么不同 volatile 类型变量提供什么保证？能使得一个非原子操作变成原子操作吗 能创建 volatile 数组吗？ transient变量有什么特点 super什么时候使用 public static void 写成 static public void会怎样 说明一下public static void main(String args[])这段声明里每个关键字的作用 请说出作用域public, private, protected, 以及不写时的区别 sizeof 是Java 的关键字吗 static static class 与 non static class的区别 static 关键字是什么意思？Java中是否可以覆盖(override)一个private或者是static的方法 静态类型有什么特点 main() 方法为什么必须是静态的？能不能声明 main() 方法为非静态 是否可以从一个静态（static）方法内部发出对非静态（non-static）方法的调用 静态变量在什么时候加载？编译期还是运行期？静态代码块加载的时机呢 成员方法是否可以访问静态变量？为什么静态方法不能访问成员变量 switch switch 语句中的表达式可以是什么类型数据 switch 是否能作用在byte 上，是否能作用在long 上，是否能作用在String上 while 循环和 do 循环有什么不同 操作符 &amp;操作符和&amp;&amp;操作符有什么区别? a = a + b 与 a += b 的区别？ 逻辑操作符 (&amp;,|,^)与条件操作符(&amp;&amp;,||)的区别 3*0.1 == 0.3 将会返回什么？true 还是 false？ float f=3.4; 是否正确？ short s1 = 1; s1 = s1 + 1;有什么错? 数据结构 基础类型(Primitives) 基础类型(Primitives)与封装类型(Wrappers)的区别在哪里 简述九种基本数据类型的大小，以及他们的封装类 int 和 Integer 哪个会占用更多的内存？ int 和 Integer 有什么区别？parseInt()函数在什么时候使用到 float和double的默认值是多少 如何去小数四舍五入保留小数点后两位 char 型变量中能不能存贮一个中文汉字，为什么 类型转换 怎样将 bytes 转换为 long 类型 怎么将 byte 转换为 String 如何将数值型字符转换为数字 我们能将 int 强制转换为 byte 类型的变量吗？如果该值大于 byte 类型的范围，将会出现什么现象 能在不进行强制转换的情况下将一个 double 值赋值给 long 类型的变量吗 类型向下转换是什么 数组 如何权衡是使用无序的数组还是有序的数组 怎么判断数组是 null 还是为空 怎么打印数组？ 怎样打印数组中的重复元素 Array 和 ArrayList有什么区别？什么时候应该使用Array而不是ArrayList 数组和链表数据结构描述，各自的时间复杂度 数组有没有length()这个方法? String有没有length()这个方法 队列 队列和栈是什么，列出它们的区别 BlockingQueue是什么 简述 ConcurrentLinkedQueue LinkedBlockingQueue 的用处和不同之处。 ArrayList、Vector、LinkedList的存储性能和特性 String StringBuffer ByteBuffer 与 StringBuffer有什么区别 HashMap HashMap的工作原理是什么 内部的数据结构是什么 HashMap 的 table的容量如何确定？loadFactor 是什么？ 该容量如何变化？这种变化会带来什么问题？ HashMap 实现的数据结构是什么？如何实现 HashMap 和 HashTable、ConcurrentHashMap 的区别 HashMap的遍历方式及效率 HashMap、LinkedMap、TreeMap的区别 如何决定选用HashMap还是TreeMap 如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办 HashMap 是线程安全的吗？并发下使用的 Map 是什么，它们内部原理分别是什么，比如存储方式、 hashcode、扩容、 默认容量等 HashSet HashSet和TreeSet有什么区别 HashSet 内部是如何工作的 WeakHashMap 是怎么工作的？ Set Set 里的元素是不能重复的，那么用什么方法来区分重复与否呢？是用 == 还是 equals()？ 它们有何区别? TreeMap：TreeMap 是采用什么树实现的？TreeMap、HashMap、LindedHashMap的区别。TreeMap和TreeSet在排序时如何比较元素？Collections工具类中的sort()方法如何比较元素？ TreeSet：一个已经构建好的 TreeSet，怎么完成倒排序。 EnumSet 是什么 Hash算法 Hashcode 的作用 简述一致性 Hash 算法 有没有可能 两个不相等的对象有相同的 hashcode？当两个对象 hashcode 相同怎么办？如何获取值对象 为什么在重写 equals 方法的时候需要重写 hashCode 方法？equals与 hashCode 的异同点在哪里 a.hashCode() 有什么用？与 a.equals(b) 有什么关系 hashCode() 和 equals() 方法的重要性体现在什么地方 Object：Object有哪些公用方法？Object类hashcode,equals 设计原则？ sun为什么这么设计？Object类的概述 如何在父类中为子类自动完成所有的 hashcode 和 equals 实现？这么做有何优劣。 可以在 hashcode() 中使用随机数字吗？ LinkedHashMap LinkedHashMap 和 PriorityQueue 的区别是什么 List List, Set, Map三个接口，存取元素时各有什么特点 List, Set, Map 是否继承自 Collection 接口 遍历一个 List 有哪些不同的方式 LinkedList LinkedList 是单向链表还是双向链表 LinkedList 与 ArrayList 有什么区别 描述下 Java 中集合（Collections），接口（Interfaces），实现（Implementations）的概念。LinkedList 与 ArrayList 的区别是什么？ 插入数据时，ArrayList, LinkedList, Vector谁速度较快？ ArrayList ArrayList 和 HashMap 的默认大小是多数 ArrayList 和 LinkedList 的区别，什么时候用 ArrayList？ ArrayList 和 Set 的区别？ ArrayList, LinkedList, Vector的区别 ArrayList是如何实现的，ArrayList 和 LinkedList 的区别 ArrayList如何实现扩容 Array 和 ArrayList 有何区别？什么时候更适合用Array 说出ArraList,Vector, LinkedList的存储性能和特性 Map Map, Set, List, Queue, Stack Map 接口提供了哪些不同的集合视图 为什么 Map 接口不继承 Collection 接口 Collections 介绍Java中的Collection FrameWork。集合类框架的基本接口有哪些 Collections类是什么？Collection 和 Collections的区别？Collection、Map的实现 集合类框架的最佳实践有哪些 为什么 Collection 不从 Cloneable 和 Serializable 接口继承 说出几点 Java 中使用 Collections 的最佳实践？ Collections 中 遗留类 (HashTable、Vector) 和 现有类的区别 什么是 B+树，B-树，列出实际的使用场景。 接口 Comparator 与 Comparable 接口是干什么的？列出它们的区别 对象 拷贝(clone) 如何实现对象克隆 深拷贝和浅拷贝区别 深拷贝和浅拷贝如何实现激活机制 写clone()方法时，通常都有一行代码，是什么 比较 在比较对象时，”==” 运算符和 equals 运算有何区别 如果要重写一个对象的equals方法，还要考虑什么 两个对象值相同(x.equals(y) == true)，但却可有不同的hash code，这句话对不对 构造器 构造器链是什么 创建对象时构造器的调用顺序 不可变对象 什么是不可变象（immutable object） 为什么 Java 中的 String 是不可变的（Immutable） 如何构建不可变的类结构？关键点在哪里 能创建一个包含可变对象的不可变对象吗 如何对一组对象进行排序 方法 构造器（constructor）是否可被重写（override） 方法可以同时即是 static 又是 synchronized 的吗 abstract 的 method是否可同时是 static，是否可同时是 native，是否可同时是synchronized Java支持哪种参数传递类型 一个对象被当作参数传递到一个方法，是值传递还是引用传递 当一个对象被当作参数传递到一个方法后，此方法可改变这个对象的属性，并可返回变化后的结果，那么这里到底是值传递还是引用传递 我们能否重载main()方法 如果main方法被声明为private会怎样 GC 概念 GC是什么？为什么要有GC 什么时候会导致垃圾回收 GC是怎么样运行的 新老以及永久区是什么 GC 有几种方式？怎么配置 什么时候一个对象会被GC？ 如何判断一个对象是否存活 System.gc() Runtime.gc()会做什么事情？ 能保证 GC 执行吗 垃圾回收器可以马上回收内存吗？有什么办法主动通知虚拟机进行垃圾回收？ Minor GC 、Major GC、Young GC 与 Full GC分别在什么时候发生 垃圾回收算法的实现原理 如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存？ 垃圾回收的最佳做法是什么 GC收集器有哪些 垃圾回收器的基本原理是什么？ 串行(serial)收集器和吞吐量(throughput)收集器的区别是什么 Serial 与 Parallel GC之间的不同之处 CMS 收集器 与 G1 收集器的特点与区别 CMS垃圾回收器的工作过程 JVM 中一次完整的 GC 流程是怎样的？ 对象如何晋升到老年代 吞吐量优先和响应优先的垃圾收集器选择 GC策略 举个实际的场景，选择一个GC策略 JVM的永久代中会发生垃圾回收吗 收集方法 标记清除、标记整理、复制算法的原理与特点？分别用在什么地方 如果让你优化收集方法，有什么思路 JVM 参数 说说你知道的几种主要的jvm 参数 -XX:+UseCompressedOops 有什么作用 类加载器(ClassLoader) Java 类加载器都有哪些 JVM如何加载字节码文件 内存管理 JVM内存分哪几个区，每个区的作用是什么 一个对象从创建到销毁都是怎么在这些部分里存活和转移的 解释内存中的栈(stack)、堆(heap)和方法区(method area)的用法 JVM中哪个参数是用来控制线程的栈堆栈小 简述内存分配与回收策略 简述重排序，内存屏障，happen-before，主内存，工作内存 Java中存在内存泄漏问题吗？请举例说明 简述 Java 中软引用（SoftReferenc）、弱引用（WeakReference）和虚引用 内存映射缓存区是什么 jstack，jstat，jmap，jconsole怎么用 32 位 JVM 和 64 位 JVM 的最大堆内存分别是多数？32 位和 64 位的 JVM，int 类型变量的长度是多数？ 怎样通过 Java 程序来判断 JVM 是 32 位 还是 64 位 JVM自身会维护缓存吗？是不是在堆中进行对象分配，操作系统的堆还是JVM自己管理堆 什么情况下会发生栈内存溢出 双亲委派模型是什么 多线程 基本概念 什么是线程 多线程的优点 多线程的几种实现方式 用 Runnable 还是 Thread 什么是线程安全 Vector, SimpleDateFormat 是线程安全类吗 什么 Java 原型不是线程安全的 哪些集合类是线程安全的 多线程中的忙循环是什么 如何创建一个线程 编写多线程程序有几种实现方式 什么是线程局部变量 线程和进程有什么区别？进程间如何通讯，线程间如何通讯 什么是多线程环境下的伪共享（false sharing） 同步和异步有何异同，在什么情况下分别使用他们？举例说明 Current ConcurrentHashMap 和 Hashtable的区别 ArrayBlockingQueue, CountDownLatch的用法 ConcurrentHashMap的并发度是什么 CyclicBarrier 和 CountDownLatch有什么不同？各自的内部原理和用法是什么 Semaphore的用法 Thread 启动一个线程是调用 run() 还是 start() 方法？start() 和 run() 方法有什么区别 调用start()方法时会执行run()方法，为什么不能直接调用run()方法 sleep() 方法和对象的 wait() 方法都可以让线程暂停执行，它们有什么区别 yield方法有什么作用？sleep() 方法和 yield() 方法有什么区别 Java 中如何停止一个线程 stop() 和 suspend() 方法为何不推荐使用 如何在两个线程间共享数据 如何强制启动一个线程 如何让正在运行的线程暂停一段时间 什么是线程组，为什么在Java中不推荐使用 你是如何调用 wait（方法的）？使用 if 块还是循环？为什么 生命周期 有哪些不同的线程生命周期 线程状态，BLOCKED 和 WAITING 有什么区别 画一个线程的生命周期状态图 ThreadLocal 用途是什么，原理是什么，用的时候要注意什么 ThreadPool 线程池是什么？为什么要使用它 如何创建一个Java线程池 ThreadPool用法与优势 提交任务时，线程池队列已满时会发会生什么 newCache 和 newFixed 有什么区别？简述原理。构造函数的各个参数的含义是什么，比如 coreSize, maxsize 等 线程池的实现策略 线程池的关闭方式有几种，各自的区别是什么 线程池中submit() 和 execute()方法有什么区别？ 线程调度 Java中用到的线程调度算法是什么 什么是多线程中的上下文切换 你对线程优先级的理解是什么 什么是线程调度器 (Thread Scheduler) 和时间分片 (Time Slicing) 线程同步 请说出你所知的线程同步的方法 synchronized 的原理是什么 synchronized 和 ReentrantLock 有什么不同 什么场景下可以使用 volatile 替换 synchronized 有T1，T2，T3三个线程，怎么确保它们按顺序执行？怎样保证T2在T1执行完后执行，T3在T2执行完后执行 同步块内的线程抛出异常会发生什么 当一个线程进入一个对象的 synchronized 方法A 之后，其它线程是否可进入此对象的 synchronized 方法B 使用 synchronized 修饰静态方法和非静态方法有什么区别 如何从给定集合那里创建一个 synchronized 的集合 锁 Java Concurrency API 中 的 Lock 接口是什么？对比同步它有什么优势 Lock 与 Synchronized 的区别？Lock 接口比 synchronized 块的优势是什么 ReadWriteLock是什么？ 锁机制有什么用 什么是乐观锁（Optimistic Locking）？如何实现乐观锁？如何避免ABA问题 解释以下名词：重排序，自旋锁，偏向锁，轻量级锁，可重入锁，公平锁，非公平锁，乐观锁，悲观锁 什么时候应该使用可重入锁 简述锁的等级方法锁、对象锁、类锁 Java中活锁和死锁有什么区别？ 什么是死锁(Deadlock)？导致线程死锁的原因？如何确保 N 个线程可以访问 N 个资源同时又不导致死锁 死锁与活锁的区别，死锁与饥饿的区别 怎么检测一个线程是否拥有锁 如何实现分布式锁 有哪些无锁数据结构，他们实现的原理是什么 读写锁可以用于什么应用场景 Executors类是什么？ Executor和Executors的区别 什么是Java线程转储(Thread Dump)，如何得到它 如何在Java中获取线程堆栈 说出 3 条在 Java 中使用线程的最佳实践 在线程中你怎么处理不可捕捉异常 实际项目中使用多线程举例。你在多线程环境中遇到的常见的问题是什么？你是怎么解决它的 请说出与线程同步以及线程调度相关的方法 程序中有3个 socket，需要多少个线程来处理 假如有一个第三方接口，有很多个线程去调用获取数据，现在规定每秒钟最多有 10 个线程同时调用它，如何做到 如何在 Windows 和 Linux 上查找哪个线程使用的 CPU 时间最长 如何确保 main() 方法所在的线程是 Java 程序最后结束的线程 非常多个线程（可能是不同机器），相互之间需要等待协调才能完成某种工作，问怎么设计这种协调方案 你需要实现一个高效的缓存，它允许多个用户读，但只允许一个用户写，以此来保持它的完整性，你会怎样去实现它 异常 基本概念 Error 和 Exception有什么区别 UnsupportedOperationException是什么 NullPointerException 和 ArrayIndexOutOfBoundException 之间有什么相同之处 什么是受检查的异常，什么是运行时异常 运行时异常与一般异常有何异同 简述一个你最常见到的runtime exception(运行时异常) finally finally关键词在异常处理中如何使用 如果执行finally代码块之前方法返回了结果，或者JVM退出了，finally块中的代码还会执行吗 try里有return，finally还执行么？那么紧跟在这个try后的finally {}里的code会不会被执行，什么时候被执行，在return前还是后 在什么情况下，finally语句不会执行 throw 和 throws 有什么区别？ OOM你遇到过哪些情况？你是怎么搞定的？ SOF你遇到过哪些情况？ 既然我们可以用RuntimeException来处理错误，那么你认为为什么Java中还存在检查型异常 当自己创建异常类的时候应该注意什么 导致空指针异常的原因 异常处理 handle or declare 原则应该如何理解 怎么利用 JUnit 来测试一个方法的异常 catch块里别不写代码有什么问题 你曾经自定义实现过异常吗？怎么写的 什么是 异常链 在try块中可以抛出异常吗 JDBC 通过 JDBC 连接数据库有哪几种方式 阐述 JDBC 操作数据库的基本步骤 JDBC 中如何进行事务处理 什么是 JdbcTemplate 什么是 DAO 模块 使用 JDBC 操作数据库时，如何提升读取数据的性能？如何提升更新数据的性能 列出 5 个应该遵循的 JDBC 最佳实践 IO File File类型中定义了什么方法来创建一级目录 File类型中定义了什么方法来判断一个文件是否存在 流 为了提高读写性能，可以采用什么流 Java中有几种类型的流 JDK 为每种类型的流提供了一些抽象类以供继承，分别是哪些类 对文本文件操作用什么I/O流 对各种基本数据类型和String类型的读写，采用什么流 能指定字符编码的 I/O 流类型是什么 序列化 什么是序列化？如何实现 Java 序列化及注意事项 Serializable 与 Externalizable 的区别 Socket socket 选项 TCP NO DELAY 是指什么 Socket 工作在 TCP/IP 协议栈是哪一层 TCP、UDP 区别及 Java 实现方式 说几点 IO 的最佳实践 直接缓冲区与非直接缓冲器有什么区别？ 怎么读写 ByteBuffer？ByteBuffer 中的字节序是什么 当用System.in.read(buffer)从键盘输入一行n个字符后，存储在缓冲区buffer中的字节数是多少 如何使用扫描器类（Scanner Class）令牌化 面向对象编程（OOP） 解释下多态性（polymorphism），封装性（encapsulation），内聚（cohesion）以及耦合（coupling） 多态的实现原理 封装、继承和多态是什么 对象封装的原则是什么? 类 获得一个类的类对象有哪些方式 重载（Overload）和重写（Override）的区别。重载的方法能否根据返回类型进行区分？ 说出几条 Java 中方法重载的最佳实践 抽象类 抽象类和接口的区别 抽象类中是否可以有静态的main方法 抽象类是否可实现(implements)接口 抽象类是否可继承具体类(concrete class) 匿名类（Anonymous Inner Class） 匿名内部类是否可以继承其它类？是否可以实现接口 内部类 内部类分为几种 内部类可以引用它的包含类（外部类）的成员吗 请说一下 Java 中为什么要引入内部类？还有匿名内部类 继承 继承（Inheritance）与聚合（Aggregation）的区别在哪里 继承和组合之间有什么不同 为什么类只能单继承，接口可以多继承 存在两个类，B 继承 A，C 继承 B，能将 B 转换为 C 么？如 C = (C) B 如果类 a 继承类 b，实现接口c，而类 b 和接口 c 中定义了同名变量，请问会出现什么问题 接口 接口是什么 接口是否可继承接口 为什么要使用接口而不是直接使用具体类？接口有什么优点 泛型 泛型的存在是用来解决什么问题 泛型的常用特点 List能否转为List 工具类 日历 Calendar Class的用途 如何在Java中获取日历类的实例 解释一些日历类中的重要方法 GregorianCalendar 类是什么 SimpleTimeZone 类是什么 Locale类是什么 如何格式化日期对象 如何添加小时(hour)到一个日期对象(Date Objects) 如何将字符串 YYYYMMDD 转换为日期 Math Math.round()什么作用？Math.round(11.5) 等于多少？Math.round(-11.5)等于多少？ XML XML文档定义有几种形式？它们之间有何本质区别？解析XML文档有哪几种方式？DOM 和 SAX 解析器有什么不同？ Java解析XML的方式 用 jdom 解析 xml 文件时如何解决中文问题？如何解析 你在项目中用到了 XML 技术的哪些方面？如何实现 动态代理 描述动态代理的几种实现方式，分别说出相应的优缺点 设计模式 什么是设计模式（Design Patterns）？你用过哪种设计模式？用在什么场合 你知道哪些商业级设计模式？ 哪些设计模式可以增加系统的可扩展性 单例模式 除了单例模式，你在生产环境中还用过什么设计模式？ 写 Singleton 单例模式 单例模式的双检锁是什么 如何创建线程安全的 Singleton 什么是类的单例模式 写出三种单例模式实现 适配器模式 适配器模式是什么？什么时候使用 适配器模式和代理模式之前有什么不同 适配器模式和装饰器模式有什么区别 什么时候使用享元模式 什么时候使用组合模式 什么时候使用访问者模式 什么是模板方法模式 请给出1个符合开闭原则的设计模式的例子 开放问题 用一句话概括 Web 编程的特点 Google是如何在一秒内把搜索结果返回给用户 哪种依赖注入方式你建议使用，构造器注入，还是 Setter方法注入 树（二叉或其他）形成许多普通数据结构的基础。请描述一些这样的数据结构以及何时可以使用它们 某一项功能如何设计 线上系统突然变得异常缓慢，你如何查找问题 什么样的项目不适合用框架 新浪微博是如何实现把微博推给订阅者 简要介绍下从浏览器输入 URL 开始到获取到请求界面之后 Java Web 应用中发生了什么 请你谈谈SSH整合 高并发下，如何做到安全的修改同一行数据 12306网站的订票系统如何实现，如何保证不会票不被超卖 网站性能优化如何优化的 聊了下曾经参与设计的服务器架构 请思考一个方案，实现分布式环境下的 countDownLatch 请思考一个方案，设计一个可以控制缓存总体大小的自动适应的本地缓存 在你的职业生涯中，算得上最困难的技术挑战是什么 如何写一篇设计文档，目录是什么 大写的O是什么？举几个例子 编程中自己都怎么考虑一些设计原则的，比如开闭原则，以及在工作中的应用 解释一下网络应用的模式及其特点 设计一个在线文档系统，文档可以被编辑，如何防止多人同时对同一份文档进行编辑更新 说出数据连接池的工作机制是什么 怎么获取一个文件中单词出现的最高频率 描述一下你最常用的编程风格 如果有机会重新设计你们的产品，你会怎么做 如何搭建一个高可用系统 如何启动时不需输入用户名与密码 如何在基于Java的Web项目中实现文件上传和下载 如何实现一个秒杀系统，保证只有几位用户能买到某件商品。 如何实现负载均衡，有哪些算法可以实现 如何设计一个购物车？想想淘宝的购物车如何实现的 如何设计一套高并发支付方案，架构如何设计 如何设计建立和保持 100w 的长连接 如何避免浏览器缓存。 如何防止缓存雪崩 如果AB两个系统互相依赖，如何解除依 如果有人恶意创建非法连接，怎么解决 如果有几十亿的白名单，每天白天需要高并发查询，晚上需要更新一次，如何设计这个功能 如果系统要使用超大整数（超过long长度范围），请你设计一个数据结构来存储这种超大型数字以及设计一种算法来实现超大整数加法运算） 如果要设计一个图形系统，请你设计基本的图形元件(Point,Line,Rectangle,Triangle)的简单实现 如果让你实现一个并发安全的链表，你会怎么做 应用服务器与WEB 服务器的区别？应用服务器怎么监控性能，各种方式的区别？你使用过的应用服务器优化技术有哪些 大型网站在架构上应当考虑哪些问题 有没有处理过线上问题？出现内存泄露，CPU利用率标高，应用无响应时如何处理的 最近看什么书，印象最深刻的是什么 描述下常用的重构技巧 你使用什么版本管理工具？分支（Branch）与标签（Tag）之间的区别在哪里 你有了解过存在哪些反模式（Anti-Patterns）吗 你用过的网站前端优化的技术有哪些 如何分析Thread dump 你如何理解AOP中的连接点（Joinpoint）、切点（Pointcut）、增强（Advice）、引介（Introduction）、织入（Weaving）、切面（Aspect）这些概念 你是如何处理内存泄露或者栈溢出问题的 你们线上应用的 JVM 参数有哪些 怎么提升系统的QPS和吞吐量 知识面 解释什么是 MESI 协议(缓存一致性) 谈谈 reactor 模型 Java 9 带来了怎样的新功能 Java 与 C++ 对比，C++ 或 Java 中的异常处理机制的简单原理和应用 简单讲讲 Tomcat 结构，以及其类加载器流程 虚拟内存是什么 阐述下 SOLID 原则 请简要讲一下你对测试驱动开发（TDD）的认识 CDN实现原理 Maven 和 ANT 有什么区别 UML中有哪些常用的图 Linux Linux 下 IO 模型有几种，各自的含义是什么。 Linux 系统下你关注过哪些内核参数，说说你知道的 Linux 下用一行命令查看文件的最后五行 平时用到哪些 Linux 命令 用一行命令输出正在运行的 Java 进程 使用什么命令来确定是否有 Tomcat 实例运行在机器上 什么是 N+1 难题 什么是 paxos 算法 什么是 restful，讲讲你理解的 restful 什么是 zab 协议 什么是领域模型(domain model)？贫血模型(anaemic domain model) 和充血模型(rich domain model)有什么区别 什么是领域驱动开发（Domain Driven Development） 介绍一下了解的 Java 领域的 Web Service 框架 Web Server、Web Container 与 Application Server 的区别是什么 微服务（MicroServices）与巨石型应用（Monolithic Applications）之间的区别在哪里 描述 Cookie 和 Session 的作用，区别和各自的应用范围，Session工作原理 你常用的持续集成（Continuous Integration）、静态代码分析（Static Code Analysis）工具有哪些 简述下数据库正则化（Normalizations） KISS,DRY,YAGNI 等原则是什么含义 分布式事务的原理，优缺点，如何使用分布式事务？ 布式集群下如何做到唯一序列号 网络 HTTPS 的加密方式是什么，讲讲整个加密解密流程 HTTPS和HTTP的区别 HTTP连接池实现原理 HTTP集群方案 Nginx、lighttpd、Apache三大主流 Web服务器的区别 是否看过框架的一些代码 持久层设计要考虑的问题有哪些？你用过的持久层框架有哪些 数值提升是什么 你能解释一下里氏替换原则吗 你是如何测试一个应用的？知道哪些测试框架 传输层常见编程协议有哪些？并说出各自的特点 编程题计算加班费加班10小时以下加班费是时薪的1.5倍。加班10小时或以上，按4元/时算。提示：（一个月工作26天，一天正常工作8小时） 计算1000月薪，加班9小时的加班费 计算2500月薪，加班11小时的加班费 计算1000月薪，加班15小时的加班费 卖东西一家商场有红苹果和青苹果出售。（红苹果5元/个，青苹果4元/个）。 模拟一个进货。红苹果跟青苹果各进200个。 模拟一个出售。红苹果跟青苹果各买出10个。每卖出一个苹果需要进行统计。 提示：一个苹果是一个单独的实体。 日期提取有这样一个时间字符串：2008-8-8 20:08:08 ， 请编写能够匹配它的正则表达式，并编写Java代码将日期后面的时分秒提取出来，即：20:08:08 线程 8设计4个线程，其中两个线程每次对j增加1，另外两个线程对j每次减少1。写出程序。 用Java写一个多线程程序，如写四个线程，二个加1，二个对一个变量减一，输出 wait-notify 写一段代码来解决生产者-消费者问题 数字 判断101-200之间有多少个素数，并输出所有素数 用最有效率的方法算出2乘以17等于多少 有 1 亿个数字，其中有 2 个是重复的，快速找到它，时间和空间要最优 2 亿个随机生成的无序整数,找出中间大小的值 10 亿个数字里里面找最小的 10 个 1到1亿的自然数，求所有数的拆分后的数字之和，如286 拆分成2、8、6，如1到11拆分后的数字之和 =&gt; 1 + … + 9 + 1 + 0 + 1 + 1 一个数如果恰好等于它的因子之和，这个数就称为 “完数 “。例如6=1＋2＋3.编程 找出1000以内的所有完数 一个数组中所有的元素都出现了三次，只有一个元素出现了一次找到这个元素 一球从100米高度自由落下，每次落地后反跳回原高度的一半；再落下，求它在 第10次落地时，共经过多少米？第10次反弹多高？ 求100－1000内质数的和 求1到100的和的平均数 求s=a+a+aaa+aaaa+aa…a的值，其中a是一个数字。例如2+22+222+2222+22222(此时共有5个数相加)，几个数相加有键盘控制。 求出1到100的和 算出1到40的质数，放进数组里 显示放组里的数 找出第[5]个数 删除第[9]个数，再显示删除后的第[9]个 有 3n+1 个数字，其中 3n 个中是重复的，只有 1 个是不重复的，怎么找出来。 有一组数1.1.2.3.5.8.13.21.34。写出程序随便输入一个数就能给出和前一组数字同规律的头5个数 计算指定数字的阶乘 开发 Fizz Buzz 给定一个包含 N 个整数的数组，找出丢失的整数 一个排好序的数组，找出两数之和为m的所有组合 将一个正整数分解质因数。例如：输入90,打印出90=2*3*3*5。 打印出所有的 “水仙花数 “，所谓 “水仙花数 “是指一个三位数，其各位数字立方和等于该数本身。例如：153是一个 “水仙花数 “，因为153=1的三次方＋5的三次方＋3的三次方 原地交换两个变量的值 找出4字节整数的中位数 找到整数的平方根 实现斐波那契 网络 用Java Socket编程，读服务器几个字符，再写入本地显示 反射 反射机制提供了什么功能？ 反射是如何实现的 哪里用到反射机制 反射中 Class.forName 和 ClassLoader 区别 反射创建类实例的三种方式是什么 如何通过反射调用对象的方法 如何通过反射获取和设置对象私有字段的值 反射机制的优缺点 数据库 写一段 JDBC 连Oracle的程序,并实现数据查询 算法 50个人围坐一圈，当数到三或者三的倍数出圈，问剩下的人是谁，原来的位置是多少 实现一个电梯模拟器用 写一个冒泡排序 写一个折半查找 随机产生20个不能重复的字符并排序 写一个函数，传入 2 个有序的整数数组，返回一个有序的整数数组 写一段代码在遍历 ArrayList 时移除一个元素 古典问题：有一对兔子，从出生后第3个月起每个月都生一对兔子，小兔子长到第四个月后每个月又生一对兔子，假如兔子都不死，问每个月的兔子总数为多少 约瑟芬环游戏 正则 请编写一段匹配IP地址的正则表达式 写出一个正则表达式来判断一个字符串是否是一个数字 字符串 写一个方法，入一个文件名和一个字符串，统计这个字符串在这个文件中出现的次数。 写一个程序找出所有字符串的组合，并检查它们是否是回文串 写一个字符串反转函数，输入abcde转换成edcba代码 小游戏，倒转句子中的单词 将GB2312编码的字符串转换为ISO-8859-1编码的字符串 请写一段代码来计算给定文本内字符“A”的个数。分别用迭代和递归两种方式 编写一个截取字符串的函数，输入为一个字符串和字节数，输出为按字节截取的字符串。 但是要保证汉字不被截半个，如“我ABC”4，应该截为“我AB”，输入“我ABC汉DEF”，6，应该输出为“我ABC”而不是“我ABC+汉的半个” 给定 2 个包含单词列表（每行一个）的文件，编程列出交集 打印出一个字符串的所有排列 将一个键盘输入的数字转化成中文输出(例如：输入1234567，输出:一百二拾三万四千五百六拾七) 在Web应用开发过程中经常遇到输出某种编码的字符，如从 GBK 到 ISO8859-1等，如何输出一个某种编码的字符串 日期 计算两个日期之间的差距 References:[1] Java 面试题：百度前200页都在这里了[2] Java 面试题：百度前200页都在这里了[3] Java 面试题：百度前200页都在这里了]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取 谷歌翻译]]></title>
    <url>%2F2017%2F11%2F14%2Fcrawler-google-translate%2F</url>
    <content type="text"><![CDATA[领导交给我一个任务，有一些英文的文章，翻译成中文，用谷歌翻译试试。那就试试呗。 F12 经过多次测试，发现请求 https://translate.google.cn/translate_a/single 带一些参数就可以爬取谷歌翻译 第一步 访问 https://translate.google.cn/ 获取TKK 第二步 根据TKK和文本算出tk值，然后访问 https://translate.google.cn/translate_a/single 时加上参数，就可以爬取到翻译后的内容了。 步骤如下: Get方式https://translate.google.cn/translate_a/single?client=t&amp;sl=en&amp;tl=zh-CN&amp;hl=zh-CN&amp;dt=at&amp;dt=bd&amp;dt=ex&amp;dt=ld&amp;dt=md&amp;dt=qca&amp;dt=rw&amp;dt=rm&amp;dt=ss&amp;dt=t&amp;ie=UTF-8&amp;oe=UTF-8&amp;source=btn&amp;ssel=3&amp;tsel=0&amp;kc=2&amp;tk=750300.857549&amp;q=test Post方式https://translate.google.cn/translate_a/single?client=t&amp;sl=en&amp;tl=zh-CN&amp;hl=zh-CN&amp;dt=at&amp;dt=bd&amp;dt=ex&amp;dt=ld&amp;dt=md&amp;dt=qca&amp;dt=rw&amp;dt=rm&amp;dt=ss&amp;dt=t&amp;ie=UTF-8&amp;oe=UTF-8&amp;pc=1&amp;otf=1&amp;ssel=3&amp;tsel=0&amp;kc=1&amp;tk=280278.139719 1234567891011121314151617181920212223https://translate.google.cn/translate_a/single?client=t&amp;sl=en&amp;tl=zh-CN&amp;hl=zh-CN&amp;dt=at&amp;dt=bd&amp;dt=ex&amp;dt=ld&amp;dt=md&amp;dt=qca&amp;dt=rw&amp;dt=rm&amp;dt=ss&amp;dt=t&amp;ie=UTF-8&amp;oe=UTF-8&amp;pc=1&amp;otf=1&amp;ssel=3&amp;tsel=0&amp;kc=1&amp;tk=280278.139719 1234567sl=en：source language为en，即需要翻译的文字是英文 tl=zh-CN：to language，目标语言为zh-CN，即要翻译为中文简体ie=UTF-8：input encoding，输入的文字的编码为UTF-8 oe=UTF-8：output encoding，输出，翻译后，的文字的编码为UTF-8tk是用JavaScript根据TKK和输入的文字算出来的 (1) 获取TKK请求 https://translate.google.cn/ 后，响应的内容里搜索TKK，可以找见下面这行代码，每次TKK都不一样1234567TKK=eval(&apos;((function()&#123;var a\x3d3412621750;var b\x3d-891074424;return 419654+\x27.\x27+(a+b)&#125;)())&apos;);\x3d 是 =\x27 是 &apos;上面这行代码翻译过来就是TKK=eval(&apos;((function()&#123;var a=3412621750;var b=-891074424;return 419654+&apos;.&apos;+(a+b)&#125;)())&apos;); (2) 获取tk值根据TKK和文本算出tk值 1234567891011121314151617181920212223var b = function (a, b) &#123; for (var d = 0; d &lt; b.length - 2; d += 3) &#123; var c = b.charAt(d + 2), c = "a" &lt;= c ? c.charCodeAt(0) - 87 : Number(c), c = "+" == b.charAt(d + 1) ? a &gt;&gt;&gt; c : a &lt;&lt; c; a = "+" == b.charAt(d) ? a + c &amp; 4294967295 : a ^ c &#125; return a&#125;var tk = function (a, TKK) &#123; for (var e = TKK.split("."), h = Number(e[0]) || 0, g = [], d = 0, f = 0; f &lt; a.length; f++) &#123; var c = a.charCodeAt(f); 128 &gt; c ? g[d++] = c : (2048 &gt; c ? g[d++] = c &gt;&gt; 6 | 192 : (55296 == (c &amp; 64512) &amp;&amp; f + 1 &lt; a.length &amp;&amp; 56320 == (a.charCodeAt(f + 1) &amp; 64512) ? (c = 65536 + ((c &amp; 1023) &lt;&lt; 10) + (a.charCodeAt(++f) &amp; 1023), g[d++] = c &gt;&gt; 18 | 240, g[d++] = c &gt;&gt; 12 &amp; 63 | 128) : g[d++] = c &gt;&gt; 12 | 224, g[d++] = c &gt;&gt; 6 &amp; 63 | 128), g[d++] = c &amp; 63 | 128) &#125; a = h; for (d = 0; d &lt; g.length; d++) a += g[d], a = b(a, "+-a^+6"); a = b(a, "+-3^+b+-f"); a ^= Number(e[1]) || 0; 0 &gt; a &amp;&amp; (a = (a &amp; 2147483647) + 2147483648); a %= 1E6; return a.toString() + "." + (a ^ h)&#125; 得到tk以后就可以爬取谷歌翻译了 (3) 可能遇到的错误12345678910111213&lt;!DOCTYPE html&gt;&lt;html lang=en&gt; &lt;meta charset=utf-8&gt; &lt;meta name=viewport content="initial-scale=1, minimum-scale=1, width=device-width"&gt; &lt;title&gt;Error 413 (Request Entity Too Large)!!1&lt;/title&gt; &lt;style&gt; *&#123;margin:0;padding:0&#125;html,code&#123;font:15px/22px arial,sans-serif&#125;html&#123;background:#fff;color:#222;padding:15px&#125;body&#123;margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px&#125;* &gt; body&#123;background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px&#125;p&#123;margin:11px 0 22px;overflow:hidden&#125;ins&#123;color:#777;text-decoration:none&#125;a img&#123;border:0&#125;@media screen and (max-width:772px)&#123;body&#123;background:none;margin-top:0;max-width:none;padding-right:0&#125;&#125;#logo&#123;background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px&#125;@media only screen and (min-resolution:192dpi)&#123;#logo&#123;background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0&#125;&#125;@media only screen and (-webkit-min-device-pixel-ratio:2)&#123;#logo&#123;background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%&#125;&#125;#logo&#123;display:inline-block;height:54px;width:150px&#125; &lt;/style&gt; &lt;a href=//www.google.com/&gt;&lt;span id=logo aria-label=Google&gt;&lt;/span&gt;&lt;/a&gt; &lt;p&gt;&lt;b&gt;413.&lt;/b&gt; &lt;ins&gt;That’s an error.&lt;/ins&gt; &lt;p&gt;Your client issued a request that was too large. &lt;ins&gt;That’s all we know.&lt;/ins&gt;&lt;/html&gt; 文件太大，字数超过5000字 总体来说，谷歌还是比较给面子的，爬了几万条也没有封我ip。 换句话说，假如谷歌封我ip，我可能会为了翻译这几万条用代理+策略访问几十万次，反而会对它的服务器造成压力。 这一点我是非常敬佩的。 毕竟，程序员何苦为难程序员。 References[1] http://www.cnblogs.com/by-dream/p/6554340.html 破解google翻译API全过程[2] https://www.zhihu.com/question/47239748 请问如何调用谷歌翻译API?[3] https://github.com/yixianle/google-translate 翻译工具 支持网页翻译和文本翻译[4] https://cnodejs.org/topic/58c94ea659017af119c1d31b 给大家分享一个免费的谷歌翻译api[5] https://www.crifan.com/teach_you_how_to_find_free_google_translate_api/[6] http://blog.sina.com.cn/s/blog_8af106960102vci1.html 调用Google翻译 api测试 2015-01-20[7] http://www.cnblogs.com/wcymiss/p/6264847.html VBA：Google翻译（含tk算法）]]></content>
      <categories>
        <category>crawler</category>
      </categories>
      <tags>
        <tag>crawler</tag>
        <tag>google</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知识图谱]]></title>
    <url>%2F2017%2F11%2F13%2Fknowledge-graph%2F</url>
    <content type="text"><![CDATA[看了很多资料后，谈谈自己对知识图谱的理解。 比较好的一篇知识图谱的文章 领域应用 | 知识图谱的技术与应用 概论 随着移动互联网的发展，万物互联成为了可能，这种互联所产生的数据也在爆发式地增长，而且这些数据恰好可以作为分析关系的有效原料。如果说以往的智能分析专注在每一个个体上，在移动互联网时代则除了个体，这种个体之间的关系也必然成为我们需要深入分析的很重要一部分。 在一项任务中，只要有关系分析的需求，知识图谱就“有可能”派的上用场。 什么是知识图谱 Google知识图谱（英语：Google Knowledge Graph，也称Google知识图）是Google的一个知识库，其使用语义检索从多种来源收集信息，以提高Google搜索的质量。 其目标是，用户将能够使用此功能提供的信息来解决他们查询的问题，而不必导航到其他网站并自己汇总信息。 据Google称，知识图谱的信息来自许多来源，包括CIA的世界概况，Freebase和维基百科[1]。其功能与Ask.com和Wolfram Alpha等问题问答系统相似。 截至2012年，其语义网络包含超过570亿个对象，超过18亿个介绍，这些不同的对象之间有链接关系，用来理解搜索关键词的含义。 知识图谱最早被应用于搜索引擎领域，旨在通过语义把碎片化的数据关联起来，让用户能直接搜索到事务（Things），而不是文本字符串（Strings）。在搜索引擎中引入知识图谱大幅的提升和优化了搜索体验。 近年来，随着人工智能的再次兴起，知识图谱又被广泛的应用于聊天机器人和问答系统中，用于辅助深度理解人类的语言和支持推理，并提升人机问答的用户体验等。典型的如IBM的Watson，苹果的Siri，Google Allo，Amazon Echo，百度度秘，公子小白等。 此外，知识图谱还被用来提升数据分析的能力和效果。例如著名的大数据公司Palantir利用知识图谱建立数据的关联以提升上游数据分析的效果。与知识图谱有关的语义技术也被用来提升机器与机器之间的语义互操作能力，解决机器之间的语义理解问题。例如，全球最大物联网标准化组织OneM2M就把语义和知识技术作为物联设备抽象和语义封装的技术基础。 在金融、农业、电商、医疗健康、环境保护等大量的垂直领域，知识图谱都得到广泛的应用。例如，很多金融领域公司也构建了金融知识库以进行碎片化金融数据的集成与管理，并辅助金融专家进行风控控制、欺诈识别等；生物医疗专家通过集成和分析大规模的生物医学知识图谱，辅助其进行药物发现、潜在靶点识别等多方面任务。 从学术的角度，我们可以对知识图谱给一个这样的定义：“知识图谱本质上是语义网络（Semantic Network）的知识库”。但这有点抽象，所以换个角度，从实际应用的角度出发其实可以简单地把知识图谱理解成多关系图（Multi-relational Graph）。 那什么叫多关系图呢？ 学过数据结构的都应该知道什么是图（Graph）。图是由节点（Vertex）和边（Edge）来构成，但这些图通常只包含一种类型的节点和边。但相反，多关系图一般包含多种类型的节点和多种类型的边。比如左下图表示一个经典的图结构，右边的图则表示多关系图，因为图里包含了多种类型的节点和边。这些类型由不同的颜色来标记。 图片 在知识图谱里，我们通常用“实体（Entity）”来表达图里的节点、用“关系（Relation）”来表达图里的“边”。实体指的是现实世界中的事物比如人、地名、概念、药物、公司等，关系则用来表达不同实体之间的某种联系，比如人-“居住在”-北京、张三和李四是“朋友”、逻辑回归是深度学习的“先导知识”等等。 现实世界中的很多场景非常适合用知识图谱来表达。 比如一个社交网络图谱里，我们既可以有“人”的实体，也可以包含“公司”实体。人和人之间的关系可以是“朋友”，也可以是“同事”关系。人和公司之间的关系可以是“现任职”或者“曾任职”的关系。 类似的，一个风控知识图谱可以包含“电话”、“公司”的实体，电话和电话之间的关系可以是“通话”关系，而且每个公司它也会有固定的电话。 知识图谱来源于三个方面的技术进步。一是符号人工智能、二是万维网、三是自然语言处理。 在经典人工智能领域，以知识表示为中心的符号人工智能和以神经网络为中心的连接人工智能一直是两个主流方向。神经网络借助深度学习解决了大量感知层面的问题，如视觉、听觉等。知识图谱一定程度上代表符号人工智能的发展方向，被认为是进一步解决认知层面的问题，如语言理解、常识推理等，所不可或缺的技术手段。 在万维网领域，Web之父Tim Berners-Lee于1998年提出了语义网（Semantic Web）的概念。传统Web是通过建立网页之间的链接发展起来的。语义网的初衷也是希望能像传统Web一样，建立数据或对象的直接链接，形成一个庞大的链接数据库或知识库。这种结构化的链接数据将使得Web上的信息更加易于被机器所理解和处理，而不仅仅像网页那样只是供人浏览。谷歌知识图谱的主要数据来源Freebase就是早期的语义网项目。 在自然语言处理领域，从文本中自动或半自动抽取实体及实体之间关系的技术飞速发展，在一定程度上解决了传统知识库获取面临的可扩展性差的问题，从而提升了各种知识图谱构建的效率。 当前，知识图谱技术和方法正在进一步与深度学习等进一步融合。目前人工智能领域的一个重要研究方向就是怎样基于神经网络来处理符号和实习知识图谱的处理和常识推理。可以预见，知识图谱将在人工智能相关的各个领域得到更加广泛的应用。 知识图谱的表示 知识图谱应用的前提是已经构建好了知识图谱，也可以把它认为是一个知识库。这也是为什么它可以用来回答一些搜索相关问题的原因，比如在Google搜索引擎里输入“Who is the wife of Bill Gates?”，我们直接可以得到答案-“Melinda Gates”。这是因为我们在系统层面上已经创建好了一个包含“Bill Gates”和“Melinda Gates”的实体以及他俩之间关系的知识库。所以，当我们执行搜索的时候，就可以通过关键词提取（”Bill Gates”, “Melinda Gates”, “wife”）以及知识库上的匹配可以直接获得最终的答案。这种搜索方式跟传统的搜索引擎是不一样的，一个传统的搜索引擎它返回的是网页、而不是最终的答案，所以就多了一层用户自己筛选并过滤信息的过程。 图片 在现实世界中，实体和关系也会拥有各自的属性，比如人可以有“姓名”和“年龄”。当一个知识图谱拥有属性时，我们可以用属性图（Property Graph）来表示。下面的图表示一个简单的属性图。李明和李飞是父子关系，并且李明拥有一个138开头的电话号，这个电话号开通时间是2018年，其中2018年就可以作为关系的属性。类似的，李明本人也带有一些属性值比如年龄为25岁、职位是总经理等。 图片 这种属性图的表达很贴近现实生活中的场景，也可以很好地描述业务中所包含的逻辑。除了属性图，知识图谱也可以用RDF来表示，它是由很多的三元组（Triples）来组成。RDF在设计上的主要特点是易于发布和分享数据，但不支持实体或关系拥有属性，如果非要加上属性，则在设计上需要做一些修改。目前来看，RDF主要还是用于学术的场景，在工业界我们更多的还是采用图数据库（比如用来存储属性图）的方式。感兴趣的读者可以参考RDF的相关文献，在文本里不多做解释。 知识抽取 知识图谱的构建是后续应用的基础，而且构建的前提是需要把数据从不同的数据源中抽取出来。对于垂直领域的知识图谱来说，它们的数据源主要来自两种渠道：一种是业务本身的数据，这部分数据通常包含在公司内的数据库表并以结构化的方式存储；另一种是网络上公开、抓取的数据，这些数据通常是以网页的形式存在所以是非结构化的数据。 前者一般只需要简单预处理即可以作为后续AI系统的输入，但后者一般需要借助于自然语言处理等技术来提取出结构化信息。比如在上面的搜索例子里，Bill Gates和Malinda Gate的关系就可以从非结构化数据中提炼出来，比如维基百科等数据源。 举个比较实际的例子，比如想获取电影(电影名 上映时间 导演 角色) 演员(姓名 出生年月 性别 等) 等的数据，可以从从比较专业的电影网站爬取，比如豆瓣 知识图谱的应用案例 当下知识图谱已在工业领域得到了广泛应用，如搜索领域的Google搜索、百度搜索，社交领域的领英经济图谱，企业信息领域的天眼查企业图谱 著名的通用知识图谱中有，谷歌 Knowledge Graph 、百度知识图谱、搜狗 知立方 、YAGO、DBpedia等，它们具有规模大、领域宽，包含大量常识等特点。 国内应用案例 在国内，落地的，而且做得比较好的行业知识图谱，我觉得应该是天眼查的工商行业知识图谱，主要是公司和人的数据。 举个例子，滴滴 官方叫法是 北京小桔科技有限公司 的 企业关系 和 历史沿革 点击上面的企业关系和历史沿革即可查看，可以看到 人和公司的关系，而且能够根据时间查看，点击人或者公司可以跳到对应的页面。我觉得天眼查这个企业和人的知识图谱做的确实挺好的。 如果不方便查看，下面有我的截图。 备注：不是打广告，我是在使用Neo4j的时候发现有一个朋友也在使用，最后发现他们可能用Neo4j做离线存储，后来才知道是天眼查。看了一下，感觉在我所知道的国内的知识库、知识图谱里，这个是想法很好，真正落地而且应用的非常好的一个。 国内高校研究复旦大学 中文通用概念知识图谱（CN-Probase）复旦大学 中文通用百科知识图谱（CN-DBpedia） 清华大学 知识图谱 北京大学 知识图谱 如何构建知识图谱 我觉得 项目实战：如何构建知识图谱 这篇文章不错，但是商用有点难 在国内的环境，我觉得直接使用提供的结构化/半结构化数据构建知识图谱是最快的办法，比如运营商、银行、公安、12306、微信、淘宝、京东、支付宝、滴滴，因为他们有大量的结构化数据，而且是经过验证的结构化数据。不过没看到他们的成果，目前看到做的比较好的是天眼查。 领域应用 | 知识图谱的技术与应用 这篇文章里提到构建知识图谱有两种方案，一种是结构化数据稍作处理，构建知识图谱；另一种是用自然语言处理来抽取关系，然后构建知识图谱。 很有幸参与了公共安全方面的知识图谱构建，对结构化数据进行处理，使用Neo4j作为图数据库构建知识图谱，历史数据量在300亿左右，而且每天增量更新上千万。有兴趣的朋友可以交流。 在构建完知识图谱以后，会用到图的遍历算法，社区挖掘算法等。 如果是非结构化数据，首先要数据清洗，对数据进行实体抽取，关系抽取，然后构建知识图谱。 References[1] Google Inside Search[2] Knowledge Graph - Wikipedia[3] Google知识图谱 - 维基百科[4] 知识图谱研究进展[5] 知识图谱目前亟待的问题有哪些？[6] 中文知识图谱构建思路是什么？[7] 专访 | 东南大学漆桂林教授：知识图谱不仅是一项技术，更是一项工程 2016-09-11[8] 知识图谱数据管理浅讲 2017-03-08[9] 浅谈知识图谱数据管理 2017-03-08[10] 中国的资本系图谱 2017-03-19[11] 拓尔思应邀出席大数据产业峰会并就知识图谱应用发表演讲 2017-03-29[12] 知识图谱作投资决策，为何比深度学习更靠谱 2017-04-07[13] HiKnowledge | 大规模知识图谱数据存储实战解析 2017-04-25[14] 图谱在手 天下我有 2017-06-19[15] 知识图谱在金融领域应用简介 2017-07-26[16] 重磅 | 知识图谱前沿技术课程实录 2017-07-19[17] 医学知识图谱构建技术与研究进展[18] 项目实战：如何构建知识图谱[19] 知识图谱的应用[20] DB-Engines Ranking[21] 领域应用 | 知识图谱的技术与应用 2018-06-14[22] 知识图谱科普 什么是知识图谱与语义技术[23] 知识图谱的缘起[24] 一文揭秘！自底向上构建知识图谱全过程 - 阿里技术]]></content>
      <categories>
        <category>knowledge</category>
      </categories>
      <tags>
        <tag>knowledge</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取 搜狗]]></title>
    <url>%2F2017%2F11%2F10%2Fcrawler-sogou%2F</url>
    <content type="text"><![CDATA[重要参数SUID、SNUID、SUVSUID SUID具体的含义可以自行百度，这里只讲述它生成的过程。当我们访问sogou搜索首页的时候，Set-Cookie中便会生成一个SUID参数的内容，除非重启浏览器，不然短时间内SUID并不会改变。SUID的值应该是sogou服务端随便分配的，只有当重新开启一个session时它的值才会更新。 SNUID SNUID是sogou反爬虫的重点，sogou是对同一个SNUID访问次数做了限制，而超过限制后，会跳转到验证码页面，只有输入验证码重新验证以后，SNUID才会更新，访问才能继续进行。那么SNUID是如何生成的呢？经过测试，应该是由javascript生成的，当然前提是要有SUID，SUID是生成SNUID的基础。 SUV SUV参数内容是由JavaScript生成的，测试并没有发现其对于反爬虫有何影响，故本文不做详细介绍。 sct 访问次数 ld 每次请求ld都会变化，但即使不对也能获取到内容 被屏蔽现象 同样，要解决反爬虫问题，我们先来看看触发反爬虫的现象。当同一个SNUID访问次数sct多了以后，继续访问sogou会跳转到一个验证码页面。URL地址以及解码后的url地址：http://www.sogou.com/antispider/?from=%2fweb%3Fquery%3d152512wqe%26ie%3dutf8%26_ast%3d1488957312%26_asf%3dnull%26w%3d01029901%26p%3d40040100%26dp%3d1%26cid%3d%26cid%3d%26sut%3d578%26sst0%3d1488957299160%26lkt%3d3%2C1488957298718%2C1488957298893http://www.sogou.com/antispider/?from=/web?query=152512wqe&amp;ie=utf8&amp;_ast=1488957312&amp;_asf=null&amp;w=01029901&amp;p=40040100&amp;dp=1&amp;cid=&amp;cid=&amp;sut=578&amp;sst0=1488957299160&amp;lkt=3,1488957298718,1488957298893 获取SUID在爬取https://www.sogou.com/时，可以在响应的Headers里找见SUID，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162-----请求开始-----https://www.sogou.com/响应状态：HTTP/1.1 200 OKServer nginxDate Fri, 10 Nov 2017 08:33:45 GMTContent-Type text/html; charset=UTF-8Transfer-Encoding chunkedConnection keep-aliveVary Accept-EncodingSet-Cookie ABTEST=0|1510302825|v17; expires=Sun, 10-Dec-17 08:33:45 GMT; path=/P3P CP=&quot;CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR&quot;Set-Cookie IPLOC=CN1100; expires=Sat, 10-Nov-18 08:33:45 GMT; domain=.sogou.com; path=/P3P CP=&quot;CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR&quot;Set-Cookie SUID=62869B271810990A000000005A056469; expires=Thu, 05-Nov-37 08:33:45 GMT; domain=.sogou.com; path=/P3P CP=&quot;CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR&quot;Cache-Control max-age=0Content-Language zh-CNSet-Cookie black_passportid=1; domain=.sogou.com; path=/; expires=Thu, 01-Dec-1994 16:00:00 GMT Expires Fri, 10 Nov 2017 08:33:45 GMT-----请求结束----------请求开始-----https://www.sogou.com/响应状态：HTTP/1.1 200 OKServer nginxDate Fri, 10 Nov 2017 08:36:43 GMTContent-Type text/html; charset=UTF-8Transfer-Encoding chunkedConnection keep-aliveVary Accept-EncodingSet-Cookie ABTEST=4|1510303003|v17; expires=Sun, 10-Dec-17 08:36:43 GMT; path=/P3P CP=&quot;CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR&quot;Set-Cookie IPLOC=CN1100; expires=Sat, 10-Nov-18 08:36:43 GMT; domain=.sogou.com; path=/P3P CP=&quot;CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR&quot;Set-Cookie SUID=62869B271810990A000000005A05651B; expires=Thu, 05-Nov-37 08:36:43 GMT; domain=.sogou.com; path=/P3P CP=&quot;CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR&quot;Cache-Control max-age=0Content-Language zh-CNSet-Cookie black_passportid=1; domain=.sogou.com; path=/; expires=Thu, 01-Dec-1994 16:00:00 GMTExpires Fri, 10 Nov 2017 08:36:43 GMT-----请求结束----------请求开始-----https://www.sogou.com/响应状态：HTTP/1.1 200 OKServer nginxDate Mon, 13 Nov 2017 02:30:45 GMTContent-Type text/html; charset=UTF-8Transfer-Encoding chunkedConnection keep-aliveVary Accept-EncodingSet-Cookie ABTEST=0|1510540245|v17; expires=Wed, 13-Dec-17 02:30:45 GMT; path=/P3P CP=&quot;CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR&quot;Set-Cookie IPLOC=CN1100; expires=Tue, 13-Nov-18 02:30:45 GMT; domain=.sogou.com; path=/P3P CP=&quot;CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR&quot;Set-Cookie SUID=62869B271810990A000000005A0903D5; expires=Sun, 08-Nov-37 02:30:45 GMT; domain=.sogou.com; path=/P3P CP=&quot;CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR&quot;Cache-Control max-age=0Content-Language zh-CNSet-Cookie black_passportid=1; domain=.sogou.com; path=/; expires=Thu, 01-Dec-1994 16:00:00 GMTExpires Mon, 13 Nov 2017 02:30:45 GMT-----请求结束----- 获取SNUID搜狗重新生成SNUID的HTML源码 在源码里面可以看到重新生成SNUID的步骤 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;link rel=&quot;shortcut icon&quot; href=&quot;//www.sogou.com/images/logo2014/new/favicon.ico&quot; type=&quot;image/x-icon&quot;&gt; &lt;title&gt;搜狗搜索&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;static/css/anti.min.css?v=1&quot;/&gt; &lt;script src=&quot;//dlweb.sogoucdn.com/common/lib/jquery/jquery-1.11.0.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;static/js/antispider.min.js?v=2&quot;&gt;&lt;/script&gt; &lt;script&gt; var domain = getDomain(); window.imgCode = -1; (function() &#123; function checkSNUID() &#123; var cookieArr = document.cookie.split(&apos;; &apos;), count = 0; for(var i = 0, len = cookieArr.length; i &lt; len; i++) &#123; if (cookieArr[i].indexOf(&apos;SNUID=&apos;) &gt; -1) &#123; count++; &#125; &#125; return count &gt; 1; &#125; if(checkSNUID()) &#123; var date = new Date(), expires; date.setTime(date.getTime() -100000); expires = date.toGMTString(); document.cookie = &apos;SNUID=1;path=/;expires=&apos; + expires; document.cookie = &apos;SNUID=1;path=/;expires=&apos; + expires + &apos;;domain=.www.sogou.com&apos;; document.cookie = &apos;SNUID=1;path=/;expires=&apos; + expires + &apos;;domain=.weixin.sogou.com&apos;; document.cookie = &apos;SNUID=1;path=/;expires=&apos; + expires + &apos;;domain=.sogou.com&apos;; document.cookie = &apos;SNUID=1;path=/;expires=&apos; + expires + &apos;;domain=.snapshot.sogoucdn.com&apos;; sendLog(&apos;delSNUID&apos;); &#125; if(getCookie(&apos;seccodeRight&apos;) === &apos;success&apos;) &#123; sendLog(&apos;verifyLoop&apos;); setCookie(&apos;seccodeRight&apos;, 1, getUTCString(-1), location.hostname, &apos;/&apos;); &#125; if(getCookie(&apos;refresh&apos;)) &#123; sendLog(&apos;refresh&apos;); &#125; &#125;)(); function setImgCode(code) &#123; try &#123; var t = new Date().getTime() - imgRequestTime.getTime(); sendLog(&apos;imgCost&apos;,&quot;cost=&quot;+t); &#125; catch (e) &#123; &#125; window.imgCode = code; &#125; sendLog(&apos;index&apos;); function changeImg2() &#123; if(window.event) &#123; window.event.returnValue=false &#125; &#125; &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div class=&quot;header&quot;&gt; &lt;div class=&quot;logo&quot;&gt;&lt;a href=&quot;/&quot;&gt;&lt;img width=&quot;180&quot; height=&quot;60&quot; src=&quot;//www.sogou.com/images/logo2014/error180x60.png&quot;&gt;&lt;/a&gt;&lt;/div&gt; &lt;div class=&quot;other&quot;&gt;&lt;span class=&quot;s1&quot;&gt;您的访问出错了&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&lt;a href=&quot;/&quot;&gt;返回首页&amp;gt;&amp;gt;&lt;/a&gt;&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class=&quot;content-box&quot;&gt; &lt;p class=&quot;ip-time-p&quot;&gt;IP:xxx.xxx.xxx.xxx&lt;br&gt;访问时间：2017.11.10 16:16:01&lt;/p&gt; &lt;p class=&quot;p2&quot;&gt;用户您好，您的访问过于频繁，为确认本次访问为正常用户行为，需要您协助验证。&lt;/p&gt; &lt;p class=&quot;p3&quot;&gt;&lt;label for=&quot;seccodeInput&quot;&gt;验证码：&lt;/label&gt;&lt;/p&gt; &lt;form name=&quot;authform&quot; method=&quot;POST&quot; id=&quot;seccodeForm&quot; action=&quot;/&quot;&gt; &lt;p class=&quot;p4&quot;&gt; &lt;input type=text name=&quot;c&quot; value=&quot;&quot; placeholder=&quot;请输入验证码&quot; id=&quot;seccodeInput&quot;&gt; &lt;input type=&quot;hidden&quot; name=&quot;tc&quot; id=&quot;tc&quot; value=&quot;&quot;&gt; &lt;input type=&quot;hidden&quot; name=&quot;r&quot; id=&quot;from&quot; value=&quot;%2Fweb%3Fquery%3D152512wqe&quot; &gt; &lt;input type=&quot;hidden&quot; name=&quot;m&quot; value=&quot;0&quot; &gt; &lt;span class=&quot;s1&quot;&gt; &lt;script&gt;imgRequestTime=new Date();&lt;/script&gt; &lt;a onclick=&quot;changeImg2();&quot; href=&quot;javascript:void(0)&quot;&gt; &lt;img id=&quot;seccodeImage&quot; onload=&quot;setImgCode(1)&quot; onerror=&quot;setImgCode(0)&quot; src=&quot;util/seccode.php?tc=1510301761&quot; width=&quot;100&quot; height=&quot;40&quot; alt=&quot;请输入图中的验证码&quot; title=&quot;请输入图中的验证码&quot;&gt; &lt;/a&gt; &lt;/span&gt; &lt;a href=&quot;javascript:void(0);&quot; id=&quot;change-img&quot; onclick=&quot;changeImg2();&quot; style=&quot;padding-left:50px;&quot;&gt;换一张&lt;/a&gt; &lt;span class=&quot;s2&quot; id=&quot;error-tips&quot; style=&quot;display: none;&quot;&gt;&lt;/span&gt; &lt;/p&gt; &lt;/form&gt; &lt;p class=&quot;p5&quot;&gt; &lt;a href=&quot;javascript:void(0);&quot; id=&quot;submit&quot;&gt;提交&lt;/a&gt; &lt;span&gt;提交后没解决问题？欢迎&lt;a href=&quot;http://fankui.help.sogou.com/index.php/web/web/index?type=10&amp;anti_time=1510301761&amp;domain=www.sogou.com&quot; target=&quot;_blank&quot;&gt;反馈&lt;/a&gt;。&lt;/span&gt; &lt;/p&gt;&lt;/div&gt;&lt;div id=&quot;ft&quot;&gt;&lt;a href=&quot;http://fuwu.sogou.com/&quot; target=&quot;_blank&quot;&gt;企业推广&lt;/a&gt;&lt;a href=&quot;http://corp.sogou.com/&quot; target=&quot;_blank&quot;&gt;关于搜狗&lt;/a&gt;&lt;a href=&quot;/docs/terms.htm?v=1&quot; target=&quot;_blank&quot;&gt;免责声明&lt;/a&gt;&lt;a href=&quot;http://fankui.help.sogou.com/index.php/web/web/index?type=10&amp;anti_time=1510301761&amp;domain=www.sogou.com&quot; target=&quot;_blank&quot;&gt;意见反馈&lt;/a&gt;&lt;br&gt;&amp;nbsp;&amp;copy;&amp;nbsp;2017&lt;span id=&quot;footer-year&quot;&gt;&lt;/span&gt;&amp;nbsp;Sogou Inc.&amp;nbsp;-&amp;nbsp;&lt;a href=&quot;http://www.miibeian.gov.cn&quot; target=&quot;_blank&quot; class=&quot;g&quot;&gt;京ICP证050897号&lt;/a&gt;&amp;nbsp;-&amp;nbsp;京公网安备1100&lt;span class=&quot;ba&quot;&gt;00000025号&lt;/span&gt;&lt;/div&gt;&lt;script src=&quot;static/js/index.min.js?v=0.1.4&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;&lt;!--zly--&gt; 通过访问一个搜狗url，获取响应header里面的SNUID12345678910111213141516171819202122-----请求开始-----https://www.sogou.com/sogou?ori=%E6%B5%8B%E8%AF%95+&amp;site=news.qq.com&amp;query=%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7&amp;pid=sogou-wsse-b58ac8403eb9cf17-0004&amp;idx=f&amp;page=2&amp;duppid=1&amp;ie=utf8Invalid cookie header: &quot;set-cookie: ld=qyllllllll2zjStRlllllVoJVZGlllllJimpYkllll9lllllRylll5@@@@@@@@@@; path=/; expires=Wed, 13 Dec 2017 03:52:35 GMT; domain=.sogou.com&quot;. Invalid &apos;expires&apos; attribute: Wed, 13 Dec 2017 03:52:35 GMT响应状态：HTTP/1.1 200 OKServer nginxDate Mon, 13 Nov 2017 03:52:35 GMTContent-Type text/html; charset=utf-8Transfer-Encoding chunkedConnection keep-aliveVary Accept-EncodingSet-Cookie ABTEST=5|1510545155|v17; expires=Wed, 13-Dec-17 03:52:35 GMT; path=/P3P CP=&quot;CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR&quot;Set-Cookie SNUID=0DF6EB4870752DA55C005265702D6FF2; expires=Tue, 13-Nov-18 03:52:35 GMT; domain=.sogou.com; path=/Set-Cookie IPLOC=CN1100; expires=Tue, 13-Nov-18 03:52:35 GMT; domain=.sogou.com; path=/P3P CP=&quot;CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR&quot;Set-Cookie SUID=62869B27541C940A000000005A091703; expires=Sun, 08-Nov-37 03:52:35 GMT; domain=.sogou.com; path=/P3P CP=&quot;CURa ADMa DEVa PSAo PSDo OUR BUS UNI PUR INT DEM STA PRE COM NAV OTC NOI DSP COR&quot;set-cookie ld=qyllllllll2zjStRlllllVoJVZGlllllJimpYkllll9lllllRylll5@@@@@@@@@@; path=/; expires=Wed, 13 Dec 2017 03:52:35 GMT; domain=.sogou.comCache-Control max-age=0x_ad_pagesize adpagesize=1059Set-Cookie black_passportid=1; domain=.sogou.com; path=/; expires=Thu, 01-Dec-1994 16:00:00 GMTExpires Mon, 13 Nov 2017 03:52:35 GMT-----请求结束----- 注意: Set-Cookie 和 set-cookie 大小写不一样 12//通过访问一个搜狗url，获取响应header里面的SNUID// 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102#! -*- coding:utf-8 -*-&apos;&apos;&apos;获取SNUID的值&apos;&apos;&apos;import requestsimport jsonimport timeimport random&apos;&apos;&apos;方法（一）通过phantomjs访问sogou搜索结果页面，获取SNUID的值&apos;&apos;&apos;def phantomjs_getsnuid(): from selenium import webdriver d=webdriver.PhantomJS(&apos;D:\python27\Scripts\phantomjs.exe&apos;,service_args=[&apos;--load-images=no&apos;,&apos;--disk-cache=yes&apos;]) try: d.get(&quot;https://www.sogou.com/web?query=&quot;) Snuid=d.get_cookies()[5][&quot;value&quot;] except: Snuid=&quot;&quot; d.quit() return Snuid &apos;&apos;&apos;方法（二）通过访问特定url，获取body里面的id&apos;&apos;&apos;def Method_one(): url=&quot;http://www.sogou.com/antispider/detect.php?sn=E9DA81B7290B940A0000000058BFAB0&amp;wdqz22=12&amp;4c3kbr=12&amp;ymqk4p=37&amp;qhw71j=42&amp;mfo5i5=7&amp;3rqpqk=14&amp;6p4tvk=27&amp;eiac26=29&amp;iozwml=44&amp;urfya2=38&amp;1bkeul=41&amp;jugazb=31&amp;qihm0q=8&amp;lplrbr=10&amp;wo65sp=11&amp;2pev4x=23&amp;4eyk88=16&amp;q27tij=27&amp;65l75p=40&amp;fb3gwq=27&amp;azt9t4=45&amp;yeyqjo=47&amp;kpyzva=31&amp;haeihs=7&amp;lw0u7o=33&amp;tu49bk=42&amp;f9c5r5=12&amp;gooklm=11&amp;_=1488956271683&quot; headers=&#123;&quot;Cookie&quot;: &quot;ABTEST=0|1488956269|v17;\ IPLOC=CN3301;\ SUID=E9DA81B7290B940A0000000058BFAB6D;\ PHPSESSID=rfrcqafv5v74hbgpt98ah20vf3;\ SUIR=1488956269&quot; &#125; try: f=requests.get(url,headers=headers).content f=json.loads(f) Snuid=f[&quot;id&quot;] except: Snuid=&quot;&quot; return Snuid &apos;&apos;&apos;方法（三）访问特定url，获取header里面的内容&apos;&apos;&apos;def Method_two(): url=&quot;https://www.sogou.com/web?query=333&amp;_asf=www.sogou.com&amp;_ast=1488955851&amp;w=01019900&amp;p=40040100&amp;ie=utf8&amp;from=index-nologin&quot; headers=&#123;&quot;Cookie&quot;: &quot;ABTEST=0|1488956269|v17;\ IPLOC=CN3301;\ SUID=E9DA81B7290B940A0000000058BFAB6D;\ PHPSESSID=rfrcqafv5v74hbgpt98ah20vf3;\ SUIR=1488956269&quot; &#125; f=requests.head(url,headers=headers).headers print f&apos;&apos;&apos;方法（四）通过访问需要输入验证码解封的页面，可以获取SNUID&apos;&apos;&apos;def Method_three(): &apos;&apos;&apos; http://www.sogou.com/antispider/util/seccode.php?tc=1488958062 验证码地址 &apos;&apos;&apos; &apos;&apos;&apos; http://www.sogou.com/antispider/?from=%2fweb%3Fquery%3d152512wqe%26ie%3dutf8%26_ast%3d1488957312%26_asf%3dnull%26w%3d01029901%26p%3d40040100%26dp%3d1%26cid%3d%26cid%3d%26sut%3d578%26sst0%3d1488957299160%26lkt%3d3%2C1488957298718%2C1488957298893 访问这个url，然后填写验证码，发送以后就是以下的包内容，可以获取SNUID。 &apos;&apos;&apos; import socket import re res=r&quot;id\&quot;\: \&quot;([^\&quot;]*)\&quot;&quot; s=socket.socket(socket.AF_INET,socket.SOCK_STREAM) s.connect((&apos;www.sogou.com&apos;,80)) s.send(&apos;&apos;&apos;POST http://www.sogou.com/antispider/thank.php HTTP/1.1Host: www.sogou.comContent-Length: 223X-Requested-With: XMLHttpRequestContent-Type: application/x-www-form-urlencoded; charset=UTF-8Cookie: CXID=65B8AE6BEE1CE37D4C63855D92AF339C; SUV=006B71D7B781DAE95800816584135075; IPLOC=CN3301; pgv_pvi=3190912000; GOTO=Af12315; ABTEST=8|1488945458|v17; PHPSESSID=f78qomvob1fq1robqkduu7v7p3; SUIR=D0E3BB8E393F794B2B1B02733A162729; SNUID=B182D8EF595C126A7D67E4E359B12C38; sct=2; sst0=958; ld=AXrrGZllll2Ysfa1lllllVA@rLolllllHc4zfyllllYllllljllll5@@@@@@@@@@; browerV=3; osV=1; LSTMV=673%2C447; LCLKINT=6022; ad=6FwTnyllll2g@popQlSGTVA@7VCYx98tLueNukllll9llllljpJ62s@@@@@@@@@@; SUID=EADA81B7516C860A57B28911000DA424; successCount=1|Wed, 08 Mar 2017 07:51:18 GMT; seccodeErrorCount=1|Wed, 08 Mar 2017 07:51:45 GMTc=6exp2e&amp;r=%252Fweb%253Fquery%253Djs%2B%25E6%25A0%25BC%25E5%25BC%258F%25E5%258C%2596%2526ie%253Dutf8%2526_ast%253D1488957312%2526_asf%253Dnull%2526w%253D01029901%2526p%253D40040100%2526dp%253D1%2526cid%253D%2526cid%253D&amp;v=5 &apos;&apos;&apos;) buf=s.recv(1024) p=re.compile(res) L=p.findall(buf) if len(L)&gt;0: Snuid=L[0] else: Snuid=&quot;&quot; return Snuiddef getsnuid(q): while 1: if q.qsize()&lt;10: Snuid=random.choice([Method_one(),Method_three(),phantomjs_getsnuid()]) if Snuid!=&quot;&quot;: q.put(Snuid) print Snuid time.sleep(0.5)if __name__==&quot;__main__&quot;: import Queue q=Queue.Queue() getsnuid(q) SUV 通过JS生成，可以直接用cookies里的，一般不会改变 参考[1] https://thief.one/2017/03/19/爬取搜索引擎之搜狗/[2] https://blog.gaoqixhb.com/p/56e92e1e7b71cea107c700ba 记搜狗微信号搜索反爬虫]]></content>
      <categories>
        <category>crawler</category>
      </categories>
      <tags>
        <tag>crawler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MariaDB 笔记]]></title>
    <url>%2F2017%2F11%2F05%2Fmariadb-notes%2F</url>
    <content type="text"><![CDATA[看到一篇新闻，MariaDB 完成 C 轮 2700 万美金融资，阿里巴巴领投，看了以后觉得MariaDB挺眼熟的，就查了一下，然后感觉这个挺有意思，试了试。而且对于像谷歌、亚马逊、FaceBook、微软、Twitter、阿里巴巴等这种公司，我觉得他们使用的产品很有可能在将来会比较流行。 11 月 2 日，MariaDB 宣布完成由阿里巴巴领投的 C 轮 2700 万美元的融资。融资完成后，MariaDB 社区将具备更强的实力参与数据库日益激烈的竞争。MariaDB 是一家欧洲公司，其开发维护的 MariaDB 数据库是最受欢迎的开源数据库之一。总部位于芬兰赫尔辛基，在瑞典和美国设有办事处，拥有大约 1200 万名全球数据库用户。包括 booking.com、惠普、维珍移动、维基百科等。MariaDB 是作为 MySQL 的一个分支。MySQL 被甲骨文收购后，MySQL 之父 Monty 为保证有一个始终开源的兼容 MySQL 的分支可用，创立了 MariaDB，名称来自其女儿 Maria 的名字。目前 MariaDB 在 Gartner 统计中是发展最快的开源数据库。 MariaDB数据库管理系统是MySQL的一个分支，主要由开源社区在维护，采用GPL授权许可 MariaDB的目的是完全兼容MySQL，包括API和命令行，使之能轻松成为MySQL的代替品。在存储引擎方面，使用XtraDB（英语：XtraDB）来代替MySQL的InnoDB。 MariaDB由MySQL的创始人Michael Widenius（英语：Michael Widenius）主导开发，他早前曾以10亿美元的价格，将自己创建的公司MySQL AB卖给了SUN，此后，随着SUN被甲骨文收购，MySQL的所有权也落入Oracle的手中。MariaDB名称来自Michael Widenius的女儿Maria的名字。MariaDB基于事务的Maria存储引擎，替换了MySQL的MyISAM存储引擎，它使用了Percona的 XtraDB，InnoDB的变体，分支的开发者希望提供访问即将到来的MySQL 5.4 InnoDB性能。这个版本还包括了 PrimeBase XT (PBXT) 和 FederatedX存储引擎。 安装 MariaDB我的MariaDB解压在 C:\ProfessionalSoftware\MariaDB\mariadb-10.2.10-winx64 目录下，想把数据放到 D:/ProfessionalSoftWare/MariaDB/data 目录下，所以需要把解压完 C:\ProfessionalSoftware\MariaDB\mariadb-10.2.10-winx64\data 目录下的所有文件复制到 D:/ProfessionalSoftWare/MariaDB/data如果不复制，会在启动的时候报 发生系统错误 1067 的错误，原因是 Fatal error: Can&#39;t open and lock privilege tables: Table &#39;mysql.user&#39; doesn&#39;t exist 安装服务123C:\ProfessionalSoftware\MariaDB\mariadb-10.2.10-winx64\binλ mysqld.exe --install MariaDB --defaults-file=&quot;C:\ProfessionalSoftware\MariaDB\mariadb-10.2.10-winx64\my.ini&quot;Service successfully installed. 移除服务123C:\ProfessionalSoftware\MariaDB\mariadb-10.2.10-winx64\binλ mysqld.exe --remove MariaDBService successfully removed. 启动MariaDB服务1234C:\ProfessionalSoftware\MariaDB\mariadb-10.2.10-winx64\binλ net start MariaDBMariaDB 服务正在启动 .MariaDB 服务已经启动成功。 添加用户12C:\ProfessionSofware\Mariadb\Mariadb-10.2\binλ mysqladmin -u root password &quot;root&quot; 123456789101112131415161718192021222324252627282930313233343536373839C:\ProfessionSofware\Mariadb\Mariadb-10.2\binλ mysqld --console2017-11-05 10:10:18 11052 [Note] mysqld (mysqld 10.2.10-MariaDB) starting as process 10168 ...2017-11-05 10:10:18 11052 [Note] InnoDB: Mutexes and rw_locks use Windows interlocked functions2017-11-05 10:10:18 11052 [Note] InnoDB: Uses event mutexes2017-11-05 10:10:18 11052 [Note] InnoDB: Compressed tables use zlib 1.2.32017-11-05 10:10:18 11052 [Note] InnoDB: Number of pools: 12017-11-05 10:10:18 11052 [Note] InnoDB: Using generic crc32 instructions2017-11-05 10:10:18 11052 [Note] InnoDB: Initializing buffer pool, total size = 128M, instances = 1, chunk size = 128M2017-11-05 10:10:18 11052 [Note] InnoDB: Completed initialization of buffer pool2017-11-05 10:10:18 11052 [Note] InnoDB: Highest supported file format is Barracuda.2017-11-05 10:10:18 11052 [Note] InnoDB: 128 out of 128 rollback segments are active.2017-11-05 10:10:18 11052 [Note] InnoDB: Creating shared tablespace for temporary tables2017-11-05 10:10:18 11052 [Note] InnoDB: Setting file &apos;.\ibtmp1&apos; size to 12 MB. Physically writing the file full; Please wait ...2017-11-05 10:10:18 11052 [Note] InnoDB: File &apos;.\ibtmp1&apos; size is now 12 MB.2017-11-05 10:10:18 11052 [Note] InnoDB: Waiting for purge to start2017-11-05 10:10:18 11052 [Note] InnoDB: 5.7.20 started; log sequence number 16199872017-11-05 10:10:18 9084 [Note] InnoDB: Loading buffer pool(s) from C:\ProfessionSofware\Mariadb\Mariadb-10.2\data\ib_buffer_pool2017-11-05 10:10:18 9084 [Note] InnoDB: Buffer pool(s) load completed at 171104 10:10:182017-11-05 10:10:18 11052 [Note] Plugin &apos;FEEDBACK&apos; is disabled.2017-11-05 10:10:18 11052 [Note] Server socket created on IP: &apos;::&apos;.2017-11-05 10:10:18 11052 [Note] Reading of all Master_info entries succeded2017-11-05 10:10:18 11052 [Note] Added new Master_info &apos;&apos; to hash table2017-11-05 10:10:18 11052 [Note] mysqld: ready for connections.Version: &apos;10.2.10-MariaDB&apos; socket: &apos;&apos; port: 3306 mariadb.org binary distribution2017-11-05 11:00:24 10868 [Note] mysqld (unknown): Normal shutdown2017-11-05 11:00:24 10868 [Note] Event Scheduler: Purging the queue. 0 events2017-11-05 11:00:24 8456 [Note] InnoDB: FTS optimize thread exiting.2017-11-05 11:00:24 10868 [Note] InnoDB: Starting shutdown...2017-11-05 11:00:24 9084 [Note] InnoDB: Dumping buffer pool(s) to C:\ProfessionSofware\Mariadb\Mariadb-10.2\data\ib_buffer_pool2017-11-05 11:00:24 9084 [Note] InnoDB: Buffer pool(s) dump completed at 171104 11:00:242017-11-05 11:00:25 10868 [Note] InnoDB: Shutdown completed; log sequence number 16200152017-11-05 11:00:25 10868 [Note] InnoDB: Removed temporary tablespace data file: &quot;ibtmp1&quot;2017-11-05 11:00:25 10868 [Note] mysqld: Shutdown complete 12345678910C:\ProfessionSofware\Mariadb\Mariadb-10.2\binλ net start mariadbMariaDB 服务正在启动 .MariaDB 服务已经启动成功。C:\ProfessionSofware\Mariadb\Mariadb-10.2\binλ net stop mariadbMariaDB 服务正在停止.MariaDB 服务已成功停止。 my.ini 配置123456789101112131415161718192021222324252627282930313233343536[mysqld]#datadir=C:/ProfessionalSoftware/MariaDB/MariaDB-10.3/datadatadir=D:/ProfessionalSoftWare/MariaDB/MariaDB-10.3/dataport=3306#character-set-server=utf8# utf8mb4 is a superset of utf8character-set-server=utf8mb4# mkdir for every databaseinnodb_file_per_table=1# ignore lowercaselower_case_table_names=1# all import biggest 1024M file to mysqlmax_allowed_packet=1024Minnodb_buffer_pool_size=1017M#log# Binary Loglog-bin=mysql-bin# if query_time &gt; 1s sql will loglong_query_time=1# if query is slow, query will log version 5.6slow-query-log=1slow-query-log-file=D:/ProfessionalSoftWare/MariaDB/MariaDB-10.3/log/slow_query.loggeneral_log=ONgeneral_log_file=D:/ProfessionalSoftWare/MariaDB/MariaDB-10.3/log/all_query.log[client]port=3306plugin-dir=C:/ProfessionalSoftware/MariaDB/MariaDB-10.3/lib/plugin#default-character-set = utf8# utf8mb4 is a superset of utf8default-character-set = utf8mb4 程序中使用1234567891011121314151617181920212223242526&lt;dependency&gt; &lt;groupId&gt;org.mariadb.jdbc&lt;/groupId&gt; &lt;artifactId&gt;mariadb-java-client&lt;/artifactId&gt; &lt;version&gt;2.1.2&lt;/version&gt;&lt;/dependency&gt;driverClassName = org.mariadb.jdbc.Driverurl = jdbc:mariadb://localhost:3306/DB&amp;useSSL=true&amp;trustServerCertificate=trueuser = adminpassword = adminConnection connection = DriverManager.getConnection(&quot;jdbc:mariadb://localhost:3306/DB?user=root&amp;password=myPassword&quot;);final HikariDataSource ds = new HikariDataSource();ds.setMaximumPoolSize(20);ds.setDriverClassName(&quot;org.mariadb.jdbc.Driver&quot;);ds.setJdbcUrl(&quot;jdbc:mariadb://localhost:3306/db&quot;);ds.addDataSourceProperty(&quot;user&quot;, &quot;root&quot;);ds.addDataSourceProperty(&quot;password&quot;, &quot;myPassword&quot;);ds.setAutoCommit(false);jdbc:(mysql|mariadb):[replication:|failover:|sequential:|aurora:]//&lt;hostDescription&gt;[,&lt;hostDescription&gt;...]/[database][?&lt;key1&gt;=&lt;value1&gt;[&amp;&lt;key2&gt;=&lt;value2&gt;]] 12345678910&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt;&lt;/dependency&gt; 12345678910111213141516171819&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;logger name=&quot;org.mariadb.jdbc&quot; level=&quot;trace&quot; additivity=&quot;false&quot;&gt; &lt;appender-ref ref=&quot;STDOUT&quot;/&gt; &lt;/logger&gt; &lt;root level=&quot;error&quot;&gt; &lt;appender-ref ref=&quot;STDOUT&quot;/&gt; &lt;/root&gt;&lt;/configuration&gt; References[1] mariadb官网[2] mariadb中文资料[3] mariadb zip包安装[4] mysql_install_dbexe[5] mysql windows intall archive[6] 在Windows下用 MSI 包安装 MariaDB[7] 安装 MariaDB 并同时运行 MySQL[8] 怎样从MySQL升级到 MariaDB？[9] 升级mariadb[10] java-connector-using-maven[11] failover-and-high-availability-with-mariadb-connector-j[12] option-batchmultisend-description[13] 设置字符集和排序规则[14] 一般查询日志[15] 索引-缓存[16] 在Windows安裝Zip (noinstall)版本的MariaDB[17] Windows下如何安装MariaDB[18] Windows 下 MariaDB (zip 免安装) 的手动安装与使用]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Filddler 笔记]]></title>
    <url>%2F2017%2F10%2F30%2Ffilddler-notes%2F</url>
    <content type="text"><![CDATA[确保你的Android设备和你安装Fiddler的电脑都连接到一个WiFi AP上 Fiddler默认是不抓取HTTPS包的，需要进行相应的配置。 打开Fiddler， Tools -&gt; Fiddler Options -&gt; HTTPS 勾选 “ Capture HTTPS CONNECTs “ ， 勾选 “ Decrypt HTTPS traffic “ 如果你要监听的程序访问的HTTPS站点使用的是不可信的证书，则请接着把下面的 “ Ignore server certificate errors “ 勾选上。 Tools -&gt; Fiddler Options -&gt; Connections 监听端口默认是8888，当然你可以把它设置成任何你想要的端口。请一定要勾选上 “ Allow remote computers to connect “ 。 打开手机wifi，输入wifi ssid password ，勾选 “ 显示高级选项 “ 。在接下来显示的页面中，点击 “代理”，选择 “手动”， 在 “代理服务器主机名” 和 “代理服务器端口” 中写上前面得到的地址和端口，最后点“保存”。 Fiddler本质上是一个HTTPS代理服务器，其自己带的证书显然不会在Android设备的受信任证书列表里。 有些应用程序会查看服务器端的证书是否是由受信任的根证书签名的，如果不是就直接跳出。为了保险起见，我们要将Fiddler代理服务器的证书导到Android设备上。导入的过程非常简单，打开设备自带的浏览器，在地址栏中输入代理服务器的IP和端口，例如本例中我们会输入192.169.191.1:8888，进入之后会看到一个Fiddler提供的页面：点击页面中的“FiddlerRoot certificate”链接，接着系统会弹出对话框,输入一个证书名称，然后直接点“确定”就好了。 Fiddler抓包优点: 手机不需要root就可以抓包； 可以用真机抓包，有些程序是抗动态分析的，能够判断自己运行在模拟器中。 缺点： 必须要用WiFi连接 要抓包分析的应用程序必须自己支持代理服务器的设置。 可能遇到的问题手机无法连到代理1 在电脑中开启了猎豹wifi，开启fidder软件，配置代理端口8888，手机连接猎豹wifi，在wifi高级中配置fidder代理 手机配置代理：wifi(连接猎豹wifi)–&gt;高级 代理：手动 主机名：通过ipconfig 查询出的电脑IP地址 端口：fidder 设置的端口8888 (fidder–tools–options–connections–fidder listens on port：) 在使用过程中发现手机连接上猎豹wifi，并设置代理主机名、代理端口后，依然无法正常上网，无法使用fidder代理，最终找到原因是因为IE导致，修改IE 配置如下所示： IE -&gt; 设置 -&gt; Internet选项 -&gt; 连接 -&gt; 局域网(LAN)设置 -&gt; 局域网设置(L) -&gt; 代理服务器 -&gt; 为LAN使用代理服务器(这些设置不用于拨号或VPN链接)(X) 把前面的复选框取消掉 手机无法连到代理2 控制面板 -&gt; 系统和安全 -&gt; Windows Defender 防火墙 -&gt; 允许的应用 References[1] Fiddler教程 [2] Fiddler 教程 [3] Fiddler抓取Android真机上的HTTPS包 [4] fiddler抓取https请求（android/ios） [5] Wireshark和Fiddler分析Android中的TLS协议包数据(附带案例样本)[6] 无法连接使用Fidder代理[7] Fiddler：手机连接代理后无法上网的问题[8] ConfigureFiddler]]></content>
      <categories>
        <category>crawler</category>
      </categories>
      <tags>
        <tag>crawler</tag>
        <tag>spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Next 主题 配置]]></title>
    <url>%2F2017%2F10%2F26%2Fnext-theme-config%2F</url>
    <content type="text"><![CDATA[hexo next 主题的配置及个性化 (1) next主题的配置next主题目录结构 12345678910111213141516171819202122232425262728293031323334353637├── .github #git信息├── languages #多语言| ├── default.yml #默认语言| └── zh-Hans.yml #简体中文| └── zh-tw.yml #繁体中文├── layout #布局，根目录下的*.ejs文件是对主页，分页，存档等的控制| ├── _custom #可以自己修改的模板，覆盖原有模板| | ├── _header.swig #头部样式| | ├── _sidebar.swig #侧边栏样式| ├── _macro #可以自己修改的模板，覆盖原有模板| | ├── post.swig #文章模板| | ├── reward.swig #打赏模板| | ├── sidebar.swig #侧边栏模板| ├── _partial #局部的布局| | ├── head #头部模板| | ├── search #搜索模板| | ├── share #分享模板| ├── _script #局部的布局| ├── _third-party #第三方模板| ├── _layout.swig #主页面模板| ├── index.swig #主页面模板| ├── page #页面模板| └── tag.swig #tag模板├── scripts #script源码| ├── tags #tags的script源码| ├── marge.js #页面模板├── source #源码| ├── css #css源码| | ├── _common #*.styl基础css| | ├── _custom #*.styl局部css| | └── _mixins #mixins的css| ├── fonts #字体| ├── images #图片| ├── uploads #添加的文件| └── js #javascript源代码├── _config.yml #主题配置文件└── README.md #用GitHub的都知道 (2) Hexo+Next主题 文章添加阅读次数，访问量等不蒜子统计打开 Hexo 目录下的 \themes\next\ _config.yml 文件 1234567891011121314151617# Show PV/UV of the website/page with busuanzi.# Get more information on http://ibruce.info/2015/04/04/busuanzi/busuanzi_count: # count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: &lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt; site_uv_footer: # custom pv span for the whole site site_pv: true site_pv_header: &lt;i class=&quot;fa fa-eye&quot;&gt;&lt;/i&gt; 总访问量 site_pv_footer: 次 # custom pv span for one page only page_pv: true page_pv_header: &lt;i class=&quot;fa fa-file-o&quot;&gt;&lt;/i&gt; 浏览 page_pv_footer: 次 (3) 在文章底部增加版权信息在目录 next/layout/_macro/下添加 my-copyright.swig：1234567891011121314151617181920212223242526272829303132&#123;% if page.copyright %&#125;&lt;div class=&quot;my_post_copyright&quot;&gt; &lt;script src=&quot;//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js&quot;&gt;&lt;/script&gt; &lt;!-- JS库 sweetalert 可修改路径 --&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js&quot;&gt;&lt;/script&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css&quot;&gt; &lt;p&gt;&lt;span&gt;本文标题:&lt;/span&gt;&lt;a href=&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot;&gt;&#123;&#123; page.title &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;文章作者:&lt;/span&gt;&lt;a href=&quot;/&quot; title=&quot;访问 &#123;&#123; theme.author &#125;&#125; 的个人博客&quot;&gt;&#123;&#123; theme.author &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;发布时间:&lt;/span&gt;&#123;&#123; page.date.format(&quot;YYYY年MM月DD日 - HH:MM&quot;) &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;最后更新:&lt;/span&gt;&#123;&#123; page.updated.format(&quot;YYYY年MM月DD日 - HH:MM&quot;) &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;原始链接:&lt;/span&gt;&lt;a href=&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot; title=&quot;&#123;&#123; page.title &#125;&#125;&quot;&gt;&#123;&#123; page.permalink &#125;&#125;&lt;/a&gt; &lt;span class=&quot;copy-path&quot; title=&quot;点击复制文章链接&quot;&gt;&lt;i class=&quot;fa fa-clipboard&quot; data-clipboard-text=&quot;&#123;&#123; page.permalink &#125;&#125;&quot; aria-label=&quot;复制成功！&quot;&gt;&lt;/i&gt;&lt;/span&gt; &lt;/p&gt; &lt;p&gt;&lt;span&gt;许可协议:&lt;/span&gt;&lt;i class=&quot;fa fa-creative-commons&quot;&gt;&lt;/i&gt; &lt;a rel=&quot;license&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/&quot; target=&quot;_blank&quot; title=&quot;Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)&quot;&gt;署名-非商业性使用-禁止演绎 4.0 国际&lt;/a&gt; 转载请保留原文链接及作者。&lt;/p&gt; &lt;/div&gt;&lt;script&gt; var clipboard = new Clipboard(&apos;.fa-clipboard&apos;); clipboard.on(&apos;success&apos;, $(function()&#123; $(&quot;.fa-clipboard&quot;).click(function()&#123; swal(&#123; title: &quot;&quot;, text: &apos;复制成功&apos;, html: false, timer: 500, showConfirmButton: false &#125;); &#125;); &#125;)); &lt;/script&gt;&#123;% endif %&#125; 在目录next/source/css/_common/components/post/下添加my-post-copyright.styl:123456789101112131415161718192021222324252627282930313233343536373839404142434445.my_post_copyright &#123; width: 85%; max-width: 45em; margin: 2.8em auto 0; padding: 0.5em 1.0em; border: 1px solid #d3d3d3; font-size: 0.93rem; line-height: 1.6em; word-break: break-all; background: rgba(255,255,255,0.4);&#125;.my_post_copyright p&#123;margin:0;&#125;.my_post_copyright span &#123; display: inline-block; width: 5.2em; color: #b5b5b5; font-weight: bold;&#125;.my_post_copyright .raw &#123; margin-left: 1em; width: 5em;&#125;.my_post_copyright a &#123; color: #808080; border-bottom:0;&#125;.my_post_copyright a:hover &#123; color: #a3d2a3; text-decoration: underline;&#125;.my_post_copyright:hover .fa-clipboard &#123; color: #000;&#125;.my_post_copyright .post-url:hover &#123; font-weight: normal;&#125;.my_post_copyright .copy-path &#123; margin-left: 1em; width: 1em; +mobile()&#123;display:none;&#125;&#125;.my_post_copyright .copy-path:hover &#123; color: #808080; cursor: pointer;&#125; 修改next/layout/_macro/post.swig，在代码12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;wechat-subscriber.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 之前增加12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;my-copyright.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 修改next/source/css/_common/components/post/post.styl文件，在最后一行增加代码：1@import &quot;my-post-copyright&quot; 保存重新生成即可。 如果要在该博文下面增加版权信息的显示，需要在 Markdown 中增加copyright: true的设置，类似：1234567---title: 前端小项目：使用canvas绘画哆啦A梦date: 2017-05-22 22:53:53tags: canvascategories: 前端copyright: true--- (4) 添加顶部加载条最新版的next主题，升级后只需修改主题配置文件(_config.yml)将pace: false改为pace: true就行了，你还可以换不同样式的加载条。 如果不是最新版，按照以下配置 12&lt;script src=&quot;//cdn.bootcss.com/pace/1.0.2/pace.min.js&quot;&gt;&lt;/script&gt;&lt;link href=&quot;//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css&quot; rel=&quot;stylesheet&quot;&gt; 但是，默认的是粉色的，要改变颜色可以在/themes/next/layout/_partials/head.swig文件中添加如下代码（接在刚才link的后面） 12345678910111213&lt;style&gt; .pace .pace-progress &#123; background: #1E92FB; /*进度条颜色*/ height: 3px; &#125; .pace .pace-progress-inner &#123; box-shadow: 0 0 10px #1E92FB, 0 0 5px #1E92FB; /*阴影颜色*/ &#125; .pace .pace-activity &#123; border-top-color: #1E92FB; /*上边框颜色*/ border-left-color: #1E92FB; /*左边框颜色*/ &#125;&lt;/style&gt; 12345678910111213141516171819&lt;meta charset=&quot;UTF-8&quot;/&gt;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot; /&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, maximum-scale=1&quot;/&gt;&lt;meta name=&quot;theme-color&quot; content=&quot;&#123;&#123; theme.android_chrome_color &#125;&#125;&quot;&gt;&lt;script src=&quot;//cdn.bootcss.com/pace/1.0.2/pace.min.js&quot;&gt;&lt;/script&gt;&lt;link href=&quot;//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css&quot; rel=&quot;stylesheet&quot;&gt;&lt;style&gt; .pace .pace-progress &#123; background: #1E92FB; /*进度条颜色*/ height: 3px; &#125; .pace .pace-progress-inner &#123; box-shadow: 0 0 10px #1E92FB, 0 0 5px #1E92FB; /*阴影颜色*/ &#125; .pace .pace-activity &#123; border-top-color: #1E92FB; /*上边框颜色*/ border-left-color: #1E92FB; /*左边框颜色*/ &#125;&lt;/style&gt; (5) 实现点击出现桃心效果1http://7u2ss1.com1.z0.glb.clouddn.com/love.js 然后将里面的代码copy一下，新建love.js文件并且将代码复制进去，然后保存。将love.js文件放到路径/themes/next/source/js/src里面，然后打开\themes\next\layout_layout.swig文件,在末尾前面（在前面引用会出现找不到的bug）添加以下代码：12&lt;!-- 页面点击小红心 --&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;/js/src/love.js&quot;&gt;&lt;/script&gt; (6) 文章加密访问打开themes-&gt;next-&gt;layout-&gt;_partials-&gt;head.swig文件，在以下位置插入这样一段代码： 12345678910&lt;script&gt; (function()&#123; if(&apos;&#123;&#123; page.password &#125;&#125;&apos;)&#123; if (prompt(&apos;请输入文章密码&apos;) !== &apos;&#123;&#123; page.password &#125;&#125;&apos;)&#123; alert(&apos;密码错误！&apos;); history.back(); &#125; &#125; &#125;)();&lt;/script&gt; 123456789101112131415&lt;meta charset=&quot;UTF-8&quot;/&gt;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot; /&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, maximum-scale=1&quot;/&gt;&lt;meta name=&quot;theme-color&quot; content=&quot;&#123;&#123; theme.android_chrome_color &#125;&#125;&quot;&gt;&lt;script&gt; (function()&#123; if(&apos;&#123;&#123; page.password &#125;&#125;&apos;)&#123; if (prompt(&apos;请输入文章密码&apos;) !== &apos;&#123;&#123; page.password &#125;&#125;&apos;)&#123; alert(&apos;密码错误！&apos;); history.back(); &#125; &#125; &#125;)();&lt;/script&gt; (7) 博文置顶修改 hero-generator-index 插件，把文件：node_modules/hexo-generator-index/lib/generator.js 内的代码替换为：12345678910111213141516171819202122232425262728&apos;use strict&apos;;var pagination = require(&apos;hexo-pagination&apos;);module.exports = function(locals)&#123; var config = this.config; var posts = locals.posts; posts.data = posts.data.sort(function(a, b) &#123; if(a.top &amp;&amp; b.top) &#123; // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排 &#125; else if(a.top &amp;&amp; !b.top) &#123; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1; &#125; else if(!a.top &amp;&amp; b.top) &#123; return 1; &#125; else return b.date - a.date; // 都没定义按照文章日期降序排 &#125;); var paginationDir = config.pagination_dir || &apos;page&apos;; return pagination(&apos;&apos;, posts, &#123; perPage: config.index_generator.per_page, layout: [&apos;index&apos;, &apos;archive&apos;], format: paginationDir + &apos;/%d/&apos;, data: &#123; __index: true &#125; &#125;);&#125;; 在文章中添加 top 值，数值越大文章越靠前，如12345678910111213---title: Next主题配置date: 2017-10-26 09:49:17updated:categories: blogtype:tags: - blogcomments:copyright:mathjax:top: 10--- (8) Hexo去掉码市(gitee.com)跳转页 找到themes/next/layout/_partials/footer.swig文件，12&lt;div class="theme-info"&gt;，在&lt;/div&gt;前加入下面文字 123&lt;span&gt;｜one of Hosted by &lt;a href="https://pages.coding.me" rel="external nofollow"&gt;Coding Pages&lt;/a&gt;&lt;/span&gt; (9) 添加博客更新时间在 /themes/next/layout/_macro/post.swig 文件，在 &lt;span class=&quot;post-time&quot;&gt;...&lt;/span&gt; 标签后添加以下内容12345678&#123;%if post.updated and post.updated &gt; post.date%&#125; &lt;span class=&quot;post-updated&quot;&gt; &amp;nbsp; | &amp;nbsp; &#123;&#123; __(&apos;post.updated&apos;) &#125;&#125; &lt;time itemprop=&quot;dateUpdated&quot; datetime=&quot;&#123;&#123; moment(post.updated).format() &#125;&#125;&quot; content=&quot;&#123;&#123; date(post.updated, config.date_format) &#125;&#125;&quot;&gt; &#123;&#123; date(post.updated, config.date_format) &#125;&#125; &lt;/time&gt; &lt;/span&gt;&#123;% endif %&#125; (10) 自定义网站配置 常见问题 NexT 对于内容的宽度的设定如下： 700px，当屏幕宽度 &lt; 1600px 900px，当屏幕宽度 &gt;= 1600px 移动设备下，宽度自适应 如果你需要修改内容的宽度，同样需要编辑样式文件。 编辑主题的 source/css/_variables/custom.styl 文件，新增变量： 修改 themes/hexo-theme-next/source/css/_variables/custom.styl 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// 修改成你期望的宽度$content-desktop = 700px// 当视窗超过 1600px 后的宽度$content-desktop-large = 900px``` 以下方法弃用themes/hexo-theme-next/source/css/_custom/custom.styl```text// Custom styles.// 给页面添加背景图片// https://source.unsplash.com/ 素材// http://oty1v077k.bkt.clouddn.com/body_girlasideflower.jpgbody&#123; background: #C7EDCC // #C7EDCC //background:url(/img/backgroud.png); background-size:cover; background-repeat:no-repeat; background-attachment:fixed; background-position:center; &#125;// 文字背景色以及半透明的设置.content &#123; border-radius: 10px; margin-top: 30px; background:rgba(199, 237, 204, 0.6) none repeat scroll !important; &#125;// 页面宽度.main-inner &#123;width: 80%;&#125;// 代码颜色#posts code &#123;color: black; background-color:white;&#125;// 页面头部和底部栏的背景色修改// 0, 0, 255, 0.4.header &#123; background:rgba(199, 179, 229, 1) none repeat scroll !important; &#125;.footer &#123; background:rgba(199, 179, 229, 1) none repeat scroll !important; &#125; // 侧栏背景图以及内部文字颜色的修改#sidebar &#123; background: #FDFFDF; // #FDFFDF background-size: cover; background-position:center; background-repeat:no-repeat; p,span,a &#123;color: black;&#125;&#125;// 评论(来必力)添加背景色#lv-container &#123; border-radius: 10px; background:rgba(0, 0, 0, 0.3) none repeat scroll !important; &#125; (11) hexo-algolia全文搜索hexo-theme-next 从5.1.4升级到6.0.3，发现hexo-algoliasearch不能全文搜索了hexo algolia报错123456789$ hexo algoliaFATAL Something&apos;s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlAlgoliaSearchError: Please provide an application ID. Usage: algoliasearch(applicationID, apiKey, opts) at AlgoliaSearchNodeJS.AlgoliaSearchCore (C:\blog\node_modules\algoliasearch\src\AlgoliaSearchCore.js:51:11) at AlgoliaSearchNodeJS.AlgoliaSearch (C:\blog\node_modules\algoliasearch\src\AlgoliaSearch.js:11:21) at AlgoliaSearchNodeJS.AlgoliaSearchServer (C:\blog\node_modules\algoliasearch\src\server\builds\AlgoliaSearchServer.js:17:17) at new AlgoliaSearchNodeJS (C:\blog\node_modules\algoliasearch\src\server\builds\node.js:79:23) at algoliasearch (C:\blog\node_modules\algoliasearch\src\server\builds\node.js:68:10) at C:\blog\node_modules\hexo-algolia\lib\command.js:67:16 这个是使用hexo-algoliasearch组件出现的问题 首先安装组件1npm install hexo-algoliasearch --save 如果不确定，建议同时设置 applicationID 和 appId 修改3处地方，把以下3个文件里的applicationID 替换为 appId 博客的_config.yml (1处) next主题的 source/js/src/algolia-search.js (2处) next主题的 layout/_partials/head/head.swig (2处) blog/_config.yml 修改完如下 1234567891011121314151617algolia: appId: xxxx applicationID: xxxx apiKey: xxxx adminApiKey: xxxx indexName: xxxx chunkSize: 5000 fields: - title - tags - slug - excerpt - excerpt:strip - photos - gallery - permalink - content:strip (12) 标签页图标 修改以下两个文件 hexo-theme-next/source/images/favicon-16x16-next.png hexo-theme-next/source/images/favicon-32x32-next.png (13) hexo algolia 不能用 can not find lib/algolia-instant-search/instantsearch.min.css lib/algolia-instant-search/instantsearch.min.js [need to install libs separately] 缺少文件了，按照以下方法获取文件 12cd hexo-theme-next/source/lib/algolia-instant-search/git clone https://github.com/theme-next/theme-next-algolia-instant-search (14) Algolia Settings are invalid. 更新hexo-theme-next后，并且把配置文件放到blog/source/_data/next.yml，然后algolia就不能用了，用以下办法解决 修改blog/_config.yml 把appId修改为applicationID hexo clean --config source/_data/next.yml &amp;&amp; hexo g --config source/_data/next.yml (15) 图片素材图片素材 图标库 (16) Forbidden to create by class ‘Counter’ permissions 403 Forbidden to create by class ‘Counter’ permissions 修改 Counter 和 Comment 类的权限 References[1] hexo-theme-next官方配置文档[2] next主题官方文档[3] HEXO-NEXT主题个性化配置[4] hexo的next主题个性化配置教程[5] hexo的next主题个性化配置教程[6] Hexo搭建博客教程[7] Unsplash Source | A Simple API for Embedding Free Photos from Unsplash[8] 个人博客网站接入来必力评论系统[9] Hexo去掉码市跳转页[10] UPDATE-FROM-5.1.X.md[11] ALGOLIA-SEARCH.md[12] hexo-algoliasearch-error[13] hexo-algoliasearch[14] hexo-algolia[15] can not find lib instantsearch.min.css[16] DATA-FILES.md[17] github hexo-leancloud-counter-security[18] hexo-leancloud-counter-security–Errors see plugin configuration[19] Leancloud访客统计插件重大安全漏洞修复指南[20] Hexo Next leancloud文章阅读次数配置以及插件无效问题解决[21] Valine 评论系统中的邮件提醒设置[22] 分享[23] theme-next-algolia-instant-search]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java线程池笔记]]></title>
    <url>%2F2017%2F10%2F23%2Fjava-thread-pool%2F</url>
    <content type="text"><![CDATA[在软件开发中，池一直都是一种非常优秀的设计思想，通过建立池可以有效的利用系统资源，节约系统性能。Java 中的线程池就是一种非常好的实现，从 JDK 1.5 开始 Java 提供了一个线程工厂 Executors 用来生成线程池，通过 Executors 可以方便的生成不同类型的线程池。但是要更好的理解使用线程池，就需要了解线程池的配置参数意义以及线程池的具体工作机制 (1) 使用线程池的好处 引用自http://ifeve.com/java-threadpool/ 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 (1.1) 线程池可以节约什么资源 多线程流行的原因是因为他能够处理与多进程一样的功能，并且创建线程耗费的时间、资源少，共享进程的资源。多线程有各自的线程ID，栈，PC，寄存器集合组成。共享代码段，文件，数据。 在实际使用中，每个请求创建新线程的服务器在创建和销毁线程上花费的时间和消耗的系统资源，甚至可能要比花在处理实际的用户请求的时间和资源要多得多。除了创建和销毁线程的开销之外，活动的线程也需要消耗系统资源。如果在一个JVM里创建太多的线程，可能会导致系统由于过度消耗内存或“切换过度”而导致系统资源不足。为了防止资源不足，服务器应用程序需要一些办法来限制任何给定时刻处理的请求数目，尽可能减少创建和销毁线程的次数，特别是一些资源耗费比较大的线程的创建和销毁，尽量利用已有对象来进行服务，这就是“池化资源”技术产生的原因。 进程是资源管理的最小单元；而线程是程序执行的最小单元。 (2) 线程池的使用(2.1) 使用12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152 /** * 参数初始化 */ private static final int CPU_COUNT = Runtime.getRuntime().availableProcessors(); /** * 核心线程数量大小 */ private static final int corePoolSize = Math.max(2, Math.min(CPU_COUNT - 1, 4)); /** * 线程池最大容纳线程数 */ private static final int maximumPoolSize = CPU_COUNT * 2 + 1; /** * 线程空闲后的存活时长 */ private static final int keepAliveTime = 30; /** * 任务过多后，存储任务的一个阻塞队列 */ BlockingQueue&lt;Runnable&gt; workQueue = new LinkedBlockingQueue&lt;&gt;(); /** * 线程的创建工厂 */ ThreadFactory threadFactory = new ThreadFactory() &#123; private final AtomicInteger mCount = new AtomicInteger(1); @Override public Thread newThread(Runnable r) &#123; return new Thread(r, "AdvacnedAsyncTask #" + mCount.getAndIncrement()); &#125; &#125;; /** * 线程池任务满载后采取的任务拒绝策略 */ RejectedExecutionHandler rejectHandler = new ThreadPoolExecutor.CallerRunsPolicy(); /** * 线程池对象，创建线程 */ ThreadPoolExecutor execute = new ThreadPoolExecutor( corePoolSize, maximumPoolSize, keepAliveTime, TimeUnit.SECONDS, workQueue, threadFactory, rejectHandler); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142import org.junit.Test;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.ArrayList;import java.util.List;import java.util.concurrent.*;/** * ThreadPoolTest * * @author: weikeqin.cn@gmail.com * @date: 2017-10-23 07:52 **/public class ThreadPoolTest &#123; private transient static final Logger log = LoggerFactory.getLogger(ThreadPoolTest.class); /** * */ @Test public void testExecute() &#123; log.info("开始"); ThreadPoolExecutor executor = new ThreadPoolExecutor(5, 10, 200, TimeUnit.MILLISECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(5)); for (int i = 0; i &lt; 15; i++) &#123; MyTask myTask = new MyTask(i); executor.execute(myTask); log.info("线程池中线程数目：&#123;&#125;，队列中等待执行的任务数目：&#123;&#125;，已执行玩别的任务数目：&#123;&#125;", executor.getPoolSize(), executor.getQueue().size(), executor.getCompletedTaskCount()); &#125; while (!executor.isTerminated()) &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; log.info("还有线程未执行完成"); &#125; executor.shutdown(); log.info("线程池关闭。"); log.info("执行完了。"); &#125; /** * Runnable任务 */ class MyTask implements Runnable &#123; private int taskNum; public MyTask(int num) &#123; this.taskNum = num; &#125; /** * */ @Override public void run() &#123; log.info("正在执行task " + taskNum); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; log.error(" &#123;&#125;", e); &#125; log.info("task " + taskNum + "执行完毕"); &#125; &#125; /** * */ @Test public void testSubmit() &#123; ExecutorService executorService = new ThreadPoolExecutor(5, 10, 200, TimeUnit.MILLISECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(5)); List&lt;Future&lt;String&gt;&gt; resultList = new ArrayList&lt;&gt;(); // 创建10个任务并执行 for (int i = 0; i &lt; 10; i++) &#123; // 使用ExecutorService执行Callable类型的任务，并将结果保存在future变量中 Future&lt;String&gt; future = executorService.submit(new TaskWithResult(i)); // 将任务执行结果存储到List中 resultList.add(future); &#125; // 遍历任务的结果 for (Future&lt;String&gt; fs : resultList) &#123; try &#123; // Future返回如果没有完成，则一直循环等待，直到Future返回完成 while (!fs.isDone()) &#123; TimeUnit.SECONDS.sleep(1); &#125; // 打印各个线程（任务）执行的结果 log.info("执行结果：&#123;&#125;", fs.get()); &#125; catch (InterruptedException e) &#123; log.error("", e); &#125; catch (ExecutionException e) &#123; log.error("", e); &#125; &#125; executorService.shutdown(); &#125; /** * Callable任务 */ class TaskWithResult implements Callable&lt;String&gt; &#123; private int id; public TaskWithResult(int id) &#123; this.id = id; &#125; /** * 任务的具体过程，一旦任务传给ExecutorService的submit方法， 则该方法自动在一个线程上执行 */ @Override public String call() throws Exception &#123; log.info("call()方法被自动调用。 当前线程名：&#123;&#125;", Thread.currentThread().getName()); // 该返回结果将被Future的get方法得到 return "call()方法被自动调用，任务返回的结果是：" + id + " " + Thread.currentThread().getName(); &#125; &#125; // end class TaskWithResult&#125; // end class (3) 源码解读 在Java中，线程池的概念是Executor这个接口，具体实现为ThreadPoolExecutor类，学习Java中的线程池，就可以直接学习ThreadPoolExecutor 123456789101112131415161718192021222324252627282930313233/** * Creates a new &#123;@code ThreadPoolExecutor&#125; with the given initial * parameters and default thread factory and rejected execution handler. * It may be more convenient to use one of the &#123;@link Executors&#125; factory * methods instead of this general purpose constructor. * * @param corePoolSize the number of threads to keep in the pool, even * if they are idle, unless &#123;@code allowCoreThreadTimeOut&#125; is set * @param maximumPoolSize the maximum number of threads to allow in the * pool * @param keepAliveTime when the number of threads is greater than * the core, this is the maximum time that excess idle threads * will wait for new tasks before terminating. * @param unit the time unit for the &#123;@code keepAliveTime&#125; argument * @param workQueue the queue to use for holding tasks before they are * executed. This queue will hold only the &#123;@code Runnable&#125; * tasks submitted by the &#123;@code execute&#125; method. * @throws IllegalArgumentException if one of the following holds:&lt;br&gt; * &#123;@code corePoolSize &lt; 0&#125;&lt;br&gt; * &#123;@code keepAliveTime &lt; 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt;= 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt; corePoolSize&#125; * @throws NullPointerException if &#123;@code workQueue&#125; is null */public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), defaultHandler);&#125; 12345678910111213141516171819202122232425262728293031323334/** * Creates a new &#123;@code ThreadPoolExecutor&#125; with the given initial * parameters and default rejected execution handler. * * @param corePoolSize the number of threads to keep in the pool, even * if they are idle, unless &#123;@code allowCoreThreadTimeOut&#125; is set * @param maximumPoolSize the maximum number of threads to allow in the * pool * @param keepAliveTime when the number of threads is greater than * the core, this is the maximum time that excess idle threads * will wait for new tasks before terminating. * @param unit the time unit for the &#123;@code keepAliveTime&#125; argument * @param workQueue the queue to use for holding tasks before they are * executed. This queue will hold only the &#123;@code Runnable&#125; * tasks submitted by the &#123;@code execute&#125; method. * @param threadFactory the factory to use when the executor * creates a new thread * @throws IllegalArgumentException if one of the following holds:&lt;br&gt; * &#123;@code corePoolSize &lt; 0&#125;&lt;br&gt; * &#123;@code keepAliveTime &lt; 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt;= 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt; corePoolSize&#125; * @throws NullPointerException if &#123;@code workQueue&#125; * or &#123;@code threadFactory&#125; is null */public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory, defaultHandler);&#125; 12345678910111213141516171819202122232425262728293031323334/** * Creates a new &#123;@code ThreadPoolExecutor&#125; with the given initial * parameters and default thread factory. * * @param corePoolSize the number of threads to keep in the pool, even * if they are idle, unless &#123;@code allowCoreThreadTimeOut&#125; is set * @param maximumPoolSize the maximum number of threads to allow in the * pool * @param keepAliveTime when the number of threads is greater than * the core, this is the maximum time that excess idle threads * will wait for new tasks before terminating. * @param unit the time unit for the &#123;@code keepAliveTime&#125; argument * @param workQueue the queue to use for holding tasks before they are * executed. This queue will hold only the &#123;@code Runnable&#125; * tasks submitted by the &#123;@code execute&#125; method. * @param handler the handler to use when execution is blocked * because the thread bounds and queue capacities are reached * @throws IllegalArgumentException if one of the following holds:&lt;br&gt; * &#123;@code corePoolSize &lt; 0&#125;&lt;br&gt; * &#123;@code keepAliveTime &lt; 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt;= 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt; corePoolSize&#125; * @throws NullPointerException if &#123;@code workQueue&#125; * or &#123;@code handler&#125; is null */public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler) &#123; this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, Executors.defaultThreadFactory(), handler);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * Creates a new &#123;@code ThreadPoolExecutor&#125; with the given initial * parameters. * * @param corePoolSize the number of threads to keep in the pool, even * if they are idle, unless &#123;@code allowCoreThreadTimeOut&#125; is set * @param maximumPoolSize the maximum number of threads to allow in the * pool * @param keepAliveTime when the number of threads is greater than * the core, this is the maximum time that excess idle threads * will wait for new tasks before terminating. * @param unit the time unit for the &#123;@code keepAliveTime&#125; argument * @param workQueue the queue to use for holding tasks before they are * executed. This queue will hold only the &#123;@code Runnable&#125; * tasks submitted by the &#123;@code execute&#125; method. * @param threadFactory the factory to use when the executor * creates a new thread * @param handler the handler to use when execution is blocked * because the thread bounds and queue capacities are reached * @throws IllegalArgumentException if one of the following holds:&lt;br&gt; * &#123;@code corePoolSize &lt; 0&#125;&lt;br&gt; * &#123;@code keepAliveTime &lt; 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt;= 0&#125;&lt;br&gt; * &#123;@code maximumPoolSize &lt; corePoolSize&#125; * @throws NullPointerException if &#123;@code workQueue&#125; * or &#123;@code threadFactory&#125; or &#123;@code handler&#125; is null */public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 线程池工作原则1、当线程池中线程数量小于 corePoolSize 则创建线程，并处理请求。2、当线程池中线程数量大于等于 corePoolSize 时，则把请求放入 workQueue 中,随着线程池中的核心线程们不断执行任务，只要线程池中有空闲的核心线程，线程池就从 workQueue 中取任务并处理。3 、当 workQueue 已存满，放不下新任务时则新建非核心线程入池，并处理请求直到线程数目达到 maximumPoolSize（最大线程数量设置值）。4、如果线程池中线程数大于 maximumPoolSize 则使用 RejectedExecutionHandler 来进行任务拒绝处理。任务队列 BlockingQueue任务队列 workQueue 是用于存放不能被及时处理掉的任务的一个队列，它是 一个 BlockingQueue 类型。 关于 BlockingQueue，虽然它是 Queue 的子接口，但是它的主要作用并不是容器，而是作为线程同步的工具，他有一个特征，当生产者试图向 BlockingQueue 放入(put)元素，如果队列已满，则该线程被阻塞；当消费者试图从 BlockingQueue 取出(take)元素，如果队列已空，则该线程被阻塞。(From 疯狂Java讲义) (3.1) 参数介绍corePoolSize 核心线程数 corePoolSize 线程池的核心线程数。 在没有设置 allowCoreThreadTimeOut 为 true 的情况下，核心线程会在线程池中一直存活，即使处于闲置状态。 maximumPoolSize 线程池所能容纳的最大线程数。 maximumPoolSize 线程池所能容纳的最大线程数。 当活动线程(核心线程+非核心线程)达到这个数值后，后续任务将会根据 RejectedExecutionHandler 来进行拒绝策略处理。 keepAliveTime 非核心线程闲置时的超时时长。 keepAliveTime 非核心线程闲置时的超时时长。 超过该时长，非核心线程就会被回收。若线程池通过 allowCoreThreadTimeOut() 方法设置 allowCoreThreadTimeOut 属性为 true，则该时长同样会作用于核心线程，AsyncTask 配置的线程池就是这样设置的。 keepAliveTime 时长对应的单位 keepAliveTime 时长对应的单位。 unit TimeUnit是一个枚举类型，其包括： NANOSECONDS ： 1纳秒 = 1微秒 / 1000 MICROSECONDS ： 1微秒 = 1毫秒 / 1000 MILLISECONDS ： 1毫秒 = 1秒 /1000 SECONDS ： 秒 MINUTES ： 分 HOURS ： 小时 DAYS ： 天 workQueue 线程池中的任务队列 workQueue 线程池中的任务队列，通过线程池的 execute() 方法提交的 Runnable 对象会存储在该队列中。当所有的核心线程都在干活时，新添加的任务会被添加到这个队列中等待处理，如果队列满了，则新建非核心线程执行任务。 常用的workQueue类型SynchronousQueue SynchronousQueue：这个队列接收到任务的时候，会直接提交给线程处理，而不保留它， 如果所有线程都在工作怎么办？ 那就新建一个线程来处理这个任务！ 所以为了保证不出现&lt;线程数达到了maximumPoolSize而不能新建线程&gt;的错误，使用这个类型队列的时候，maximumPoolSize一般指定成Integer.MAX_VALUE，即无限大 LinkedBlockingQueue LinkedBlockingQueue：这个队列接收到任务的时候，如果当前线程数小于核心线程数，则新建线程(核心线程)处理任务； 如果当前线程数等于核心线程数，则进入队列等待。 ArrayBlockingQueue ArrayBlockingQueue：可以限定队列的长度，接收到任务的时候， 如果没有达到corePoolSize的值，则新建线程(核心线程)执行任务， 如果达到了，则入队等候， 如果队列已满，则新建线程(非核心线程)执行任务， 如果总线程数到了maximumPoolSize，并且队列也满了，则发生错误 DelayQueue DelayQueue：队列内元素必须实现Delayed接口，这就意味着你传进去的任务必须先实现Delayed接口。这个队列接收到任务时，首先先入队，只有达到了指定的延时时间，才会执行任务 ThreadFactory 线程工厂 ThreadFactory 线程工厂，功能很简单，就是为线程池提供创建新线程的功能。这是一个接口，可以通过自定义，做一些自定义线程名的操作。 RejectedExecutionHandler 任务拒绝策略 RejectedExecutionHandler 当任务无法被执行时(超过线程最大容量 maximum 并且 workQueue 已经被排满了)的处理策略，这里有四种任务拒绝类型。 ThreadPoolExecutor.AbortPolicy 拒绝 ThreadPoolExecutor.AbortPolicy: 当线程池中的数量等于最大线程数时抛 java.util.concurrent.RejectedExecutionException 异常，涉及到该异常的任务也不会被执行，线程池默认的拒绝策略就是该策略。 ThreadPoolExecutor.DiscardPolicy() 默默丢弃 ThreadPoolExecutor.DiscardPolicy():当线程池中的数量等于最大线程数时,默默丢弃不能执行的新加任务，不报任何异常。 ThreadPoolExecutor.CallerRunsPolicy() 重试 ThreadPoolExecutor.CallerRunsPolicy():当线程池中的数量等于最大线程数时，重试添加当前的任务；它会自动重复调用execute()方法。 ThreadPoolExecutor.DiscardOldestPolicy() 抛弃最开始的任务 ThreadPoolExecutor.DiscardOldestPolicy(): 当线程池中的数量等于最大线程数时,抛弃线程池中工作队列头部的任务(即等待时间最久的任务)，并执行新传入的任务。 我们可以通过调用线程池的shutdown或shutdownNow方法来关闭线程池，但是它们的实现原理不同，shutdown的原理是只是将线程池的状态设置成SHUTDOWN状态，然后中断所有没有正在执行任务的线程。shutdownNow的原理是遍历线程池中的工作线程，然后逐个调用线程的interrupt方法来中断线程，所以无法响应中断的任务可能永远无法终止。shutdownNow会首先将线程池的状态设置成STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表。 只要调用了这两个关闭方法的其中一个，isShutdown方法就会返回true。当所有的任务都已关闭后,才表示线程池关闭成功，这时调用isTerminaed方法会返回true。至于我们应该调用哪一种方法来关闭线程池，应该由提交到线程池的任务特性决定，通常调用shutdown来关闭线程池，如果任务不一定要执行完，则可以调用shutdownNow。 References[1] 关于线程池的执行原则及配置参数详解[2] 线程池，这一篇或许就够了[3] 多线程 线程池ThreadPoolExecutor介绍[4] 聊聊并发（三）Java线程池的分析和使用[5] ThreadPoolExecutor里面4种拒绝策略（详细）[6] 聊聊并发（三）——JAVA线程池的分析和使用[7] Java并发编程：线程池的使用[8] Java四种线程池的使用[9] 并发新特性—Executor 框架与线程池[10] 线程池踩坑记 –load飙高的原因[11] 操作系统之线程[12] 为什么要使用线程池]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid 连接池 配置 (java无框架)]]></title>
    <url>%2F2017%2F10%2F16%2Fdruid-db-pool-config-java%2F</url>
    <content type="text"><![CDATA[为什么必须使用数据库连接池: 1.普通的JDBC数据库连接使用DriverManager来获取，每次向数据库建立连接时都要将Connection加载到内存中，再验证用户名和密码。需要数据库连接的时候，就向数据库要求一个，执行完后就断开连接，这样的方式会消耗大量的资源和时间，数据库的连接资源并没有得到很好的重复利用。若是同时有几百人甚至几千人在线，频繁地进行数据库连接操作，这将会占用很多的系统资源，严重的甚至会造成服务器的奔溃。 2.使用DriverManager方式获取JDBC数据库连接，每一次数据库连接，操作完会后都要断开，否则，如果程序出现异常而未能关闭，将会导致数据库系统内存泄漏，最终将导致重启数据库。 3.同时，这种开发不能控制被创建的连接对象数，系统资源会被毫无顾忌地分配出去，若果连接过多，也可能导致内存泄漏，服务器奔溃。 使用数据库连接池，可以避免以上几种问题。数据库连接池允许应用程序重复使用一个现有的数据库连接，而不是再重新建立一个；释放空闲空闲时间超过最大空闲时间的数据库连接来避免因为没有释放数据库连接而引起的数据库连接漏洞；连接池的数据库连接数受最大连接数和最小连接数控制，因此不会出现连接过多内存泄漏的情况。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596import java.io.IOException;import java.io.InputStream;import java.sql.Connection;import java.sql.SQLException;import java.util.Properties;import javax.sql.DataSource;import com.alibaba.druid.pool.DruidDataSourceFactory;/** * * @date 2017年6月28日 下午10:31:49 */public class DBPoolHelper &#123; /** 默认配置文件名 */ public static String confile = "druid.properties"; /** 配置文件 */ public static Properties p = null; /** 唯一dateSource，保证全局只有一个数据库连接池 */ public static DataSource dataSource = null; static &#123; p = new Properties(); InputStream inputStream = null; try &#123; // java应用 读取配置文件 inputStream = DBPoolHelper.class.getClassLoader().getResourceAsStream(confile); p.load(inputStream); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (inputStream != null) &#123; inputStream.close(); &#125; &#125; catch (IOException e) &#123; // ignore &#125; &#125; // end finally try &#123; //通过工厂类获取DataSource对象 dataSource = DruidDataSourceFactory.createDataSource(p); &#125; catch (Exception e) &#123; logger.error("获取连接异常 ", e); &#125; &#125; // end static /** * 获取连接 * * @return */ public static Connection getConnection() throws SQLException &#123; try &#123; return dataSource.getConnection(); &#125; catch (SQLException e) &#123; throw new SQLException("获取连接时异常", e); &#125; &#125; /** * 关闭连接 * * @param con * @date : 2017-10-16 10:08:10 */ public static void close(Connection con) throws SQLException &#123; try &#123; if (con != null) &#123; con.close(); &#125; &#125; catch (SQLException e) &#123; throw new SQLException("关闭连接时异常", e); &#125;finally &#123; try &#123; if (con != null) &#123; con.close(); &#125; &#125; catch (SQLException e) &#123; throw new SQLException("关闭连接时异常", e); &#125; &#125; &#125; // end method &#125; 12345678910111213141516171819202122232425262728293031323334353637383940driverClassName=com.mysql.jdbc.Driverurl=jdbc:mysql://127.0.0.1:3306/crawler?useUnicode=true&amp;characterEncoding=UTF-8&amp;autoReconnect=true&amp;useSSL=falseusername=adminpassword=admin# 配置参数，让ConfigFilter解密密码#connectionProperties=config.decrypt=true;config.decrypt.key=xxxx# 监控统计拦截的filtersfilters=stat# 初始化时建立物理连接的个数，初始化发生在显示调用init方法，或者第一次getConnection时initialSize=1# 最大连接池数量maxActive=10# 最小连接池数量minIdle:1# 获取连接等待超时的时间，单位毫秒maxWait=60000# 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒# 有两个含义：1) Destroy线程会检测连接的间隔时间 2) testWhileIdle的判断依据，详细看testWhileIdle属性的说明timeBetweenEvictionRunsMillis=60000# 一个连接在池中最小生存的时间，单位是毫秒minEvictableIdleTimeMillis=300000# 用来检测连接是否有效validationQuery=SELECT 1# 申请连接的时候检测，如果空闲时间大于timeBetweenEvictionRunsMillis，执行validationQuery检测连接是否有效testWhileIdle=true# 申请连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能testOnBorrow=false# 归还连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能testOnReturn=false# 是否缓存preparedStatement，也就是PSCachepoolPreparedStatements=truemaxPoolPreparedStatementPerConnectionSize=200 References[1] druid简单教程[2] 淘宝druid数据库连接池使用示例[3] Druid连接池配置(java无框架)[4] 使用数据库连接池获取JDBC数据库]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>pool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫 笔记]]></title>
    <url>%2F2017%2F10%2F12%2Fweb-crawler-notes%2F</url>
    <content type="text"><![CDATA[爬虫(Spider)，反爬虫(Anti-Spider)，反反爬虫(Anti-Anti-Spider)，这之间的斗争恢宏壮阔… Day 1 小莫想要某站上所有的电影，写了标准的爬虫(基于HttpClient库)，不断地遍历某站的电影列表页面，根据 Html 分析电影名字存进自己的数据库。这个站点的运维小黎发现某个时间段请求量陡增，分析日志发现都是 IP(1.1.1.1)这个用户，并且 useragent 还是 JavaClient1.6 ，基于这两点判断非人类后直接在Nginx 服务器上封杀。 Day 2 小莫电影只爬了一半，于是也针对性的变换了下策略：1. useragent 模仿百度(“Baiduspider…”)，2. IP每爬半个小时就换一个IP代理。小黎也发现了对应的变化，于是在 Nginx 上设置了一个频率限制，每分钟超过120次请求的再屏蔽IP。 同时考虑到百度家的爬虫有可能会被误伤，想想市场部门每月几十万的投放，于是写了个脚本，通过 hostname 检查下这个 ip 是不是真的百度家的，对这些 ip 设置一个白名单。 Day 3 小莫发现了新的限制后，想着我也不急着要这些数据，留给服务器慢慢爬吧，于是修改了代码，随机1-3秒爬一次，爬10次休息10秒，每天只在8-12，18-20点爬，隔几天还休息一下。小黎看着新的日志头都大了，再设定规则不小心会误伤真实用户，于是准备换了一个思路，当3个小时的总请求超过50次的时候弹出一个验证码弹框，没有准确正确输入的话就把 IP 记录进黑名单。 Day 4 小莫看到验证码有些傻脸了，不过也不是没有办法，先去学习了图像识别（关键词 PIL，tesseract），再对验证码进行了二值化，分词，模式训练之后，识别了小黎的验证码（关于验证码，验证码的识别，验证码的反识别也是一个恢弘壮丽的斗争史，这里先不展开….），之后爬虫又跑了起来。小黎是个不折不挠的好同学，看到验证码被攻破后，和开发同学商量了变化下开发模式，数据并不再直接渲染，而是由前端同学异步获取，并且通过 js 的加密库生成动态的 token，同时加密库再进行混淆（比较重要的步骤的确有网站这样做，参见微博的登陆流程）。 Day5混淆过的加密库就没有办法了么？当然不是，可以慢慢调试，找到加密原理，不过小莫不准备用这么耗时耗力的方法，他放弃了基于 HttpClient的爬虫，选择了内置浏览器引擎的爬虫(关键词：PhantomJS，Selenium)，在浏览器引擎中js 加密脚本算出了正确的结果，又一次拿到了对方的数据。小黎：….. 爬虫与发爬虫的斗争还在继续。不过实际应用时候，一般大家做到根据 IP 限制频次就结束了，除非很核心的数据，不会再进行更多的验证，毕竟工程的问题一半是成本的问题。至于高效部分，一些 Tips：1.尽量减少请求次数，能抓列表页就不抓详情页2.不要只看 Web 网站，还有 App 和 H5，他们的反爬虫措施一般比较少3.如果真的对性能要求很高，可以考虑多线程(一些成熟的框架如 scrapy都已支持)，甚至分布式 本文来自携程酒店研发部研发经理崔广宇在第三期【携程技术微分享】上的分享 你被爬虫侵扰过么？当你看到“爬虫”两个字的时候，是不是已经有点血脉贲张的感觉了？千万要忍耐，稍稍做点什么，就可以在名义上让他们胜利，实际上让他们受损失。 一、为什么要反爬虫1、爬虫占总PV比例较高，这样浪费钱（尤其是三月份爬虫）。三月份爬虫是个什么概念呢？每年的三月份我们会迎接一次爬虫高峰期。 最初我们百思不得其解。直到有一次，四月份的时候，我们删除了一个url，然后有个爬虫不断的爬取url，导致大量报错，测试开始找我们麻烦。我们只好特意为这个爬虫发布了一次站点，把删除的url又恢复回去了。 但是当时我们的一个组员表示很不服，说，我们不能干掉爬虫，也就罢了，还要专门为它发布，这实在是太没面子了。于是出了个主意，说：url可以上，但是，绝对不给真实数据。 于是我们就把一个静态文件发布上去了。报错停止了，爬虫没有停止，也就是说对方并不知道东西都是假的。这个事情给了我们一个很大的启示，也直接成了我们反爬虫技术的核心：变更。 后来有个学生来申请实习。我们看了简历发现她爬过携程。后来面试的时候确认了下，果然她就是四月份害我们发布的那个家伙。不过因为是个妹子，技术也不错，后来就被我们招安了。现在已经快正式入职了。 后来我们一起讨论的时候，她提到了，有大量的硕士在写论文的时候会选择爬取OTA数据，并进行舆情分析。因为五月份交论文，所以嘛，大家都是读过书的，你们懂的，前期各种DotA，LOL，到了三月份了，来不及了，赶紧抓数据，四月份分析一下，五月份交论文。就是这么个节奏。 2、公司可免费查询的资源被批量抓走，丧失竞争力，这样少赚钱。OTA的价格可以在非登录状态下直接被查询，这个是底线。如果强制登陆，那么可以通过封杀账号的方式让对方付出代价，这也是很多网站的做法。但是我们不能强制对方登录。那么如果没有反爬虫，对方就可以批量复制我们的信息，我们的竞争力就会大大减少。 竞争对手可以抓到我们的价格，时间长了用户就会知道，只需要去竞争对手那里就可以了，没必要来携程。这对我们是不利的。 3、爬虫是否涉嫌违法？ 如果是的话，是否可以起诉要求赔偿？这样可以赚钱。这个问题我特意咨询了法务，最后发现这在国内还是个擦边球，就是有可能可以起诉成功，也可能完全无效。所以还是需要用技术手段来做最后的保障。 二、反什么样的爬虫1、十分低级的应届毕业生开头我们提到的三月份爬虫，就是一个十分明显的例子。应届毕业生的爬虫通常简单粗暴，根本不管服务器压力，加上人数不可预测，很容易把站点弄挂。 顺便说下，通过爬携程来获取offer这条路已经行不通了。因为我们都知道，第一个说漂亮女人像花的人，是天才。而第二个。。。你们懂的吧？ 2、十分低级的创业小公司现在的创业公司越来越多，也不知道是被谁忽悠的然后大家创业了发现不知道干什么好，觉得大数据比较热，就开始做大数据。 分析程序全写差不多了，发现自己手头没有数据。 怎么办？写爬虫爬啊。于是就有了不计其数的小爬虫，出于公司生死存亡的考虑，不断爬取数据。 3、不小心写错了没人去停止的失控小爬虫携程上的点评有的时候可能高达60%的访问量是爬虫。我们已经选择直接封锁了，它们依然孜孜不倦地爬取。 什么意思呢？就是说，他们根本爬不到任何数据，除了http code是200以外，一切都是不对的，可是爬虫依然不停止这个很可能就是一些托管在某些服务器上的小爬虫，已经无人认领了，依然在辛勤地工作着。 4、成型的商业对手这个是最大的对手，他们有技术，有钱，要什么有什么，如果和你死磕，你就只能硬着头皮和他死磕。 5、抽风的搜索引擎大家不要以为搜索引擎都是好人，他们也有抽风的时候，而且一抽风就会导致服务器性能下降，请求量跟网络攻击没什么区别。 三、什么是爬虫和反爬虫因为反爬虫暂时是个较新的领域，因此有些定义要自己下。我们内部定义是这样的： 爬虫：使用任何技术手段，批量获取网站信息的一种方式。关键在于批量。反爬虫：使用任何技术手段，阻止别人批量获取自己网站信息的一种方式。关键也在于批量。误伤：在反爬虫的过程中，错误的将普通用户识别为爬虫。误伤率高的反爬虫策略，效果再好也不能用。拦截：成功地阻止爬虫访问。这里会有拦截率的概念。通常来说，拦截率越高的反爬虫策略，误伤的可能性就越高。因此需要做个权衡。资源：机器成本与人力成本的总和。这里要切记，人力成本也是资源，而且比机器更重要。因为，根据摩尔定律，机器越来越便宜。而根据IT行业的发展趋势，程序员工资越来越贵。因此，让对方加班才是王道，机器成本并不是特别值钱。 四、知己知彼：如何编写简单爬虫要想做反爬虫，我们首先需要知道如何写个简单的爬虫。 目前网络上搜索到的爬虫资料十分有限，通常都只是给一段python代码。python是一门很好的语言，但是用来针对有反爬虫措施的站点做爬虫，真的不是最优选择。 更讽刺的是，通常搜到的python爬虫代码都会使用一个lynx的user-agent。你们应该怎么处理这个user-agent，就不用我来说了吧？ 通常编写爬虫需要经过这么几个过程： 分析页面请求格式创建合适的http请求批量发送http请求，获取数据举个例子，直接查看携程生产url。在详情页点击“确定”按钮，会加载价格。假设价格是你想要的，那么抓出网络请求之后，哪个请求才是你想要的结果呢？ 答案出乎意料的简单，你只需要用根据网络传输数据量进行倒序排列即可。因为其他的迷惑性的url再多再复杂，开发人员也不会舍得加数据量给他。 五、知己知彼：如何编写高级爬虫那么爬虫进阶应该如何做呢？通常所谓的进阶有以下几种： 分布式通常会有一些教材告诉你，为了爬取效率，需要把爬虫分布式部署到多台机器上。这完全是骗人的。分布式唯一的作用是：防止对方封IP。封IP是终极手段，效果非常好，当然，误伤起用户也是非常爽的。 模拟JavaScript有些教程会说，模拟javascript，抓取动态网页，是进阶技巧。但是其实这只是个很简单的功能。因为，如果对方没有反爬虫，你完全可以直接抓ajax本身，而无需关心js怎么处理的。如果对方有反爬虫，那么javascript必然十分复杂，重点在于分析，而不仅仅是简单的模拟。 换句话说：这应该是基本功。 PhantomJs这个是一个极端的例子。这个东西本意是用来做自动测试的，结果因为效果很好，很多人拿来做爬虫。但是这个东西有个硬伤，就是：效率。此外PhantomJs也是可以被抓到的，出于多方面原因，这里暂时不讲。 六、不同级别爬虫的优缺点越是低级的爬虫，越容易被封锁，但是性能好，成本低。越是高级的爬虫，越难被封锁，但是性能低，成本也越高。 当成本高到一定程度，我们就可以无需再对爬虫进行封锁。经济学上有个词叫边际效应。付出成本高到一定程度，收益就不是很多了。 那么如果对双方资源进行对比，我们就会发现，无条件跟对方死磕，是不划算的。应该有个黄金点，超过这个点，那就让它爬好了。毕竟我们反爬虫不是为了面子，而是为了商业因素。 七、如何设计一个反爬虫系统(常规架构)有个朋友曾经给过我这样一个架构： 1、对请求进行预处理，便于识别；2、识别是否是爬虫；3、针对识别结果，进行适当的处理； 当时我觉得，听起来似乎很有道理，不愧是架构，想法就是和我们不一样。后来我们真正做起来反应过来不对了。因为： 如果能识别出爬虫，哪还有那么多废话？想怎么搞它就怎么搞它。如果识别不出来爬虫，你对谁做适当处理？ 三句话里面有两句是废话，只有一句有用的，而且还没给出具体实施方式。那么：这种架构(师)有什么用？ 因为当前存在一个架构师崇拜问题，所以很多创业小公司以架构师名义招开发。给出的title都是：初级架构师，架构师本身就是个高级岗位，为什么会有初级架构。这就相当于：初级将军/初级司令。 最后去了公司，发现十个人，一个CTO，九个架构师，而且可能你自己是初级架构师，其他人还是高级架构师。不过初级架构师还不算坑爹了，有些小创业公司还招CTO做开发呢。 传统反爬虫手段1、后台对访问进行统计，如果单个IP访问超过阈值，予以封锁。 这个虽然效果还不错，但是其实有两个缺陷，一个是非常容易误伤普通用户，另一个就是，IP其实不值钱，几十块钱甚至有可能买到几十万个IP。所以总体来说是比较亏的。不过针对三月份呢爬虫，这点还是非常有用的。 2、后台对访问进行统计，如果单个session访问超过阈值，予以封锁。 这个看起来更高级了一些，但是其实效果更差，因为session完全不值钱，重新申请一个就可以了。 3、后台对访问进行统计，如果单个userAgent访问超过阈值，予以封锁。 这个是大招，类似于抗生素之类的，效果出奇的好，但是杀伤力过大，误伤非常严重，使用的时候要非常小心。至今为止我们也就只短暂封杀过mac下的火狐。 4、以上的组合 组合起来能力变大，误伤率下降，在遇到低级爬虫的时候，还是比较好用的。 由以上我们可以看出，其实爬虫反爬虫是个游戏，RMB玩家才最牛逼。因为上面提到的方法，效果均一般，所以还是用JavaScript比较靠谱。 也许有人会说：javascript做的话，不是可以跳掉前端逻辑，直接拉服务吗？怎么会靠谱呢？因为啊，我是一个标题党啊。JavaScript不仅仅是做前端。跳过前端不等于跳过JavaScript。也就是说：我们的服务器是nodejs做的。 思考题：我们写代码的时候，最怕碰到什么代码？什么代码不好调试？eval eval已经臭名昭著了，它效率低下，可读性糟糕。正是我们所需要的。 goto js对goto支持并不好，因此需要自己实现goto。 混淆 目前的minify工具通常是minify成abcd之类简单的名字，这不符合我们的要求。我们可以minify成更好用的，比如阿拉伯语。为什么呢？ 因为阿拉伯语有的时候是从左向右写，有的时候是从右向左写，还有的时候是从下向上写。除非对方雇个阿拉伯程序员，否则非头疼死不可。 不稳定代码 什么bug不容易修？不容易重现的bug不好修。因此，我们的代码要充满不确定性，每次都不一样。 代码演示 下载代码本身，可以更容易理解。这里简短介绍下思路： 纯JAVASCRIPT反爬虫DEMO，通过更改连接地址，来让对方抓取到错误价格。这种方法简单，但是如果对方针对性的来查看，十分容易被发现。纯JAVASCRIPT反爬虫DEMO，更改key。这种做法简单，不容易被发现。但是可以通过有意爬取错误价格的方式来实现。纯JAVASCRIPT反爬虫DEMO，更改动态key。这种方法可以让更改key的代价变为0，因此代价更低。纯JAVASCRIPT反爬虫DEMO，十分复杂的更改key。这种方法，可以让对方很难分析，如果加了后续提到的浏览器检测，更难被爬取。到此为止。 前面我们提到了边际效应，就是说，可以到此为止了。后续再投入人力就得不偿失了。除非有专门的对手与你死磕。不过这个时候就是为了尊严而战，不是为了商业因素了。 浏览器检测 针对不同的浏览器，我们的检测方式是不一样的。 IE 检测bug；FF 检测对标准的严格程度；Chrome 检测强大特性。 八、我抓到你了——然后该怎么办不会引发生产事件——直接拦截 可能引发生产事件——给假数据(也叫投毒) 此外还有一些发散性的思路。例如是不是可以在响应里做SQL注入？毕竟是对方先动的手。不过这个问题法务没有给具体回复，也不容易和她解释。因此暂时只是设想而已。 1、技术压制 我们都知道，DotA AI里有个de命令，当AI被击杀后，它获取经验的倍数会提升。因此，前期杀AI太多，AI会一身神装，无法击杀。 正确的做法是，压制对方等级，但是不击杀。反爬虫也是一样的，不要一开始就搞太过分，逼人家和你死磕。 2、心理战 挑衅、怜悯、嘲讽、猥琐。 以上略过不提，大家领会精神即可。 3、放水 这个可能是是最高境界了。 程序员都不容易，做爬虫的尤其不容易。可怜可怜他们给他们一小口饭吃吧。没准过几天你就因为反爬虫做得好，改行做爬虫了。 比如，前一阵子就有人找我问我会不会做爬虫。。。。。我这么善良的人，能说不会吗？？？？ References[1] https://segmentfault.com/a/1190000012293292 如果有人问你爬虫抓取技术的门道，请叫他来看这篇文章[2] https://segmentfault.com/a/1190000005840672 关于反爬虫，看这一篇就够了[3] http://techshow.ctrip.com/archives/2213.html 那些你不知道的爬虫反爬虫套路[4] http://dp.imysql.com:8080/2013/05/13/nginx-limit-req-to-limit-spider.html 修改配置nginx，限制无良爬虫频率[5] https://www.bbsmax.com/A/LPdoG2jd3a/ 基于webmagic的爬虫项目经验小结[6] http://www.oschina.net/project/tag/64/spider?lang=0&amp;os=0&amp;sort=view&amp; 网络爬虫[7] https://zhuanlan.zhihu.com/p/22097627 几篇入门级的爬虫相关文章[8] https://zhuanlan.zhihu.com/p/20334680 一看就明白的爬虫入门讲解-基础理论篇（上篇）[9] https://zhuanlan.zhihu.com/p/20336750?refer=zhugeio 一看就明白的爬虫入门讲解-基础理论篇（下篇）[10] http://www.bijishequ.com/detail/535159?p= 爬虫与反爬虫的博弈[11] https://www.zhihu.com/question/28168585[12] https://zhuanlan.zhihu.com/p/20410446[13] https://zhuanlan.zhihu.com/p/20413379[14] https://zhuanlan.zhihu.com/p/20413828[15] https://www.liaoxuefeng.com/article/001509844125769eafbb65df0a04430a2d010a24a945bfa000 使用Nginx过滤网络爬虫]]></content>
      <categories>
        <category>crawler</category>
      </categories>
      <tags>
        <tag>crawler</tag>
        <tag>spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取 携程]]></title>
    <url>%2F2017%2F09%2F21%2Fcrawler-xiecheng%2F</url>
    <content type="text"><![CDATA[主页面urlhttp://flights.ctrip.com/booking/BJS-KMG-day-1.html?DDate1=2017-09-22 获取数据urlhttp://flights.ctrip.com/domesticsearch/search/SearchFirstRouteFlights?DCity1=BJS&amp;ACity1=KMG&amp;SearchType=S&amp;DDate1=2017-09-22&amp;IsNearAirportRecommond=0&amp;LogToken=c802fde883e64a3c8fba0e99e45d023c&amp;rk=2.7090996922689414192821&amp;CK=1D7795E4EB96B7578AA673D6EF346C33&amp;r=0.2337708871934405872019 首次搜索 http://flights.ctrip.com/domestic/Search/FirstRoute/?ddate1=2017-09-22&amp;ddate2=2017-09-22&amp;dcity1=BJS&amp;acity1=KMG http://flights.ctrip.com/domestic/booking/BJS-KMG---D-adu-1/?dayoffset=1&amp;ddate1=2017-09-22&amp;ddate2=2017-09-22重新搜索 http://flights.ctrip.com/booking/bjs-kmg---d-adu-1/?ddate1=2017-09-22&amp;ddate2=2017-09-22 往返 http://flights.ctrip.com/booking/BJS-KMG-day-1.html?DDate1=2017-09-22 单程 往返 var url = &quot;//flights.ctrip.com/domesticsearch/search/SearchRoundRecommend?DCity1=BJS&amp;ACity1=KMG&amp;SearchType=D&amp;DDate1=2017-09-22&amp;ACity2=BJS&amp;DDate2=2017-09-22&amp;IsNearAirportRecommond=0&amp;LogToken=3c10647ccad04412a33567f25335875a&amp;CK=97017DBB47A37AFE6BE4D99105F78AFE&quot;; 单程 var url = &quot;//flights.ctrip.com/domesticsearch/search/SearchFirstRouteFlights?DCity1=BJS&amp;ACity1=KMG&amp;SearchType=S&amp;DDate1=2017-09-22&amp;IsNearAirportRecommond=0&amp;LogToken=66c9c383c12f4dc685a9ad3e5834a7c4&amp;CK=D6BF57FFFC0A4E89AF51CF7186A1944E&quot;; 根据直接获取json的url拿到数据解析后就能获得航班信息，这个url里有4个参数，需要从搜索主页的页面里获取单程和往返的原理一样，就是往返多了一个返回日期，其他都一样 LogToken不变 r不变 CK有变化 rk用Math.random LogToken rk CK r 在主页面的第34行 LogToken和CK根据主页面24行的url可以得到，r值在主页面34行可以得到 获取的json数据的url，爬取数据用的url 单程 http://flights.ctrip.com/domesticsearch/search/SearchFirstRouteFlights?DCity1=BJS&amp;ACity1=KMG&amp;SearchType=S&amp;DDate1=2017-09-22&amp;IsNearAirportRecommond=0&amp;LogToken=c96a69a7ca9142be9f0e3192f44f9f07&amp;rk=1.445840834132689163412&amp;CK=6A53EEBD27EBBE6785AF7951EC03EB1E&amp;r=0.3795051054613884121115 往返 http://flights.ctrip.com/domesticsearch/search/SearchFirstRouteFlights?DCity1=BJS&amp;ACity1=KMG&amp;SearchType=D&amp;DDate1=2017-09-22&amp;ACity2=BJS&amp;DDate2=2017-09-22&amp;IsNearAirportRecommond=0&amp;LogToken=584465ec999045db9a546e1ce6a04a3f&amp;rk=4.057381591787214165342&amp;CK=D76BF5FFFC0A4E89AF51CF7186A1944E&amp;r=0.543402675163338674414 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv="Cache-Control" content="no-transform " /&gt; &lt;meta http-equiv="Content-Type" content="text/html; charset=gb2312" /&gt;&lt;meta name="description" content="携程旅行网为您提供北京到昆明特价机票以及北京到昆明航班查询。携程旅行网于2003年在美国纳斯达克上市，拥有覆盖全国的服务网络。提供国内国际各大航空公司的航线航班，安全支付值得信赖，是您网上订购北京到昆明机票的首选。免费咨询800-820-6666。" /&gt;&lt;meta name="keywords" content="北京到昆明特价，北京到昆明机票预订，北京到昆明航班查询，携程机票" /&gt;&lt;title&gt;北京到昆明机票预订 - 北京到昆明特价机票 - 北京到昆明航班查询预订 - 携程国内机票预订&lt;/title&gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" /&gt; &lt;link rel="dns-prefetch" href="//webresource.c-ctrip.com" /&gt; &lt;link rel="dns-prefetch" href="//pic.c-ctrip.com" /&gt; &lt;link rel="dns-prefetch" href="//images3.c-ctrip.com" /&gt; &lt;link rel="dns-prefetch" href="//crm.ws.ctrip.com" /&gt; &lt;link rel="dns-prefetch" href="//s.c-ctrip.com" /&gt; &lt;link rel="dns-prefetch" href="//www.google-analytics.com" /&gt; &lt;link rel="canonical" href="//flights.ctrip.com/domestic/booking/BJS-KMG-day-1.html"/&gt; &lt;link href="//webresource.c-ctrip.com/ResFlightOnline/R2/Booking/css/fltdomestic111027/searchresult_v2.1.css?ReleaseNo=CR_2017_09_20_21_00_00" type="text/css" rel="stylesheet" /&gt;&lt;/head&gt;&lt;body class="gray_body"&gt; &lt;script type="text/javascript"&gt; var url = "//flights.ctrip.com/domesticsearch/search/SearchFirstRouteFlights?DCity1=BJS&amp;ACity1=KMG&amp;SearchType=S&amp;DDate1=2017-09-22&amp;IsNearAirportRecommond=0&amp;LogToken=c802fde883e64a3c8fba0e99e45d023c&amp;CK=D7795E14EB96B7578AA673D6EF346C33"; var _searchCount_c = 0; function ajaxRequest(n,t)&#123;var i=null,e,f,l,o,s,r,c,u,h;if(typeof XMLHttpRequest!="undefined")i=new XMLHttpRequest;else if(typeof ActiveXObject!="undefined")&#123;if(typeof arguments.callee.aciveXString!="string")for(e=["MSXML2.XMLHttp.6.0","MSXML2.XMLHttp.3.0","MSXML2.XMLHttp"],f=0,l=e.length;f&lt;l;f++)try&#123;i=new ActiveXObject(e[f]);arguments.callee.activeXString=e[f];break&#125;catch(a)&#123;&#125;i==null&amp;&amp;(i=new ActiveXObject(arguments.callee.activeXString))&#125;i.onreadystatechange=function()&#123;try&#123;if(i.readyState==4)if(i.status&gt;=200&amp;&amp;i.status&lt;300||i.status==304)&#123;var r=eval("("+i.responseText+")");if(_searchCount_c==0&amp;&amp;r&amp;&amp;r.Error&amp;&amp;(r.Error.Code==104||r.Error.Code==1004)&amp;&amp;(r.Error.Message==""||!r.Error.Message))&#123;_searchCount_c++;setTimeout(function()&#123;var i=n.split("&amp;");i.pop();ajaxRequest(i.join("&amp;")+"&amp;rt="+Math.random()*1e3,t)&#125;,1e3);return&#125;jsonCallback.done(r)&#125;else i.status!=0&amp;&amp;jsonCallback.onError()&#125;catch(u)&#123;jsonCallback.onError()&#125;&#125;;window.location.hash&amp;&amp;(o=window.location.hash.match(/DDate1=\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;/),o&amp;&amp;o.length&gt;0&amp;&amp;(n=n.replace(/DDate1=(\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;)/ig,o[0])),s=window.location.hash.match(/DDate2=\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;/),s&amp;&amp;s.length&gt;0&amp;&amp;(n=n.replace(/DDate2=(\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;)/ig,s[0])));r=n.replace(/^[\s\xA0]+|[\s\xA0]+$/g,"");(r.indexOf("ClassType=CF")==-1||r.indexOf("ClassType=&amp;")!=-1)&amp;&amp;(r+=getStorage("FD_SearchPage_onlyCf")=="CF"?"&amp;ClassType=CF":"");_searchCount_c&gt;0&amp;&amp;(c=t.split(".")[1],t="0."+c.substring(1,c.length-1));u=r.split("&amp;");h=r.indexOf("rk=")&gt;=0||r.indexOf("rt=")&gt;=0?u.splice(u.length-2,1)[0]:u.pop();u.push("CK=");h=h.split("=")[1];var fn=(function(u,r,k,t)&#123;var Z21=1,M21Z=1;Z21=Z21+=parseInt(Math.cos(7) * 0xa);Z21=Z21-=parseInt(Math.tan(7) * 0xa);if(Z21&lt;0)Z21=-Z21; while(Z21&gt;30)Z21=Z21%10;M21Z=M21Z+=parseInt(Math.sin(6) * 0xa);M21Z=M21Z+=parseInt(Math.cos(6) * 0xa);M21Z=M21Z*=parseInt(Math.log(6) * 0xa);if(M21Z&lt;0)M21Z=-M21Z; while(M21Z&gt;30)M21Z=M21Z%10;(function(r,u,x,y,t,k)&#123;if(!window.location.href)&#123;return;&#125;var l=r.split(''); var c=l.splice(y,1);l.splice(x,0,c);t.open('GET', u.join('&amp;')+l.join('') + '&amp;r=' + k, !0);t.send(null);&#125;)(r,u,Z21,M21Z,t,k)&#125;);fn(u,h,t,i)&#125;var jsonCallback=&#123;isError:!1,isReady:!1,data:&#123;&#125;,readyList:[],errorList:[],ready:function(n)&#123;this.isReady==!1?this.readyList.push(n):n(this.data)&#125;,done:function(n)&#123;this.isReady=!0;this.data=n;for(var t=0;this.readyList[t];)this.readyList[t](n),t++&#125;,error:function(n)&#123;this.isError==!1?this.errorList.push(n):n()&#125;,onError:function()&#123;this.isError=!0;for(var n=0;this.errorList[n];)this.errorList[n](),n++&#125;&#125;,getStorage=function(n)&#123;var i,r,t;try&#123;if(i="&#123;&#125;",window.localStorage)i=localStorage.getItem("jStorage");else if(window.globalStorage)i=window.globalStorage[window.location.hostname];else&#123;r=document.head||document.getElementsByTagName("head")[0];t=document.createElement("link");t.style.behavior="url(#default#userData)";r.appendChild(t);try&#123;t.load("jStorage")&#125;catch(u)&#123;t.setAttribute("jStorage","&#123;&#125;");t.save("jStorage");t.load("jStorage")&#125;i=t.getAttribute("jStorage")||"&#123;&#125;";r.removeChild(t)&#125;return!i||i=="&#123;&#125;"?"":eval("("+i+")")[n]&#125;catch(f)&#123;return""&#125;&#125; var searchRouteIndex = "0"; var isCivil = false; var roundTripCombinationSwitch = false; ajaxRequest(url + '&amp;rk=' + Math.random()*10+'192821','0.2337708871934405872019');&lt;/script&gt;&lt;div&gt; 中间省略&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107function ajaxRequest(n, t) &#123; var i = null, e, f, l, o, s, r, c, u, h; if (typeof XMLHttpRequest != "undefined") i = new XMLHttpRequest; else if (typeof ActiveXObject != "undefined") &#123; if (typeof arguments.callee.aciveXString != "string") for (e = ["MSXML2.XMLHttp.6.0", "MSXML2.XMLHttp.3.0", "MSXML2.XMLHttp"], f = 0, l = e.length; f &lt; l; f++) try &#123; i = new ActiveXObject(e[f]); arguments.callee.activeXString = e[f]; break &#125; catch (a) &#123; &#125; i == null &amp;&amp; (i = new ActiveXObject(arguments.callee.activeXString)) &#125; i.onreadystatechange = function () &#123; try &#123; if (i.readyState == 4) if (i.status &gt;= 200 &amp;&amp; i.status &lt; 300 || i.status == 304) &#123; var r = eval("(" + i.responseText + ")"); if (_searchCount_c == 0 &amp;&amp; r &amp;&amp; r.Error &amp;&amp; (r.Error.Code == 104 || r.Error.Code == 1004) &amp;&amp; (r.Error.Message == "" || !r.Error.Message)) &#123; _searchCount_c++; setTimeout(function () &#123; var i = n.split("&amp;"); i.pop(); ajaxRequest(i.join("&amp;") + "&amp;rt=" + Math.random() * 1e3, t) &#125;, 1e3); return &#125; jsonCallback.done(r) &#125; else i.status != 0 &amp;&amp; jsonCallback.onError() &#125; catch (u) &#123; jsonCallback.onError() &#125; &#125;; window.location.hash &amp;&amp; (o = window.location.hash.match(/DDate1=\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;/), o &amp;&amp; o.length &gt; 0 &amp;&amp; (n = n.replace(/DDate1=(\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;)/ig, o[0])), s = window.location.hash.match(/DDate2=\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;/), s &amp;&amp; s.length &gt; 0 &amp;&amp; (n = n.replace(/DDate2=(\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;)/ig, s[0]))); r = n.replace(/^[\s\xA0]+|[\s\xA0]+$/g, ""); (r.indexOf("ClassType=CF") == -1 || r.indexOf("ClassType=&amp;") != -1) &amp;&amp; (r += getStorage("FD_SearchPage_onlyCf") == "CF" ? "&amp;ClassType=CF" : ""); _searchCount_c &gt; 0 &amp;&amp; (c = t.split(".")[1], t = "0." + c.substring(1, c.length - 1)); u = r.split("&amp;"); h = r.indexOf("rk=") &gt;= 0 || r.indexOf("rt=") &gt;= 0 ? u.splice(u.length - 2, 1)[0] : u.pop(); u.push("CK="); h = h.split("=")[1]; var fn = (function (u, r, k, t) &#123; var Z21 = 1, M21Z = 1; Z21 = Z21 += parseInt(Math.cos(7) * 0xa); Z21 = Z21 -= parseInt(Math.tan(7) * 0xa); if (Z21 &lt; 0) Z21 = -Z21; while (Z21 &gt; 30) Z21 = Z21 % 10; M21Z = M21Z += parseInt(Math.sin(6) * 0xa); M21Z = M21Z += parseInt(Math.cos(6) * 0xa); M21Z = M21Z *= parseInt(Math.log(6) * 0xa); if (M21Z &lt; 0) M21Z = -M21Z; while (M21Z &gt; 30) M21Z = M21Z % 10; (function (r, u, x, y, t, k) &#123; if (!window.location.href) &#123; return; &#125; var l = r.split(''); var c = l.splice(y, 1); l.splice(x, 0, c); t.open('GET', u.join('&amp;') + l.join('') + '&amp;r=' + k, !0); t.send(null); &#125;)(r, u, Z21, M21Z, t, k) &#125;); fn(u, h, t, i)&#125;var jsonCallback = &#123; isError: !1, isReady: !1, data: &#123;&#125;, readyList: [], errorList: [], ready: function (n) &#123; this.isReady == !1 ? this.readyList.push(n) : n(this.data) &#125;, done: function (n) &#123; this.isReady = !0; this.data = n; for (var t = 0; this.readyList[t];) this.readyList[t](n), t++ &#125;, error: function (n) &#123; this.isError == !1 ? this.errorList.push(n) : n() &#125;, onError: function () &#123; this.isError = !0; for (var n = 0; this.errorList[n];) this.errorList[n](), n++ &#125;&#125;, getStorage = function (n) &#123; var i, r, t; try &#123; if (i = "&#123;&#125;", window.localStorage) i = localStorage.getItem("jStorage"); else if (window.globalStorage) i = window.globalStorage[window.location.hostname]; else &#123; r = document.head || document.getElementsByTagName("head")[0]; t = document.createElement("link"); t.style.behavior = "url(#default#userData)"; r.appendChild(t); try &#123; t.load("jStorage") &#125; catch (u) &#123; t.setAttribute("jStorage", "&#123;&#125;"); t.save("jStorage"); t.load("jStorage") &#125; i = t.getAttribute("jStorage") || "&#123;&#125;"; r.removeChild(t) &#125; return !i || i == "&#123;&#125;" ? "" : eval("(" + i + ")")[n] &#125; catch (f) &#123; return "" &#125;&#125; 数据解析1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253├─ als 航空公司 &#123;MU: &quot;东方航空&quot;, ZH: &quot;深圳航空&quot;, CA: &quot;中国国航&quot;&#125;├─ apb 机场 &#123;TYN12: &quot;武宿国际机场T2&quot;, PEK2: &quot;首都国际机场T2&quot;, YCU1169: &quot;关公机场&quot;, PEK3: &quot;首都国际机场T3&quot;, NKG987: &quot;禄口国际机场T2&quot;&#125;├─ fis ├─ acc 到达地 YCU ├─ acn 目的地 运城 ├─ alc CA 航空公司 ├─ apbn 机场名 关公机场 ├─ apc YCU ├─ asmsn 航站楼 ├─ at 到达时间 2017-10-20 13:50:00 ├─ ├─ confort 里面有历史准点率 ├─ ArrivedBridge ├─ BoardingWay ├─ DepartBridge ├─ HistoryPunctuality ├─ HistoryPunctualityArr ├─ SubClassList ├─ ├─ ├─ dcc 出发地 BJS ├─ dcn 出发地 北京 ├─ dpbn 出发机场 首都国际机场T3 ├─ dpc PEK ├─ dsmsn 航站楼 ├─ dt 2017-10-20 11:55:00 ├─ fn 航班号 CA1237 ├─ lcfp 票价 ├─ lp 实际票价 ├─ pr 准点率 96.6100006103516 ├─ rt 折扣 ├─ sdft ├─ tax 税 ├─ lps 半年内 每天的机票价格├─ tf 中转组合 ├─ Routes ├─ 0 ├─ fis ├─ ├─ tcn ├─ at ├─ c ├─ cn ├─ desc ├─ dt ├─ fcrtp ├─ fut ├─ icp ├─ ics ├─ imre ├─ ire ├─ k ├─ References[1] 用Python抓取携程网机票信息 过程纪实（上篇）[2] 用Python抓取携程网机票信息 过程纪实（下篇）[3] get-ctrip-data[4] post-803]]></content>
      <categories>
        <category>crawler</category>
      </categories>
      <tags>
        <tag>carwler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM 监控]]></title>
    <url>%2F2017%2F09%2F02%2Fjvm-monitoring%2F</url>
    <content type="text"><![CDATA[昨天天发现Tomcat占用2G物理内存，11G虚拟内存，感觉就起了一个web项目，而且是在内网，没几个人用呀。内存占用怎么这么高。 Tomcat使用的大多是默认配置。就改了个conf/context.xml server.xml 遇到问题了，先想想怎么解决，暂时能想到的是用分析工具分析一下了。我用的是jdk1.8.0_111， ${JAVA_HOME}/bin目录里有两个工具jmc.exe jvisualvm.exe这两个工具就是今天的主角了。用来监控JVM的。 用这两个工具监控本地的java程序特别简单，打开找到运行的程序就能用。但是监控远程的java程序就得没那么简单了。先看看前辈们的作法 使用本地JConsole监控远程JVM(最权威的总结) jvisualvm远程监控Tomcat他们虽然写的挺好的，但是太啰嗦，对于想要快速解决问题的同志们，想要通篇看完文章有点难度。 JMC VisualVM 连接远程的配置上硬货。1234567# 修改配置文件 $&#123;TOMCAT_HOME&#125;/bin/catalina.sh 103行左右，添加以下内容# OS specific support. $var _must_ be set to either true or false. if [ &quot;$1&quot; = &quot;start&quot; ];then JAVA_OPTS=&quot;-Xms256m -Xmx2048m -XX:PermSize=128M -XX:MaxPermSize=256m -Dcom.sun.management.jmxremote.port=6688 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management .jmxremote.authenticate=false -XX:+UnlockCommercialFeatures -XX:+FlightRecorder&quot;else echo &quot;shutdown&quot;;fi; 1234567891011// 参数解释-Xms256m #JVM初始分配的堆内存-Xmx2048m #JVM最大允许分配的堆内存，按需分配-XX:PermSize=128M #JVM初始分配的非堆内存-XX:MaxPermSize=256M #JVM最大允许分配的非堆内存，按需分配-Dcom.sun.management.jmxremote.port #这个是配置远程 connection 的端口号的，要确定这个端口没有被占用-Dcom.sun.management.jmxremote.ssl=false #是否启用ssl-Dcom.sun.management.jmxremote.authenticate=false #指定了JMX 是否启用鉴权（需要用户名，密码鉴权）-Djava.rmi.server.hostname #这个是配置server的IP的，可以不配置-XX:+UnlockCommercialFeatures -XX:+FlightRecorder #不用于商业用途(使用JMC bean时要配置该项)-Dcom.sun.management.jmxremote.pwd.file #密码文件路径(例如: /root/soft/jdk8/jre/lib/management/jmxremote.password) 不使用密码可以不配置该项 还有加if else的原因:12Error: Exception thrown by the agent : java.rmi.server.ExportException: Port already in use: 6688; nested exception is: java.net.BindException: Address already in use (Bind failed) 如果不加if else，在关闭tomcat的时候会报错，提示端口已经占用。 123456789101112# // 也可以使用下面这种配置，和上面效果一样JAVA_OPTS=&quot;$JAVA_OPTS -Xms256m -Xmx1024m -XX:PermSize=128M -XX:MaxPermSize=256m&quot;if [ &quot;$1&quot; = &quot;start&quot; ];then echo &quot;set console&quot;; JAVA_OPTS=&quot;$JAVA_OPTS -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=6688 -Djava.rmi.server.hostname=192.168.1.6&quot;; JAVA_OPTS=&quot;$JAVA_OPTS -Dcom.sun.management.jmxremote.authenticate=false&quot;; JAVA_OPTS=&quot;$JAVA_OPTS -Dcom.sun.management.jmxremote.ssl=false&quot;;# JAVA_OPTS=&quot;$JAVA_OPTS -Dcom.sun.management.jmxremote.pwd.file=/root/soft/jdk8/jre/lib/management/jmxremote.password&quot; JAVA_OPTS=&quot;$JAVA_OPTS -XX:+UnlockCommercialFeatures -XX:+FlightRecorderelse echo &quot;shutdown&quot;;fi; 虚拟内存大可能是程序mmap了什么大文件过着tomcat映射了一部分用户空间的内存地址到物理内存，方便后续直接操作而避免内存拷贝（请搜索zero copy概念），比如nio的很多方法都用mmap实现的。 虚拟内存越大，说明有更多的物理空间被映射到用户空间的地址，避免你访问物理内存的时候内存拷贝，是好事。 top的cpu显示的是瞬时值，只有参考意义，实际意义不大，应该更关注平均值。另外很多人觉得cpu高很可怕，其实正好相反，高了说明你的程序iowait少啊，是好事啊，除非是程序的计算量预期很低但是实际很高。 References[1] 使用本地JConsole监控远程JVM(最权威的总结)[2] jvisualvm远程监控Tomcat[3] 配置远程JConsole[4] Tomcat虚拟内存占用很高？[5] security-windows[6] agent]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[String StringBuilder StringBuffer 对比]]></title>
    <url>%2F2017%2F08%2F29%2Fstring-stringbuilder-stringbuffer-compare%2F</url>
    <content type="text"><![CDATA[我们都知道在一个循环内拼接字符串不能直接用+，而是要用StringBuilder或者StringBuffer代替。 但是在循环内拼接字符串影响会有多大呢，会对程序产生多大影响呢，是0.1s还是1s，还是… ，用数据来说话。 还有一个问题就是，循环内每次要输出一句话，用String+还是用StringBuilder好呢。 顺便也测一下。 动手 下面是代码和测试结果 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158/** * @version V1.0 */public class StringStringBuilderStringBufferTest &#123; public static void main(String[] args) &#123; int count = 100000; int iteatorCount = 10; boolean outFlag = false; // 测试输出字符串对效率的影响 StringStringBuilderStringBufferTest test = new StringStringBuilderStringBufferTest(); // 用第一个for循环抵消创建int的影响 for (int i = 0; i &lt; count; i++) &#123; &#125; /** * 测试一个循环内拼接字符串的影响 */ Double[][] array = new Double[iteatorCount][3]; for (int i = 0; i &lt; iteatorCount; i++) &#123; test.effect(count, array, i); &#125; // 打印多次测试结果 for (int i = 0; i &lt; iteatorCount; i++) &#123; // 测试多少次 StringBuilder sb = new StringBuilder(); for (int j = 0; j &lt; 3; j++) &#123; sb.append(array[i][j]); sb.append(" "); &#125; System.out.println(sb.toString()); // 一行是一次测试结果 &#125; /** * 测试一个循环内，每次拼接一个字符串的影响 */ // 多次测试，比较结果，避免随机对测试结果的影响 Double[][] array2 = new Double[iteatorCount][3]; for (int i = 0; i &lt; iteatorCount; i++) &#123; test.effect2(count, outFlag, array2, i); &#125; // 打印多次测试结果 for (int i = 0; i &lt; iteatorCount; i++) &#123; // 测试多少次 StringBuilder sb = new StringBuilder(); for (int j = 0; j &lt; 3; j++) &#123; sb.append(array2[i][j]); sb.append(" "); &#125; System.out.println(sb.toString()); // 一行是一次测试结果 &#125; &#125; /** * * @param count * 循环次数 * @param array * 结果数据 * @param iteatorCount * 第几次测试 */ private void effect(int count, Double[][] array, int iteatorCount) &#123; /** * 测试String */ long stringStart = System.currentTimeMillis(); String str = null; for (int i = 0; i &lt; count; i++) &#123; str += i; &#125; long stringEnd = System.currentTimeMillis(); /** * 测试StringBuilder */ long stringBuildStrat = System.currentTimeMillis(); StringBuilder stringBuilder = new StringBuilder(); for (int i = 0; i &lt; count; i++) &#123; stringBuilder.append(i); &#125; String str2 = stringBuilder.toString(); long stringBuildEnd = System.currentTimeMillis(); /** * 测试StringBuffer */ long stringBufferStrat = System.currentTimeMillis(); StringBuffer stringBuffer = new StringBuffer(); for (int i = 0; i &lt; count; i++) &#123; stringBuffer.append(i); &#125; stringBuffer.toString(); long stringBufferEnd = System.currentTimeMillis(); array[iteatorCount][0] = 1.0 * (stringEnd - stringStart) / 1000; array[iteatorCount][1] = 1.0 * (stringBuildEnd - stringBuildStrat) / 1000; array[iteatorCount][2] = 1.0 * (stringBufferEnd - stringBufferStrat) / 1000; &#125; /** * @param count * 循环次数 * @param outFlag * 是否输出字符串 * @param array * 结果数据 * @param iteatorCount * 第几次测试 */ private void effect2(int count, boolean outFlag, Double[][] array, int iteatorCount) &#123; /** * 测试String */ long stringStart = System.currentTimeMillis(); for (int i = 0; i &lt; count; i++) &#123; String str = "i:" + i; if (outFlag) &#123; System.out.println(str); &#125; &#125; long stringEnd = System.currentTimeMillis(); /** * 测试StringBuilder */ long stringBuildStrat = System.currentTimeMillis(); for (int i = 0; i &lt; count; i++) &#123; String str = new StringBuilder().append("i:").append(i).toString(); if (outFlag) &#123; System.out.println(str); &#125; &#125; long stringBuildEnd = System.currentTimeMillis(); /** * 测试StringBuffer */ long stringBufferStrat = System.currentTimeMillis(); for (int i = 0; i &lt; count; i++) &#123; String str = new StringBuffer().append("i:").append(i).toString(); if (outFlag) &#123; System.out.println(str); &#125; &#125; long stringBufferEnd = System.currentTimeMillis(); array[iteatorCount][0] = 1.0 * (stringEnd - stringStart) / 1000; array[iteatorCount][1] = 1.0 * (stringBuildEnd - stringBuildStrat) / 1000; array[iteatorCount][2] = 1.0 * (stringBufferEnd - stringBufferStrat) / 1000; &#125;&#125; effect方法里拼接1000000次字符串打印出的结果，String StringBuilder StringBuffer16.887 0.004 0.00411.244 0.002 0.0028.889 0.002 0.0029.098 0.003 0.0039.134 0.002 0.0039.027 0.002 0.0038.996 0.002 0.0029.081 0.002 0.0038.985 0.003 0.0029.016 0.003 0.003 可以看到，String和StringBuilder的差别很大，StringBuilder和StringBuffer效率差不多，总体还是StringBuilder好一点。 effect2方法打印出的结果，String StringBuilder StringBuffer0.095 0.07 0.0720.063 0.038 0.0320.035 0.036 0.0390.04 0.04 0.0280.028 0.03 0.0310.03 0.03 0.0370.042 0.038 0.0460.046 0.047 0.050.033 0.028 0.0380.028 0.041 0.044 effect2结果看着不明显，再测一次String StringBuilder StringBuffer0.12 0.071 0.0680.033 0.056 0.0350.036 0.045 0.050.04 0.034 0.0310.034 0.034 0.0310.037 0.028 0.0410.042 0.042 0.0490.043 0.031 0.0320.031 0.031 0.0320.035 0.029 0.036 其实每次输出一个字符串的话，这几个都是可以的嘛，String+用起来比较方便以后可以放心的用System.out.println(“第” + count + “行:” + line);了 搬运点东西 String StringBuilder StringBuffer 对比String 字符串常量StringBuffer 字符串变量（线程安全）StringBuilder 字符串变量（非线程安全） 简要的说， String 类型和 StringBuffer 类型的主要性能区别其实在于 String 是不可变的对象, 因此在每次对 String 类型进行改变的时候其实都等同于生成了一个新的 String 对象，然后将指针指向新的 String 对象，所以经常改变内容的字符串最好不要用 String ，因为每次生成对象都会对系统性能产生影响，特别当内存中无引用对象多了以后， JVM 的 GC 就会开始工作，那速度是一定会相当慢的。 而如果是使用 StringBuffer 类则结果就不一样了，每次结果都会对 StringBuffer 对象本身进行操作，而不是生成新的对象，再改变对象引用。所以在一般情况下我们推荐使用 StringBuffer ，特别是字符串对象经常改变的情况下。而在某些特别情况下， String 对象的字符串拼接其实是被 JVM 解释成了 StringBuffer 对象的拼接，所以这些时候 String 对象的速度并不会比 StringBuffer 对象慢，而特别是以下的字符串对象生成中， String 效率是远要比 StringBuffer 快的： String S1 = “This is only a” + “ simple” + “ test”; StringBuffer Sb = new StringBuilder(“This is only a”).append(“ simple”).append(“ test”); 你会很惊讶的发现，生成 String S1 对象的速度简直太快了，而这个时候 StringBuffer 居然速度上根本一点都不占优势。其实这是 JVM 的一个把戏，在 JVM 眼里，这个 String S1 = “This is only a” + “ simple” + “test”; 其实就是： String S1 = “This is only a simple test”; 所以当然不需要太多的时间了。但大家这里要注意的是，如果你的字符串是来自另外的 String 对象的话，速度就没那么快了，譬如： String S2 = “This is only a”; String S3 = “ simple”; String S4 = “ test”; String S1 = S2 +S3 + S4;这时候 JVM 会规规矩矩的按照原来的方式去做 在大部分情况下 StringBuffer &gt; StringStringBufferJava.lang.StringBuffer线程安全的可变字符序列。一个类似于 String 的字符串缓冲区，但不能修改。虽然在任意时间点上它都包含某种特定的字符序列，但通过某些方法调用可以改变该序列的长度和内容。可将字符串缓冲区安全地用于多个线程。可以在必要时对这些方法进行同步，因此任意特定实例上的所有操作就好像是以串行顺序发生的，该顺序与所涉及的每个线程进行的方法调用顺序一致。StringBuffer 上的主要操作是 append 和 insert 方法，可重载这些方法，以接受任意类型的数据。每个方法都能有效地将给定的数据转换成字符串，然后将该字符串的字符追加或插入到字符串缓冲区中。append 方法始终将这些字符添加到缓冲区的末端；而 insert 方法则在指定的点添加字符。例如，如果 z 引用一个当前内容是“start”的字符串缓冲区对象，则此方法调用 z.append(“le”) 会使字符串缓冲区包含“startle”，而 z.insert(4, “le”) 将更改字符串缓冲区，使之包含“starlet”。在大部分情况下 StringBuilder &gt; StringBufferjava.lang.StringBuildejava.lang.StringBuilder一个可变的字符序列是5.0新增的。此类提供一个与 StringBuffer 兼容的 API，但不保证同步。该类被设计用作 StringBuffer 的一个简易替换，用在字符串缓冲区被单个线程使用的时候（这种情况很普遍）。如果可能，建议优先采用该类，因为在大多数实现中，它比 StringBuffer 要快。两者的方法基本相同。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取 微博]]></title>
    <url>%2F2017%2F08%2F23%2Fcrawler-weibo%2F</url>
    <content type="text"><![CDATA[电脑版：http://weibo.com 手机版wap版: https://weibo.cn/ 手机app版：https://m.weibo.cn/ 我采用的是爬取手机wap版，weibo.cn 最重要的是这四个参数：_T_WM、SUB、SUHB、SCF 爬取微博主要思路是登录后获取cookie，cookie可以一直用 一个账号1分钟爬取次数不要太快，尽量小于10次。 最近登录记录https://security.weibo.com/loginrecord/active References[1] 新浪微博爬虫分享（一天可抓取 1300 万条数据）[2] Python 爬虫如何机器登录新浪微博并抓取内容？[3] 微博模拟登陆研究[4] 微博爬虫开源项目汇总大全（长期更新、欢迎补充）[5] 【java】微博爬虫（一）：小试牛刀——网易微博爬虫（自定义关键字爬取微博数据）[6] 详解抓取网站，模拟登陆，抓取动态网页的原理和实现（Python，C#等）[7] 用python对鹿晗、关晓彤微博进行情感分析[8] 模拟新浪微博登录：从原理分析到实现[9] 【网络爬虫】【java】微博爬虫（二）：如何抓取HTML页面及HttpClient使用[10] 网络爬虫系列[11] 新浪微博的 GSID 泄露是什么样一个漏洞？怎么被人发现的？[12] 零授权，抓取新浪微博任何用户的微博内容[13] 新浪微博gsid劫持[14] 新浪微博爬取笔记[15] 新浪微博爬取笔记（2）：wap端模拟登陆 python[16] 模拟新浪微博wap登录[17] Python模拟新浪微博登录]]></content>
      <categories>
        <category>crawler</category>
      </categories>
      <tags>
        <tag>crawler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 使用]]></title>
    <url>%2F2017%2F08%2F15%2Fredis-notes%2F</url>
    <content type="text"><![CDATA[12345wget http://download.redis.io/releases/redis-4.0.2.tar.gztar xzf redis-4.0.2.tar.gzcd redis-4.0.2sudo make test suto make install Redis对于Linux是官方支持的,普通安装、使用按照官方指导，5分钟以内就能搞定。详情请参考：Redis官网下载链接 http://redis.io/download 但有时候又想在windows下使用redis,可以从redis下载页面看到如下提示(在页面中搜索 “windows”)： 123Win64 Unofficial The Redis project does not directly support Windows, however the Microsoft Open Tech group develops and maintains an Windows port targeting Win64. 大意就是 Redis官方是不支持windows的，只是 Microsoft Open Tech group 在 GitHub上开发了一个Win64的版本,项目地址是：项目地址 打开以后，可以直接使用浏览器下载，或者Git克隆。 https://github.com/MicrosoftArchive/redis/releases 在 Release 页面中，可以找到 msi 安装文件以及 .zip 文件 下载解压，在解压后的bin目录下有以下这些文件：123456789101112131415161718C:\ProfessionSofware\Redis\Redis-x64-3.2.100 的目录2017-08-15 22:04 &lt;DIR&gt; .2017-08-15 22:04 &lt;DIR&gt; ..2016-07-01 16:27 1,024 EventLog.dll2016-07-01 16:07 12,509 Redis on Windows Release Notes.docx2016-07-01 16:07 16,727 Redis on Windows.docx2016-07-01 16:28 409,088 redis-benchmark.exe2016-07-01 16:28 4,370,432 redis-benchmark.pdb2016-07-01 16:28 257,024 redis-check-aof.exe2016-07-01 16:28 3,518,464 redis-check-aof.pdb2016-07-01 16:28 499,712 redis-cli.exe2016-07-01 16:28 4,526,080 redis-cli.pdb2016-07-01 16:28 1,666,560 redis-server.exe2016-07-01 16:28 7,081,984 redis-server.pdb2016-07-01 16:07 48,212 redis.windows-service.conf2016-07-01 16:07 48,201 redis.windows.conf2016-07-01 09:17 14,265 Windows Service Documentation.docx 123456redis-benchmark.exe #基准测试 redis-check-aof.exe # aof redis-check-dump.exe # dump redis-cli.exe # 客户端 redis-server.exe # 服务器 redis.windows.conf # 配置文件 当然，还有一个 RedisService.docx 文件，看似是一些启动和安装服务的说明文档, 但是照着他的指示来,你就会死的很惨，莫名其妙的死了，不知道原因。 [换机器重新测试后已查明,如果不是Administrator用户，就会出各种问题，服务安装以后启动不了等等问题， 应该可以修改服务的属性–&gt;登录用户等选项来修正.] 如果你安装的windows没有Administrator账户，请参考这篇文章: Windows 7 启用超级管理员Administrator账户的N种方法 启动脚本如下:1redis-server redis.windows.conf 可以将其保存为文件 startup.bat ; 下次就可以直接启动了。 启动,OK,成功.123456789101112131415161718192021222324252627282930313233343536WKQ@WKQ-PC C:\ProfessionSofware\Redis\Redis-x64-3.2.100&gt; redis-server redis.windows.conf _._ _.-``__ &apos;&apos;-._ _.-`` `. `_. &apos;&apos;-._ Redis 3.2.100 (00000000/0) 64 bit .-`` .-```. ```\/ _.,_ &apos;&apos;-._ ( &apos; , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|&apos;` _.-&apos;| Port: 6379 | `-._ `._ / _.-&apos; | PID: 9740 `-._ `-._ `-./ _.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | http://redis.io `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; `-._ `-.__.-&apos; _.-&apos; `-._ _.-&apos; `-.__.-&apos;[9740] 15 Aug 22:05:36.713 # Server started, Redis version 3.2.100[9740] 15 Aug 22:05:36.713 * The server is now ready to accept connections on port 6379使用 ctrl c 退出后[9740] 15 Aug 22:20:37.003 * 1 changes in 900 seconds. Saving...[9740] 15 Aug 22:20:37.048 * Background saving started by pid 9752[9740] 15 Aug 22:20:37.148 # fork operation complete[9740] 15 Aug 22:20:37.148 * Background saving terminated with success[9740] 15 Aug 22:45:46.762 # User requested shutdown...[9740] 15 Aug 22:45:46.762 * Saving the final RDB snapshot before exiting.[9740] 15 Aug 22:45:46.789 * DB saved on disk[9740] 15 Aug 22:45:46.790 # Redis is now ready to exit, bye bye...WKQ@WKQ-PC C:\ProfessionSofware\Redis\Redis-x64-3.2.100 然后可以使用自带的客户端工具进行测试。 双击打开 redis-cli.exe , 如果不报错,则连接上了本地服务器,然后测试，比如 set命令，get命令:1234567891011121314151617181920212223WKQ@WKQ-PC C:\ProfessionSofware\Redis\Redis-x64-3.2.100&gt; redis-cli127.0.0.1:6379&gt; helpredis-cli 3.2.100To get help about Redis commands type: &quot;help @&lt;group&gt;&quot; to get a list of commands in &lt;group&gt; &quot;help &lt;command&gt;&quot; for help on &lt;command&gt; &quot;help &lt;tab&gt;&quot; to get a list of possible help topics &quot;quit&quot; to exitTo set redis-cli perferences: &quot;:set hints&quot; enable online hints &quot;:set nohints&quot; disable online hintsSet your preferences in ~/.redisclirc127.0.0.1:6379&gt; set str teststrOK127.0.0.1:6379&gt; get str&quot;teststr&quot;127.0.0.1:6379&gt; exitWKQ@WKQ-PC C:\ProfessionSofware\Redis\Redis-x64-3.2.100 you need tcl 8.5 可能遇到的问题sudo make test 时 [exception]: Executing test client: couldn’t execute “src/redis-benchmark”: no such file or directory.123456[exception]: Executing test client: couldn&apos;t execute &quot;src/redis-benchmark&quot;: no such file or directory.couldn&apos;t execute &quot;src/redis-benchmark&quot;: no such file or directory while executing&quot;exec src/redis-benchmark -p $R_port(0) -n 10000000 -r 1000 incr __rand_int__ &gt; /dev/null &amp;&quot; (&quot;uplevel&quot; body line 31) invoked from within 忽略这个错误，对安装没影响，安装完可以用 使用连接 redis-cli -h host -p port -a password select dbIndex默认16个数据库：0-15，进入redis后默认是0库。不建议使用多个数据库 123λ redis-cli.exe -h 127.0.0.1 -p 6379127.0.0.1:6379&gt; select 1OK 123λ redis-cli.exe -h 127.0.0.1 -p 6379127.0.0.1:6379&gt; auth &quot;DtJwUZaKl&quot;OK string1234567891011121314127.0.0.1:6379[1]&gt; set key1 value1OK127.0.0.1:6379[1]&gt; keys *1) &quot;key1&quot;127.0.0.1:6379[1]&gt; set key2 value2OK127.0.0.1:6379[1]&gt; keys *1) &quot;key1&quot;2) &quot;key2&quot;127.0.0.1:6379[1]&gt; set key3 value3 key4 value4(error) ERR syntax error127.0.0.1:6379[1]&gt; keys *1) &quot;key1&quot;2) &quot;key2&quot; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191127.0.0.1:6379[1]&gt; HMSET key1_hash value1_hash(error) ERR wrong number of arguments for &apos;hmset&apos; command127.0.0.1:6379[1]&gt; hmset key1_hash field1 hello filed2 worldOK127.0.0.1:6379[1]&gt; hget key1_hash filed1(nil)127.0.0.1:6379[1]&gt; hget key1_hash field1&quot;hello&quot;127.0.0.1:6379[1]&gt; hget key1_hash field2(nil)127.0.0.1:6379[1]&gt; hget key1_hash filed2&quot;world&quot;127.0.0.1:6379[1]&gt; hmset key1 field1 value1 field2 vaule2(error) WRONGTYPE Operation against a key holding the wrong kind of value127.0.0.1:6379[1]&gt; hmset key2_hash field1 value1 field2 vaule2OK127.0.0.1:6379[1]&gt; hget key2_hash field1&quot;value1&quot;127.0.0.1:6379[1]&gt; hget key2_hash field2&quot;vaule2&quot;127.0.0.1:6379[1]&gt; keys *1) &quot;key2_hash&quot;2) &quot;key1_hash&quot;3) &quot;key1&quot;4) &quot;key2&quot;127.0.0.1:6379[1]&gt; del key2_hash field1(integer) 1127.0.0.1:6379[1]&gt; hget key2_hash field1(nil)127.0.0.1:6379[1]&gt; hget key2_hash field2(nil)127.0.0.1:6379[1]&gt; hget key1_hash fiel1(nil)127.0.0.1:6379[1]&gt; hget key1_hash field1&quot;hello&quot;127.0.0.1:6379[1]&gt; hget key1_hash filed2&quot;world&quot;127.0.0.1:6379[1]&gt; del key1_hash filed2(integer) 1127.0.0.1:6379[1]&gt; hget key1_hash filed1(nil)127.0.0.1:6379[1]&gt; hget key1_hash fiield1(nil)127.0.0.1:6379[1]&gt; hget key1_hash field1(nil)127.0.0.1:6379[1]&gt; keys *1) &quot;key1&quot;2) &quot;key2&quot;127.0.0.1:6379[1]&gt; hset key1_hash field1 value1 field2 value2(error) ERR wrong number of arguments for &apos;hset&apos; command127.0.0.1:6379[1]&gt; hmset key1_hash field1 value1 field2 value2OK127.0.0.1:6379[1]&gt; keys *1) &quot;key1_hash&quot;2) &quot;key1&quot;3) &quot;key2&quot;127.0.0.1:6379[1]&gt; hget key1_hash(error) ERR wrong number of arguments for &apos;hget&apos; command127.0.0.1:6379[1]&gt; hget key1_hash field1&quot;value1&quot;127.0.0.1:6379[1]&gt; hget key1_hash field2&quot;value2&quot;127.0.0.1:6379[1]&gt; keys *1) &quot;key1_hash&quot;2) &quot;key1&quot;3) &quot;key2&quot;127.0.0.1:6379[1]&gt; del key1_hash field1(integer) 1127.0.0.1:6379[1]&gt; keys *1) &quot;key1&quot;2) &quot;key2&quot;127.0.0.1:6379[1]&gt; hmset key1_hash field1 value1 field2 value2OK127.0.0.1:6379[1]&gt; keys *1) &quot;key1_hash&quot;2) &quot;key1&quot;3) &quot;key2&quot;127.0.0.1:6379[1]&gt; del key1_hash(integer) 1127.0.0.1:6379[1]&gt; keys *1) &quot;key1&quot;2) &quot;key2&quot;127.0.0.1:6379[1]&gt; lpush key1_list value1 value2 value3(integer) 3127.0.0.1:6379[1]&gt; keys *1) &quot;key1_list&quot;2) &quot;key1&quot;3) &quot;key2&quot;127.0.0.1:6379[1]&gt; lrange key1_list(error) ERR wrong number of arguments for &apos;lrange&apos; command127.0.0.1:6379[1]&gt; lrange key1_list 0 101) &quot;value3&quot;2) &quot;value2&quot;3) &quot;value1&quot;127.0.0.1:6379[1]&gt; lpush key1_list value4(integer) 4127.0.0.1:6379[1]&gt; lrange key1_list 0 101) &quot;value4&quot;2) &quot;value3&quot;3) &quot;value2&quot;4) &quot;value1&quot;127.0.0.1:6379[1]&gt; lpush key2_list value1 value2 value3(integer) 3127.0.0.1:6379[1]&gt; lrange key2_lsit(error) ERR wrong number of arguments for &apos;lrange&apos; command127.0.0.1:6379[1]&gt; lrange key2_list(error) ERR wrong number of arguments for &apos;lrange&apos; command127.0.0.1:6379[1]&gt; lrange key2_list 0 101) &quot;value3&quot;2) &quot;value2&quot;3) &quot;value1&quot;127.0.0.1:6379[1]&gt; keys *1) &quot;key1_list&quot;2) &quot;key1&quot;3) &quot;key2_list&quot;4) &quot;key2&quot;127.0.0.1:6379[1]&gt; del key2_list value(integer) 1127.0.0.1:6379[1]&gt; keys *1) &quot;key1_list&quot;2) &quot;key1&quot;3) &quot;key2&quot;127.0.0.1:6379[1]&gt; sadd key1_set value(integer) 1127.0.0.1:6379[1]&gt; keys *1) &quot;key1_list&quot;2) &quot;key1&quot;3) &quot;key1_set&quot;4) &quot;key2&quot;127.0.0.1:6379[1]&gt; semebers key1_set(error) ERR unknown command &apos;semebers&apos;127.0.0.1:6379[1]&gt; smembers key1_set1) &quot;value&quot;127.0.0.1:6379[1]&gt; sadd key1_set value2(integer) 1127.0.0.1:6379[1]&gt; smembers key1_set1) &quot;value2&quot;2) &quot;value&quot;127.0.0.1:6379[1]&gt; sadd key1_set value3 value4(integer) 2127.0.0.1:6379[1]&gt; smembers key1_set1) &quot;value4&quot;2) &quot;value3&quot;3) &quot;value2&quot;4) &quot;value&quot;127.0.0.1:6379[1]&gt; sadd key1_set value value1 value2(integer) 1127.0.0.1:6379[1]&gt; smembers key1_set1) &quot;value4&quot;2) &quot;value2&quot;3) &quot;value3&quot;4) &quot;value&quot;5) &quot;value1&quot;127.0.0.1:6379[1]&gt; zadd key1_zset 0 redis(integer) 1127.0.0.1:6379[1]&gt; keys *1) &quot;key2&quot;2) &quot;key1_set&quot;3) &quot;key1_zset&quot;4) &quot;key1_list&quot;5) &quot;key1&quot;127.0.0.1:6379[1]&gt; zadd key1_zset 0 mysql(integer) 1127.0.0.1:6379[1]&gt; keys *1) &quot;key2&quot;2) &quot;key1_set&quot;3) &quot;key1_zset&quot;4) &quot;key1_list&quot;5) &quot;key1&quot;127.0.0.1:6379[1]&gt; zrangetbyscore key1_set 0 100(error) ERR unknown command &apos;zrangetbyscore&apos;127.0.0.1:6379[1]&gt; zrangetyscore key1_set 0 100(error) ERR unknown command &apos;zrangetyscore&apos;127.0.0.1:6379[1]&gt; zrangebyscore key1_set 0 100(error) WRONGTYPE Operation against a key holding the wrong kind of value127.0.0.1:6379[1]&gt; zrangebyscore key1_zset 0 1001) &quot;mysql&quot;2) &quot;redis&quot;127.0.0.1:6379[1]&gt; zadd key1_zset 1 neo4j(integer) 1127.0.0.1:6379[1]&gt; zadd key1_zset 1 orientdb(integer) 1127.0.0.1:6379[1]&gt; zrangebyscore key1_zset(error) ERR wrong number of arguments for &apos;zrangebyscore&apos; command127.0.0.1:6379[1]&gt; zrangebyscore key1_zset 0 1001) &quot;mysql&quot;2) &quot;redis&quot;3) &quot;neo4j&quot;4) &quot;orientdb&quot;127.0.0.1:6379[1]&gt;127.0.0.1:6379[1]&gt; References[1] https://redis.io/[2] https://redis.io/download[3] http://blog.csdn.net/renfufei/article/details/38474435[4] http://keenwon.com/1275.html[5] http://blog.csdn.net/renfufei/article/details/52876083[6] Redis常用命令[7] 2018整理最全的50道Redis面试题[8] Mac环境下安装Redis[9] mac安装redis]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取 天眼查]]></title>
    <url>%2F2017%2F08%2F05%2Fcrawler-tianyancha%2F</url>
    <content type="text"><![CDATA[由于天眼查反爬措施较严，只写了思路，有部分内容没公开。仅供参考。 需要一万条左右的企业信息，然后就想到 天眼查 企查查 启信宝 去爬一点，发现天眼查做的比较好，然后就想从天眼查爬一点。 结果发现天眼查2017-06的时候改版了，改版以后全部使用https，而且反爬措施也严了。 如果不差钱，直接用天眼查提供的接口 https://open.tianyancha.com/ 爬取的时候发现并没有想象的那么简单，然后再网上搜了搜，发现2017-06以前的博客代码都不能用了。2017-06以后的资料非常少。没办法，只能靠自己了。 2018-03-01 又改版了，改完感觉颜色没以前好看了。不过增加了一些功能。 如果是毕业写论文需要几万条数据，可以直接找我要，免得花费太多时间，Email: weikeqin.cn@gmail.com 天眼查数据获取分为两块，第一块为大量索引信息获取。第二块为企业详细信息获取。 现在一个城市能爬取5页，一页20个公司信息，大概有362个城市，这么一算能爬36200条，就算爬一半也够了好了，动手 https://bj.tianyancha.com/search/p1…https://bj.tianyancha.com/search/p5 Cookie里主要有以下参数12345678910111213TYCID undefined ssuid auth_token tyc-user-info RTYCID aliyungf_tc csrfToken OA _csrf _csrf_bkHm_lvt_e92c8d65d92d534b0fc290df538b4758 Hm_lpvt_e92c8d65d92d534b0fc290df538b4758 登陆后获取几个参数，然后模拟1个参数，爬取时动态得到几个参数，这几个参数就全了。然后就可以爬取了。 不过爬取到100页以后，就会发现被反爬了，要想继续爬，第一个办法是换用户登录，第二个办法是换ip，第三个办法是输入验证码。 因为我需要的数据量并不大，所以试了试用代理。然后试了试多注册几个账号。结果发现还是多注册账号好用点，把1810页全爬下来了，解析完一共有32158个公司信息。 如果想爬取全量数据，无需登录，需要研究穿插在几万行代码里的加密算法 Cookie中有两串字符串，分别是token和utm，今天我们分别讲一讲两种破解算法； token获取https://www.tianyancha.com/tongji/3871135.json?_=15100445xxxxxtoken隐藏在https://www.tianyancha.com/tongji/+企业id.json他返回的是一串数字，我们用代码直接获取 1234567891011import requestsurl=&quot;http://www.tianyancha.com/tongji/216908186.json&quot;headers=&#123; &quot;Accept&quot;:&quot;application/json, text/plain, */*&quot;&#125;data=requests.get(url,headers=headers)print(data.text)&#123;&quot;state&quot;:&quot;ok&quot;,&quot;message&quot;:&quot;&quot;,&quot;data&quot;:&#123;&quot;name&quot;:&quot;216908186&quot;,&quot;uv&quot;:740581,&quot;pv&quot;:138691,&quot;v&quot;:&quot;33,102,117,110,99,116,105,111,110,40,110,41,123,100,111,99,117,109,101,110,116,46,99,111,111,107,105,101,61,39,116,111,107,101,110,61,49,101,101,55,97,54,98,101,48,102,57,98,52,48,54,56,56,98,97,99,97,97,55,99,48,101,49,98,53,100,99,102,59,112,97,116,104,61,47,59,39,59,110,46,119,116,102,61,102,117,110,99,116,105,111,110,40,41,123,114,101,116,117,114,110,39,55,44,51,50,44,51,52,44,49,52,44,49,52,44,51,52,44,51,52,44,50,57,44,51,44,55,44,49,44,50,57,44,50,57,44,51,44,49,52,44,49,56,44,49,56,44,49,51,44,49,51,44,51,52,44,51,50,44,50,57,44,49,57,44,50,55,44,55,44,48,44,52,44,52,44,49,51,44,48,44,52,44,51,39,125,125,40,119,105,110,100,111,119,41,59&quot;&#125;&#125; 以上字符串的v对应的一串数字有什么用呢？仔细看可以发现，这串数字最大值也没有超过130，是不是和Ascii有点类似？通过以下代码解码可以得到这么一串字符 123456def strfromcode(strcode): arr=strcode.split(&quot;,&quot;) stringfromcode1=&quot;&quot; for lin in arr: stringfromcode1+=chr(int(lin)) return stringfromcode1 1!function(n)&#123;document.cookie=&apos;token=8cdd0625160146c1909dda40448e7c69;path=/;&apos;;n.wtf=function()&#123;return&apos;28,7,4,28,3,1,31,7,7,32,28,34,29,19,29,14,18,30,28,31,4,29,34,7,30,13,4,0,1,31,4,18&apos;&#125;&#125;(window); 从上不难发现有我们需要的token字段，再通过代码将其取出，则是我们所需要的token字段，同时除了这段之外，还有一段return数字，这串数字和接下来我们要说的utm关系非常密切； token的获取就已经完成了 utm的获取utm的获取需要使用此链接：http://static.tianyancha.com/wap/resources/scripts/app-ce05b92dbf.js此链接返回的字符串中有许多appendChlid字段，此为获取utm的关键字段将其进行拆分，获取相应的字段，同时再将非数字及字母及-的字符使用正则表达式去除，获取相应的字符串列表；再取列表中的每个元素的第１个字符，将第１个字符相同的链接在一起变成新的列表；再使用企业id和10进行求余数，此余数为列表的索引值同时再将第二点中的return列表作为该字符串的索引获取字符串中的值，得到utm字符串具体代码如下所示，由于网上对于天眼查的代码稀少，作者不知是否存在侵权，因此关键代码已删除； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106Sub Main() &apos;根据企业在天眼查内的ID来查询企业信息 &apos;原创：wcymiss Dim strText As String Dim objHttp As Object Dim strURL As String Dim ID As String Dim sgArr() As String Dim strToken As String Dim strUtm As String Dim strV As String Dim strCode As String Dim Index As Integer ID = &quot;812498657&quot; Set objHttp = CreateObject(&quot;WinHttp.WinHttpRequest.5.1&quot;) strURL = &quot;http://www.tianyancha.com/tongji/&quot; &amp; ID &amp; &quot;.json&quot; With objHttp .Open &quot;GET&quot;, strURL, False .setRequestHeader &quot;Accept&quot;, &quot;application/json, text/plain, */*&quot; .Send strText = .responsetext End With strCode = Split(Split(strText, &quot;,&quot;&quot;v&quot;&quot;:&quot;&quot;&quot;)(1), &quot;&quot;&quot;&quot;)(0) strV = StringFromCode(strCode) strToken = Split(Split(strV, &quot;&apos;token=&quot;)(1), &quot;;&quot;)(0) strCode = Split(Split(strV, &quot;return&apos;&quot;)(1), &quot;&apos;&quot;)(0) strURL = &quot;http://static.tianyancha.com/wap/resources/scripts/app-ce05b92dbf.js&quot; With objHttp .Open &quot;GET&quot;, strURL, False .Send strText = .responsetext End With sgArr = GetSoGou(strText) Index = Asc(Left(ID, 1)) Mod 10 strUtm = GetUtm(sgArr, Index, strCode)&apos; Debug.Print strToken&apos; Debug.Print strUtm strURL = &quot;http://www.tianyancha.com/company/&quot; &amp; ID &amp; &quot;.json&quot; With objHttp .Open &quot;GET&quot;, strURL, False .setRequestHeader &quot;Accept&quot;, &quot;application/json, text/plain, */*&quot; .setRequestHeader &quot;Cookie&quot;, &quot;token=&quot; &amp; strToken &amp; &quot;;_utm=&quot; &amp; strUtm .Send strText = .responsetext End With Set objHttp = Nothing Debug.Print strTextEnd SubPrivate Function GetSoGou(strText As String) As String() Dim arr() As String Dim i As Integer Dim objReg As Object Dim sgArr(0 To 9) As String Dim Index As Integer Set objReg = CreateObject(&quot;VBScript.Regexp&quot;) objReg.Global = True arr = Split(strText, &quot;appendChlid(&quot;) For i = 1 To UBound(arr) arr(i) = Split(Split(arr(i), &quot;&gt;&quot;)(1), &quot;&lt;&quot;)(0) Next objReg.Pattern = &quot;&amp;[^;]*;&quot; For i = 1 To UBound(arr) arr(i) = objReg.Replace(arr(i), &quot;&quot;) Next objReg.Pattern = &quot;[^0-9a-z-]&quot; For i = 1 To UBound(arr) arr(i) = objReg.Replace(arr(i), &quot;&quot;) Next Set objReg = Nothing For i = 1 To UBound(arr) If Len(arr(i)) &gt; 1 Then Index = Left(arr(i), 1) sgArr(Index) = sgArr(Index) &amp; Mid(arr(i), 2) End If Next GetSoGou = sgArrEnd FunctionPrivate Function GetUtm(sgArr() As String, Index As Integer, strCode As String) As String Dim i As Integer Dim arr() As String arr = Split(strCode, &quot;,&quot;) For i = 0 To UBound(arr) GetUtm = GetUtm &amp; Mid(sgArr(Index), arr(i) + 1, 1) NextEnd FunctionPrivate Function StringFromCode(strCode As String) As String Dim i As Integer Dim arr() As String arr = Split(strCode, &quot;,&quot;) For i = 0 To UBound(arr) StringFromCode = StringFromCode &amp; Chr(arr(i)) NextEnd Function 1 Github上天眼查爬虫项目https://github.com/guapier/tianyancha（关键词：phantomjs，xpath）https://github.com/felixglow/Tianyancha（关键词：scrapy）https://github.com/haijunt/tianyancha_example（关键词：scrapy, splash）https://github.com/kestiny/PythonCrawler（关键词：phantomjs） 2 各类博客https://ask.hellobi.com/blog/jasmine3happy/6200（关键词：selenium, phantomjs）http://blog.csdn.net/chlk118/article/details/52937671（关键词：phantomjs）https://sanwen.net/a/njbicqo.html（关键词：utm，token）http://www.bubuko.com/infodetail-1917809.html（关键词：utm, token） References[1] 破解天眼查token,_utm,paaptp的过程 2017-07-06[2] 天眼查企业信息获取[3] 天眼查接口token, _utm获取[4] selenium+chromedriver爬取天眼查 2017-10-26[5] 利用python爬虫抓取天眼查企业信息数据，反反爬虫的一些实践[6] 用天眼查查询企业信息（含token和_utm值算法）[7] 用天眼查查询企业信息（含token和_utm值算法）[8] 简单爬取天眼查数据 附代码[9] 【爬虫】大杀器——phantomJS-selenium[10] Python访问天眼查[11] 理解CSRF(跨站请求伪造)[12] 天眼查企业数据获取 [13] 天眼查柳超：公开数据里挖金矿，腾讯给了我最大启发]]></content>
      <categories>
        <category>crawler</category>
      </categories>
      <tags>
        <tag>crawler</tag>
        <tag>spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日志]]></title>
    <url>%2F2017%2F08%2F02%2Flog%2F</url>
    <content type="text"><![CDATA[为什么要写日志 举个简单的例子，你把项目写完后，交给客户，然后客户运行的时候出现问题，很紧急，你要解决这个问题，怎么办？ 这就是写日志的原因，在程序出现问题的时候，日志可以第一时间定位到问题的所在，方便及时排查、解决问题。 在系统开发中，日志是很重要的一个环节，日志写得好对于我们开发调试，线上问题追踪等都有很大的帮助。但记日志并不是简单的输出信息，需要考虑很多问题，比如日志输出的速度，日志输出对于系统内存,CPU的影响等，为此，出现了很多日志框架，以帮助开发者解决这些问题。 为什么项目里不让使用System.out 出错了不知道哪个类哪个方法第几行有问题 System.out.println(“”) 很费性能 你写了以后过几天你就找不见写在哪了 1234567891011public static void main(String[] args) &#123; long start = System.currentTimeMillis(); for(int i = 0; i &lt; 1000000; i ++)&#123; // 注释掉试试 System.out.println("测试System.out性能"); &#125; long end = System.currentTimeMillis(); System.out.println("共耗时"+(1.0*(end - start)/1000) + "s"); &#125; 不注释System.out共耗时6.83s注释System.out共耗时0.004s 6.83 / 0.004 = 1707.5差距够大吧 怎么写日志 假如你开发完项目，交给其他人，他们遇到问题，找你解决，你远在千里，解决的时候需要什么信息，你写日志的时候就写什么信息。就这么简单。 写日志用什么 j.u.l (即java.util.logging) log4j commons-logging logback SLF4J 这些都可以，但是推荐使用 SLF4J + logback Java日志SLF4J+logback1234567891011121314151617181920212223242526272829&lt;!-- SLF4J --&gt;&lt;!-- https://mvnrepository.com/artifact/org.slf4j/slf4j-api --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.21&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Logback --&gt;&lt;!-- https://mvnrepository.com/artifact/ch.qos.logback/logback-core --&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/ch.qos.logback/logback-classic --&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;!-- lombok --&gt;&lt;!-- https://mvnrepository.com/artifact/org.projectlombok/lombok --&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.18&lt;/version&gt;&lt;/dependency&gt; logback.xml放src目录下或者src/main/resources目录下就行123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration scan="true" scanPeriod="60 seconds" debug="false"&gt;&lt;!-- scan="true" scanPeriod="60 seconds" debug="false" --&gt;&lt;!-- scan：当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true。 --&gt;&lt;!-- scanPeriod：设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒当scan为true时，此属性生效。默认的时间间隔为1分钟。 --&gt;&lt;!-- debug：当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。 --&gt; &lt;!--定义日志文件的存储地址 勿在 LogBack 的配置中使用相对路径--&gt; &lt;property name="LOG_HOME" value="./logs" /&gt; &lt;property name="LOG_LEVEL" value="INFO" /&gt; &lt;property name="appName" value="appName"&gt;&lt;/property&gt; &lt;!-- 控制台输出 --&gt; &lt;appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level [%logger&#123;50&#125;] [%line] - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 按照每天生成日志文件 --&gt; &lt;appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;!--日志文件输出的文件名--&gt; &lt;FileNamePattern&gt;$&#123;LOG_HOME&#125;/$&#123;appName&#125;.log.%d&#123;yyyy-MM-dd&#125;.log&lt;/FileNamePattern&gt; &lt;!--日志文件保留天数--&gt; &lt;MaxHistory&gt;30&lt;/MaxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder"&gt; &lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level [%logger&#123;50&#125;] [%line] - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;!--日志文件最大的大小--&gt; &lt;triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"&gt; &lt;MaxFileSize&gt;10MB&lt;/MaxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;/appender&gt; &lt;!-- 日志输出级别 --&gt; &lt;root level="$&#123;LOG_LEVEL&#125;"&gt; &lt;appender-ref ref="STDOUT" /&gt; &lt;appender-ref ref="FILE" /&gt; &lt;/root&gt;&lt;/configuration&gt; 常用写法12345678910111213141516import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class LogTest &#123; private static final Logger log = LoggerFactory.getLogger(CrawlerUmetrip.class); public static void main(String[] args) &#123; log.debug("调试信息"); log.info("详细信息"); log.warn("警告信息"); log.error("错误信息"); &#125; &#125; 更简洁的写法1234567891011121314import lombok.extern.slf4j.Slf4j;@Slf4jpublic class LogTest &#123; public static void main(String[] args) &#123; log.debug("调试信息"); log.info("详细信息"); log.warn("警告信息"); log.error("错误信息"); &#125; &#125; SLF4J+log4j12345678910111213141516&lt;!-- SLF4J --&gt;&lt;!-- https://mvnrepository.com/artifact/org.slf4j/slf4j-api --&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.21&lt;/version&gt;&lt;/dependency&gt;&lt;!-- lombok --&gt;&lt;!-- https://mvnrepository.com/artifact/org.projectlombok/lombok --&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.18&lt;/version&gt;&lt;/dependency&gt; 12345678910111213141516171819202122232425262728293031### log4j配置 ###log4j.rootLogger = DEBUG, console, D, Elog4j.additivity.org.apache = false### 输出信息到控制台 ###log4j.appender.console = org.apache.log4j.ConsoleAppenderlog4j.appender.console.Target = System.out#log4j.appender.console.layout = org.apache.log4j.PatternLayoutlog4j.appender.console.layout = com.custom.CustomPatternLayoutlog4j.appender.console.layout.ConversionPattern = %d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; -(%T)- [%t] %p: -%m%n### 输出DEBUG 级别以上的日志到=E://logs/log.log ####log4j.appender.D = org.apache.log4j.RollingFileAppenderlog4j.appender.D = com.custom.TestRollingFileAppenderlog4j.appender.D.File = E://logs/test/log.loglog4j.appender.D.Append = truelog4j.appender.D.Threshold = DEBUG log4j.appender.D.MaxFileSize = 10MB #log4j.appender.D.MaxBackupIndex = 10 #MaxBackupIndex在TestRollingFileAppender没用，可以不写。log4j.appender.D.layout = com.custom.CustomPatternLayoutlog4j.appender.D.layout.ConversionPattern = %d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; -(%T)- [%t] %p: -%m%n### 输出ERROR 级别以上的日志到=E://logs/error.log ###log4j.appender.E = org.apache.log4j.RollingFileAppenderlog4j.appender.E.File = E://logs/test/error.log log4j.appender.E.Append = truelog4j.appender.E.Threshold = ERROR log4j.appender.E.MaxFileSize = 10MB log4j.appender.E.MaxBackupIndex = 10log4j.appender.E.layout = com.custom.CustomPatternLayoutlog4j.appender.E.layout.ConversionPattern = %d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; -(%T)- [%t] %p: -%m%n log4j.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE log4j:configuration SYSTEM "log4j.dtd"&gt; &lt;log4j:configuration xmlns:log4j='http://jakarta.apache.org/log4j/' &gt; &lt;!-- 输出到控制台 --&gt; &lt;appender name="console" class="org.apache.log4j.ConsoleAppender"&gt; &lt;!-- 设置布局 --&gt; &lt;layout class="com.custom.CustomPatternLayout"&gt;&lt;!-- org.apache.log4j.PatternLayout --&gt; &lt;!-- 输出格式/转换格式 --&gt; &lt;param name="ConversionPattern" value="%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; -(%T)- [%t] %p: -%m%n" /&gt; &lt;/layout&gt; &lt;!--过滤器设置输出的级别--&gt; &lt;filter class="org.apache.log4j.varia.LevelRangeFilter"&gt; &lt;!-- 设置最低级别 --&gt; &lt;param name="levelMin" value="debug" /&gt; &lt;!-- 设置最高级别 --&gt; &lt;param name="levelMax" value="fatal" /&gt; &lt;param name="AcceptOnMatch" value="true" /&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 设置输出到文件 设置滚动方式 --&gt; &lt;appender name="fileD" class="com.custom.TestRollingFileAppender"&gt; &lt;!-- org.apache.log4j.RollingFileAppender --&gt; &lt;!-- 设置日志输出文件名 --&gt; &lt;param name="File" value="E:/logs/test4/log.log" /&gt; &lt;!-- 设置是否在重新启动服务时，在原有日志的基础添加新日志 --&gt; &lt;param name="Append" value="true" /&gt; &lt;!-- 设置滚动时最多保存几个日志文件 --&gt; &lt;param name="MaxBackupIndex" value="10" /&gt; &lt;!-- 设置文件大小达到多大时滚动--&gt; &lt;param name="MaxFileSize" value="10MB"/&gt; &lt;!-- 设置布局 --&gt; &lt;layout class="com.custom.CustomPatternLayout"&gt;&lt;!-- org.apache.log4j.PatternLayout --&gt; &lt;param name="ConversionPattern" value="%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; -(%T)- [%t] %p: -%m%n" /&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;!-- 设置输出到文件 设置滚动方式 --&gt; &lt;appender name="fileE" class="org.apache.log4j.RollingFileAppender"&gt; &lt;!-- 设置日志输出文件名 --&gt; &lt;param name="File" value="E:/logs/test4/error.log" /&gt; &lt;!-- 设置是否在重新启动服务时，在原有日志的基础添加新日志 --&gt; &lt;param name="Append" value="true" /&gt; &lt;!-- 设置滚动时最多保存几个日志文件 --&gt; &lt;param name="MaxBackupIndex" value="10" /&gt; &lt;!-- 设置文件大小达到多大时滚动--&gt; &lt;param name="MaxFileSize" value="10MB"/&gt; &lt;!-- 设置布局 --&gt; &lt;layout class="com.custom.CustomPatternLayout"&gt;&lt;!-- org.apache.log4j.PatternLayout --&gt; &lt;param name="ConversionPattern" value="%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; -(%T)- [%t] %p: -%m%n" /&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;!-- --&gt; &lt;!-- 设置输出到文件 设置滚动方式 --&gt; &lt;appender name="fileF" class="org.apache.log4j.DailyRollingFileAppender"&gt; &lt;param name="File" value="E:/logs/test4/fatal.log" /&gt; &lt;param name="DatePattern" value="'.'yyyy-MM-dd'.log'" /&gt; &lt;layout class="org.apache.log4j.PatternLayout"&gt; &lt;param name="ConversionPattern" value="%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; -(%T)- [%t] %p: -%m%n" /&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;!-- 设置根logger--&gt; &lt;root&gt; &lt;priority value ="debug"/&gt; &lt;!-- 设置输出到哪，和appender name相对应 --&gt; &lt;appender-ref ref="console"/&gt; &lt;appender-ref ref="fileD"/&gt; &lt;appender-ref ref="fileE"/&gt; &lt;/root&gt; &lt;/log4j:configuration&gt; springboot项目日志输出到控制台在application.properties文件里添加以下两行：12logging.file=account.loglogging.config= logback-spring.xml的绝对路径 再把logback-spring.xml文件放到对应的路径下，通过slf4j+logback来管理，在配置里配置不往控制台输出日志。默认的logback-spring.xml在 classpath/logback-spring.xml References[1] 【Java深入学习系列】之那些年我们用过的日志框架[2] Java日志框架（Commons-logging,SLF4j,Log4j,Logback)[3] 封装SLF4J/Log4j，不再处处定义logger变量[4] 日志工具现状调研[5] logback layoutInsteadOfEncoder[6] Logback源码赏析-日志按时间滚动（切割）[7] features/log[8] mavenrepo/index[9] boot-features-logging.html[10] boot-features-logging[11] spring howto-logging[12] log4j/1.2/faq.html[13] Spring Boot干货系列：（七）默认日志logback配置解析[14] 面试官：Logback如何配置，才能提升TPS?[15] logback异步日志配置]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[neo4j 资料]]></title>
    <url>%2F2017%2F07%2F24%2Fneo4j-documents%2F</url>
    <content type="text"><![CDATA[对于中国的Neo4j学习者的一些推荐资料 Neo4j官网 Neo4j中文社区 neo4j亚太区技术专家 俞方桦 博士 中文社区文章 neo4j亚太区技术专家 俞方桦 博士 csdn文章 大白 知乎 回答 developer-manual ogm-manual java rest-docs neo4j-rest-documentation-3.4.pdf neo4j-java-reference-3.4.pdf neo4j-ogm-manual-3.1.pdf neo4j-graph-algorithms-3.4.pdf neo4j-developer-manual-3.4-java.pdf docs Free Neo4j e-books: Graph Databases Online training classes operations-manual operations-manual.pdf groups.google.neo4j neo4j导入csv文件https://neo4j.com/developer/kb/how-do-i-define-a-load-csv-fieldterminator-in-hexidecimal-notation/ 使用batch-import工具向neo4j中导入海量数据https://my.oschina.net/u/2538940/blog/883829 前端展示https://bl.ocks.org/mbostock/4062045 Force-Directed Graphhttp://visjs.org/]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP 资料]]></title>
    <url>%2F2017%2F07%2F19%2Fnlp-notes%2F</url>
    <content type="text"><![CDATA[NLP(Nature Language Processing)自然语言处理 funNLP https://github.com/fighting41love/funNLP 中英文敏感词、语言检测、中外手机/电话归属地/运营商查询、名字推断性别、手机号抽取、身份证抽取、邮箱抽取、中日文人名库、中文缩写库、拆字词典、词汇情感值、停用词、反动词表、暴恐词表、繁简体转换、英文模拟中文发音、汪峰歌词生成器、职业名称词库、同义词库、反义词库、否定词库、汽车品牌词库、汽车零件词库、连续英文切割、各种中文词向量、公司名字大全、古诗词库、IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库、中文聊天语料、中文谣言数据、百度中文问答数据集、句子相似度匹配算法集合、bert资源、文本生成&amp;摘要相关工具、cocoNLP信息抽取工具、国内电话号码正则匹配、清华大学XLORE:中英文跨语言百科知识图谱、清华大学人工智能技术系列报告、自然语言生成、NLU太难了系列、自动对联数据及机器人、用户名黑名单列表、罪名法务名词及分类模型、微信公众号语料、cs224n深度学习自然语言处理课程、中文手写汉字识别、中文自然语言处理 语料/数据集、变量命名神器、分词语料库+代码、任务型对话英文数据集、ASR 语音数据集 + 基于深度学习的中文语音识别系统、笑声检测器、Microsoft多语言数字/单位/如日期时间识别包、中华新华字典数据库及api(包括常用歇后语、成语、词语和汉字)、文档图谱自动生成、SpaCy 中文模型、Common Voice语音识别数据集新版、神经网络关系抽取、基于bert的命名实体识别、关键词(Keyphrase)抽取包pke、基于医疗领域知识图谱的问答系统、基于依存句法与语义角色标注的事件三元组抽取、依存句法分析4万句高质量标注数据、cnocr：用来做中文OCR的Python3包、中文人物关系知识图谱项目、中文nlp竞赛项目及代码汇总、中文字符数据、speech-aligner: 从“人声语音”及其“语言文本”产生音素级别时间对齐标注的工具、AmpliGraph: 知识图谱表示学习(Python)库：知识图谱概念链接预测、Scattertext 文本可视化(python)、语言/知识表示工具：BERT &amp; ERNIE、中文对比英文自然语言处理NLP的区别综述、Synonyms中文近义词工具包、HarvestText领域自适应文本挖掘工具（新词发现-情感分析-实体链接等）、word2word：(Python)方便易用的多语言词-词对集：62种语言/3,564个多语言对、语音识别语料生成工具：从具有音频/字幕的在线视频创建自动语音识别(ASR)语料库、构建医疗实体识别的模型（包含词典和语料标注）、单文档非监督的关键词抽取、Kashgari中使用gpt-2语言模型、开源的金融投资数据提取工具、文本自动摘要库TextTeaser: 仅支持英文、人民日报语料处理工具集、一些关于自然语言的基本模型、基于14W歌曲知识库的问答尝试–功能包括歌词接龙and已知歌词找歌曲以及歌曲歌手歌词三角关系的问答、基于Siamese bilstm模型的相似句子判定模型并提供训练数据集和测试数据集、用Transformer编解码模型实现的根据Hacker News文章标题自动生成评论、用BERT进行序列标记和文本分类的模板代码、LitBank：NLP数据集——支持自然语言处理和计算人文学科任务的100部带标记英文小说语料、百度开源的基准信息抽取系统、虚假新闻数据集、Facebook: LAMA语言模型分析，提供Transformer-XL/BERT/ELMo/GPT预训练语言模型的统一访问接口、CommonsenseQA：面向常识的英文QA挑战、中文知识图谱资料、数据及工具、各大公司内部里大牛分享的技术文档 PDF 或者 PPT、自然语言生成SQL语句（英文）、中文NLP数据增强（EDA）工具、英文NLP数据增强工具 、基于医药知识图谱的智能问答系统、京东商品知识图谱、基于mongodb存储的军事领域知识图谱问答项目、基于远监督的中文关系抽取、语音情感分析、中文ULMFiT-情感分析-文本分类-语料及模型、一个拍照做题程序、世界各国大规模人名库、一个利用有趣中文语料库 qingyun 训练出来的中文聊天机器人、中文聊天机器人seqGAN、省市区镇行政区划数据带拼音标注、教育行业新闻语料库包含自动文摘功能、开放了对话机器人-知识图谱-语义理解-自然语言处理工具及数据、中文知识图谱：基于百度百科中文页面-抽取三元组信息-构建中文知识图谱、masr: 中文语音识别-提供预训练模型-高识别率、Python音频数据增广库、中文全词覆盖BERT及两份阅读理解数据、ConvLab：开源多域端到端对话系统平台、中文自然语言处理数据集、基于最新版本rasa搭建的对话系统、基于TensorFlow和BERT的管道式实体及关系抽取、一个小型的证券知识图谱/知识库、复盘所有NLP比赛的TOP方案、OpenCLaP：多领域开源中文预训练语言模型仓库、UER：基于不同语料+编码器+目标任务的中文预训练模型仓库、中文自然语言处理向量合集、基于金融-司法领域(兼有闲聊性质)的聊天机器人、g2pC：基于上下文的汉语读音自动标记模块、Zincbase 知识图谱构建工具包、诗歌质量评价/细粒度情感诗歌语料库、快速转化「中文数字」和「阿拉伯数字」、百度知道问答语料库、基于知识图谱的问答系统、jieba_fast 加速版的jieba、正则表达式教程、中文阅读理解数据集、基于BERT等最新语言模型的抽取式摘要提取、Python利用深度学习进行文本摘要的综合指南、知识图谱深度学习相关资料整理、维基大规模平行文本语料、StanfordNLP 0.2.0：纯Python版自然语言处理包、NeuralNLP-NeuralClassifier：腾讯开源深度学习文本分类工具、端到端的封闭域对话系统、中文命名实体识别：NeuroNER vs. BertNER、新闻事件线索抽取、2019年百度的三元组抽取比赛：“科学空间队”源码、基于依存句法的开放域文本知识三元组抽取和知识库构建、中文的GPT2训练代码、ML-NLP - 机器学习(Machine Learning)NLP面试中常考到的知识点和代码实现、nlp4han:中文自然语言处理工具集(断句/分词/词性标注/组块/句法分析/语义分析/NER/N元语法/HMM/代词消解/情感分析/拼写检查、XLM：Facebook的跨语言预训练语言模型、用基于BERT的微调和特征提取方法来进行知识图谱百度百科人物词条属性抽取、中文自然语言处理相关的开放任务-数据集-当前最佳结果、CoupletAI - 基于CNN+Bi-LSTM+Attention 的自动对对联系统、抽象知识图谱、MiningZhiDaoQACorpus - 580万百度知道问答数据挖掘项目。 比较好的文章中文自然语言处理可能是 NLP 中最难的？未来数据领域的珠穆朗玛峰之中文自然语言处理 比较好的个人博客Poll的笔记计算广告与机器学习 http://www.cnblogs.com/jerrylead/tag/Machine%20Learning/ Machine Learninghttp://www.javashuo.com/tag/NLP/list-7.htmlhttp://www.cnblogs.com/DianaCody/ 国内自然语言处理产品百度自然语言处理 https://cloud.baidu.com/product/nlp腾讯文智 阿里巴巴 比较好的开源项目友情链接http://thunlp.org/site2/index.php/zh 清华大学自然语言处理与社会人文计算实验室http://thunlp.org/site2/index.php/zh/people 清华大学研究队伍http://thunlp.org/site2/index.php/zh/project 研究项目http://thunlp.org/site2/index.php/zh/course 课程讲授http://thunlp.org/site2/index.php/zh/codes 开源代码http://thunlp.org/site2/index.php/zh/resources 开放资源 比较好的自然语言处理团队中国大陆地区：微软亚洲研究院自然语言计算组 Natural Language Computing (NLC) Group https://www.microsoft.com/en-us/research/group/natural-language-computing/腾讯人工智能实验室（Tencent AI Lab） http://ai.tencent.com/ailab/头条人工智能实验室（Toutiao AI Lab） http://lab.toutiao.com/清华大学自然语言处理与社会人文计算实验室 http://nlp.csai.tsinghua.edu.cn/site2/清华大学智能技术与系统国家重点实验室信息检索组 http://www.thuir.cn/cms/北京大学计算语言学教育部重点实验室 http://www.klcl.pku.edu.cn/北京大学计算机科学技术研究所语言计算与互联网挖掘研究室 http://www.icst.pku.edu.cn/lcwm/index.php?title=%E9%A6%96%E9%A1%B5哈工大社会计算与信息检索研究中心 http://ir.hit.edu.cn/哈工大机器智能与翻译研究室 http://www.contem.org/哈尔滨工业大学智能技术与自然语言处理实验室 http://www.insun.hit.edu.cn/home/中科院计算所自然语言处理研究组 http://nlp.ict.ac.cn/index_zh.php中科院自动化研究所语音语言技术研究组 http://nlpr-web.ia.ac.cn/cip/introduction.htm南京大学自然语言处理研究组 http://nlp.nju.edu.cn/homepage/复旦大学自然语言处理研究组 http://nlp.fudan.edu.cn/东北大学自然语言处理实验室 http://www.nlplab.com/厦门大学智能科学与技术系自然语言处理实验室 http://nlp.xmu.edu.cn/苏州大学自然语言处理实验室 http://nlp.suda.edu.cn/苏州大学人类语言技术研究所 http://hlt.suda.edu.cn/郑州大学自然语言处理实验室 http://nlp.zzu.edu.cn/ 中国 香港/澳门/台湾 地区：Huawei Noah’s Ark Lab http://www.noahlab.com.hkHuman Language Technology Center at Hong Kong University of Science &amp; Technology http://www.cse.ust.hk/~hltc/Natural Language Processing &amp; Portuguese-Chinese Machine Translation Laboratory at University of Macau http://nlp2ct.cis.umac.mo/Natural Language Processing Lab at National Taiwan University http://nlg.csie.ntu.edu.tw/ References[1] 机器学习，数据挖掘以及深度学习算法及相关应用[2] 机器学习常见算法分类汇总[3] 国内外自然语言处理(NLP)研究组[4] 机器学习小知识[5] 公司名语料库。机构名语料库。公司简称,缩写,品牌词,企业名。可用于中文分词、机构名实体识别。[6] 用python写一个简单的中文搜索引擎]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL与SQLServer相互转换]]></title>
    <url>%2F2017%2F07%2F15%2Fmysql-sqlserver-translate%2F</url>
    <content type="text"><![CDATA[环境： Windows XP sp2 MS SQL Server 2OOO sp1 MySql 5.0.41 1：MSSQLServer数据库导入到MySql数据库步骤： 1.安装mysql数据库的ODBC驱动，mysql-connector-odbc-3.51.23-win32.msi，下载并安装。 2.在Mysql中创建数据库实例。 3.打开控制面板 –&gt; 管理工具 –&gt; 数据源ODBC，在用户DSN中添加一个MySQL ODBC 3.51数据源。 4.在登录login选项卡中输入数据源名称Data Source Name，此处输入MysqlDNS（也可以自己随便命名， 只要在后面导入数据的时候选择正确的数据源名字就行），然后输入服务器Server，用户User，密码Password， 输入正确后选择要导入的数据库，Database选择你需要导入的数据库。 在连接选项connect options中根据需要设置MySql使用的端口port和字符集Character Set。 注：字符集一定要和Mysql服务器相对应，如果Mysql使用了gbk字符集，则一定要设置字符集为gbk， 否则导入到Sql Server可能会出现问号乱码。 5.打开sql server企业管理器，选择该数据库，单击右键选择所有任务 –&gt; 导出数据。 6.‘选择数据源’为默认，‘选择目的’为刚刚安装的mySQL数据源，用户/系统DSN为MysqlDNS。 在‘指定表复制或查询’中选择‘从源数据库复制表和视图’，在‘选择源表和视图’里，选择需要导入的表， 即可将数据从MSSQLServer数据库导入到MySql数据库中。 2：MySql数据库导入到MSSQL数据库中1.安装mysql数据库的ODBC驱动，mysql-connector-odbc-3.51.19-win32.msi 2.打开控制面板\管理工具\数据源ODBC，在用户DSN中添加一个MySQL ODBC 3.51数据源。 3.在登录login选项卡中输入数据源名称Data Source Name，此处输入MysqlDNS;然后输入服务器， 用户User，密码Password，输入正确后选择要导入的数据库。 在连接选项connect options中根据需要设置MySql使用的端口port和字符集Character Set。 注：字符集一定要和Mysql服务器相对应，如果Mysql使用了gbk字符集，则一定要设置字符集为gbk， 否则导入到Sql 可能会出现问号乱码。 4.打开sql server企业管理器，新建一数据库MySql。选择该数据库，单击右键选择所有任务\导入数据。 5.选择数据源为其它(ODBC数据源)，用户/系统DSN为MysqlDNS。其余根据向导进行， 即可将数据从MySql数据库导入到MSSQL数据库中 References[1] Mysql和SqlServer互相转换]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[12306 买 卧铺 下铺票]]></title>
    <url>%2F2017%2F07%2F13%2Fbuy-tickets-on-12306%2F</url>
    <content type="text"><![CDATA[在12306上买票时推荐使用电信、联通的网，其他运营商的网质量不太好。可能会影响抢票结果。 有的时候甚至查不到票。我用的是方正的网，发现手机上可以查到票，电脑上查不到票。 结果用手机开热点(电信网)，电脑连上以后很快就查出票了。 在12306买票 首先要登录 在车票预订页面输入出发站、目的地、出发日，然后点击查询，选择合适的车次，然后点击预订。 点击预定后会跳转到 确认订单 页面，在这个页面选择想要席别和位置，这里我选硬卧，此时还没法选下铺，没关系，接下来根据下面说的做就可以了，在浏览器里打开开发者模式(快捷键是F12)然后点击左上角的第一个按钮(一个方框带了一个箭头)，然后再点击确认订单页面的硬卧(附近就行)，然后就会找到如下图1的select的那块，点击select，点击鼠标右键，选择 Edit as HTML，在&lt;/select&gt;后添加以下代码，再点击页面就会发现可以选择卧铺的位置了。如下图2。然后选择下铺，然后确认订单，支付，就可以买到想要的车票了。 在 &lt;/select&gt; 后面直接粘贴以下代码： 1234567&lt;select name=&quot;passenger_1_seat_detail_select&quot; style=&quot;display:block&quot; id=&quot;passenger_1_seat_detail_select&quot; onchange=&quot;setSeatDetail(&apos;1&apos;)&quot;&gt; &lt;option value=&quot;0&quot;&gt;随机&lt;/option&gt; &lt;option value=&quot;3&quot;&gt;上铺&lt;/option&gt; &lt;option value=&quot;2&quot;&gt;中铺&lt;/option&gt; &lt;option value=&quot;1&quot;&gt;下铺&lt;/option&gt;&lt;/select&gt; References[1] http://jingyan.baidu.com/article/c33e3f48a84d46ea15cbb53e.html]]></content>
      <categories>
        <category>daily</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Tomcat 笔记]]></title>
    <url>%2F2017%2F06%2F22%2Ftomcat-notes%2F</url>
    <content type="text"><![CDATA[使用Tomcat时遇到的一些问题，记的一些笔记 Tomcat配置1234567891011&lt;!-- 并发配置 --&gt;&lt;Connector executor=&quot;tomcatThreadPool&quot; port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; URIEncoding=&quot;UTF-8&quot; maxThreads=&quot;30000&quot; minSpareThreads=&quot;512&quot; maxSpareThreads=&quot;2048&quot; connectionTimeout=&quot;20000&quot; keepAliveTimeout=&quot;15000&quot; maxKeepAliveRequests=&quot;1&quot; redirectPort=&quot;8443&quot; enableLookups=&quot;false&quot; acceptCount=&quot;35000&quot; disableUploadTimeout=&quot;true&quot; /&gt; 配置 ${TOMCAT_HOME}/bin/catalina.sh1234567891011121314# 修改配置文件 $&#123;TOMCAT_HOME&#125;/bin/catalina.sh 103行左右，添加以下内容# OS specific support. $var _must_ be set to either true or false. JAVA_OPTS=&quot;$JAVA_OPTS -Xms256m -Xmx1024m -XX:PermSize=128M -XX:MaxPermSize=256m&quot;if [ &quot;$1&quot; = &quot;start&quot; ];then echo &quot;set console&quot;; JAVA_OPTS=&quot;$JAVA_OPTS -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=6688 -Djava.rmi.server.hostname=192.168.1.6&quot;; JAVA_OPTS=&quot;$JAVA_OPTS -Dcom.sun.management.jmxremote.authenticate=false&quot;; JAVA_OPTS=&quot;$JAVA_OPTS -Dcom.sun.management.jmxremote.ssl=false&quot;;# JAVA_OPTS=&quot;$JAVA_OPTS -Dcom.sun.management.jmxremote.pwd.file=/root/soft/jdk8/jre/lib/management/jmxremote.password&quot; JAVA_OPTS=&quot;$JAVA_OPTS -XX:+UnlockCommercialFeatures -XX:+FlightRecorderelse echo &quot;shutdown&quot;;fi; 1234567891011// 参数解释-Xms256m #JVM初始分配的堆内存-Xmx2048m #JVM最大允许分配的堆内存，按需分配-XX:PermSize=128M #JVM初始分配的非堆内存-XX:MaxPermSize=256M #JVM最大允许分配的非堆内存，按需分配-Dcom.sun.management.jmxremote.port #这个是配置远程 connection 的端口号的，要确定这个端口没有被占用-Dcom.sun.management.jmxremote.ssl=false #是否启用ssl-Dcom.sun.management.jmxremote.authenticate=false #指定了JMX 是否启用鉴权（需要用户名，密码鉴权）-Djava.rmi.server.hostname #这个是配置server的IP的，可以不配置-XX:+UnlockCommercialFeatures -XX:+FlightRecorder #不用于商业用途(使用JMC bean时要配置该项)-Dcom.sun.management.jmxremote.pwd.file #密码文件路径(例如: /root/soft/jdk8/jre/lib/management/jmxremote.password) 不使用密码可以不配置该项 还有加if else的原因:12Error: Exception thrown by the agent : java.rmi.server.ExportException: Port already in use: 6688; nested exception is: java.net.BindException: Address already in use (Bind failed) 如果不加if else，在关闭tomcat的时候会报错，提示端口已经占用。 org.apache.catalina.webresources.Cache.getResource Unable to add the resource at [/WEB-INF/lib/xml-apis-1.0.b2.jar] to the cache1222-Jun-2017 08:21:34.719 WARNING [ContainerBackgroundProcessor[StandardEngine[Catalina]]] org.apache.catalina.webresources.Cache.getResource Unable to add the resource at [/WEB-INF/lib/xml-apis-1.0.b2.jar] to the cache because there was insufficient free space available after evicting expired cache entries - consider increasing the maximum size of the cache22-Jun-2017 08:21:34.720 INFO [ContainerBackgroundProcessor[StandardEngine[Catalina]]] org.apache.catalina.webresources.Cache.backgroundProcess The background cache eviction process was unable to free [10] percent of the cache for Context [/m] - consider increasing the maximum size of the cache. After eviction approximately [10,239] KB of data remained in the cache. 这个问题会在Tomcat 8里遇到 123456方法一 增大缓存 Tomcat conf/context.xml 在&lt;/Context&gt;之前添加&lt;Resources cachingAllowed=&quot;true&quot; cacheMaxSize=&quot;102400&quot; /&gt;方法二 关闭缓存 &lt;Resources cachingAllowed=&quot;false&quot; cacheMaxSize=&quot;0&quot; /&gt; 1234567In your $CATALINA_BASE/conf/context.xml add block below before &lt;/Context&gt;&lt;Resources cachingAllowed=&quot;true&quot; cacheMaxSize=&quot;102400&quot; /&gt;or&lt;Resources cachingAllowed=&quot;false&quot; cacheMaxSize=&quot;0&quot; /&gt; Exception thrown by the agent : java.rmi.server.ExportException: Port already in use: 668812Error: Exception thrown by the agent : java.rmi.server.ExportException: Port already in use: 6688; nested exception is: java.net.BindException: Address already in use (Bind failed) References[1] https://stackoverflow.com/questions/26893297/tomcat-8-throwing-org-apache-catalina-webresources-cache-getresource-unable-to[2] http://tomcat.apache.org/tomcat-8.0-doc/config/resources.html[3] http://www.youyong.top/article/1158d1fece13[4] http://www.jianshu.com/p/5a04ae2fca8c[5] http://www.toutiao.com/a6434416811978834177/ Tomcat 的优化 献给java 小白]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 错误 笔记]]></title>
    <url>%2F2017%2F06%2F21%2Fmysql-error-notes%2F</url>
    <content type="text"><![CDATA[(1) -bash: mysql: command not found原因:这是由于系统默认会查找/usr/bin下的命令，如果这个命令不在这个目录下，当然会找不到命令，我们需要做的就是映射一个链接到/usr/bin目录下，相当于建立一个链接文件。[root@localhost bin]# ln -s /usr/local/mysql/bin/mysql /usr/bin (2) cannot connect to local MySQL Server through socket ‘/var/lib/mysql/mysql.sock’配置 /etc/my.cnfsock = /var/lib/mysql/mysql.sock (3) can’t connect to local mysql server through socket ‘/tmp/mysql.sock’客户端连接时会默认去找/tmp路径下的mysql.sock 所以 我们这里的第二个方案是 看能不呢把 mysql.sock复制到 /tmp路径下 ln -s /var/lib/mysql/mysql.sock /tmp/mysql.sock error (3) MySQL server PID file could not be found配置 /etc/my.cnfsock = /var/lib/mysql/mysql.sock (4) Starting MySQL. ERROR! The server quit without updating PID file (/data00/usr/local/mysql/data/host-172-16-11-125.pid).data目录最好放到 /usr/local/mysql/datadata目录所属用户不是mysql 首先看文件对应的用户 1234-rw-rw---- 1 root root 4 Dec 5 18:10 mysqld_safe.pid[root@host-172-16-11-125 data]# chown -R mysql /data00/usr/local/mysql/data/mysqld_safe.pid[root@host-172-16-11-125 data]# chgrp -R mysql /data00/usr/local/mysql/data/mysqld_safe.pid 可能是刚修改my.ini，哪里配置的有问题 (5) ln: failed to create symbolic link ‘/usr/bin/mysql’: File exists1234567[root@localhost bin]# ln -s /usr/local/mysql/bin/mysql /usr/binln: failed to create symbolic link ‘/usr/bin/mysql’: File exists[root@localhost bin]# rm /usr/bin/mysqlrm: remove symbolic link ‘/usr/bin/mysql’? yes[root@localhost bin]# pwd/usr/local/mysql/bin[root@localhost bin]# ln -s /usr/local/mysql/bin/mysql /usr/bin (6) mysql启动不了1234567891011[root@localhost /]# /etc/init.d/mysqld restartMySQL server PID file could not be found! [FAILED]Starting MySQL...The server quit without updating PID file [FAILED]cal/mysql/data/localhost.localdomain.pid).[root@localhost /]# service mysql startRedirecting to /bin/systemctl start mysql.serviceJob for mysqld.service failed because the control process exited with error code. See "systemctl status mysqld.service" and "journalctl -xe" for details.[root@localhost /]# service mysql startRedirecting to /bin/systemctl start mysql.service[root@localhost /]# systemctl start mariadb.serviceFailed to start mariadb.service: Unit mariadb.service failed to load: No such file or directory.[root@localhost /]# systemctl start mysql (7) 发生系统错误 1067123456789net start mysqlMySQL 服务正在启动 ...MySQL 服务无法启动。系统出错。发生系统错误 1067。进程意外终止。 一定是配置文件里的某些配置不对(注意MySQL各个版本间的配置可能不一样) (8) your password has expired. to log in you must change it using a client that supports expired passwords 在mysql bin目录下用./mysql -u root -p 登录 账户密码过期，修改密码 (9) 忘记MySQL密码Linux下如果忘记MySQL的root密码，可以通过修改配置的方法，重置root密码修改MySQL的配置文件（默认为/etc/my.cnf）,在[mysqld]下添加一行skip-grant-tables保存配置文件后，重启MySQL服务 service mysqld restart再次进入MySQL命令行 mysql -uroot -p,输入密码时直接回车，就会进入MySQL数据库了，这个时候按照常规流程修改root密码即可。12345678910update user set password=password('newpassword') where user='root'; #MySQL-5.6修改密码命令update user set authentication_string=password('root') where user='root' ; #MySQL-5.7修改密码命令flush privileges;mysql&gt; SET PASSWORD = PASSWORD('123456');Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) (10) ERROR 1820 (HY000): You must reset your password1ERROR 1820 (HY000): You must reset your password using ALTER USER statement befo re executing this statement. 解决办法：12345678910111213141516Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt; show databases;# 第一次登录会提示你修改密码，修改密码就可以了，修改完重新登录。ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement. mysql&gt; SET PASSWORD = PASSWORD('abc2017qwer'); # 修改密码Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; alter user 'root'@'localhost' password expire never; # 设置密码永不过期Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges; # 把表里的修改同步到内存Query OK, 0 rows affected (0.00 sec)mysql&gt; exit;Bye (11) ERROR 1045 (28000): Access denied for user ‘root‘@’localhost’ (using password: YES)1ERROR 1045 (28000): Access denied for user &apos;root&apos;@&apos;localhost&apos; (using password: YES) 可能是密码输错了 root用户没有localhost登录的权限，在mysql数据库的user表里插入或者更新root用户的权限 (12) Establishing SSL connection without server’s identity verification is not recommended.1Wed Jun 14 10:47:21 CST 2017 WARN: Establishing SSL connection without server&apos;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&apos;t set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to &apos;false&apos;. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification. 连接MySQL的时候url参数里加上&amp;useSSL=false1url=jdbc:mysql://localhost:3306/text?useUnicode=true&amp;characterEncoding=UTF-8&amp;autoReconnect=true&amp;useSSL=false (13) 1215 cannot add foreign key constraint可能原因：（1）外键对应的字段数据类型不一致 （2）设置外键时“删除时”设置为“SET NULL” （3）两张表的存储引擎不一致 解决办法：12show table status from db_name where name=&apos;table_name&apos;;alter table table_name engine=innodb; (14) ERROR 2003 (HY000): Can’t connect to MySQL server on ‘172.16.1.23’ (113)113是防火墙的原因修改防火墙，允许3306端口访问，或者关闭防火墙123456/sbin/iptables -I INPUT -p tcp --dport 3306 -j ACCEPT service iptables stop #关闭防火墙systemctl stop firewalld #关闭防火墙 (15) java.io.IOException: java.sql.SQLException: Incorrect string value: ‘\xF0\x9F\x8C\xA3’ for column ‘page_title’ at row 892 UTF-8编码有可能是两个、三个、四个字节。Emoji表情是4个字节，而Mysql的utf8编码最多3个字节，所以数据插不进去。 解决方案就是：将Mysql的编码从utf8转换成utf8mb4。 1234567891011121314151617181920212223241. 修改my.cnf(windows为my.ini)[client] default-character-set = utf8mb4 [mysql]default-character-set=utf8mb4[mysqld]character-set-server = utf8mb4 #collation-server = utf8mb4_unicode_ci #character-set-client-handshake = FALSE #init_connect=&apos;SET NAMES utf8mb4&apos;2. 将已经建好的表也转换成utf8mb4alter database DATABASE_NAME character set utf8mb4 collate utf8mb4_general_ci; # 更改数据库编码 utf8mb4_unicode_cialter table TABLE_NAME convert to character set utf8mb4 collate utf8mb4_bin; # 更改表的编码 utf8mb4_unicode_cialter table TABLE modify COL varchar(50) CHARACTER SET utf8mb4; # 更改字段的编码修改后重启Mysql第三步可以省略3. 以root身份登录Mysql，修改环境变量，将character_set_client,character_set_connection,character_set_database,character_set_results,character_set_server 都修改成utf8mb4 (16) 1071 - Specified key was too long; max key length is 767 bytes12[SQL] alter table page convert to character set utf8mb4 collate utf8mb4_bin;[Err] 1071 - Specified key was too long; max key length is 767 bytes 数据库中的某两个字段设置unique索引的时候，出现了Specified key was too long; max key length is 767 bytes错误是Mysql的字段设置的太长了，于是我把这两个字段的长度改了一下就好了 (17) java.io.IOException: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry ‘15649431’ for key ‘PRIMARY’12Exception in thread &quot;main&quot; java.io.IOException: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry &apos;15649431&apos; for key &apos;PRIMARY&apos;Caused by: org.xml.sax.SAXException: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry &apos;15649431&apos; for key &apos;PRIMARY&apos; 主键重复了 (18) Cannot truncate a table referenced in a foreign key constraint12[SQL] truncate table `program`;[Err] 1701 - Cannot truncate a table referenced in a foreign key constraint (`m`.`task`, CONSTRAINT `FK_7kxuyirn7pko5dshnh5969xay` FOREIGN KEY (`pid`) REFERENCES `m`.`program` (`pid`)) 12sql : INSERT INTO page (page_id,page_namespace,page_title,page_restrictions,page_counter,page_is_redirect,page_is_new,page_random,page_touched,page_latest,page_len) VALUES (133506,0,'陳任','',0,0,0,RAND(),DATE_ADD('1970-01-01', INTERVAL UNIX_TIMESTAMP() SECOND)+0,38317407,3004) ...java.io.IOException: java.sql.SQLException: Incorrect string value: '\xA2\xE3' for column 'page_title' at row 642 (19) mysql error: Table “mysql”.“innodb_table_stats” not found123456789101112131415161718192021CREATE TABLE `innodb_index_stats` ( `database_name` varchar(64) COLLATE utf8_bin NOT NULL, `table_name` varchar(64) COLLATE utf8_bin NOT NULL, `index_name` varchar(64) COLLATE utf8_bin NOT NULL, `last_update` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `stat_name` varchar(64) COLLATE utf8_bin NOT NULL, `stat_value` bigint(20) unsigned NOT NULL, `sample_size` bigint(20) unsigned DEFAULT NULL, `stat_description` varchar(1024) COLLATE utf8_bin NOT NULL, PRIMARY KEY (`database_name`,`table_name`,`index_name`,`stat_name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin STATS_PERSISTENT=0;CREATE TABLE `innodb_table_stats` ( `database_name` varchar(64) COLLATE utf8_bin NOT NULL, `table_name` varchar(64) COLLATE utf8_bin NOT NULL, `last_update` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `n_rows` bigint(20) unsigned NOT NULL, `clustered_index_size` bigint(20) unsigned NOT NULL, `sum_of_other_index_sizes` bigint(20) unsigned NOT NULL, PRIMARY KEY (`database_name`,`table_name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin STATS_PERSISTENT=0; (20) Data truncated for column由数据类型的长度不一致导致的。alter table user modify column sex enum(‘0’,’1’); (21) ERROR 1290 (HY000): –secure-file-priv option so it cannot execute this statement1ERROR 1290 (HY000): The MySQL server is running with the --secure-file-priv option so it cannot execute this statement On Ubuntu 14 and Mysql 5.5.53 this setting seems to be enabled by default. To disable it you need to add secure-file-priv = “” to your my.cnf file under the mysqld config group. eg:-12[mysqld]secure-file-priv = &quot;&quot; 1234567Stop the MySQL server service by going into services.mscGo to C:\ProgramData\MySQL\MySQL Server 5.6 (ProgramData was a hidden folder in my case).Open the my.ini file in Notepad.Search for &apos;secure-file-priv&apos;.Comment the line out by adding &apos;#&apos; at the start of the line.Save the file.Start the MySQL server service by going into services.msc # (22) Can’t create/write to file ‘/tmp/ML6MLk2I’ (Errcode: 28 - No space left on device)1java.sql.BatchUpdateException: Can&apos;t create/write to file &apos;/tmp/ML6MLk2I&apos; (Errcode: 28 - No space left on device) (23) mysql如何查看自己数据库文件所在的位置 2016.11.12打开mysqml文件夹，显示的文件如下，然后找到my.ini文件,打开my.ini文件后，按ctrl+f搜索”datadir”就可以找到你数据库的物理路径。如果你修改数据库文件的存储位置，可以直接在这里修改。如果想对数据库进行搬迁，只需要复制”data“这个文件夹，覆盖新环境下的”data“文件就可以了。 (24) Got a packet bigger than ‘max_allowed_packet’ bytes导入文件时提示文件太大[Err] 1153 - Got a packet bigger than ‘max_allowed_packet’ bytes解决方法：在MySQL安装目录下找到文件my.ini，搜索[mysqld]，在其下面添加一句话max_allowed_packet=1024M （大小根据自己的情况定）重启MySQL服务（windows下 计算机 右键 管理 服务和应用程序 服务 找到mysql服务，重启）或者 以管理员身份运行cmd net mysql stop net mysql start (25) MySQL ibdata1文件太大如何缩小2016.3.1 MySql innodb如果是共享表空间，ibdata1文件越来越大，达到了30多个G，对一些没用的表进行清空：truncate table xxx;然后optimize table xxx; 没有效果因为对共享表空间不起作用。mysql ibdata1存放数据，索引等，是MYSQL的最主要的数据。 如果不把数据分开存放的话，这个文件的大小很容易就上了G，甚至几十G。对于某些应用来说，并不是太合适。因此要把此文件缩小。无法自动收缩，必须数据导出，删除ibdata1，然后数据导入，比较麻烦，因此需要改为每个表单独的文件。 解决方法：数据文件单独存放(共享表空间如何改为每个表独立的表空间文件)。步骤如下：1234567891011121314151617181920212223242526272829303132333435361）备份数据库从命令行进入MySQL Server 5.0\bin备份全部数据库，执行命令D:\&gt;mysqldump -q -u mysql -ppassword --add-drop-table --all-databases &gt; c:/all.sql做完此步后，停止数据库服务。2）找到my.ini或my.cnf文件linux下执行 ./mysqld --verbose --help | grep -A 1 &apos;Default options&apos;会有类似显示：Default options are read from the following files in the given order:/etc/my.cnf ~/.my.cnf /usr/local/service/mysql3306/etc/my.cnfwindows环境下可以：mysqld --verbose --help &gt; mysqlhelp.txtnotepad mysqlhelp.txt在里面查找Default options，可以看到查找my.ini的顺序，以找到真实目录3）修改mysql配置文件打开my.ini或my.cnf文件[mysqld]下增加下面配置innodb_file_per_table=1验证配置是否生效，可以重启mysql后,执行show variables like &apos;%per_table%&apos;看看innodb_file_per_table变量是否为ON4）删除原数据文件删除原来的ibdata1文件及日志文件ib_logfile*，删除data目录下的应用数据库文件夹(mysql文件夹不要删)5）还原数据库启动数据库服务从命令行进入MySQL Server 5.0\bin还原全部数据库，执行命令mysql -uusername -pyourpassword &lt; c:/all.sql经过以上几步后，可以看到新的ibdata1文件就只有几十M了，数据及索引都变成了针对单个表的小ibd文件了，它们在相应数据库的文件夹下面。 (26) 同一局域网内一台电脑(windows)访问另一条电脑(windows)上的mysql 被连接的那台电脑 打开3306端口 允许远程用户访问(把mysql数据库user表里的host改成%) 关闭防火墙备注：和telnet没关系 (27) can not connet mysql 1006012345mysqld_safe The file /usr/local/mysql/bin/mysqlddoes not exist or is not executable. Please cd to the mysql installationdirectory and restart this script from there as follows:./bin/mysqld_safe&amp;See http://dev.mysql.com/doc/mysql/en/mysqld-safe.html for more information mysqld_safe只认识/usr/local/mysql/bin/mysqld路径的mysqld 于是调整在当前目录创建了一个软链接，问题解决。 [root@edu local]# ln -s mysql5.7/ mysql (28) ERROR 2013 (HY000): Lost connection to MySQL server at ‘reading initial communication packet’, system error: 1101ERROR 2013 (HY000): Lost connection to MySQL server at &apos;reading initial communication packet&apos;, system error: 110 mysql设置文件中“bind-address”值的问题； 访问权限限制问题； 防火墙、杀毒软件阻拦的问题（特别是Windows）； 负载过大、最大连接限制了访问（特别正式提供服务的mysql） 网络问题 Lost connection to MySQL server at ‘reading initial communication packet’, system error: 0 References[1] MySQL-5.7 官方文档[2] MySQL-5.6 创建用户[3] 安装完绿色版MySQL修改密码：ERROR 1820 (HY000): You must reset your password using ALTER USER statement befo re executing this statement.[4] alter user[5] 让Mysql支持Emoji表情[6] MySQL乱码问题以及utf8mb4字符集[7] 清官谈mysql中utf8和utf8mb4区别[8] mysql-error-table-mysql-innodb-table-stats-not-found[9] Incorrect string value: ‘\xF0\x9F…’ for column ‘XXX’ at row [10] charset-unicode-conversion[11] mysql-utf8mb4[12] MySQL server PID file could not be found![13] Mysql JDBC 连接串参数说明[14] connector-j-reference-configuration-properties[15] mysql导出导入文件问题整理[16] how-should-i-tackle-secure-file-priv-in-mysql [17] Mysql导出逗号分隔的csv文件[18] mysql导出数据 null值被处理成\N[19] mysql-writing-file-error-errcode-28[20] mysql-cant-create-write-to-file-tmp-sql-3c6-0-myi-errcode-2-what-does [21] Mysql删除数据后，磁盘空间未释放的解决办法]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[word2vec]]></title>
    <url>%2F2017%2F06%2F21%2Fword2vec%2F</url>
    <content type="text"><![CDATA[word2vec的使用和学习… word2vec使用 先跑起来，跑出一个结果，然后再详细了解。 word2vec有多个版本，我是用NLPchina的这个版本 代码下载12git clone https://github.com/NLPchina/Word2VEC_java.gitcd Word2VEC_java 准备训练方法需要的语料 在Word2VEC_java目录下新建library目录，然后下载语料到这个路径下。 语料要求是分完词的文件，一行是分完词的一句话或者一篇文章。行数不限。 训练模型修改Word2VEC_java/src/main/java/com/ansj/vec/Learn.java文件中的main函数为：12345Learn learn = new Learn();long start = System.currentTimeMillis();learn.learnFile(new File(&quot;library/corpus.txt&quot;)); // 语料的路径System.out.println(&quot;use time &quot; + (System.currentTimeMillis() - start));learn.saveModel(new File(&quot;library/javaVector.model&quot;)); // 模型的路径 执行main方法 1234567891011121314151617181920alpha:0.025 Progress: 0%alpha:0.02498111730866601 Progress: 0%alpha:0.024964508501374084 Progress: 0%alpha:0.02494698416965384 Progress: 0%alpha:0.024929642279395765 Progress: 0%alpha:0.024912590636918405 Progress: 0%...alpha:1.4140374308845295E-4 Progress: 99%alpha:1.245129808095463E-4 Progress: 99%alpha:1.043813947391059E-4 Progress: 99%alpha:8.721199459071072E-5 Progress: 99%alpha:6.699748058291988E-5 Progress: 99%alpha:4.9765055202141695E-5 Progress: 99%alpha:3.306005150289226E-5 Progress: 99%Vocab size: 348846Words in train file: 15073327sucess train over!use time 2519393 以上就完成了本文的目标，生成的模型文件路径为：library/javaVector.model。训练之后，下一步的目标就是进行得到词向量之后计算句向量了，这就是后文的目标了。 123456789101112131415161718192021222324import java.io.IOException;import com.ansj.vec.Word2VEC;public class Test &#123; public static void main(String[] args) throws IOException &#123; Word2VEC w1 = new Word2VEC() ; //w1.loadGoogleModel(&quot;C:/WorkSpaces/NLP/Word2VEC_java/library/javaVector.model&quot;) ; w1.loadJavaModel(&quot;C:/WorkSpaces/NLP/Word2VEC_java/library/javaVector.model&quot;); System.out.println(w1.distance(&quot;军事&quot;)); &#125;&#125;运行结果(结果是一行，因为展示原因，我放到多行)：[军队 0.62896645, 战争 0.5970405, 转兵 0.57379395, 部队 0.5631516, 79天 0.5608308, 轴心国 0.5573783, 畴型 0.5520164, 反革命战争 0.54803735, 武力 0.5451811, 李德作主 0.5450375, clausewity 0.5428826, 十有八九学校 0.53841996, 颠复 0.53727686, 克劳塞维茨 0.5356221, 核武器 0.5323216, 时分时合 0.53146726, 武器 0.53097516, 费一枪 0.5285448, 678号 0.5270526, 核武库 0.52657473, 鲁登道夫 0.5246394, 近卫军 0.5244502, 后延 0.5233266, 武装 0.5232187, 博古作主 0.52262974, 叛匪 0.5221401, 反苏 0.522086, 英法 0.5194732, 武一文 0.518501, 政变 0.5179417, 金门岛 0.51790655, 军事化 0.5175235, 非党化 0.5155129, 和谈 0.51519865, 陆基洲际战略 0.5141574, 武装力量 0.5139615, 人民解放军 0.5128906, 宣传战 0.5125072, 明春可 0.5116397] References[1] http://blog.csdn.net/android_ruben/article/details/65935621[2] https://code.google.com/archive/p/word2vec/[3] https://my.oschina.net/magicly007/blog/851583 用word2vec分析中文维基语料库[4] http://www.52nlp.cn/中英文维基百科语料上的word2vec实验[5] http://blog.csdn.net/itplus/article/details/37969519 word2vec 中的数学原理详解（一）目录和前言[6] http://blog.csdn.net/itplus/article/details/37969635 word2vec 中的数学原理详解（二）预备知识[7] http://blog.csdn.net/itplus/article/details/37969817 word2vec 中的数学原理详解（三）背景知识[8] http://blog.csdn.net/itplus/article/details/37969979 word2vec 中的数学原理详解（四）基于 Hierarchical Softmax 的模型[9] http://blog.csdn.net/itplus/article/details/37998797 word2vec 中的数学原理详解（五）基于 Negative Sampling 的模型[10] http://blog.csdn.net/itplus/article/details/37999613 word2vec 中的数学原理详解（六）若干源码细节[11] http://blog.csdn.net/github_31448565/article/details/48598611 word2vec源码详细赏析（一）[12] http://cikuapi.com/index.php?content=%E6%90%9C%E7%8B%90&amp;bs= 在线word2vec网站]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 笔记]]></title>
    <url>%2F2017%2F06%2F14%2Flinux-notes%2F</url>
    <content type="text"><![CDATA[(1) 新linux机器配置 en_US.UTF-8 zh_CN.UTF-8 都是 UTF-8编码 en_US.UTF-8：你说英语，你在美国，字符集是utf-8 zh_CN.UTF-8：你说中文，你在中国，字符集是utf-8 建议设置成 en_US.UTF-8，因为命令行的结果是英文，不用担心乱码。 (1.1) 配置编码 配置文件一般在 /etc/sysconfig/i18n /etc/locale.conf (1.1.1) 查看当前区域编码配置 localectl status1234[wkq@VM_77_25_centos ~]$ localectl status System Locale: LANG=en_US.utf8 VC Keymap: us X11 Layout: us (1.1.2) 查看系统默认的语言设置 locale1234567891011121314151617[wkq@VM_77_25_centos ~]$ localelocale: Cannot set LC_CTYPE to default locale: No such file or directorylocale: Cannot set LC_ALL to default locale: No such file or directoryLANG=en_US.utf8LC_CTYPE=UTF-8LC_NUMERIC="en_US.utf8"LC_TIME="en_US.utf8"LC_COLLATE="en_US.utf8"LC_MONETARY="en_US.utf8"LC_MESSAGES="en_US.utf8"LC_PAPER="en_US.utf8"LC_NAME="en_US.utf8"LC_ADDRESS="en_US.utf8"LC_TELEPHONE="en_US.utf8"LC_MEASUREMENT="en_US.utf8"LC_IDENTIFICATION="en_US.utf8"LC_ALL= (1.1.3) 查看系统支持的汉语区域语言 localectl list-locales | grep zh1234567891011121314151617[wkq@VM_77_25_centos ~]$ localectl list-locales | grep zhzh_CNzh_CN.gb18030zh_CN.gb2312zh_CN.gbkzh_CN.utf8zh_HKzh_HK.big5hkscszh_HK.utf8zh_SGzh_SG.gb2312zh_SG.gbkzh_SG.utf8zh_TWzh_TW.big5zh_TW.euctwzh_TW.utf8 (1.1.4) 设置语言 localectl set-locale LANG=en_US.utf812345678[root@VM_77_25_centos ~]# localectl set-locale LANG=en_US.utf8[root@VM_77_25_centos ~]# [root@VM_77_25_centos ~]# source /etc/locale.conf [root@VM_77_25_centos ~]# [root@VM_77_25_centos ~]# [root@VM_77_25_centos ~]# tail -100 /etc/locale.confLANG=en_US.utf8[root@VM_77_25_centos ~]# Centos7 locale区域语言设置 (1.2) VI编辑器设置配置 ~/.vimrc 加上如下几行： 12345set fileencodings=utf-8,ucs-bom,gb18030,gbk,gb2312,cp936set termencoding=utf-8set encoding=utf-8set nu (1.3) 其它配置12345678910111213141516171819202122alias ll='ls -l'# Java JDK config export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Homeexport PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar# Maven configexport MAVEN_HOME=/Users/weikeqin1/SoftWare/apache-maven-3.6.1export NODE_PATH=/Users/weikeqin1/SoftWare/node/node-v10.16.0-darwin-x64export PATH=$PATH:$MAVEN_HOME/bin:$NODE_PATH/bin:$NODE_PATH/node_global# go config export GOROOT=/usr/local/goexport GOPATH=/Users/weikeqin1/SoftWare/gopathexport PATH=$PATH:$GOROOT/bin:$GOPATH/bin# Setting PATH for Python 2.7PATH="/System/Library/Frameworks/Python.framework/Versions/2.7/bin:$&#123;PATH&#125;"# Setting PATH for Python 3.7PATH="/Library/Frameworks/Python.framework/Versions/3.7/bin:$&#123;PATH&#125;"export PATH (2) linux常用命令(2.1) 远程登录Linux远程登录 ssh wkq@192.168.66.88 (2.2) Linux文件和目录1234567[admin@user01 software]$ lltotal 16-rw-rw-r-- 1 admin admin 425 Jun 16 14:32 HelloWorld.class-rw-rw-r-- 1 admin admin 110 Jun 16 14:31 HelloWorld.java-rwxrwxrwx 1 usdp user1 9271609 Aug 16 11:49 apache-tomcat-8.5.4.tar.gz-rwxrwxrwx 1 root root 185540433 Aug 16 10:36 jdk-8u131-linux-x64.tar.gz-rwxrwxrwx 1 root root 641555814 Aug 16 10:35 mysql-5.7.16-linux-glibc2.5-x86_64.tar.gz 最前面那个 – 代表的是类型rwx 代表的是 可读、可写、可执行中间那三个 rw- 代表的是所有者（user）然后那三个 rw- 代表的是组群（group）最后那三个 r–- 代表的是其他人（other） 是追加内容 &gt; 是覆盖原有内容 find命令 find ./company -type f | wc -l grep命令grep -rn &quot;字符串&quot; * sed 命令统计文件个数123456789ls -l |grep &quot;^-&quot;|wc -l #统计当前目录下文件的个数ls -l |grep &quot;^d&quot;|wc -l #统计当前目录下目录的个数ls -lR|grep &quot;^-&quot;|wc -l #统计当前目录下文件的个数，包括子目录里的 find ./company -type f | wc -lls -lR|grep &quot;^d&quot;|wc -l #统计目录下目录的个数，包括子目录里的 (2.3) 用户和用户组(2.3.1) 添加用户并设置密码 查看用户 tail -10 /etc/passwd 添加用户 useradd -m user_admin 设置密码 passwd user_admin ******** 123456789101112131415[root@VM_77_25_centos ~]$ tail -6 /etc/passwdchrony:x:995:993::/var/lib/chrony:/sbin/nologinsshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologintcpdump:x:72:72::/:/sbin/nologinwkq:x:1000:1000::/home/wkq:/bin/bashuser_admin:x:1001:1001::/home/user_admin:/bin/bashuser_1:x:1002:1002::/home/user_1:/bin/bash[root@VM_77_25_centos ~]# [root@VM_77_25_centos ~]# useradd -m user_admin[root@VM_77_25_centos ~]# passwd user_adminChanging password for user user_admin.New password:Retype new password:passwd: all authentication tokens updated successfully.[root@VM_77_25_centos ~]# (2.3.2) 用户所属组123cat /etc/group 查看用户组cat /etc/passwd|grep -v nologin|grep -v halt|grep -v shutdown|awk -F&quot;:&quot; &apos;&#123; print $1&quot;|&quot;$3&quot;|&quot;$4 &#125;&apos;|more (2.4) Linux磁盘管理(2.5) VI文本编辑器 编辑文本 vi test.txt (2.6) 压缩和解压缩(2.7) 安装RPM或源码包(2.8) shell(2.9) 正则(3) 常用运维监控命令df1234567891011121314151617181920212223242526272829303132[wkq@host-wkq support-files]$ df -hlFilesystem Size Used Avail Use% Mounted on/dev/mapper/rhel-root 8.0G 6.8G 1.3G 84% /devtmpfs 16G 0 16G 0% /devtmpfs 16G 0 16G 0% /dev/shmtmpfs 16G 177M 16G 2% /runtmpfs 16G 0 16G 0% /sys/fs/cgroup/dev/mapper/rhel-mnt_disk1 1014M 33M 982M 4% /mnt/disk1/dev/mapper/rhel-home 1017M 184M 834M 19% /home/dev/vda1 497M 125M 373M 25% /boottmpfs 3.2G 0 3.2G 0% /run/user/0/dev/mapper/datavg-data01 1023G 251G 773G 25% /data00tmpfs 3.2G 0 3.2G 0% /run/user/501tmpfs 3.2G 0 3.2G 0% /run/user/1001tmpfs 3.2G 0 3.2G 0% /run/user/1006[wkq@host-wkq support-files]$ df -iFilesystem Inodes IUsed IFree IUse% Mounted on/dev/mapper/rhel-root 5498672 90856 5407816 2% /devtmpfs 4095200 370 4094830 1% /devtmpfs 4097815 1 4097814 1% /dev/shmtmpfs 4097815 560 4097255 1% /runtmpfs 4097815 13 4097802 1% /sys/fs/cgroup/dev/mapper/rhel-mnt_disk1 1048576 3 1048573 1% /mnt/disk1/dev/mapper/rhel-home 1044480 4599 1039881 1% /home/dev/vda1 512000 328 511672 1% /boottmpfs 4097815 1 4097814 1% /run/user/0/dev/mapper/datavg-data01 1072693248 6508644 1066184604 1% /data00tmpfs 4097815 1 4097814 1% /run/user/501tmpfs 4097815 1 4097814 1% /run/user/1001tmpfs 4097815 1 4097814 1% /run/user/1006 lsblk12345678910111213141516171819202122232425262728lsblk[root@host local]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTvda 252:0 0 22G 0 disk ├─vda1 252:1 0 500M 0 part /boot└─vda2 252:2 0 18G 0 part ├─rhel-root 253:0 0 8G 0 lvm / ├─rhel-swap 253:1 0 8G 0 lvm [SWAP] ├─rhel-home 253:2 0 1020M 0 lvm /home └─rhel-mnt_disk1 253:3 0 1G 0 lvm /mnt/disk1vdb 252:16 0 1T 0 disk └─datavg-data01 253:4 0 1023G 0 lvm /data00``` ## 查看操作系统信息uname -a```text[root@localhost ~]# uname -aLinux localhost.localdomain 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linuxlocalhost.localdomain 主机名3.10.0-327.el7.x86_64 内核版本#1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 内核编译日期x86_64 操作系统版本x86_64 处理器类型x86_64 硬件平台GNU/Linux 操作系统 查看cpu统计信息1234567891011121314151617181920212223[root@localhost local]# lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 2On-line CPU(s) list: 0,1Thread(s) per core: 1Core(s) per socket: 1Socket(s): 2NUMA node(s): 1Vendor ID: GenuineIntelCPU family: 6Model: 61Model name: Intel Core Processor (Broadwell)Stepping: 2CPU MHz: 2099.998BogoMIPS: 4199.99Hypervisor vendor: KVMVirtualization type: fullL1d cache: 32KL1i cache: 32KL2 cache: 4096KNUMA node0 CPU(s): 0,1 每个cpu信息1[root@localhost local]# cat /proc/cpuinfo 概要查看内存情况1234[root@localhost local]# free -m (单位是MB) total used free shared buff/cache availableMem: 15887 215 14819 8 852 15378Swap: 8191 0 8191 查看内存详细使用1[root@localhost ~]# cat /proc/meminfo 查看内存硬件信息1[root@localhost ~]# dmidecode -t memory 查看硬盘和分区分布12345678[root@localhost ~]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTvda 252:0 0 100G 0 disk ├─vda1 252:1 0 512M 0 part /boot└─vda2 252:2 0 99.5G 0 part ├─osvg-lv_root 253:0 0 80G 0 lvm / └─osvg-lv_swap 253:1 0 8G 0 lvm [SWAP]vdb 252:16 0 500G 0 disk 看硬盘和分区的详细信息1234567891011121314151617181920212223242526272829[root@localhost ~]# fdisk -lDisk /dev/vda: 107.4 GB, 107374182400 bytes, 209715200 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x0009350c Device Boot Start End Blocks Id System/dev/vda1 * 2048 1050623 524288 83 Linux/dev/vda2 1050624 209715199 104332288 8e Linux LVMDisk /dev/mapper/osvg-lv_root: 85.9 GB, 85899345920 bytes, 167772160 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk /dev/mapper/osvg-lv_swap: 8589 MB, 8589934592 bytes, 16777216 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk /dev/vdb: 536.9 GB, 536870912000 bytes, 1048576000 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes 定时执行任务 定时执行任务使用crontab命令 首先自己写好shell脚本，然后使用crontab命令调用。 下面是我的crontab配置和对应的脚本 在linux终端输入crontab -e就可以调出定时器的设置vi界面。然后输入下面的语句就可以了 00 01 * /usr/local/selectwritefile.sh 查看是否创建成功 crontab -l 启动crontab服务1234567[admin@localhost selectwritefile]$ /sbin/service crond startRedirecting to /bin/systemctl start crond.service==== AUTHENTICATING FOR org.freedesktop.systemd1.manage-units ===Authentication is required to manage system services or units.Authenticating as: rootPassword: ==== AUTHENTICATION COMPLETE === 123456789101112131415#!/bin/bash# selectwritefile.sh# program:# 定时执行java脚本，把数据库中的数据写到磁盘#导入环境变量 export LANG=&quot;en_US.UTF-8&quot; export JAVA_HOME=/home/admin/software/jdk1.8.0_111export PATH=$JAVA_HOME/bin:$PATH export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarecho -e &quot;[`date +%Y-%m-%d_%H:%M:%S`] 开始执行linux shell脚本&quot; # 注意 这儿路径用绝对路径，用相对路径会导致找不见jar包，java脚本不能运行java -jar /home/admin/workspace/selectwritefile/selectwritefile-0.0.1.jar /home/admin/corpusecho -e &quot;[`date +%Y-%m-%d_%H:%M:%S`] linux shell脚本运行完毕。&quot; crontab参数说明12345678910For details see man 4 crontabsExample of job definition:.---------------- minute (0 - 59)| .------------- hour (0 - 23)| | .---------- day of month (1 - 31)| | | .------- month (1 - 12) OR jan,feb,mar,apr ...| | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat| | | | |* * * * * user-name command to be executed 在crontab文件中写入需要执行的命令和时间，该文件中每行都包括六个域，其中前五个域是指定命令被执行的时间，最后一个域是要被执行的命令。每个域之间使用空格或者制表符分隔。 除了root用户之外的用户可以执行crontab配置计划任务。所有用户定义的crontab存储在目录/var/spool/cron下，任务会以创建者的身份被执行。要以特定用户创建一个crontab，先以该用户登录，执行命令crontab -e，系统会启动在VISUAL或者EDITOR中指定的的编辑软件编辑crontab。文件内容与/etc/crontab格式相同。 当更改的crontab需要保存时，文件会保存在成如下文件/var/spool/cron/username。文件名会根据用户名而不同。 cron服务会每分钟检查一次/etc/crontab、/etc/cron.d/、/var/spool/cron文件下的变更。如果发现变化，就会下载到存储器中。 因此，即使crontab文件改变了，程序也不需要重新启动。 推荐自定义的任务使用crontab -e命令添加， 退出后用/etc/init.d/crond restart命令重启crond进程，官方文件说不用重启进程，但我遇到不重启无法运行任务的情况。 开始不知道/etc/crontab文件中的run-parts是什么意思，直接把命令按照/etc/crontab的格式加上总是无法运行， 后来才知道run-parts是指后面跟着的是文件夹。 crontab服务的启动关闭1234/sbin/service crond start //启动服务/sbin/service crond stop //关闭服务/sbin/service crond restart //重启服务/sbin/service crond reload //重新载入配置 查看进程12345678ps -ef ps -ef | grep javatopls /proc/ # 查看进程cd 26489/ #进入该进程对应文件ls -ail #查看详细信息 (4) 常见问题Permission denied1-bash: ./test.jar: Permission denied 权限不够使用chmod命令修改权限1chmod 754 test.jar Cannot mkdir: Permission denied权限不够使用chmod命令修改权限 bin/bash: bad interpreter: No such file or directory有可能是你的脚本文件是DOS格式的, 即每一行的行尾以/r/n来标识使用vim编辑器打开脚本, 运行::set ff?可以看到DOS或UNIX的字样. 使用set ff=unix把它强制为unix格式的, 然后保存退出, 即可。 Failed to restart iptables.service: Unit iptables.service failed to load: No such file or directory.12345678910111213141516iptables --list #查看防火墙上的可用规则下例说明当前系统没有定义防火墙，你可以看到，它显示了默认的filter表，以及表内默认的input链, forward链, output链。[root@host_wkq local]# iptables --listChain INPUT (policy ACCEPT)target prot opt source destination Chain FORWARD (policy ACCEPT)target prot opt source destination Chain OUTPUT (policy ACCEPT)target prot opt source destination [root@host_wkq local]# service iptables stop #关闭防火墙systemctl stop firewalld #关闭防火墙 No space left on device-bash: vim: command not found12345678910[usdp@host-172-16-11-125 workspaces]$ rpm -qa|grep vimvim-minimal-7.4.160-1.el7.x86_64vim-filesystem-7.4.160-1.el7_3.1.x86_64vim-common-7.4.160-1.el7_3.1.x86_64vim-enhanced-7.4.160-1.el7_3.1.x86_64yum -y install vim-enhancedyum -y install vim* account locked due to 17 failed logins登陆次数过多后的处理 Account locked due to failed logins1pam_tally2 --user=username --reset linux登录后出现-bash-4.1$，解决办法以及造成这样的原因12345678910111213##首先切换到故障用户su - test##复制对应的文件（不要用root直接复制，否则复制过去的东西属主，数组都是root的）-bash-4.1$ cp /etc/skel/.bash* ~ ##(/etc/skel 新用户老家的样子，所以从这里复制)-bash-4.1$ ls -latotal 24drwx------ 2 test test 4096 Nov 5 14:51 .drwxr-xr-x. 6 root root 4096 Nov 5 14:44 ..-rw------- 1 test test 21 Nov 5 14:45 .bash_history-rw-r--r-- 1 test test 18 Nov 5 14:51 .bash_logout-rw-r--r-- 1 test test 176 Nov 5 14:51 .bash_profile-rw-r--r-- 1 test test 124 Nov 5 14:51 .bashrc-bash-4.1$ logout References[1] Linux教程[2] Linux文件系统目录结构[3] Linux磁盘简介[4] Linux下定时运行脚本[5] bin/bash: bad interpreter: No such file or directory[6] bin/bash: bad interpreter: No such file or directory[7] Linux下定时执行脚本[8] Linux Crontab 定时任务 命令详解[9] Linux通过PID查看进程完整信息[10] linux 中防火墙配置 iptables 命令参数的含义介绍[11] 在linux中使用vi 打开文件时，能显示行号[12] 新建用户到指定的目录[13] Centos7 locale区域语言设置]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j Cypher 笔记]]></title>
    <url>%2F2017%2F06%2F09%2Fcypher%2F</url>
    <content type="text"><![CDATA[遇到的问题修改Label、删除Label、增加Label neo4j是no-schema的，允许一个节点有0-n个label，也就是可以没有Label，可以有一个Label，也可以有多个 删除Label match (n) where id(n) = 2 remove n:Label return n; 123456789101112131415161718192021222324252627neo4j-sh (?)$ match (n) where id(n) = 2 set n:E return id(n), labels(n), n;+------------------------------------------------+| id(n) | labels(n) | n |+------------------------------------------------+| 2 | [&quot;Person&quot;,&quot;T&quot;,&quot;E&quot;] | Node[2]&#123;name:&quot;t&quot;&#125; |+------------------------------------------------+1 rowLabels added: 1136 msneo4j-sh (?)$ match (n) where id(n) = 2 remove n:E return id(n), labels(n), n;+--------------------------------------------+| id(n) | labels(n) | n |+--------------------------------------------+| 2 | [&quot;Person&quot;,&quot;T&quot;] | Node[2]&#123;name:&quot;t&quot;&#125; |+--------------------------------------------+1 rowLabels removed: 1107 msneo4j-sh (?)$ match (n) where id(n) = 2 remove n:Person:T return id(n), labels(n), n;+---------------------------------------+| id(n) | labels(n) | n |+---------------------------------------+| 2 | [] | Node[2]&#123;name:&quot;t&quot;&#125; |+---------------------------------------+1 rowLabels removed: 246 ms 可以看到，第一次操作，原来id=2的节点有3个Label Person、T、E ，移除一个E后，还有2个第二次操作，原来id=2的节点有2个Label Person、T ，移除两个后，有0个 修改Label123456789101112131415161718neo4j-sh (?)$ match (n) where id(n) = 2 set n:E return id(n), labels(n), n;+---------------------------------------+| id(n) | labels(n) | n |+---------------------------------------+| 2 | [&quot;E&quot;] | Node[2]&#123;name:&quot;t&quot;&#125; |+---------------------------------------+1 rowLabels added: 113 msneo4j-sh (?)$ match (n) where id(n) = 2 set n:T return id(n), labels(n), n;+---------------------------------------+| id(n) | labels(n) | n |+---------------------------------------+| 2 | [&quot;T&quot;,&quot;E&quot;] | Node[2]&#123;name:&quot;t&quot;&#125; |+---------------------------------------+1 rowLabels added: 129 ms 不remove，直接set，会导致修改完有两个Label，所以需要先remove，然后再set (先set再remove也可以) 12 增加Labelneo4j里如何判断一个属性是什么类型123456789101112131415CREATE (n &#123;a:1, b:&quot;a&quot;, c:[1,2,3]&#125;)MATCH (n)RETURN size(n.a),CASE n.aWHEN toInt(n.a)THEN &apos;int&apos;WHEN toFloat(n.a)THEN &apos;float&apos;WHEN toString(n.a)THEN &apos;string&apos;WHEN [x IN n.a | x]THEN &apos;coll&apos;WHEN NULL THEN &apos;null&apos;ELSE &apos;unknown&apos; END , size(n.b), size(n.c) how-to-determine-property-value-type-within-a-node-in-neo4j 基本语法123456789unwind // 把列转为单独的行// With UNWIND, any list can be transformed back into individual rows.// The example matches all names from a list of names.with // 变量传递// The WITH syntax is similar to RETURN. It separates query parts explicitly,// allowing you to declare which variables to carry over to the next part. 12345678910// 查询所有关系的节点的idmatch (n)-[r]-(m) with count(r) as count, id(n) as idwhere count &gt; 0return collect(id);// 查询孤立节点match (n) where not (n)--() return n;// 查询孤立节点的所有标签match (n) where not (n)--() return distinct (labels(n)); Introduction to Cypher123456789101112Chapter 3. Introduction to CypherTable of Contents3.1. Background and Motivation3.2. Graphs, Patterns, and Cypher3.3. Patterns in Practice3.4. Getting the Results You Want3.5. How to Compose Large Statements3.6. Labels, Constraints and Indexes3.7. Loading Data3.8. Utilizing Data Structures3.9. Cypher vs. SQL This friendly guide will introduce you to Cypher, Neo4j’s query language. The guide will help you: start thinking about graphs and patterns, apply this knowledge to simple problems, learn how to write Cypher statements, use Cypher for loading data, transition from SQL to Cypher. If you want to keep a reference at your side while reading, please see the Cypher Refcard. Background and MotivationCypher provides a convenient way to express queries and other Neo4j actions. Although Cypher is particularly useful for exploratory work, it is fast enough to be used in production. Java-based approaches (eg, unmanaged extensions) can also be used to handle particularly demanding use cases. Query processingTo use Cypher effectively, it’s useful to have an idea of how it works. So, let’s take a high-level look at the way Cypher processes queries. Parse and validate the query. Generate the execution plan. Locate the initial node(s). Select and traverse relationships. Change and/or return values. PreparationParsing and validating the Cypher statement(s) is important, but mundane. However, generating an optimal search strategy can be far more challenging. The execution plan must tell the database how to locate initial node(s), select relationships for traversal, etc. This involves tricky optimization problems (eg, which actions should happen first), but we can safely leave the details to the Neo4j engineers. So, let’s move on to locating the initial node(s). Locate the initial node(s)Neo4j is highly optimized for traversing property graphs. Under ideal circumstances, it can traverse millions of nodes and relationships per second, following chains of pointers in the computer’s memory. However, before traversal can begin, Neo4j must know one or more starting nodes. Unless the user (or, more likely, a client program) can provide this information, Neo4j will have to search for these nodes. A “brute force” search of the database (eg, for a specified property value) can be very time consuming. Every node must be examined, first to see if it has the property, then to see if the value meets the desired criteria. To avoid this effort, Neo4j creates and uses indexes. So, Neo4j uses a separate index for each label/property combination. Traversal and actionsOnce the initial nodes are determined, Neo4j can traverse portions of the graph and perform any requested actions. The execution plan helps Neo4j to determine which nodes are relevant, which relationships to traverse, etc. Graphs, Patterns, and CypherNodes, Relationships, and PatternsNeo4j’s Property Graphs are composed of nodes and relationships, either of which may have properties (ie, attributes). Nodes represent entities (eg, concepts, events, places, things); relationships (which may be directed) connect pairs of nodes. However, nodes and relationships are simply low-level building blocks. The real strength of the Property Graph lies in its ability to encode patterns of connected nodes and relationships. A single node or relationship typically encodes very little information, but a pattern of nodes and relationships can encode arbitrarily complex ideas. Cypher, Neo4j’s query language, is strongly based on patterns. Specifically, patterns are used to match desired graph structures. Once a matching structure has been found (or created), Neo4j can use it for further processing. Simple and Complex PatternsA simple pattern, which has only a single relationship, connects a pair of nodes (or, occasionally, a node to itself). For example, a Person LIVES_IN a City or a City is PART_OF a Country. Complex patterns, using multiple relationships, can express arbitrarily complex concepts and support a variety of interesting use cases. For example, we might want to match instances where a Person LIVES_IN a Country. The following Cypher code combines two simple patterns into a (mildly) complex pattern which performs this match:1(:Person) -[:LIVES_IN]-&gt; (:City) -[:PART_OF]-&gt; (:Country) Pattern recognition is fundamental to the way that the brain works. Consequently, humans are very good at working with patterns. When patterns are presented visually (eg, in a diagram or map), humans can use them to recognize, specify, and understand concepts. As a pattern-based language, Cypher takes advantage of this capability. Cypher ConceptsLike SQL (used in relational databases), Cypher is a textual, declarative query language. It uses a form of ASCII art to represent graph-related patterns. SQL-like clauses and keywords (eg, MATCH, WHERE, DELETE) are used to combine these patterns and specify desired actions. This combination tells Neo4j which patterns to match and what to do with the matching items (eg, nodes, relationships, paths, collections). However, as a declarative language, Cypher does not tell Neo4j how to find nodes, traverse relationships, etc. (This level of control is available from Neo4j’s Java APIs, see Section 32.2, “Unmanaged Extensions”) Diagrams made up of icons and arrows are commonly used to visualize graphs; textual annotations provide labels, define properties, etc. Cypher’s ASCII-art syntax formalizes this approach, while adapting it to the limitations of text. Node SyntaxCypher uses a pair of parentheses (usually containing a text string) to represent a node, eg: (), (foo). This is reminiscent of a circle or a rectangle with rounded end caps. Here are some ASCII-art encodings for example Neo4j nodes, providing varying types and amounts of detail:123456()(matrix)(:Movie)(matrix:Movie)(matrix:Movie &#123;title: &quot;The Matrix&quot;&#125;)(matrix:Movie &#123;title: &quot;The Matrix&quot;, released: 1997&#125;) The simplest form, (), represents an anonymous, uncharacterized node. If we want to refer to the node elsewhere, we can add an identifier, eg: (matrix). Identifiers are restricted (ie, scoped) to a single statement: an identifier may have different (or no) meaning in another statement. The Movie label (prefixed in use with a colon) declares the node’s type. This restricts the pattern, keeping it from matching (say) a structure with an Actor node in this position. Neo4j’s node indexes also use labels: each index is specific to the combination of a label and a property. The node’s properties (eg, title) are represented as a list of key/value pairs, enclosed within a pair of braces, eg: {…}. Properties can be used to store information and/or restrict patterns. For example, we could match nodes whose title is “The Matrix”. Relationship SyntaxCypher uses a pair of dashes (–) to represent an undirected relationship. Directed relationships have an arrowhead at one end (eg, &lt;–, –&gt;). Bracketed expressions (eg: […]) can be used to add details. This may include identifiers, properties, and/or type information, eg:12345--&gt;-[role]-&gt;-[:ACTED_IN]-&gt;-[role:ACTED_IN]-&gt;-[role:ACTED_IN &#123;roles: [&quot;Neo&quot;]&#125;]-&gt; The syntax and semantics found within a relationship’s bracket pair are very similar to those used between a node’s parentheses. An identifier (eg, role) can be defined, to be used elsewhere in the statement. The relationship’s type (eg, ACTED_IN) is analogous to the node’s label. The properties (eg, roles) are entirely equivalent to node properties. (Note that the value of a property may be an array.) Pattern SyntaxCombining the syntax for nodes and relationships, we can express patterns. The following could be a simple pattern (or fact) in this domain:123(keanu:Person:Actor &#123;name: &quot;Keanu Reeves&quot;&#125; )-[role:ACTED_IN &#123;roles: [&quot;Neo&quot;] &#125; ]-&gt;(matrix:Movie &#123;title: &quot;The Matrix&quot;&#125; ) Like with node labels, the relationship type ACTED_IN is added as a symbol, prefixed with a colon: :ACTED_IN. Identifiers (eg, role) can be used elsewhere in the statement to refer to the relationship. Node and relationship properties use the same notation. In this case, we used an array property for the roles, allowing multiple roles to be specified.[Note] Pattern Nodes vs. Database NodesWhen a node is used in a pattern, it describes zero or more nodes in the database. Similarly, each pattern describes zero or more paths of nodes and relationships.Pattern Identifiers To increase modularity and reduce repetition, Cypher allows patterns to be assigned to identifiers. This allow the matching paths to be inspected, used in other expressions, etc.1acted_in = (:Person)-[:ACTED_IN]-&gt;(:Movie) The acted_in variable would contain two nodes and the connecting relationship for each path that was found or created. There are a number of functions to access details of a path, including nodes(path), rels(path) (same as relationships(path)), and length(path). ClausesCypher statements typically have multiple clauses, each of which performs a specific task, eg: create and match patterns in the graph filter, project, sort, or paginate results connect/compose partial statements By combining Cypher clauses, we can compose more complex statements that express what we want to know or create. Neo4j then figures out how to achieve the desired goal in an efficient manner. Neo4j TutorialFundamentalsStore any kind of data using the following graph concepts: Node: Graph data records Relationship: Connect nodes (has direction and a type) Property: Stores data in key-value pair in nodes and relationships Label: Groups nodes and relationships (optional) Browser editorCLIExamples: :help :clear CypherMatchMatch node123MATCH (ee:Person)WHERE ee.name = &quot;Emil&quot;RETURN ee; MATCH clause to specify a pattern of nodes and relationships (ee:Person) a single node pattern with label ‘Person’ which will assign matches to the variable ee WHERE clause to constrain the results ee.name = “Emil” compares name property to the value “Emil” RETURN clause used to request particular results Gets gets the id and id nodes and creates a :KNOWS relationship between them Match nodes and relationships123MATCH (ee:Person)-[:KNOWS]-(friends)WHERE ee.name = &quot;Emil&quot;RETURN ee, friends MATCH clause to describe the pattern from known Nodes to found Nodes (ee) starts the pattern with a Person (qualified by WHERE) -[:KNOWS]- matches “KNOWS” relationships (in either direction) (friends) will be bound to Emil’s friends Match labels12MATCH (n:Person)RETURN n or 123MATCH (n)WHERE n:PersonRETURN n Match multiple labels:Car OR :Person labels 123MATCH (n)WHERE n:Person OR n:CarRETURN n :Car AND :Person labels 123MATCH (n)WHERE n:Person:CarRETURN n Match same properties123MATCH (a:Person)WHERE a.from = &quot;Sweden&quot;RETURN a Returns every node (and their relationships) where there’s a property from with “Sweden” value Match friends of friends with same hobbiesJohan is learning surfing, and wants to know any friend of his friends who already knows surfing 123MATCH (js:Person)-[:KNOWS]-()-[:KNOWS]-(surfer)WHERE js.name = &quot;Johan&quot; AND surfer.hobby = &quot;surfing&quot;RETURN DISTINCT surfer () empty parenthesis to ignore these nodes DISTINCT because more than one path will match the pattern surfer will contain Allison, a friend of a friend who surfs Match by IDEvery node and relationship has an internal autonumeric ID, which can be queried using &lt;, &lt;=, =, =&gt;, &lt;&gt; and IN operators: Search node by ID 123MATCH (n)WHERE id(n) = 0RETURN n Search multiple nodes by ID 123MATCH (n)WHERE id(n) IN [1, 2, 3]RETURN n Search relationship by ID 123MATCH ()-[n]-()WHERE id(n) = 0RETURN n CreateCreate node1CREATE (ee:Person &#123; name: &quot;Emil&quot;, from: &quot;Sweden&quot;, klout: 99 &#125;) CREATE clause to create data () parenthesis to indicate a node ee:Person a variable ee and label Person for the new node {} brackets to add properties (key-value pairs) to the node Create nodes and relationships123456789MATCH (ee:Person) WHERE ee.name = &quot;Emil&quot;CREATE (js:Person &#123; name: &quot;Johan&quot;, from: &quot;Sweden&quot;, learn: &quot;surfing&quot; &#125;),(ir:Person &#123; name: &quot;Ian&quot;, from: &quot;England&quot;, title: &quot;author&quot; &#125;),(rvb:Person &#123; name: &quot;Rik&quot;, from: &quot;Belgium&quot;, pet: &quot;Orval&quot; &#125;),(ally:Person &#123; name: &quot;Allison&quot;, from: &quot;California&quot;, hobby: &quot;surfing&quot; &#125;),(ee)-[:KNOWS &#123;since: 2001&#125;]-&gt;(js),(ee)-[:KNOWS &#123;rating: 5&#125;]-&gt;(ir),(js)-[:KNOWS]-&gt;(ir),(js)-[:KNOWS]-&gt;(rvb),(ir)-[:KNOWS]-&gt;(js),(ir)-[:KNOWS]-&gt;(ally),(rvb)-[:KNOWS]-&gt;(ally) MATCH clause to get “Emil” in ee variable CREATE clause to create multiple nodes (comma separated) with their labels and properties. Also creates directed relationships (a)-[:Label {key: value}]-&gt;(b) Create relationship between 2 unrelated nodes123MATCH (n), (m)WHERE n.name = &quot;Allison&quot; AND m.name = &quot;Emil&quot;CREATE (n)-[:KNOWS]-&gt;(m) Alternative with MERGE, which ensures that the relationship is created only once 12MATCH (n:User &#123;name: &quot;Allison&quot;&#125;), (m:User &#123;name: &quot;Emil&quot;&#125;)MERGE (n)-[:KNOWS]-&gt;(m) Create node with multiple labels1CREATE (n:Actor:Director) UpdateUpdate node properties (add new or modify)Add new .owns property or modify (if exists) 123MATCH (n)WHERE n.name = &quot;Rik&quot;SET n.owns = &quot;Audi&quot; Replace all node properties for the new onesDanger: It will delete all previous properties and create .plays and .age properties 123MATCH (n)WHERE n.name = &quot;Rik&quot;SET n = &#123;plays: &quot;Piano&quot;, age: 23&#125; Add new node properties without deleting old onesDanger: If .plays or .age properties are already set, it will overwrite them 123MATCH (n)WHERE n.name = &quot;Rik&quot;SET n += &#123;plays: &quot;Piano&quot;, age: 23&#125; Add new node property if property not already set123MATCH (n)WHERE n.plays = &quot;Guitar&quot; AND NOT (EXISTS (n.likes))SET n.likes = &quot;Movies&quot; Rename a property in all nodes1234MATCH (n)WHERE NOT (EXISTS (n.instrument))SET n.instrument = n.playsREMOVE n.plays Alternative 1234MATCH (n)WHERE n.instrument is nullSET n.instrument = n.playsREMOVE n.plays Add label to existing nodeAdds the :Food label to nodes id and id 123MATCH (n)WHERE id(n) IN [7, 8]SET n:Food Creates the node if not exists and updates (or creates) a property12MERGE (n:Person &#123;name: &quot;Rik&quot;&#125;)SET n.owns = &quot;Audi&quot; DeleteDelete nodesTo delete a node (p.e. id), first we need to delete its relationships. Then, the node can be deleted 123MATCH (n)-[r]-()WHERE id(n) = 5DELETE r, n To delete multiple nodes (must have their relationships previously deleted) 123MATCH (n)WHERE id(n) IN [1, 2, 3]DELETE n Deletes a property in a specific node123MATCH (n)WHERE n:Person AND n.name = &quot;Rik&quot; AND n.plays is NOT nullREMOVE n.plays Alternative 123MATCH (n)WHERE n:Person AND n.name = &quot;Rik&quot; AND EXISTS (n.plays)REMOVE n.plays Delete a label from all nodesDeletes the :Person label from all nodes 12MATCH (n)REMOVE n:Person Delete a label from nodes with specific labelsDeletes the :Person label from nodes with :Food and :Person labels 123MATCH (n)WHERE n:Food:PersonREMOVE n:Person Delete multiple labels from nodesDeletes the :Food and :Person labels from nodes which have both labels 123MATCH (n)WHERE n:Food:PersonREMOVE n:Food:Person Danger: Deletes the :Food and :Person labels from nodes which have :Food or :Person or :Food:Person labels 12MATCH (n)REMOVE n:Food:Person Delete entire database123MATCH (n)OPTIONAL MATCH (n)-[r]-()DELETE n, r Other clausesShow execution planUse PROFILE or EXPLAIN before the query PROFILE: Shows the execution plan, query information and db hits. Example: Cypher version: CYPHER 3.0, planner: COST, runtime: INTERPRETED. 84 total db hits in 32 ms. EXPLAIN: Shows the execution plan and query information. Example: Cypher version: CYPHER 3.0, planner: COST, runtime: INTERPRETED. CountCount all nodes 12MATCH (n)RETURN count(n) Count all relationships 12MATCH ()--&gt;()RETURN count(*); LimitReturns up to 2 nodes (and their relationships) where there’s a property from with “Sweden” value 1234MATCH (a:Person)WHERE a.from = &quot;Sweden&quot;RETURN aLIMIT 2 Create unique property constraintMake .name property unique on nodes with :Person label 12CREATE CONSTRAINT ON (n:Person)ASSERT n.name IS UNIQUE Drop unique property constraintMake .name property unique on nodes with :Person label 12DROP CONSTRAINT ON (n:Person)ASSERT n.name IS UNIQUE References[1] https://neo4j.com/developer/cypher/[2] https://neo4j.com/docs/cypher-refcard/current/[3] https://neo4j.com/docs/developer-manual/3.1/cypher/clauses/set/[4] http://www.jexp.de/blog/html/compiled-runtime-32.html]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
        <tag>cypher</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[wikipedia 数据 下载 使用]]></title>
    <url>%2F2017%2F06%2F02%2Fwikipedia-data-downlaod%2F</url>
    <content type="text"><![CDATA[维基百科的数据有什么用 可以用来分词、做实体识别、word2vec、新词发现等 数据下载下载地址 https://dumps.wikimedia.org/zhwiki/ 中文 ftp://ftpmirror.your.org/pub/wikimedia/dumps/ 所有语言 镜像 https://dumps.wikimedia.org/mirrors.html https://dumps.wikimedia.org/elwiktionary/ https://dumps.wikimedia.org/enwiki/latest/ https://meta.wikimedia.org/wiki/Mirroring_Wikimedia_project_XML_dumps 数据库及表的创建维基百科数据库的结构https://www.mediawiki.org/wiki/Manual:Database_layout 创建数据库及表结构的语句https://phabricator.wikimedia.org/source/mediawiki/browse/master/maintenance/tables.sql (最新版，添加了很多字段，但是删除了一个旧字段) 部分表结构12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697/*Navicat MySQL Data TransferSource Server : localhostSource Server Version : 50505Source Host : localhost:3306Source Database : wikipedia_zhTarget Server Type : MYSQLTarget Server Version : 50505File Encoding : 65001Date: 2018-07-11 17:16:22*/SET FOREIGN_KEY_CHECKS=0;-- ------------------------------ Table structure for `page`-- ----------------------------DROP TABLE IF EXISTS `page`;CREATE TABLE `page` (`page_id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT ,`page_namespace` int(11) NOT NULL ,`page_title` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_bin NOT NULL ,`page_restrictions` tinyblob NOT NULL ,`page_counter` bigint(20) UNSIGNED NOT NULL DEFAULT 0 ,`page_is_redirect` tinyint(3) UNSIGNED NOT NULL DEFAULT 0 ,`page_is_new` tinyint(3) UNSIGNED NOT NULL DEFAULT 0 ,`page_random` double UNSIGNED NOT NULL ,`page_touched` binary(14) NOT NULL DEFAULT '\0\0\0\0\0\0\0\0\0\0\0\0' ,`page_latest` int(10) UNSIGNED NOT NULL ,`page_len` int(10) UNSIGNED NOT NULL ,PRIMARY KEY (`page_id`))ENGINE=InnoDBDEFAULT CHARACTER SET=utf8mb4 COLLATE=utf8mb4_general_ciAUTO_INCREMENT=1;-- ------------------------------ Table structure for `revision`-- ----------------------------DROP TABLE IF EXISTS `revision`;CREATE TABLE `revision` (`rev_id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT ,`rev_page` int(10) UNSIGNED NOT NULL ,`rev_text_id` int(10) UNSIGNED NOT NULL ,`rev_comment` blob NOT NULL ,`rev_user` int(10) UNSIGNED NOT NULL DEFAULT 0 ,`rev_user_text` varchar(255) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL DEFAULT '' ,`rev_timestamp` binary(14) NOT NULL DEFAULT '\0\0\0\0\0\0\0\0\0\0\0\0' ,`rev_minor_edit` tinyint(3) UNSIGNED NOT NULL DEFAULT 0 ,`rev_deleted` tinyint(3) UNSIGNED NOT NULL DEFAULT 0 ,`rev_len` int(10) UNSIGNED NULL DEFAULT '' ,`rev_parent_id` int(10) UNSIGNED NULL DEFAULT '' ,`rev_sha1` varbinary(32) NOT NULL DEFAULT '' ,PRIMARY KEY (`rev_id`))ENGINE=InnoDBDEFAULT CHARACTER SET=utf8mb4 COLLATE=utf8mb4_general_ciAUTO_INCREMENT=1;-- ------------------------------ Table structure for `text`-- ----------------------------DROP TABLE IF EXISTS `text`;CREATE TABLE `text` (`old_id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT ,`old_text` mediumblob NOT NULL ,`old_flags` tinyblob NOT NULL ,PRIMARY KEY (`old_id`))ENGINE=InnoDBDEFAULT CHARACTER SET=utf8mb4 COLLATE=utf8mb4_general_ciAUTO_INCREMENT=1;-- ------------------------------ Auto increment value for `page`-- ----------------------------ALTER TABLE `page` AUTO_INCREMENT=1;-- ------------------------------ Auto increment value for `revision`-- ----------------------------ALTER TABLE `revision` AUTO_INCREMENT=1;-- ------------------------------ Auto increment value for `text`-- ----------------------------ALTER TABLE `text` AUTO_INCREMENT=1; 数据导入 mwdumper源代码 git clone https://phabricator.wikimedia.org/diffusion/MWDU/mwdumper.git mediawiki-tools-mwdumper github地址 git clone https://github.com/wikimedia/mediawiki-tools-mwdumper.git 直接使用源代码导入12345678用eclipse导入mwdumper工程，运行Dumper类 (windows下可能会报各种错)run configuration:program argument: --output=mysql://localhost/wikipedia_zh?user=root&amp;password=root&amp;characterEncoding=utf8 --format=sql:1.5 C:/workspace/data/zhwiki-latest-pages-articles.xml.bz2vm argument: -Xms128m -Xmx2048 把.xml.bz2转化成.sql，再通过.sql导入123456program argument: --output=mysql://localhost/wikipedia_zh?user=root&amp;password=root&amp;characterEncoding=utf8 --format=mysql:1.5 C:/WorkSpaces/data/zhwiki-latest-pages-articles.xml.bz2vm argument: -Xms128m -Xmx2048m 使用jar包导入123456// 推荐在linux下运行 // windows下可能会报各种错 java -jar mwdumper.jar --format=mysql:1.5 zhwiki-latest-pages-articles.xml.bz2 | mysql -u &lt;username&gt; -p &lt;databasename&gt; java -jar mwdumper-1.25.jar --format=sql:1.5 zhwiki-latest-pages-articles.xml.bz2 | mysql -u user01 -p wikipedia_zh // 然后会提示你输入mysql数据库的密码 使用 用于word2vec 三种不同格式的离线文档，这个演示xml.bz2wiki离线文档介绍https://www.wikidata.org/wiki/Wikidata:Database_download/zhxowa介绍http://xowa.org/home/wiki/Help/Download_XOWA.html中文数据库的下载介绍https://zh.wikipedia.org/wiki/Wikipedia:%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8B%E8%BD%BD中文数据库源https://dumps.wikimedia.org/zhwiki/ References[1] Data_dumps[2] Tools_for_importing[3] Manual:MWDumper[4] Import_examples[5] 把wikipedia中文数据库导入mysql[6] 维基百科简体中文语料的获取[7] mediawiki - github[8] 网上可供下载的数据集合大整理[9] Wikipedia:数据库下载[10] 维基百科简体中文语料的获取[11] backup[12] linux，windows下如何使用XOWA查看离线的维基百科 XOWA 导入离线wiki[13] 维基百科中文语料的获取]]></content>
      <categories>
        <category>data</category>
      </categories>
      <tags>
        <tag>data</tag>
        <tag>wikipedia</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j error 笔记]]></title>
    <url>%2F2017%2F06%2F01%2Fneo4j-error-notes%2F</url>
    <content type="text"><![CDATA[把使用neo4j时遇到的一些常见的问题、错误分享给大家 这要包括 启动问题、前端问题、导入数据问题、工程问题、部署问题，欢迎大家补充。 (1) 启动问题(1.1) Neo4j启动没反应但也不报错 在服务器上用neo4j start启动neo4j，没有任何反应，用neo4j console启动也没反应 JDK的问题，换成Oracle JDK-1.8，不要用Open JDK (1.2) org.neo4j.server.database.LifecycleManagingDatabase was successfully initialized, but failed to start12345678910111213141516171819警告: This command does not appear to be running with administrativerights. Some commands may fail e.g. Start/Stop2017-03-23 03:01:33.868+0000 INFO Starting...2017-03-23 03:01:34.729+0000 INFO Bolt enabled on 0.0.0.0:7687.2017-03-23 03:01:34.838+0000 ERROR Failed to start Neo4j: Starting Neo4j failed: Component &apos;org.neo4j.server.database.LifecycleManagingDatabase@5dc1b1c4&apos; was successfully initialized, but failed to start. Please see attached cause exception. Starting Neo4j failed: Component &apos;org.neo4j.server.database.LifecycleManagingDatabase@5dc1b1c4&apos; was successfully initialized, but failed to start. Please seeattached cause exception.org.neo4j.server.ServerStartupException: Starting Neo4j failed: Component &apos;org.neo4j.server.database.LifecycleManagingDatabase@5dc1b1c4&apos; was successfully initialized, but failed to start. Please see attached cause exception.Caused by: org.neo4j.kernel.lifecycle.LifecycleException: Component &apos;org.neo4j.server.database.LifecycleManagingDatabase@5dc1b1c4&apos; was successfully initialized, but failed to start. Please see attached cause exception.Caused by: java.lang.RuntimeException: Error starting org.neo4j.kernel.impl.factory.GraphDatabaseFacadeFactory, C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\C:ProfessionSofwareNeo4j\databases\charles.graph.dbCaused by: org.neo4j.kernel.lifecycle.LifecycleException: Component &apos;org.neo4j.kernel.internal.StoreLockerLifecycleAdapter@3edf2098&apos; was successfully initialized, but failed to start. Please see attached cause exception.Caused by: org.neo4j.kernel.StoreLockException: Unable to create path for store dir: C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\C:ProfessionSofwareNeo4j\databases\charles.graph.db. Please ensureno other process is using this database, and that the directory is writable (required even for read-only access) at org.neo4j.kernel.internal.StoreLocker.storeLockException(StoreLocker.java:94)Caused by: java.io.IOException: Unable to create directory path [C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\C:ProfessionSofwareNeo4j\databases\charles.graph.db] for Neo4j store. at org.neo4j.io.fs.DefaultFileSystemAbstraction.mkdirs(DefaultFileSystemAbstraction.java:113) at org.neo4j.kernel.internal.StoreLocker.checkLock(StoreLocker.java:64) Unable to create path for store dir， 没有写权限 遇到这个问题，一般都是和路径或者权限有关 最后发现原因是配置文件里dbms.directories.data路径写错了 (1.3) TLS private key found, but missing certificate12345678910111213C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\bin&gt;neo4j console警告: This command does not appear to be running with administrative rights. Somecommands may fail e.g. Start/Stop2017-04-24 07:06:54.426+0000 ERROR Failed to start Neo4j: TLS private key found, but missing certificate at &apos;C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\certificates\neo4j.cert&apos;. Cannot start server without certificate. TLS private key found, but missing certificate at &apos;C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\certificates\neo4j.cert&apos;. Cannot start server without certificate.org.neo4j.server.ServerStartupException: TLS private key found, but missing certificate at &apos;C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\certificates\neo4j.cert&apos;. Cannot start server without certificate. at org.neo4j.server.AbstractNeoServer.createKeyStore(AbstractNeoServer.java:387) at org.neo4j.server.AbstractNeoServer.init(AbstractNeoServer.java:183) at org.neo4j.server.AbstractNeoServer.start(AbstractNeoServer.java:196) at org.neo4j.server.ServerBootstrapper.start(ServerBootstrapper.java:91) at org.neo4j.server.ServerBootstrapper.start(ServerBootstrapper.java:68) at org.neo4j.server.CommunityEntryPoint.main(CommunityEntryPoint.java:28) 有neo4j.key但是没有neo4j.cert文件，刚才把neo4j.cert误删了 (1.4) /browser/scripts/8eea4b31.components.js org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutException123456789101112131415161718192021222324252627282930313233[admin@user01 neo4j-community-3.1.3]$ ./bin/neo4j consoleStarting Neo4j.WARNING: Max 1024 open files allowed, minimum of 40000 recommended. See the Neo4j manual.2017-05-03 12:00:12.496+0000 INFO ======== Neo4j 3.1.3 ========2017-05-03 12:00:12.880+0000 INFO Starting...2017-05-03 12:00:13.825+0000 INFO Bolt enabled on 0.0.0.0:7687.2017-05-03 12:00:19.550+0000 INFO Started.2017-05-03 12:00:20.975+0000 INFO Remote interface available at http://0.0.0.0:7474/2017-05-03 12:02:23.689+0000 WARN /browser/scripts/8eea4b31.components.js org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutExceptionjava.io.IOException: org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutException at org.eclipse.jetty.util.SharedBlockingCallback$Blocker.block(SharedBlockingCallback.java:234)Caused by: org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutException2017-05-03 12:02:23.690+0000 WARN /browser/scripts/8eea4b31.components.js org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutExceptionjava.io.IOException: org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutExceptionCaused by: org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutException2017-05-03 12:03:23.538+0000 WARN /browser/scripts/8eea4b31.components.js org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutExceptionCaused by: org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutException2017-05-03 12:03:23.540+0000 WARN /browser/scripts/8eea4b31.components.js org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutExceptionjava.io.IOException: org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutExceptionCaused by: org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutException at org.eclipse.jetty.util.SharedBlockingCallback$Blocker.block(SharedBlockingCallback.java:216) ... 33 more^C2017-05-03 12:10:26.531+0000 INFO Neo4j Server shutdown initiated by request2017-05-03 12:10:26.566+0000 INFO Stopping...2017-05-03 12:10:27.132+0000 INFO Stopped.[admin@user01 neo4j-community-3.1.3]$ (1.5) Not possible to upgrade a store with version ‘v0.A.8’ to current store version v0.A.7 (Neo4j 3.1.0).1234567891011121314151617ERROR Failed to start Neo4j: Starting Neo4j failed: Component &apos;org.neo4j.server.database.LifecycleManagingDatabase@77b9886e&apos; was successfully initialized, but failed to start. Please see attached cause exception. Starting Neo4j failed: Component &apos;org.neo4j.server.database.LifecycleManagingDatabase@77b9886e&apos; was successfully initialized, but failed to start. Please see attached cause exception.org.neo4j.server.ServerStartupException: Starting Neo4j failed: Component &apos;org.neo4j.server.database.LifecycleManagingDatabase@77b9886e&apos; was successfully initialized, but failed to start. Please see attached cause exception.Caused by: org.neo4j.kernel.lifecycle.LifecycleException: Component &apos;org.neo4j.server.database.LifecycleManagingDatabase@77b9886e&apos; was successfully initialized, but failed to start. Please see attached cause exception.Caused by: java.lang.RuntimeException: Error starting org.neo4j.kernel.impl.factory.GraphDatabaseFacadeFactory, /home/user1/neo4j_5687/neo4j-community-3.1.0/data/databases/graph.dbCaused by: org.neo4j.kernel.lifecycle.LifecycleException: Component &apos;org.neo4j.kernel.NeoStoreDataSource@7c18987f&apos; was successfully initialized, but failed to start. Please see attached cause exception.Caused by: org.neo4j.kernel.impl.storemigration.StoreUpgrader$UnexpectedUpgradingStoreVersionException: Not possible to upgrade a store with version &apos;v0.A.8&apos; to current store version `v0.A.7` (Neo4j 3.1.0). at org.neo4j.kernel.impl.storemigration.UpgradableDatabase.checkUpgradeable(UpgradableDatabase.java:135) at org.neo4j.kernel.impl.storemigration.StoreUpgrader.migrateIfNeeded(StoreUpgrader.java:130) at org.neo4j.kernel.impl.storemigration.DatabaseMigrator.migrate(DatabaseMigrator.java:98) at org.neo4j.kernel.NeoStoreDataSource.upgradeStore(NeoStoreDataSource.java:571) at org.neo4j.kernel.NeoStoreDataSource.start(NeoStoreDataSource.java:440) at org.neo4j.kernel.lifecycle.LifeSupport$LifecycleInstance.start(LifeSupport.java:433) ... 14 more 把高版本的数据直接复制到低版本报错。通过neo4j提供的数据迁移jar包迁移数据 (1.6) java.lang.IllegalArgumentException: Base directory ‘C:\ProfessionSofware\Neo4j\neo4j-community-3.3.4\certificates\default’ for SSL policy with name ‘default’ does not exist.1234567891011121314151617181920C:\ProfessionSofware\Neo4j\neo4j-community-3.3.4\binλ neo4j console2018-04-10 02:29:48.937+0000 INFO ======== Neo4j 3.3.4 ========2018-04-10 02:29:48.981+0000 INFO Starting...2018-04-10 02:29:49.477+0000 ERROR Failed to start Neo4j: Starting Neo4j failed: Component &apos;org.neo4j.server.database.LifecycleManagingDatabase@1f2586d6&apos; was successfully initialized, but failed to start. Please see the attached cause exception &quot;Base directory &apos;C:\ProfessionSofware\Neo4j\neo4j-community-3.3.4\certificates\default&apos; for SSL policy with name &apos;default&apos; does not exist.&quot;. Starting Neo4j failed: Component &apos;org.neo4j.server.database.LifecycleManagingDatabase@1f2586d6&apos; was successfully initialized, but failed to start. Please see the attached cause exception &quot;Base directory &apos;C:\ProfessionSofware\Neo4j\neo4j-community-3.3.4\certificates\default&apos; for SSL policy with name &apos;default&apos; does not exist.&quot;.org.neo4j.server.ServerStartupException: Starting Neo4j failed: Component &apos;org.neo4j.server.database.LifecycleManagingDatabase@1f2586d6&apos; was successfully initialized, but failed to start. Please see the attached cause exception &quot;Base directory &apos;C:\ProfessionSofware\Neo4j\neo4j-community-3.3.4\certificates\default&apos; for SSL policy with name &apos;default&apos; does not exist.&quot;.Caused by: org.neo4j.kernel.lifecycle.LifecycleException: Component &apos;org.neo4j.server.database.LifecycleManagingDatabase@1f2586d6&apos; was successfully initialized, but failed to start. Please see the attached cause exception &quot;Base directory &apos;C:\ProfessionSofware\Neo4j\neo4j-community-3.3.4\certificates\default&apos; for SSL policy with name &apos;default&apos; does not exist.&quot;.Caused by: java.lang.IllegalArgumentException: Base directory &apos;C:\ProfessionSofware\Neo4j\neo4j-community-3.3.4\certificates\default&apos; for SSL policy with name &apos;default&apos; does not exist. at org.neo4j.kernel.configuration.ssl.SslPolicyLoader.load(SslPolicyLoader.java:184) at org.neo4j.kernel.configuration.ssl.SslPolicyLoader.create(SslPolicyLoader.java:94) at org.neo4j.kernel.impl.factory.CommunityEditionModule.&lt;init&gt;(CommunityEditionModule.java:103) at org.neo4j.kernel.impl.factory.GraphDatabaseFacadeFactory.initFacade(GraphDatabaseFacadeFactory.java:159) at org.neo4j.kernel.impl.factory.GraphDatabaseFacadeFactory.newFacade(GraphDatabaseFacadeFactory.java:126) at org.neo4j.server.CommunityNeoServer.lambda$static$0(CommunityNeoServer.java:58) at org.neo4j.server.database.LifecycleManagingDatabase.start(LifecycleManagingDatabase.java:88) at org.neo4j.kernel.lifecycle.LifeSupport$LifecycleInstance.start(LifeSupport.java:445) ... 5 more2018-04-10 02:29:49.483+0000 INFO Neo4j Server shutdown initiated by request ssl配置问题修改neo4j.conf里的配置 (2) Neo4j-DespTop(前端展示)问题(2.1) WebSocket connection failure1WebSocket connection failure. Due to security constraints in your web browser, the reason for the failure is not available to this Neo4j Driver. Please use your browsers development console to determine the root cause of the failure. Common reasons include the database being unavailable, using the wrong connection URL or temporary network problems. If you have enabled encryption, ensure your browser is configured to trust the certificate Neo4j is configured to use. WebSocket `readyState` is: 3 分2种情况： neo4j版本在3.2.0以前1.1 换浏览器1.2 在右边的设置里(齿轮图标)，把Do not use Bolt勾选上 neo4j版本3.2.0及以上 在浏览器里禁用bolt 在neo4j.conf里禁用bolt 检查JDK版本，推荐使用JDK 1.8， JDK 1.7或者1.9 可能出问题 前端页面打不开12345678910111213141516171819202122232425262728293031322019-06-18 10:37:55.940+0800 WARN /browser/vendor-d594bc0d621572ced6b1.js org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutExceptionjava.io.IOException: org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutException at org.eclipse.jetty.util.SharedBlockingCallback$Blocker.block(SharedBlockingCallback.java:213) ... at java.lang.Thread.run(Thread.java:748)Caused by: org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutException at org.eclipse.jetty.util.SharedBlockingCallback$Blocker.block(SharedBlockingCallback.java:195) ... 33 more2019-06-18 10:37:55.941+0800 WARN /browser/vendor-d594bc0d621572ced6b1.js org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutExceptionjava.io.IOException: org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutException at org.eclipse.jetty.util.SharedBlockingCallback$Blocker.block(SharedBlockingCallback.java:213) ... at java.lang.Thread.run(Thread.java:748)Caused by: org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutException at org.eclipse.jetty.util.SharedBlockingCallback$Blocker.block(SharedBlockingCallback.java:195) ... 33 more2019-06-18 10:37:55.949+0800 WARN /browser/app-d594bc0d621572ced6b1.js org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutExceptionjava.io.IOException: org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutException at org.eclipse.jetty.util.SharedBlockingCallback$Blocker.block(SharedBlockingCallback.java:213) ... at java.lang.Thread.run(Thread.java:748)Caused by: org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutException at org.eclipse.jetty.util.SharedBlockingCallback$Blocker.block(SharedBlockingCallback.java:195) ... 33 more2019-06-18 10:37:55.950+0800 WARN /browser/app-d594bc0d621572ced6b1.js org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutExceptionjava.io.IOException: org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutException at org.eclipse.jetty.util.SharedBlockingCallback$Blocker.block(SharedBlockingCallback.java:213) ... at java.lang.Thread.run(Thread.java:748)Caused by: org.eclipse.jetty.util.SharedBlockingCallback$BlockerTimeoutException at org.eclipse.jetty.util.SharedBlockingCallback$Blocker.block(SharedBlockingCallback.java:195) ... 33 more (3) 导入数据问题(3.1) Expected ‘–nodes’ to have at least 1 valid item, but had 0 []1234567891011121314C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\bin&gt;neo4j-import --into retail.db --id-type string --nodes:Customer customers.csv --nodes products.csv --nodes orders_header.csv,orders1.csv,orders2.csv --relationships:CONTAINS order_details.csv --relationships:ORDERED customer_orders_header.csv,orders1.csv,orders2.csv警告: This command does not appear to be running with administrative rights. Some commands may fail e.g. Start/StopWARNING: neo4j-import is deprecated and support for it will be removed in a futureversion of Neo4j; please use neo4j-admin import instead.Input error: Expected &apos;--nodes&apos; to have at least 1 valid item, but had 0 []Caused by:Expected &apos;--nodes&apos; to have at least 1 valid item, but had 0 []java.lang.IllegalArgumentException: Expected &apos;--nodes&apos; to have at least 1 valid item, but had 0 [] at org.neo4j.kernel.impl.util.Validators.lambda$atLeast$6(Validators.java:125) at org.neo4j.helpers.Args.validated(Args.java:640) at org.neo4j.helpers.Args.interpretOptionsWithMetadata(Args.java:608) at org.neo4j.tooling.ImportTool.extractInputFiles(ImportTool.java:503) at org.neo4j.tooling.ImportTool.main(ImportTool.java:388) at org.neo4j.tooling.ImportTool.main(ImportTool.java:334) (3.2) Error in input data123456789101112131415161718192021222324C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\bin&gt;neo4j-import -into test.db --id-type string --nodes:Test C:/User/wdb/2017-04-06_test.csv --stacktrace truektrace true --skip-duplicate-nodes true警告: This command does not appear to be running with administrative rights. Some commands may fail e.g. Start/StopWARNING: neo4j-import is deprecated and support for it will be removed in a futureversion of Neo4j; please use neo4j-admin import instead.Neo4j version: 3.1.2Importing the contents of these files into test.db:Nodes: :Test C:\User\wdb\2017-04-06_test.csvAvailable resources: Free machine memory: 3.74 GB Max heap memory : 1.77 GB Processors: 4NodesError in input dataCaused by:ERROR in input data source: BufferedCharSeeker[source:C:\User\wdb\2017-04-06_test.csv, position:654, line:11] in field: mobile:int:6 for header: [dep:string, uid:int, name:string, tel:int, fax:string, mobile:int, email:string, id:string] raw field value: 13900001111 original error: Not supported a.t.m 错误原因是 用int表示电话号码，超范围了。最后换成用String保存电话号码。 (3.2) Missing header od type start_idimport date has problem , please check the data数据有问题，检查csv文件里的数据格式，看少了什么东西 (4) 工程问题## (4.1) Cannot determine embedded database driver class for database type NONE.1234Caused by: org.springframework.beans.factory.BeanCreationException: Could not autowire field: private javax.sql.DataSource org.springframework.boot.autoconfigure.orm.jpa.JpaBaseConfiguration.dataSource; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name &apos;dataSource&apos; defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceAutoConfiguration$NonEmbeddedConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [javax.sql.DataSource]: Factory method &apos;dataSource&apos; threw exception; nested exception is org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Cannot determine embedded database driver class for database type NONE. If you want an embedded database please put a supported one on the classpath. If you have database settings to be loaded from a particular profile you may need to active it (no profiles are currently active).Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name &apos;dataSource&apos; defined in class path resource [org/springframework/boot/autoconfigure/jdbc/DataSourceAutoConfiguration$NonEmbeddedConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [javax.sql.DataSource]: Factory method &apos;dataSource&apos; threw exception; nested exception is org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Cannot determine embedded database driver class for database type NONE. If you want an embedded database please put a supported one on the classpath. If you have database settings to be loaded from a particular profile you may need to active it (no profiles are currently active).Caused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [javax.sql.DataSource]: Factory method &apos;dataSource&apos; threw exception; nested exception is org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Cannot determine embedded database driver class for database type NONE. If you want an embedded database please put a supported one on the classpath. If you have database settings to be loaded from a particular profile you may need to active it (no profiles are currently active).Caused by: org.springframework.boot.autoconfigure.jdbc.DataSourceProperties$DataSourceBeanCreationException: Cannot determine embedded database driver class for database type NONE. If you want an embedded database please put a supported one on the classpath. If you have database settings to be loaded from a particular profile you may need to active it (no profiles are currently active). spring-data-jpa 和 spring-data-neo4j 不兼容，只能升级版本解决了 (4.2) org.neo4j.driver.v1.exceptions.ClientException: You cannot begin a transaction on a session with an open transaction;1234567891011121314151617181920212223242526272829org.neo4j.driver.v1.exceptions.ClientException: You cannot begin a transaction on a session with an open transaction; either run from within the transaction or use a different session. at org.neo4j.driver.internal.InternalSession.ensureNoOpenTransactionBeforeOpeningTransaction(InternalSession.java:196) at org.neo4j.driver.internal.InternalSession.ensureConnectionIsValidBeforeOpeningTransaction(InternalSession.java:167) at org.neo4j.driver.internal.InternalSession.beginTransaction(InternalSession.java:135) at org.neo4j.jdbc.bolt.BoltPreparedStatement.executeInternal(BoltPreparedStatement.java:89) at org.neo4j.jdbc.bolt.BoltPreparedStatement.executeQuery(BoltPreparedStatement.java:54) at com.xxxx.xxxx.odk.neo4j.utils.CypherResult.getCypherResultList(CypherResult.java:34) at com.xxxx.xxxx.odk.neo4j.search.impl.JdbcSearchImpl.getRelationshipsBetweenTwoDifferentLabelNodes(JdbcSearchImpl.java:399) at com.xxxx.xxxx.xxxxxxxx.service.impl.OperationServiceImpl.getDetailInformation(OperationServiceImpl.java:95) at com.xxxx.xxxx.xxxxxxxx.controller.OperationController.getDetailInformation(OperationController.java:34) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221) at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:136) at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:114) at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:827) at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:738) at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:963) at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:897) at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) at javax.servlet.http.HttpServlet.service(HttpServlet.java:635) at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) at javax.servlet.http.HttpServlet.service(HttpServlet.java:742) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:230) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:165) 事务的问题，多看看事务的资料 (4.3) Exception in thread “Thread-0” org.neo4j.driver.v1.exceptions.ClientException: Unable to connect to ‘172.23.22.27’ on port 7687, ensure the database is running and that there is a working network connection to it.12Exception in thread &quot;Thread-0&quot; org.neo4j.driver.v1.exceptions.ClientException: Unable to connect to &apos;172.23.22.27&apos; on port 7687, ensure the database is running and that there is a working network connection to it. at org.neo4j.driver.internal.connector.socket.SocketClient.start(SocketClient.java:82) 使用neo4j-java-driver时，用完连接没有释放，导致连接用尽。在使用完注意加上 session.close(); driver.close(); (4.4) org.neo4j.driver.v1.exceptions.clientException General SSLEngine problem1org.neo4j.driver.v1.exceptions.clientException General SSLEngine problem 在当前用户的目录下找 .neo4j/known_hosts 文件，找到后删除就可以了 在新版本里，SSL认证方式发生变化，需要修改配置文件neo4j.conf，修改70行，102行，103行，把前面的#去掉，重启neo4j数据库 dbms.connector.bolt.tls_level=DISABLED 1234567891011121314151617181920212223242526# Bolt connectordbms.connector.bolt.enabled=truedbms.connector.bolt.tls_level=OPTIONALdbms.connector.bolt.listen_address=:5687#*****************************************************************# SSL system configuration#*****************************************************************# Names of the SSL policies to be used for the respective components.# The legacy policy is a special policy which is not defined in# the policy configuration section, but rather derives from# dbms.directories.certificates and associated files# (by default: neo4j.key and neo4j.cert). Its use will be deprecated.# The policies to be used for connectors.## N.B: Note that a connector must be configured to support/require# SSL/TLS for the policy to actually be utilized.## see: dbms.connector.*.tls_levelbolt.ssl_policy=legacyhttps.ssl_policy=legacy I think you need to delete the file ~/neo/known_hosts (or at least open it and delete the line that starts with localhost:7687). When you are using trust-on-first-use the driver will accept the first certificate it sees and store it in the known_hosts-file, if the driver sees any other certificates it will reject them. However when you upgrade the server it will generate a new certificate and it will no longer match the certificate in the known_hosts-file. (4.5) Caused by: java.io.IOException: Broken pipe123456789101112131415161718org.neo4j.driver.v1.exceptions.ClientException: Unable to send messages to server: Broken pipeorg.neo4j.driver.v1.exceptions.ClientException: Unable to send messages to server: Broken pipeCaused by: java.io.IOException: Broken pipe at sun.nio.ch.FileDispatcherImpl.write0(Native Method) at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47) at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93) at sun.nio.ch.IOUtil.write(IOUtil.java:51) at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471) at org.neo4j.driver.internal.connector.socket.TLSSocketChannel.wrap(TLSSocketChannel.java:282) at org.neo4j.driver.internal.connector.socket.TLSSocketChannel.write(TLSSocketChannel.java:390) at org.neo4j.driver.internal.connector.socket.ChunkedOutput.flush(ChunkedOutput.java:62) at org.neo4j.driver.internal.packstream.PackStream$Packer.flush(PackStream.java:173) at org.neo4j.driver.internal.messaging.PackStreamMessageFormatV1$Writer.flush(PackStreamMessageFormatV1.java:329) at org.neo4j.driver.internal.messaging.PackStreamMessageFormatV1$Writer.flush(PackStreamMessageFormatV1.java:100) at org.neo4j.driver.internal.connector.socket.SocketClient.send(SocketClient.java:114) at org.neo4j.driver.internal.connector.socket.SocketConnection.flush(SocketConnection.java:108) ... 113 more 程序有问题，报错太多了 ulimit没有优化，设置ulimit 这个异常是客户端读取超时关闭了连接,这时候服务器端再向客户端已经断开的连接写数据时就发生了broken pipe异常！ (4.x)123456C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\binλ neo4j console2018-04-28 01:40:12.128+0000 INFO Starting...2018-04-28 01:40:13.614+0000 INFO Bolt enabled on 0.0.0.0:7687.2018-04-28 01:40:19.501+0000 INFO Started.2018-04-28 01:40:20.920+0000 INFO Remote interface available at http://localhost:7474/ 1234567891011121314151617181920212223242526272829303132333435WKQ@WKQ-PC MINGW64 /c/ProfessionSofware/Neo4j/neo4j-community-3.1.0/data/databases$ rm -rf graph.db/rm: cannot remove &apos;graph.db/neostore&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.counts.db.a&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.id&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.labeltokenstore.db&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.labeltokenstore.db.id&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.labeltokenstore.db.names&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.labeltokenstore.db.names.id&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.nodestore.db&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.nodestore.db.id&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.nodestore.db.labels&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.nodestore.db.labels.id&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.propertystore.db&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.propertystore.db.arrays&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.propertystore.db.arrays.id&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.propertystore.db.id&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.propertystore.db.index&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.propertystore.db.index.id&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.propertystore.db.index.keys&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.propertystore.db.index.keys.id&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.propertystore.db.strings&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.propertystore.db.strings.id&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.relationshipgroupstore.db&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.relationshipgroupstore.db.id&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.relationshipstore.db&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.relationshipstore.db.id&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.relationshiptypestore.db&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.relationshiptypestore.db.id&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.relationshiptypestore.db.names&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.relationshiptypestore.db.names.id&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.schemastore.db&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.schemastore.db.id&apos;: Device or resource busyrm: cannot remove &apos;graph.db/neostore.transaction.db.0&apos;: Device or resource busyrm: cannot remove &apos;graph.db/store_lock&apos;: Device or resource busy 12345678910111213141516171819202122232425262728293031323334353637383940414243442018-04-28 01:44:15.449+0000 INFO Neo4j Server shutdown initiated by request2018-04-28 01:44:15.517+0000 INFO Stopping...终止批处理操作吗(Y/N)? Exception in thread &quot;Thread-11&quot; java.lang.RuntimeException: org.neo4j.kernel.lifecycle.LifecycleException: Failed to transition component &apos;org.neo4j.kernel.NeoStoreDataSource$5@58dcfcb2&apos; from STOPPED to SHUTTING_DOWN. Please see attached cause exceptionCaused by: org.neo4j.kernel.lifecycle.LifecycleException: Failed to transition component &apos;org.neo4j.kernel.NeoStoreDataSource$5@58dcfcb2&apos; from STOPPED to SHUTTING_DOWN. Please see attached cause exception Suppressed: org.neo4j.kernel.lifecycle.LifecycleException: Failed to transition component &apos;org.neo4j.kernel.impl.storageengine.impl.recordstorage.RecordStorageEngine@6661b54d&apos; from STOPPED to SHUTTING_DOWN. Please see attached cause exception Caused by: java.io.IOException: Exception closing multiple resources Caused by: java.io.IOException: Exception closing multiple resources Caused by: java.nio.file.NoSuchFileException: C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\schema\label\lucene\labelStore\1\write.lock Suppressed: java.nio.file.NoSuchFileException: C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\schema\label\lucene\labelStore\1\_1.cfe at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:79) Suppressed: org.neo4j.kernel.lifecycle.LifecycleException: Failed to transition component &apos;org.neo4j.kernel.impl.pagecache.PageCacheLifecycle@6df2bf5d&apos; from STOPPED to SHUTTING_DOWN. Please see attached cause exception Caused by: java.lang.IllegalStateException: Cannot close the PageCache while files are still mapped: C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\neostore (1 mapping) C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\neostore.counts.db.a (1 mapping) C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\neostore.relationshipgroupstore.db (1 mapping) C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\neostore.schemastore.db (1 mapping) C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\neostore.labeltokenstore.db (1 mapping) C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\neostore.labeltokenstore.db.names (1 mapping) C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\neostore.relationshiptypestore.db (1 mapping) C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\neostore.relationshiptypestore.db.names (1 mapping) C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\neostore.relationshipstore.db (1 mapping) C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\neostore.propertystore.db (1 mapping) C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\neostore.propertystore.db.arrays (1 mapping) C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\neostore.propertystore.db.strings (1 mapping) C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\neostore.propertystore.db.index (1 mapping) C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\neostore.propertystore.db.index.keys (1 mapping) C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\neostore.nodestore.db (1 mapping) C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\neostore.nodestore.db.labels (1 mapping) at org.neo4j.io.pagecache.impl.muninn.MuninnPageCache.close(MuninnPageCache.java:610) at org.neo4j.kernel.impl.pagecache.PageCacheLifecycle.shutdown(PageCacheLifecycle.java:42) at org.neo4j.kernel.lifecycle.LifeSupport$LifecycleInstance.shutdown(LifeSupport.java:488) ... 10 moreCaused by: org.neo4j.kernel.impl.store.UnderlyingStorageException: java.nio.file.NoSuchFileException: C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\schema\label\lucene\labelStore\1\write.lockCaused by: java.nio.file.NoSuchFileException: C:\ProfessionSofware\Neo4j\neo4j-community-3.1.0\data\databases\graph.db\schema\label\lucene\labelStore\1\write.lock (5) 部署问题(5.1) no suitable driver founeno suitable driver foune for jdbc:neo4j:http:http://127.0.0.1:7474 Class.forName(url).newInstance() 把jar放在$jre/lib/ext下面 (5.2) WARNING: Max 1024 open files allowed, minimum of 40000 recommended. See the Neo4j manual.123456[admin@user01 neo4j-community-3.1.3]$ ./bin/neo4j startStarting Neo4j.WARNING: Max 1024 open files allowed, minimum of 40000 recommended. See the Neo4j manual.Started neo4j (pid 8480). It is available at http://localhost:7474/There may be a short delay until the server is ready.See /home/admin/databases/neo4j-community-3.1.3/logs/neo4j.log for current status. Linux平台对用户可能打开的并发文件的数量施加了上限。默认是1024。使用ulimit -n命令为当前用户和会话查看 ulimit -n 把用户可打开的并发文件数量改大点，比如改成40000。 切换成root用户 编辑/etc/security/limits.conf并添加以下两行： neo4j soft nofile 40000 neo4j hard nofile 40000 编辑/etc/pam.d/su并取消注释或添加以下行： 会话需要pam_limits.so 需要重新启动才能使设置生效。 在上述过程之后，neo4j用户将具有40,000个同时打开文件的限制。如果您继续遇到太多打开文件或Could not stat（）目录的异常，则可能需要进一步提高限制。 (6) neo4j-community限制问题org.springframework.dao.DataRetrievalFailureException: ‘type‘ onorg.springframework.dao.DataRetrievalFailureException: &#39;__type__&#39; on neo4j-community 最多允许 2^35个节点，2^35个关系，2^36个属性，属性类型不能超过2^16个属性类型超过65535个了 (类似于MySQL里字段超过65535个) References[1] https://neo4j.com/docs/[2] https://stackoverflow.com/questions/13803609/dataretrievalfailureexception-type-not-found-in-spring-data-neo4j[3] https://groups.google.com/forum/#!topic/neo4j/aFZCp5T3ABo[4] https://github.com/neo4j/neo4j-java-driver/issues/174[5] https://stackoverflow.com/questions/37240461/general-sslengine-error-in-neo4j]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么要写博客]]></title>
    <url>%2F2017%2F05%2F30%2Fblog-notes%2F</url>
    <content type="text"><![CDATA[工作很长时间以后，发现以前别人说的一句话很对。你想成为哪类人，取决于你的选择。 我认为，博客也是一样。 写博客的原因 最开始是想把学到的东西沉淀下来。 不写到纸上很快就忘了。 最原始用的txt，然后是word，但是在另一个电脑或者没网的环境下很不方便。机缘巧合，最后想到了写博客。 给同事的资料，提高效率，节省时间。 新入职的同学，有些东西不会，不想给他讲，扔一篇博客和一些参考资料给他，让他自己学，有问题再问我。 还别说，真管用。 想有个自己的网站。 博客算是技术的对外展示，方便沟通。 看到别人写，自己也想写。 数十年后，可以看到自己写的博客。也算是一个小成绩、小贡献吧。 为什么要自建网站 最开始是看到别人这么做，然后自己也试了试，就用hexo自己建了一个网站。 不用csdn、博客园、简书、开源中国、头条、微信公众号，是想自己控制。必要的时候可以用一文多发平台同步到这些平台。 后来发现自建网站接入百度统计、腾讯分析以后可以看到每个省的访问量，每个ip的访问。有点出乎意料。 不应该做的 转载或引用要在文章前面或者后面标明引用来源。 可以写的很基础。 不为了蹭流量而写。 不为了营销而写。]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python下载安装]]></title>
    <url>%2F2017%2F05%2F28%2Fpython-download-install%2F</url>
    <content type="text"><![CDATA[https://www.python.org/downloads/ https://www.python.org/downloads/source/ python文档 https://docs.python.org/3/download.html 12345678910# Setting PATH for Python 2.7PATH=&quot;/System/Library/Frameworks/Python.framework/Versions/2.7/bin:$&#123;PATH&#125;&quot;export PATH# Setting PATH for Python 3.7PATH=&quot;/Library/Frameworks/Python.framework/Versions/3.7/bin:$&#123;PATH&#125;&quot;alias python2=&apos;/System/Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7&apos;alias python3=&apos;/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7&apos;alias python=python3 pip: command not found运行Python的安装工具安装pip sudo easy_install pip 12pip install --upgrade pippip3 install --upgrade pip 1234pip3 install matplotlib -i http://pypi.douban.com/simple --trusted-host pypi.douban.com pip3 install numpy -i http://pypi.douban.com/simple --trusted-host pypi.douban.com pip3 install pandas -i http://pypi.douban.com/simple --trusted-host pypi.douban.com pip3 install seaborn scipy -i http://pypi.douban.com/simple --trusted-host pypi.douban.com 12sqlalchemy.exc.ProgrammingError: (pymysql.err.ProgrammingError) (1064, &quot;You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &apos;form car_brand_series&apos; at line 1&quot;)[SQL: select * form car_brand_series ] 1234567891011121314WKQ@WKQ-PC MINGW64 ~$ python -m pip install --upgrade pipCollecting pip Downloading https://files.pythonhosted.org/packages/00/b6/9cfa56b4081ad13874b0c6f96af8ce16cfbc1cb06bedf8e9164ce5551ec1/pip-19.3.1-py2.py3-none-any.whl (1.4MB) 100% |████████████████████████████████| 1.4MB 17kB/sInstalling collected packages: pip Found existing installation: pip 9.0.1 Uninstalling pip-9.0.1: Successfully uninstalled pip-9.0.1Successfully installed pip-19.3.1WKQ@WKQ-PC MINGW64 ~$ python -m pip install --upgrade pipRequirement already up-to-date: pip in c:\professionalsoftware\python\python35\lib\site-packages (19.3.1) References[1] Mac下安装配置Python2和Python3并相互切换使用]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 学习]]></title>
    <url>%2F2017%2F05%2F28%2Fpython-notes%2F</url>
    <content type="text"><![CDATA[因为在写算法时，很多算法用python实现，为了参考、学习、对比，觉得有必要学一下python， 简单的学一下，能看懂别人的代码就可以(其实是python的库太多了，觉得一定时间里看不完源代码) (1) 简介 Python的哲学就是简单优雅，尽量写容易看明白的代码，尽量写少的代码。 (2) 安装 https://www.python.org/downloads/ (3) 第一个python程序文件编码 .py文件默认使用UTF-8无BOM编码 即使是在windows下新建一个txt文件(gbk编码)，把txt文件重命名成.py文件并且执行后， 文件会自动变成UTF-8编码(可能是在程序执行时把文件转换成UTF-8编码了吧) 输入输出输入用print加上字符串，就可以向屏幕上输出指定的文字。 print语句也可以跟上多个字符串，用逗号“,”隔开，或者“+”，注意两个符号的效果不一样 12345678910111213141516171819202122WKQ@WKQ-PC C:\WorkSpaces\python&gt; pythonPython 2.7.13 (v2.7.13:a06454b1afa1, Dec 17 2016, 20:53:40) [MSC v.1500 64 bit (AMD64)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; print &apos;hello&apos;, &apos;world&apos;, &apos; test&apos;hello world test&gt;&gt;&gt; print &apos;hello&apos;,&apos;world&apos;, &apos; test&apos;hello world test&gt;&gt;&gt; print 300300&gt;&gt;&gt; print 100 + 200300&gt;&gt;&gt; print &apos;100&apos; + &apos;200&apos;100200&gt;&gt;&gt; print &apos;hello&apos; + &apos;world&apos;helloworld 输出Python提供了一个raw_input，可以让用户输入字符串，并存放到一个变量里。12345&gt;&gt;&gt; name = raw_input()123 + 1223 + '1234', 'hello'&gt;&gt;&gt; name"123 + 1223 + '1234', 'hello'" 自己写一个程序，执行该程序时，提示用户自己输入名字，输出 hello + 用户自定义的名字1234# hello_name.pyname = raw_input('please enter your name: ')print 'hello,', name 1234WKQ@WKQ-PC C:\WorkSpaces\python&gt; python hello_name.pytomhello, tom (4) Python基础123456# print absolute value of an integer:a = 100if a &gt;= 0: print aelse: print -a 以#开头的语句是注释，注释是给人看的，可以是任意内容，解释器会忽略掉注释。其他每一行都是一个语句， 当语句以冒号“:”结尾时，缩进的语句视为代码块。 缩进是为了让人方便阅读 Python程序是大小写敏感的，如果写错了大小写，程序会报错。 数据类型和变量整数 Python可以处理任意大小的整数，当然包括负整数，在程序中的表示方法和数学上的写法一模一样， 例如：1，100，-8080，0，等等。 计算机由于使用二进制，所以，有时候用十六进制表示整数比较方便，十六进制用0x前缀和0-9，a-f表示， 例如：0xff00，0xa5b4c3d2，等等。 浮点数 浮点数也就是小数，之所以称为浮点数，是因为按照科学记数法表示时，一个浮点数的小数点位置是可变的， 比如，1.23x109和12.3x108是相等的。浮点数可以用数学写法，如1.23，3.14，-9.01，等等。 但是对于很大或很小的浮点数，就必须用科学计数法表示，把10用e替代，1.23x109就是1.23e9， 或者12.3e8，0.000012可以写成1.2e-5，等等。 整数和浮点数在计算机内部存储的方式是不同的，整数运算永远是精确的（除法难道也是精确的？是的！）， 而浮点数运算则可能会有四舍五入的误差。 1234567891011121314# 整数除法永远是整数，即使除不尽。&gt;&gt;&gt; 10 / 33 # 精确的除法，只需把其中一个整数换成浮点数做除法就可以&gt;&gt;&gt; 10.0 / 33.3333333333333335# Python还提供一个余数运算，可以得到两个整数相除的余数&gt;&gt;&gt; 10 % 31&gt;&gt;&gt; 1.0 * 10 / 33.3333333333333335 字符串 字符串是以’’或””括起来的任意文本，比如’abc’，”xyz”等等。 请注意，’’或””本身只是一种表示方式，不是字符串的一部分，因此，字符串’abc’只有a，b，c这3个字符。 如果’本身也是一个字符，那就可以用””括起来，比如”I’m OK”包含的字符是I，’，m，空格，O，K这6个字符。 如果字符串内部既包含’又包含”怎么办？可以用转义字符\来标识，比如：1&apos;I\&apos;m \&quot;OK\&quot;!&apos; 表示的字符串内容是：1I&apos;m &quot;OK&quot;! 转义字符\可以转义很多字符，比如\n表示换行，\t表示制表符，字符\本身也要转义，所以\表示的字符就是\， 可以在Python的交互式命令行用print打印字符串看看：12345678910111213&gt;&gt;&gt; print 'I\'m ok.'I'm ok.&gt;&gt;&gt; print '1\n2'12&gt;&gt;&gt; print '\\\n\\'\\&gt;&gt;&gt; print '\\\\n\\'\\n\ 如果字符串里面有很多字符都需要转义，就需要加很多\， 为了简化，Python还允许用r’’表示’’内部的字符串默认不转义，可以自己试试：12345&gt;&gt;&gt; print '\\\t\\'\ \&gt;&gt;&gt; print r'\\\t\\'\\\t\\ 如果字符串内部有很多换行，用\n写在一行里不好阅读， 为了简化，Python允许用’’’…’’’的格式表示多行内容，可以自己试试：123456789101112&gt;&gt;&gt; print &apos;&apos;&apos;line1...line2...line3&apos;&apos;&apos;line1...line2...line3&gt;&gt;&gt; print &apos;&apos;&apos;... lint1... line2... line3... &apos;&apos;&apos;lint1line2line3 布尔值 布尔值和布尔代数的表示完全一致，一个布尔值只有True、False两种值， 在Python中，可以直接用True、False表示布尔值（请注意大小写），也可以通过布尔运算计算出来：123456789101112131415161718192021&gt;&gt;&gt; TrueTrue&gt;&gt;&gt; FalseFalse&gt;&gt;&gt; trueTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;NameError: name 'true' is not defined&gt;&gt;&gt; falseTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;NameError: name 'false' is not defined&gt;&gt;&gt; 2 &gt; 1True&gt;&gt;&gt; 2 &gt; 3False 布尔值可以用and、or和not运算。and运算是与运算，只有所有都为True，and运算结果才是Trueor运算是或运算，只要其中有一个为True，or运算结果就是Truenot运算是非运算，它是一个单目运算符，把True变成False，False变成True 空值 空值是Python里一个特殊的值，用None表示。None不能理解为0，因为0是有意义的，而None是一个特殊的空值。 此外，Python还提供了列表、字典等多种数据类型，还允许创建自定义数据类型。 变量 变量的概念基本上和初中代数的方程变量是一致的，只是在计算机程序中，变量不仅可以是数字，还可以是任意数据类型。 变量在程序中就是用一个变量名表示了，变量名必须是大小写英文、数字和_的组合，且不能用数字开头，比如：12345a = 1 # 变量a是一个整数t_007 = 'T007' # 变量t_007是一个字符串Answer = True # 变量Answer是一个布尔值True 在Python中，等号=是赋值语句，可以把任意数据类型赋值给变量，同一个变量可以反复赋值，而且可以是不同类型的变量，例如：123456a = 123 # a是整数print aa = 'ABC' # a变为字符串print aa = True # a变为布尔类型print a 这种变量本身类型不固定的语言称之为动态语言，与之对应的是静态语言。 静态语言在定义变量时必须指定变量类型，如果赋值的时候类型不匹配，就会报错。 例如Java是静态语言，赋值语句如下（在Java里，// 表示注释）12int a = 123; // a是整数类型变量a = "ABC"; // 错误：不能把字符串赋给整型变量 和静态语言相比，动态语言更灵活，就是这个原因。 请不要把赋值语句的等号等同于数学的等号。比如下面的代码：12x = 10x = x + 2 如果从数学上理解x = x + 2那无论如何是不成立的，在程序中，赋值语句先计算右侧的表达式x + 2，得到结果12，再赋给变量x。由于x之前的值是10，重新赋值后，x的值变成12。 最后，理解变量在计算机内存中的表示也非常重要。当我们写：1a = 'ABC' 时，Python解释器干了两件事情： 在内存中创建了一个’ABC’的字符串。 在内存中创建了一个名为a的变量，并把它指向’ABC’。也可以把一个变量a赋值给另一个变量b，这个操作实际上是把变量b指向变量a所指向的数据，例如下面的代码： 1234a = 'ABC'b = aa = 'XYZ'print b 最后一行打印出变量b的内容到底是’ABC’呢还是’XYZ’？如果从数学意义上理解，就会错误地得出b和a相同，也应该是’XYZ’，但实际上b的值是’ABC’，让我们一行一行地执行代码，就可以看到到底发生了什么事： 常量 所谓常量就是不能变的变量，比如常用的数学常数π就是一个常量。 在Python中，通常用全部大写的变量名表示常量1PI = 3.14159265359 但事实上PI仍然是一个变量，Python根本没有任何机制保证PI不会被改变， 所以，用全部大写的变量名表示常量只是一个习惯上的用法，如果你一定要改变变量PI的值，也没人能拦住你。 字符串和编码编码 我们已经讲过了，字符串也是一种数据类型，但是，字符串比较特殊的是还有一个编码问题。 因为计算机只能处理数字，如果要处理文本，就必须先把文本转换为数字才能处理。 最早的计算机在设计时采用8个比特（bit）作为一个字节（byte）， 所以，一个字节能表示的最大的整数就是255（二进制11111111=十进制255），如果要表示更大的整数，就必须用更多的字节。 比如两个字节可以表示的最大整数是65535，4个字节可以表示的最大整数是4294967295。 由于计算机是美国人发明的，因此，最早只有127个字母被编码到计算机里，也就是大小写英文字母、数字和一些符号， 这个编码表被称为ASCII编码，比如大写字母A的编码是65，小写字母z的编码是122。 但是要处理中文显然一个字节是不够的，至少需要两个字节，而且还不能和ASCII编码冲突， 所以，中国制定了GB2312编码，用来把中文编进去。 你可以想得到的是，全世界有上百种语言，日本把日文编到Shift_JIS里，韩国把韩文编到Euc-kr里， 各国有各国的标准，就会不可避免地出现冲突，结果就是，在多语言混合的文本中，显示出来会有乱码。 因此，Unicode应运而生。Unicode把所有语言都统一到一套编码里，这样就不会再有乱码问题了。 Unicode标准也在不断发展，但最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要4个字节）。 现代操作系统和大多数编程语言都直接支持Unicode。 现在，捋一捋ASCII编码和Unicode编码的区别：ASCII编码是1个字节，而Unicode编码通常是2个字节。 字母A用ASCII编码是十进制的65，二进制的01000001； 字符0用ASCII编码是十进制的48，二进制的00110000，注意字符’0’和整数0是不同的； 汉字中已经超出了ASCII编码的范围，用Unicode编码是十进制的20013，二进制的01001110 00101101。 你可以猜测，如果把ASCII编码的A用Unicode编码，只需要在前面补0就可以，因此，A的Unicode编码是00000000 01000001。 新的问题又出现了：如果统一成Unicode编码，乱码问题从此消失了。但是，如果你写的文本基本上全部是英文的话， 用Unicode编码比ASCII编码需要多一倍的存储空间，在存储和传输上就十分不划算。 所以，本着节约的精神，又出现了把Unicode编码转化为“可变长编码”的UTF-8编码。 UTF-8编码把一个Unicode字符根据不同的数字大小编码成1-6个字节， 常用的英文字母被编码成1个字节， 汉字通常是3个字节， 只有很生僻的字符才会被编码成4-6个字节。 如果你要传输的文本包含大量英文字符，用UTF-8编码就能节省空间： 字符 ASCII Unicode UTF-8A 01000001 00000000 01000001 01000001中 x 01001110 00101101 11100100 10111000 10101101 从上面的表格还可以发现，UTF-8编码有一个额外的好处，就是ASCII编码实际上可以被看成是UTF-8编码的一部分， 所以，大量只支持ASCII编码的历史遗留软件可以在UTF-8编码下继续工作。 搞清楚了ASCII、Unicode和UTF-8的关系，我们就可以总结一下现在计算机系统通用的字符编码工作方式： 在计算机内存中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码。 用记事本(推荐使用notepad++)编辑的时候，从文件读取的UTF-8字符被转换为Unicode字符到内存里， 编辑完成后，保存的时候再把Unicode转换为UTF-8保存到文件： 浏览网页的时候，服务器会把动态生成的Unicode内容转换为UTF-8再传输到浏览器： 所以你看到很多网页的源码上会有类似1&lt;meta charset=&quot;UTF-8&quot; /&gt; 的信息，表示该网页正是用的UTF-8编码。 Python的字符串 搞清楚了令人头疼的字符编码问题后，我们再来研究Python对Unicode的支持。 因为Python的诞生比Unicode标准发布的时间还要早，所以最早的Python只支持ASCII编码， 普通的字符串’ABC’在Python内部都是ASCII编码的。Python提供了ord()和chr()函数，可以把字母和对应的数字相互转换：1234567891011&gt;&gt;&gt; ord(&apos;A&apos;)65&gt;&gt;&gt; ord(&quot;A&quot;)65&gt;&gt;&gt; ord(&apos;a&apos;)97&gt;&gt;&gt; chr(65)&apos;A&apos; Python在后来添加了对Unicode的支持，以Unicode表示的字符串用u’…’表示，比如：1234567891011&gt;&gt;&gt; print u'中文'中文&gt;&gt;&gt; u'中'u'\u4e2d'&gt;&gt;&gt; u'中文'u'\u4e2d\u6587'&gt;&gt;&gt; print u'\u4e2d\u6587'中文 写u’中’和u’\u4e2d’是一样的，\u后面是十六进制的Unicode码。因此，u’A’和u’\u0041’也是一样的。 两种字符串如何相互转换？字符串’xxx’虽然是ASCII编码，但也可以看成是UTF-8编码，而u’xxx’则只能是Unicode编码。 把u’xxx’转换为UTF-8编码的’xxx’用encode(‘utf-8’)方法：1234567891011121314151617181920212223242526272829303132333435363738394041&gt;&gt;&gt; u&apos;ABC&apos;.encode(&apos;utf-8&apos;)&apos;ABC&apos;&gt;&gt;&gt; u&apos;ABC&apos;.encode(&apos;gbk&apos;)&apos;ABC&apos;&gt;&gt;&gt; u&apos;ABC&apos;.encode(&apos;ansi&apos;)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;LookupError: unknown encoding: ansi&gt;&gt;&gt; u&apos;ABC&apos;.encode(&apos;ANSI&apos;)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;LookupError: unknown encoding: ANSI&gt;&gt;&gt; u&apos;ABC&apos;.encode(&apos;gb2312&apos;)&apos;ABC&apos;&gt;&gt;&gt; u&apos;ABC&apos;.encode(&apos;utf-16&apos;)&apos;\xff\xfeA\x00B\x00C\x00&apos;&gt;&gt;&gt; u&apos;ABC&apos;.encode(&apos;utf-32&apos;)&apos;\xff\xfe\x00\x00A\x00\x00\x00B\x00\x00\x00C\x00\x00\x00&apos;&gt;&gt;&gt; u&apos;ABC&apos;.encode(&apos;utf-64&apos;)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;LookupError: unknown encoding: utf-64&gt;&gt;&gt; u&apos;中文&apos;.encode(&apos;utf-8&apos;)&apos;\xe4\xb8\xad\xe6\x96\x87&apos;&gt;&gt;&gt; u&apos;中文&apos;.encode(&apos;gbk&apos;)&apos;\xd6\xd0\xce\xc4&apos;&gt;&gt;&gt; u&apos;中文&apos;.encode(&apos;gb2312&apos;)&apos;\xd6\xd0\xce\xc4&apos;&gt;&gt;&gt; u&apos;中文&apos;.encode(&apos;ansi&apos;)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;LookupError: unknown encoding: ansi&gt;&gt;&gt; u&apos;中文&apos;.encode(&apos;utf-16&apos;)&apos;\xff\xfe-N\x87e&apos;&gt;&gt;&gt; u&apos;中文&apos;.encode(&apos;utf-32&apos;)&apos;\xff\xfe\x00\x00-N\x00\x00\x87e\x00\x00&apos;&gt;&gt;&gt; u&apos;中文&apos;.encode(&apos;utf-64&apos;)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;LookupError: unknown encoding: utf-64 英文字符转换后表示的UTF-8的值和Unicode值相等（但占用的存储空间不同）， 而中文字符转换后1个Unicode字符将变为3个UTF-8字符，你看到的\xe4就是其中一个字节， 因为它的值是228，没有对应的字母可以显示，所以以十六进制显示字节的数值。 len()函数可以返回字符串的长度：12345678910111213141516171819&gt;&gt;&gt; len('ABC')3&gt;&gt;&gt; len("ABC")3&gt;&gt;&gt; len(u"ABC")3&gt;&gt;&gt; len(U"ABC")3&gt;&gt;&gt; len(U'ABC')3&gt;&gt;&gt; len(u'ABC')3&gt;&gt;&gt; len(u'中文')2&gt;&gt;&gt; len(u'\u4e2d\u6587')2&gt;&gt;&gt; len('\xe4\xb8\xad\xe6\x96\x87')6 反过来，把UTF-8编码表示的字符串’xxx’转换为Unicode字符串u’xxx’用decode(‘utf-8’)方法：123456&gt;&gt;&gt; &apos;abc&apos;.decode(&apos;utf-8&apos;)u&apos;abc&apos;&gt;&gt;&gt; &apos;\xe4\xb8\xad\xe6\x96\x87&apos;.decode(&apos;utf-8&apos;)u&apos;\u4e2d\u6587&apos;&gt;&gt;&gt; print &apos;\xe4\xb8\xad\xe6\x96\x87&apos;.decode(&apos;utf-8&apos;)中文 由于Python源代码也是一个文本文件，所以，当你的源代码中包含中文的时候，在保存源代码时， 就需要务必指定保存为UTF-8编码。当Python解释器读取源代码时，为了让它按UTF-8编码读取， 我们通常在文件开头写上这两行：12#!/usr/bin/env python# -*- coding: utf-8 -*- 第一行注释是为了告诉Linux/OS X系统，这是一个Python可执行程序，Windows系统会忽略这个注释； 第二行注释是为了告诉Python解释器，按照UTF-8编码读取源代码，否则，你在源代码中写的中文输出可能会有乱码。 如果你使用Notepad++进行编辑，除了要加上# -- coding: utf-8 --外，中文字符串必须是Unicode字符串： 格式化 最后一个常见的问题是如何输出格式化的字符串。 我们经常会输出类似’亲爱的xxx你好！你xx月的话费是xx，余额是xx’之类的字符串，而xxx的内容都是根据变量变化的， 所以，需要一种简便的格式化字符串的方式。 在Python中，采用的格式化方式和C语言是一致的，用%实现，举例如下：123456789&gt;&gt;&gt; 'Hello, %s' % 'Tom''Hello, Tom'&gt;&gt;&gt; 'Hello, %s' % '123''Hello, 123'&gt;&gt;&gt; 'Hi, %s, you have $%d' % ('Tom', 10000)'Hi, Tom, you have $10000'&gt;&gt;&gt; 'Hi, %s, you have $%d' % ('123', 123)'Hi, 123, you have $123' 你可能猜到了，%运算符就是用来格式化字符串的。 在字符串内部，%s表示用字符串替换，%d表示用整数替换，有几个%?占位符，后面就跟几个变量或者值，顺序要对应好。 如果只有一个%?，括号可以省略。 常见的占位符有：%d 整数%f 浮点数%s 字符串%x 十六进制整数 其中，格式化整数和浮点数还可以指定是否补0和整数与小数的位数：12345678&gt;&gt;&gt; '%2d-%02d' % (3, 1)' 3-01'&gt;&gt;&gt; '%2d-%02d-%03d' % (3,2,1)' 3-02-001'&gt;&gt;&gt; '%.2f' % 3.1415926'3.14' 如果你不太确定应该用什么，%s永远起作用，它会把任何数据类型转换为字符串：12&gt;&gt;&gt; 'Age: %s. Gender: %s' % (25, True)'Age: 25. Gender: True' 对于Unicode字符串，用法完全一样，但最好确保替换的字符串也是Unicode字符串：12&gt;&gt;&gt; u'Hi, %s' % u'Michael'u'Hi, Michael' 有些时候，字符串里面的%是一个普通字符怎么办？这个时候就需要转义，用%%来表示一个%：12&gt;&gt;&gt; 'growth rate: %d %%' % 7'growth rate: 7 %' 使用list和tuplelistPython内置的一种数据类型是列表：list。list是一种有序的集合，可以随时添加和删除其中的元素。1234567891011121314151617181920# 比如，列出班里所有同学的名字，就可以用一个list表示：&gt;&gt;&gt; c = ['1', '2', '3']&gt;&gt;&gt; c['1', '2', '3']# 变量c就是一个list。用len()函数可以获得list元素的个数&gt;&gt;&gt; len(c)3# 用索引来访问list中每一个位置的元素，记得索引是从0开始的&gt;&gt;&gt; c[0]'1'&gt;&gt;&gt; c[1]'2'&gt;&gt;&gt; c[2]'3'&gt;&gt;&gt; c[3]Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;IndexError: list index out of range 如果要取最后一个元素，除了计算索引位置外，还可以用-1做索引，直接获取最后一个元素12345678910&gt;&gt;&gt; c[-1]'3'# 以此类推，可以获取倒数第2个、倒数第3个&gt;&gt;&gt; c[-2]'2'&gt;&gt;&gt; c[-5]Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;IndexError: list index out of range 12345678910111213141516171819202122232425262728# list是一个可变的有序表，所以，可以往list中追加元素到末尾&gt;&gt;&gt; c.append('4')&gt;&gt;&gt; c['1', '2', '3', '4']# 也可以把元素插入到指定的位置，比如索引号为1的位置&gt;&gt;&gt; c.insert(1, 'tom')&gt;&gt;&gt; c['1', 'tom', '2', '3', '4']# 要删除list末尾的元素，用pop()方法&gt;&gt;&gt; c.pop&lt;built-in method pop of list object at 0x000000000276BF48&gt;&gt;&gt;&gt; c.pop()'4'&gt;&gt;&gt; c['1', 'tom', '2', '3']# 要删除指定位置的元素，用pop(i)方法，其中i是索引位置&gt;&gt;&gt; c.pop(1)'tom'&gt;&gt;&gt; c['1', '2', '3']# 要把某个元素替换成别的元素，可以直接赋值给对应的索引位置&gt;&gt;&gt; c[1] = 'ke'&gt;&gt;&gt; c['1', 'ke', '3'] 条件判断和循环使用dict和set(5) 函数调用函数定义函数函数的参数递归函数(6) 高级特性切片迭代列表生成式生成器(7) 函数式编程高阶函数map/reducefiltersorted返回函数匿名函数装饰器偏函数(8) 模块使用模块安装第三方模块使用future(9) 面向对象编程(10) 面向对象高级编程(11) 错误、调试和测试(12) IO编程(13) 进程和线程(14) 正则表达式(15) 常用内建模块(16) 常用第三方模块(17) 图形界面(18) 网络编程(19) 电子邮件(20) 访问数据库(21) Web开发(22) 协程实战References[1] python 2.7[2] python 3.5[3] Python基础编程1小时快速实战掌握[4] 模块 logging — Python 的日志记录工具[5] Python日志库logging总结-可能是目前为止将logging库总结的最好的一篇文章]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[securecrt配置]]></title>
    <url>%2F2017%2F05%2F23%2Fsecurecrt%2F</url>
    <content type="text"><![CDATA[(1) 修改全局设置 Options –&gt; Global Options –&gt; General –&gt; Default Session –&gt; Edit Default Settings.. (1.1) 设置字符编码为UTF-8 Appearance 找到 Character encoding 选项, 设置为 UTF-8 格式。 (1.2) 设置字体 Apperance –&gt; Fonts ，设置 Normal font 为 Consolas (1.3) 设置全局缓冲区大小 Terminal –&gt; Emulation –&gt; Scrollback ，Scrollback buffer 默认为500行，可以最大调整到128000行。 (1.4) 设置日志 Terminal --&gt; Log File ， 设置 Log file name 为 D:\SoftWare\SecureCRT_x86_7.1.1.264_en\log\ssh_log\%S_%Y_%M_%D.log Options 设置 Start log upon connect 和 Append to file (1.5) 设置每300秒发个回车不掉线 Terminal –&gt; Anti-idle Send String \n every 300 seconds (2) 设置字符编码为UTF-8 Options –&gt; Session Options –&gt; Appearance 。 找到 Character encoding 选项, 设置为 UTF-8 格式。 (3) 设置背景颜色 Options –&gt; Sessions options –&gt; Terminal –&gt; Emulation，Terminal 下拉列表下选择 Linux，勾选 ANSI Color 复选框、Use color scheme 复选框 。 Options –&gt; Global Options –&gt; Terminal –&gt; Apperance –&gt; Advanced, 选中一个 Color schemes 点击右边的 Edit ，设置 Foreground(前景) 和 Background (背景) 。 (4) 设置会话的缓冲区大小 Options –&gt; Sessions options –&gt; Terminal –&gt; Emulation –&gt; Scrollback ， Scrollback buffer 默认为500行，可以最大调整到128000行。 References[1] SecureCRT优化调整、永久设置、保护眼睛和配色方案]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java解析json]]></title>
    <url>%2F2017%2F05%2F23%2Fjava-parser-json%2F</url>
    <content type="text"><![CDATA[(1) Java解析JSON 建议使用 fastjson (2) 使用 fastjson 时遇到的问题(2.1) java.lang.VerifyError12java.lang.VerifyError: (class: com/alibaba/fastjson/parser/deserializer/FastjsonASMDeserializer_3_ErpOrderResVo, method: deserialze signature: (Lcom/alibaba/fastjson/parser/DefaultJSONParser;Ljava/lang/reflect/Type;Ljava/lang/Object;I)Ljava/lang/Object;) Accessing value from uninitialized register 85 at java.lang.Class.getDeclaredConstructors0(Native Method) (2.2)fastjson 32个字段时反序列化报错的问题 1.2.28 32整数倍个字段 OOM bug 32个字段时反序列化报错 (3) 老办法 以下是老方法/笨办法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156import com.alibaba.fastjson.JSONArray;import com.alibaba.fastjson.JSONObject;import com.xxxx.file.FileUtil;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;/** * @version V1.0 * @date 2017-09-21 21:48 */public class JsonTest &#123; private static final Logger logger = LoggerFactory.getLogger(JsonTest.class); public static void main(String[] args) &#123; String str = null; String filePath = "doc/html/xiecheng/crawler_flights_bj-yc-2017-09-23_result.json"; try &#123; str = FileUtil.readFile2String(filePath); &#125; catch (IOException e) &#123; logger.error("", e); &#125; JSONObject json = JSONObject.parseObject(str); JSONArray fisArray = json.getJSONArray("fis"); int size = fisArray.size(); for(int i = 0; i &lt; size; i++)&#123; JSONObject oneFlight = (JSONObject) fisArray.get(i); logger.info(i +": &#123;&#125;", oneFlight); &#125; &#125; /** * * @param str */ public List&lt;Map&lt;String, String&gt;&gt; parserJson(String str) &#123; if (StringUtil.isEmpty(str)) &#123; logger.error("json为空"); return null; &#125; JSONObject json = JSONObject.parseObject(str); // 航空公司 JSONObject als = (JSONObject) json.get("als"); logger.info("符合要求的航空公司：&#123;&#125;", als); Set&lt;Map.Entry&lt;String, Object&gt;&gt; set = als.entrySet(); logger.info("set：&#123;&#125;", set); JSONObject apb = (JSONObject) json.get("apb"); logger.info("符合要求的机场：&#123;&#125;", apb); // 中转组合，暂时不考虑，业务上也暂时不需要，稍微有点复杂，时间允许可以解析 // json.get("tf"); // 直航 JSONArray fisArray = json.getJSONArray("fis"); if (fisArray == null || fisArray.isEmpty()) &#123; logger.error("fisArray结果为空"); return null; &#125; int size = fisArray.size(); if (size == 0) &#123; logger.error("fisArray 里元素个数为0"); return null; &#125; List&lt;Map&lt;String, String&gt;&gt; list = new ArrayList&lt;&gt;(size); for (int i = 0; i &lt; size; i++) &#123; Map&lt;String, String&gt; dataMap = new HashMap&lt;&gt;(16); // JSONObject oneFlight = (JSONObject) fisArray.get(i); logger.debug(i + ": &#123;&#125;", oneFlight); // 到达城市代码 String acc = (String) oneFlight.get("acc"); dataMap.put("acc", acc); // 目的地 String acn = (String) oneFlight.get("acn"); dataMap.put("acn", acn); // 航空公司 String alc = (String) oneFlight.get("alc"); String flightCompany = null; if (als.containsKey(alc)) &#123; flightCompany = als.getString(alc); &#125; dataMap.put("alc", flightCompany); // 到达机场 String apbn = (String) oneFlight.get("apbn"); dataMap.put("apbn", apbn); // 到达时间 String at = (String) oneFlight.get("at"); dataMap.put("at", at); // 到达城市代码 String dcc = (String) oneFlight.get("dcc"); dataMap.put("dcc", dcc); // 出发地 String dcn = (String) oneFlight.get("dcn"); dataMap.put("dcn", dcn); // 出发机场 String dpbn = (String) oneFlight.get("dpbn"); dataMap.put("dpbn", dpbn); // 航站楼 String dsmsn = (String) oneFlight.get("dsmsn"); dataMap.put("dsmsn", dsmsn); // 出发时间 String dt = (String) oneFlight.get("dt"); dataMap.put("dt", dt); // 航班号 String fn = (String) oneFlight.get("fn"); dataMap.put("fn", fn); // 实际票价 String lp = oneFlight.get("lp").toString(); dataMap.put("lp", lp); JSONObject confort = oneFlight.getJSONObject("confort"); String historyPunctualityArr = (String) confort.get("HistoryPunctualityArr"); if (StringUtil.isNotEmpty(historyPunctualityArr)) &#123; dataMap.put("historyPunctualityArr", historyPunctualityArr.replace("%", "")); &#125; logger.info("本条航班数据：&#123;&#125;", dataMap); list.add(dataMap); &#125; return list; &#125; // end method &#125; References[1] Java几种常用JSON库性能比较]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>json</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[neo4j-admin 使用 笔记]]></title>
    <url>%2F2017%2F05%2F22%2Fneo4j-admin%2F</url>
    <content type="text"><![CDATA[neo4j-admin使用笔记 建议还是先用 neo4j-import，比neo4j-admin 好用多了 neo4j-admin import 虽然在以后的版本里会把 neo4j-import 替掉，但是我感觉neo4j-import好用，比neo4j-admin import多一些参数 1234567891011121314151617181920212223242526272829303132333435363738C:\ProfessionSofware\Neo4j\neo4j-community-3.3.1\binλ neo4j-admin helpusage: neo4j-admin &lt;command&gt;Manage your Neo4j instance.environment variables: NEO4J_CONF Path to directory which contains neo4j.conf. NEO4J_DEBUG Set to anything to enable debug output. NEO4J_HOME Neo4j home directory. HEAP_SIZE Set size of JVM heap during command execution. Takes a number and a unit, for example 512m.available commands:General check-consistency Check the consistency of a database. help This help text, or help for the command specified in &lt;command&gt;. import Import from a collection of CSV files or a pre-3.0 database. store-info Prints information about a Neo4j database store.Authentication set-default-admin Sets the default admin user when no roles are present. set-initial-password Sets the initial password of the initial admin user (&apos;neo4j&apos;).Offline backup dump Dump a database into a single-file archive. load Load a database from an archive created with the dump command.Use neo4j-admin help &lt;command&gt; for more details. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455WKQ@WKQ-PC C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\bin&gt; neo4j-admin import -help警告: This command does not appear to be running with administrative rights. Some commands may fail e.g. Start/StopDirectory &apos;C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\data\databases\graph.db&apos; already contains a databaseusage: neo4j-admin import [--mode=csv] [--database=&lt;name&gt;] [--additional-config=&lt;config-file-path&gt;] [--report-file=&lt;filename&gt;] [--nodes[:Label1:Label2]=&lt;&quot;file1,file2,...&quot;&gt;] [--relationships[:RELATIONSHIP_TYPE]=&lt;&quot;file1,file2,...&quot;&gt;] [--id-type=&lt;STRING|INTEGER|ACTUAL&gt;] [--input-encoding=&lt;character-set&gt;]usage: neo4j-admin import --mode=database [--database=&lt;name&gt;] [--additional-config=&lt;config-file-path&gt;] [--from=&lt;source-directory&gt;]Import a collection of CSV files with --mode=csv (default), or a database from apre-3.0 installation with --mode=database.options: --database=&lt;name&gt; Name of database. [default:graph.db] --additional-config=&lt;config-file-path&gt; Configuration file to supply additional configuration in. [default:] --mode=&lt;database|csv&gt; Import a collection of CSV files or a pre-3.0 installation. [default:csv] --from=&lt;source-directory&gt; The location of the pre-3.0 database (e.g. &lt;neo4j-root&gt;/data/graph.db). [default:] --report-file=&lt;filename&gt; File in which to store the report of the csv-import. [default:import.report] --nodes[:Label1:Label2]=&lt;&quot;file1,file2,...&quot;&gt; Node CSV header and data. Multiple files will be logically seen as one big file from the perspective of the importer. The first line must contain the header. Multiple data sources like these can be specified in one import, where each data source has its own header. Note that file groups must be enclosed in quotation marks. [default:] --relationships[:RELATIONSHIP_TYPE]=&lt;&quot;file1,file2,...&quot;&gt; Relationship CSV header and data. Multiple files will be logically seen as one big file from the perspective of the importer. The first line must contain the header. Multiple data sources like these can be specified in one import, where each data source has its own header. Note that file groups must be enclosed in quotation marks. [default:] --id-type=&lt;STRING|INTEGER|ACTUAL&gt; Each node must provide a unique id. This is used to find the correct nodes when creating relationships. Possible values are: STRING: arbitrary strings for identifying nodes, INTEGER: arbitrary integer values for identifying nodes, ACTUAL: (advanced) actual node ids. For more information on id handling, please see the Neo4j Manual: https://neo4j.com/docs/operations-manual/current/tools/import/ [default:STRING] --input-encoding=&lt;character-set&gt; Character set that input data is encoded in. [default:UTF-8]]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK 安装 配置]]></title>
    <url>%2F2017%2F05%2F04%2Fjdk-install-config%2F</url>
    <content type="text"><![CDATA[JDK安装步骤 1.检查是否有JDK，如果有，可以用，直接完成，如果版本低或者没有，如果有open JDK(系统自带的)，可以把open JDK卸载 2.如果有JSE JDK 低版本，可以忽略低版本，也可以删除低版本，安装新版本的JDK，配置/etc/profile文件就行 3.安装JDK，并配置/etc/profile文件就行 Windows系统可以通过环境变量 (1) 卸载JDK(可以省略该步骤)1234rpm -qa | grep jdk 查看已安装的JDKrpm -qa | grep gcjyum -y remove java-1.7.0-openjdk-headless.x86_64 rpm -e –nodeps java-1.4.2-gcj-compat-1.4.2.0-40jpp.115 (2) 下载JDK12345JDK 8u151wget --no-check-certificate -c --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u151-b12/e758a0de34e24606bca991d704f6dcbf/jdk-8u151-linux-x64.tar.gzJDK 8u172wget --no-check-certificate -c --header &quot;Cookie: oraclelicense=accept-securebackup-cookie&quot; http://download.oracle.com/otn-pub/java/jdk/8u172-b11/a58eab1ec242421181065cdc37240b08/jdk-8u172-linux-x64.tar.gz 国内华为镜像 https://repo.huaweicloud.com/java/jdk/ (3) 安装JDK123456789tar -zxvf jdk-8u111-linux-x64.tar.gz #解压vi /etc/profile #修改配置文件在unset i unset -f pathmunge的下面加上export JAVA_HOME=/usr/java/jdk1.8.0_111export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarsource /etc/profile #使配置立即生效 (4) 查看JDK位置 (可以忽略该步骤)1234whereis javawhich java （java执行路径）echo $JAVA_HOMEecho $PATH 整个 /etc/profile 如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849 LOGNAME=$USER MAIL="/var/spool/mail/$USER"fi# Path manipulationif [ "$EUID" = "0" ]; then pathmunge /usr/sbin pathmunge /usr/local/sbinelse pathmunge /usr/local/sbin after pathmunge /usr/sbin afterfiHOSTNAME=`/usr/bin/hostname 2&gt;/dev/null`HISTSIZE=1000if [ "$HISTCONTROL" = "ignorespace" ] ; then export HISTCONTROL=ignorebothelse export HISTCONTROL=ignoredupsfiexport PATH USER LOGNAME MAIL HOSTNAME HISTSIZE HISTCONTROL# By default, we want umask to get set. This sets it for login shell# Current threshold for system reserved uid/gids is 200# You could check uidgid reservation validity in# /usr/share/doc/setup-*/uidgid fileif [ $UID -gt 199 ] &amp;&amp; [ "`id -gn`" = "`id -un`" ]; then umask 002else umask 022fifor i in /etc/profile.d/*.sh ; do if [ -r "$i" ]; then if [ "$&#123;-#*i&#125;" != "$-" ]; then . "$i" else . "$i" &gt;/dev/null fi fidoneunset iunset -f pathmungeexport JAVA_HOME=/usr/java/jdk1.8.0_111export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar (5) 为单个用户配置JDKvi .bash_profile #修改或新建.bash_profile .bash_profile文件如下123456789101112# .bash_profile# Get the aliases and functionsif [ -f ~/.bashrc ]; then . ~/.bashrcfi# User specific environment and startup programsPATH=$PATH:$HOME/.local/bin:$HOME/binexport JAVA_HOME=/home/admin/software/jdk1.8.0_111export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar (6) 可能遇到的错误bash: /usr/java/jdk1.8.0_111/bin/java: cannot execute binary file因为JDK是64位的，操作系统是32位的，所以会报这个错 (7) Windows环境变量配置JAVA_HOMEC:\ProfessionSofware\Java\jdk1.8.0_111 CLASSPATH.:%JAVA_HOME%\lib PATH%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin; References[1] linux wget 下载 JDK]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j 索引]]></title>
    <url>%2F2017%2F05%2F04%2Fneo4j-index%2F</url>
    <content type="text"><![CDATA[今天同事问了我一个有关neo4j索引的问题，不知道。就想对neo4j index深入了解一下。总结中… neo4j 3.1 indexThe main reason for using indexes in a graph database is to find the starting point in the graph as fastas possible. After that seek you rely on in-graph structures and the first class citizenship ofrelationships in the graph database to achieve high performance. Thus graph queries themselves donot need indexes to run fast.Indexes can be added at any time. Note that it will take some time for an index to come online whenthere’s existing data. Normally you don’t specify indexes when querying for data. They will be used automatically.This means we that can simply look up the Tom Hanks node, and the index will kick in behindthe scenes to boost performance. https://neo4j.com/docs/rest-docs/3.1/Chapter 17. Legacy indexingChapter 18. Unique indexing https://neo4j.com/docs/java-reference/current/#indexing-create-advanced6.10. Configuration and fulltext indexes https://neo4j.com/docs/developer-manual/3.1/get-started/cypher/labels-constraints-and-indexes/2.2.5. Constraints and indexes References[1] https://dzone.com/articles/indexing-neo4j-overview[2] https://neo4j.com/docs/rest-docs/3.1/ neo4j 3.1[3] https://neo4j.com/docs/java-reference/current/#indexing-create-advanced neo4j 2.3.3[4] https://dzone.com/articles/neo4j-indexes-match-merge[5] https://dzone.com/articles/spocklight-set-timeout-on-specification-methods[6] http://neo4j.com/docs/stable/indexing-create-advanced.html# neo4j 2.3.3[7] http://blog.csdn.net/u011697278/article/details/52462420[8] http://www.cnblogs.com/xiaoheike/p/5338664.html[9] http://nigelsmall.com/neo4j/index-confusion[10] https://dzone.com/articles/indexing-neo4j-overview[11] https://stackoverflow.com/questions/31770466/neo4j-auto-index-legacy-index-and-label-schema-differences-for-a-relative-to-a[12] https://github.com/neo4j/neo4j/wiki/Changelog]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>index</tag>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-5.7 安装 配置]]></title>
    <url>%2F2017%2F05%2F04%2Fmysql-install-and-config%2F</url>
    <content type="text"><![CDATA[MySQL安装配置 (1) Mysql安装整体安装步骤1.检查服务器是否有mysql，如果有，是否要卸载2.添加mysql组和mysql用户，用于设置mysql安装目录文件所有者和所属组3.mysql-5.7.16-linux-glibc2.5-x86_64.tar.gz解压，复制到/usr/local 目录下(其他目录也可以) 设置软链接4.进入mysql文件夹，也就是mysql所在的目录，并更改所属的组和用户5.安装、初始化、配置 数据库 (1.1) 检查服务器是否有mysql，如果有，是否要卸载1234567#查看是否安装mysql，grep的-i选项表示匹配时忽略大小写rpm -qa|grep -i mysql #查看是否有mysql路径下的程序在运行ps -ef | grep mysqld ps -el | grep mysqld#卸载mysqlrpm -e pcp-pmda-mysql --nodeps (1.2) 检查mysql组和用户是否存在，如无创建。12345678#检查mysql组和用户是否存在，如无创建。cat /etc/group | grep mysql #添加mysql组和mysql用户，用于设置mysql安装目录文件所有者和所属组groupadd mysql#useradd -r 参数表示mysql用户是系统用户，不可用于登录系统。useradd -r -g mysql mysql (1.3) 下载mysql安装包，解压到/usr/local目录下1234567http://dev.mysql.com/downloads/mysql/#downloads mysql官网下载地址https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.16-linux-glibc2.5-x86_64.tarhttps://cdn.mysql.com//Downloads/MySQL-5.7/mysql-5.7.21-linux-glibc2.12-x86_64.tar.gztar -zxvf mysql-5.7.16-linux-glibc2.5-x86_64.tar.gz #解压ln -s /usr/local/mysql-5.7.16-linux-glibc2.5-x86_64/ mysql #创建软链接，方便操作 (1.4) 进入mysql文件夹，也就是mysql所在的目录，并更改所属的组和用户12345cd mysql# 更改文件所属组和所属用户(后面必须要有. 否则会有chown: missing operand after ‘mysql’ Try &apos;chown --help&apos; for more information.的错误)chown -R mysql . #更改当前目录所属的用户(也替换为 chown -R mysql /usr/local/mysql )chgrp -R mysql . #更改当前目录所属的组(也替换为 chgrp -R mysql /usr/local/mysql ) (1.5) 安装、初始化、配置 数据库1234567891011121314151617181920212223242526272829303132# 执行安装脚本，--user 指的是mysql用户，--basedir是数据库的安装目录，可以自定义，--datadir是数据存放目录，可以自定义# 这一步会生成随机密码，一定要保存[root@host_wkq bin]# ./mysqld --initialize --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data2017-08-16T02:48:19.810647Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).2017-08-16T02:48:27.480885Z 0 [Warning] InnoDB: New log files created, LSN=457902017-08-16T02:48:29.962049Z 0 [Warning] InnoDB: Creating foreign key constraint system tables.2017-08-16T02:48:30.134064Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: 62d4c750-822d-11e7-98fc-fa163e50f135.2017-08-16T02:48:30.223416Z 0 [Warning] Gtid table is not ready to be used. Table &apos;mysql.gtid_executed&apos; cannot be opened.2017-08-16T02:48:30.224491Z 1 [Note] A temporary password is generated for root@localhost: wgU;&amp;c,hn6V,执行 cp -a ./support-files/my-default.cnf /etc/my.cnf 命令[root@iZ2ze5ynf8pjgvvvq0hzbfZ mysql]# cp -a ./support-files/my-default.cnf /etc/my.cnf #复制配置文件cp: overwrite ‘/etc/my.cnf’? yes执行 cp -a ./support-files/mysql.server /etc/init.d/mysqld 命令[root@iZ2ze5ynf8pjgvvvq0hzbfZ mysql]# cp -a ./support-files/mysql.server /etc/init.d/mysqld #复制配置文件进入bin目录，执行 ./mysqld_safe --user=mysql &amp; 命令[root@iZ2ze5ynf8pjgvvvq0hzbfZ bin]# ./mysqld_safe --user=mysql &amp; ##启动守护进程[1] 27812[root@iZ2ze5ynf8pjgvvvq0hzbfZ bin]# 2016-12-10T15:12:54.430174Z mysqld_safe Logging to &apos;/usr/local/mysql/data/iZ2ze5ynf8pjgvvvq0hzbfZ.err&apos;.2016-12-10T15:12:54.503677Z mysqld_safe Starting mysqld daemon with databases from /usr/local/mysql/data执行 /etc/init.d/mysqld restart 命令[root@iZ2ze5ynf8pjgvvvq0hzbfZ bin]# /etc/init.d/mysqld restart # 启动mysqlShutting down MySQL..2016-12-10T15:14:50.965214Z mysqld_safe mysqld from pid file /usr/local/mysql/data/iZ2ze5ynf8pjgvvvq0hzbfZ.pid ended [ OK ]Starting MySQL. [ OK ][1]+ Done ./mysqld_safe --user=mysql[root@iZ2ze5ynf8pjgvvvq0hzbfZ bin]# mysql安装完了，可以使用了。如果还想配置的，执行以下步骤。 用随机生成的密码登录mysql12345678910111213141516171819202122232425262728[root@localhost bin]# ./mysql -u root -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.7.16Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.mysql&gt; show databases;#第一次登录会提示你修改密码，修改密码就可以了，修改完重新登录。ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement. mysql&gt; SET PASSWORD = PASSWORD(&apos;abc2017qwer&apos;);Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; alter user &apos;root&apos;@&apos;localhost&apos; password expire never;Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)mysql&gt; exit;Bye 添加远程访问权限123456789101112131415161718192021222324mysql&gt; use mysql; Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; update user set host = &apos;%&apos; where user = &apos;root&apos;; #修改root用户的远程访问权限Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; select host, user from user; #查看设置是否成功+-----------+-----------+| host | user |+-----------+-----------+| % | root || localhost | mysql.sys |+-----------+-----------+2 rows in set (0.00 sec)mysql&gt; exitBye[root@iZ2ze5ynf8pjgvvvq0hzbfZ bin]# /etc/init.d/mysqld restartShutting down MySQL.. [ OK ]Starting MySQL. [ OK ][root@iZ2ze5ynf8pjgvvvq0hzbfZ bin]# mysql_install_db命令安装生成密码保存在/root/.mysql_secret，需要自己查看123456789101112131415mysql5.7会生成一个初始化密码，而在之前的版本首次登陆不需要登录。执行 bin/mysql_install_db --user=mysql --basedir=/usr/local/mysql/ --datadir=/usr/local/mysql/data/ 命令[root@localhost mysql]# bin/mysql_install_db --user=mysql --basedir=/usr/local/mysql/ --datadir=/usr/local/mysql/data/2016-06-01 15:23:25 [WARNING] mysql_install_db is deprecated. Please consider switching to mysqld --initialize2016-06-01 15:23:30 [WARNING] The bootstrap log isn&apos;t empty:2016-06-01 15:23:30 [WARNING] 2016-06-01T22:23:25.491840Z 0 [Warning] --bootstrap is deprecated. Please consider using --initialize instead2016-06-01T22:23:25.492256Z 0 [Warning] Changed limits: max_open_files: 1024 (requested 5000)2016-06-01T22:23:25.492260Z 0 [Warning] Changed limits: table_open_cache: 431 (requested 2000)执行 cat /root/.mysql_secret 命令[root@iZ2ze5ynf8pjgvvvq0hzbfZ bin]# cat /root/.mysql_secret #Password set for user &apos;root@localhost&apos; at 2016-12-10 23:06:16 Y*rBklxr_q&amp;) 这个是密码，有点难写 忘记MySQL密码Linux下如果忘记MySQL的root密码，可以通过修改配置的方法，重置root密码修改MySQL的配置文件（默认为/etc/my.cnf）,在[mysqld]下添加一行skip-grant-tables保存配置文件后，重启MySQL服务 service mysqld restart再次进入MySQL命令行 mysql -uroot -p,输入密码时直接回车，就会进入MySQL数据库了，这个时候按照常规流程修改root密码即可。12345678910update user set password=password(&apos;newpassword&apos;) where user=&apos;root&apos;; #MySQL-5.6修改密码命令update user set authentication_string=password(&apos;root&apos;) where user=&apos;root&apos; ; #MySQL-5.7修改密码命令flush privileges;mysql&gt; SET PASSWORD = PASSWORD(&apos;123456&apos;);Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 设置mysql开机启动12#执行 chkconfig --level 35 mysqld on 命令[root@iZ2ze5ynf8pjgvvvq0hzbfZ bin]# chkconfig --level 35 mysqld on MySQL启动、停止、重启命令1234/etc/init.d/mysqld start #使用mysqld启动MySQL/etc/init.d/mysqld stop #使用mysqld停止MySQL/etc/init.d/mysqld restart #重启MySQL (2) MySQL-5.7 my.cnf配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# For advice on how to change settings please see# http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html# *** DO NOT EDIT THIS FILE. It's a template which will be copied to the# *** default location during install, and will be replaced if you# *** upgrade to a newer version of MySQL.#[client]#port=3306#default-character-set = utf8mb4#[mysql]#default-character-set = utf8mb4[mysqld]#character-set-server=utf8mb4#innodb_file_per_table=1#lower_case_table_names=1#max_allowed_packet=1024M## Remove leading # and set to the amount of RAM for the most important data# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.# innodb_buffer_pool_size = 128M# Remove leading # to turn on a very important data integrity option: logging# changes to the binary log between backups.log_bin=mysql-binserver-id=1#if query_time &gt; 1s sql will log#long_query_time=1#slow-query-log=1#slow-query-log-file=/usr/local/mysql/logs/slow_query.log# These are commonly set, remove the # and set as required.basedir = /usr/local/mysqldatadir = /usr/local/mysql/dataport = 3306character-set-server = utf8mb4# server_id = .....sock = /var/lib/mysql/mysql.sock# Remove leading # to set options mainly useful for reporting servers.# The server defaults are faster for transactions and fast SELECTs.# Adjust sizes as needed, experiment to find the optimal values.# join_buffer_size = 128M# sort_buffer_size = 2M# read_rnd_buffer_size = 2M sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES (3) MySQL详细安装过程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284[root@host_wkq local]# pwd/usr/local[root@host_wkq local]# lltotal 807716drwxr-xr-x. 2 root root 6 May 25 2015 bindrwxr-xr-x. 2 root root 6 May 25 2015 etcdrwxr-xr-x. 2 root root 6 May 25 2015 gamesdrwxr-xr-x. 2 root root 6 May 25 2015 includedrwxr-xr-x 8 10 143 4096 Mar 15 16:35 jdk1.8.0_131-rw-r--r-- 1 root root 185540433 Aug 16 10:36 jdk-8u131-linux-x64.tar.gzdrwxr-xr-x. 2 root root 6 May 25 2015 libdrwxr-xr-x. 2 root root 6 May 25 2015 lib64drwxr-xr-x. 2 root root 6 May 25 2015 libexecdrwxr-xr-x 9 root root 120 Aug 16 10:37 mysql-5.7.16-linux-glibc2.5-x86_64-rw-r--r-- 1 root root 641555814 Aug 16 10:35 mysql-5.7.16-linux-glibc2.5-x86_64.tar.gzdrwxr-xr-x. 2 root root 6 May 25 2015 sbindrwxr-xr-x. 5 root root 46 Jul 25 10:38 sharedrwxr-xr-x. 2 root root 6 May 25 2015 src[root@host_wkq local]# tar -zxvf mysql-5.7.16-linux-glibc2.5-x86_64.tar.gz #解压mysql-5.7.16-linux-glibc2.5-x86_64/bin/myisam_ftdumpmysql-5.7.16-linux-glibc2.5-x86_64/bin/myisamchkmysql-5.7.16-linux-glibc2.5-x86_64/bin/myisamlogmysql-5.7.16-linux-glibc2.5-x86_64/bin/myisampackmysql-5.7.16-linux-glibc2.5-x86_64/bin/mysqlmysql-5.7.16-linux-glibc2.5-x86_64/bin/mysql_client_test_embeddedmysql-5.7.16-linux-glibc2.5-x86_64/bin/mysql_config_editormysql-5.7.16-linux-glibc2.5-x86_64/bin/mysql_embeddedmysql-5.7.16-linux-glibc2.5-x86_64/bin/mysql_install_dbmysql-5.7.16-linux-glibc2.5-x86_64/bin/mysql_pluginmysql-5.7.16-linux-glibc2.5-x86_64/bin/mysql_secure_installationmysql-5.7.16-linux-glibc2.5-x86_64/bin/mysql_ssl_rsa_setupmysql-5.7.16-linux-glibc2.5-x86_64/bin/mysql_tzinfo_to_sqlmysql-5.7.16-linux-glibc2.5-x86_64/bin/mysql_upgrademysql-5.7.16-linux-glibc2.5-x86_64/bin/mysqladminmysql-5.7.16-linux-glibc2.5-x86_64/bin/mysqlbinlogmysql-5.7.16-linux-glibc2.5-x86_64/bin/mysqlcheckmysql-5.7.16-linux-glibc2.5-x86_64/bin/mysqldumpmysql-5.7.16-linux-glibc2.5-x86_64/bin/mysqlimport...mysql-5.7.16-linux-glibc2.5-x86_64/share/serbian/errmsg.sysmysql-5.7.16-linux-glibc2.5-x86_64/share/slovak/errmsg.sysmysql-5.7.16-linux-glibc2.5-x86_64/share/spanish/errmsg.sysmysql-5.7.16-linux-glibc2.5-x86_64/share/swedish/errmsg.sysmysql-5.7.16-linux-glibc2.5-x86_64/share/ukrainian/errmsg.sysmysql-5.7.16-linux-glibc2.5-x86_64/support-files/mysql-log-rotatemysql-5.7.16-linux-glibc2.5-x86_64/support-files/mysqld_multi.servermysql-5.7.16-linux-glibc2.5-x86_64/lib/libmysqlclient.somysql-5.7.16-linux-glibc2.5-x86_64/lib/libmysqlclient.so.20mysql-5.7.16-linux-glibc2.5-x86_64/lib/libmysqlclient.so.20.3.3mysql-5.7.16-linux-glibc2.5-x86_64/share/install_rewriter.sqlmysql-5.7.16-linux-glibc2.5-x86_64/share/uninstall_rewriter.sqlmysql-5.7.16-linux-glibc2.5-x86_64/support-files/magicmysql-5.7.16-linux-glibc2.5-x86_64/support-files/mysql.servermysql-5.7.16-linux-glibc2.5-x86_64/docs/INFO_BINmysql-5.7.16-linux-glibc2.5-x86_64/docs/INFO_SRC[root@host_wkq local]# ln -s /usr/local/mysql-5.7.16-linux-glibc2.5-x86_64/ mysql #创建软链接，方便操作[root@host_wkq local]# lltotal 807716drwxr-xr-x. 2 root root 6 May 25 2015 bindrwxr-xr-x. 2 root root 6 May 25 2015 etcdrwxr-xr-x. 2 root root 6 May 25 2015 gamesdrwxr-xr-x. 2 root root 6 May 25 2015 includedrwxr-xr-x 8 10 143 4096 Mar 15 16:35 jdk1.8.0_131-rw-r--r-- 1 root root 185540433 Aug 16 10:36 jdk-8u131-linux-x64.tar.gzdrwxr-xr-x. 2 root root 6 May 25 2015 libdrwxr-xr-x. 2 root root 6 May 25 2015 lib64drwxr-xr-x. 2 root root 6 May 25 2015 libexeclrwxrwxrwx 1 root root 46 Aug 16 10:44 mysql -&gt; /usr/local/mysql-5.7.16-linux-glibc2.5-x86_64/drwxr-xr-x 9 root root 120 Aug 16 10:37 mysql-5.7.16-linux-glibc2.5-x86_64-rw-r--r-- 1 root root 641555814 Aug 16 10:35 mysql-5.7.16-linux-glibc2.5-x86_64.tar.gzdrwxr-xr-x. 2 root root 6 May 25 2015 sbindrwxr-xr-x. 5 root root 46 Jul 25 10:38 sharedrwxr-xr-x. 2 root root 6 May 25 2015 src[root@host_wkq local]# cd mysql #进入mysql目录[root@host_wkq mysql]# lltotal 40drwxr-xr-x 2 root root 4096 Aug 16 10:38 bin-rw-r--r-- 1 7161 31415 17987 Sep 29 2016 COPYINGdrwxr-xr-x 2 root root 52 Aug 16 10:38 docsdrwxr-xr-x 3 root root 4096 Aug 16 10:37 includedrwxr-xr-x 5 root root 4096 Aug 16 10:38 libdrwxr-xr-x 4 root root 28 Aug 16 10:37 man-rw-r--r-- 1 7161 31415 2478 Sep 29 2016 READMEdrwxr-xr-x 28 root root 4096 Aug 16 10:38 sharedrwxr-xr-x 2 root root 107 Aug 16 10:38 support-files[root@host_wkq mysql]# chown -R mysql . #更改当前目录所属的用户(也替换为 chown -R mysql /usr/local/mysql )[root@host_wkq mysql]# chgrp -R mysql . #更改当前目录所属的组(也替换为 chgrp -R mysql /usr/local/mysql )[root@host_wkq mysql]# cd bin[root@host_wkq bin]# pwd/usr/local/mysql/bin[root@host_wkq bin]# lltotal 1355456-rwxr-xr-x 1 mysql mysql 9207169 Sep 29 2016 innochecksum-rwxr-xr-x 1 mysql mysql 251228 Sep 29 2016 lz4_decompress-rwxr-xr-x 1 mysql mysql 8347930 Sep 29 2016 myisamchk-rwxr-xr-x 1 mysql mysql 7922860 Sep 29 2016 myisam_ftdump-rwxr-xr-x 1 mysql mysql 7676657 Sep 29 2016 myisamlog-rwxr-xr-x 1 mysql mysql 8050890 Sep 29 2016 myisampack-rwxr-xr-x 1 mysql mysql 5652398 Sep 29 2016 my_print_defaults-rwxr-xr-x 1 mysql mysql 10884339 Sep 29 2016 mysql-rwxr-xr-x 1 mysql mysql 9718874 Sep 29 2016 mysqladmin-rwxr-xr-x 1 mysql mysql 11780342 Sep 29 2016 mysqlbinlog-rwxr-xr-x 1 mysql mysql 10084494 Sep 29 2016 mysqlcheck-rwxr-xr-x 1 mysql mysql 217782410 Sep 29 2016 mysql_client_test_embedded-rwxr-xr-x 1 mysql mysql 4879 Sep 29 2016 mysql_config-rwxr-xr-x 1 mysql mysql 8648715 Sep 29 2016 mysql_config_editor-rwxr-xr-x 1 mysql mysql 253303409 Sep 29 2016 mysqld-rwxr-xr-x 1 mysql mysql 224181321 Sep 29 2016 mysqld-debug-rwxr-xr-x 1 mysql mysql 26705 Sep 29 2016 mysqld_multi-rwxr-xr-x 1 mysql mysql 27186 Sep 29 2016 mysqld_safe-rwxr-xr-x 1 mysql mysql 9989115 Sep 29 2016 mysqldump-rwxr-xr-x 1 mysql mysql 7424 Sep 29 2016 mysqldumpslow-rwxr-xr-x 1 mysql mysql 217423259 Sep 29 2016 mysql_embedded-rwxr-xr-x 1 mysql mysql 9734439 Sep 29 2016 mysqlimport-rwxr-xr-x 1 mysql mysql 10711017 Sep 29 2016 mysql_install_db-rwxr-xr-x 1 mysql mysql 5722776 Sep 29 2016 mysql_plugin-rwxr-xr-x 1 mysql mysql 18165530 Sep 29 2016 mysqlpump-rwxr-xr-x 1 mysql mysql 9664543 Sep 29 2016 mysql_secure_installation-rwxr-xr-x 1 mysql mysql 9669356 Sep 29 2016 mysqlshow-rwxr-xr-x 1 mysql mysql 9778641 Sep 29 2016 mysqlslap-rwxr-xr-x 1 mysql mysql 6098192 Sep 29 2016 mysql_ssl_rsa_setup-rwxr-xr-x 1 mysql mysql 216919461 Sep 29 2016 mysqltest_embedded-rwxr-xr-x 1 mysql mysql 5211835 Sep 29 2016 mysql_tzinfo_to_sql-rwxr-xr-x 1 mysql mysql 13025173 Sep 29 2016 mysql_upgrade-rwxr-xr-x 1 mysql mysql 29531087 Sep 29 2016 mysqlxtest-rwxr-xr-x 1 mysql mysql 5790065 Sep 29 2016 perror-rwxr-xr-x 1 mysql mysql 5431432 Sep 29 2016 replace-rwxr-xr-x 1 mysql mysql 5651159 Sep 29 2016 resolveip-rwxr-xr-x 1 mysql mysql 5732163 Sep 29 2016 resolve_stack_dump-rwxr-xr-x 1 mysql mysql 109100 Sep 29 2016 zlib_decompress# 执行安装脚本，--user 指的是mysql用户，--basedir是数据库的安装目录，可以自定义，--datadir是数据存放目录，可以自定义# 这一步会生成随机密码，一定要保存[root@host_wkq bin]# ./mysqld --initialize --user=mysql --basedir=/usr/local/mysql/ --datadir=/usr/local/mysql/data/2017-08-16T02:48:19.810647Z 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).2017-08-16T02:48:27.480885Z 0 [Warning] InnoDB: New log files created, LSN=457902017-08-16T02:48:29.962049Z 0 [Warning] InnoDB: Creating foreign key constraint system tables.2017-08-16T02:48:30.134064Z 0 [Warning] No existing UUID has been found, so we assume that this is the first time that this server has been started. Generating a new UUID: 62d4c750-822d-11e7-98fc-fa163e50f135.2017-08-16T02:48:30.223416Z 0 [Warning] Gtid table is not ready to be used. Table 'mysql.gtid_executed' cannot be opened.2017-08-16T02:48:30.224491Z 1 [Note] A temporary password is generated for root@localhost: wgU;&amp;c,hn6V,[root@host_wkq bin]# cd ..[root@host_wkq mysql]# pwd/usr/local/mysql[root@host_wkq mysql]# lltotal 40drwxr-xr-x 2 mysql mysql 4096 Aug 16 10:38 bin-rw-r--r-- 1 mysql mysql 17987 Sep 29 2016 COPYINGdrwxr-x--- 5 mysql mysql 139 Aug 16 10:48 datadrwxr-xr-x 2 mysql mysql 52 Aug 16 10:38 docsdrwxr-xr-x 3 mysql mysql 4096 Aug 16 10:37 includedrwxr-xr-x 5 mysql mysql 4096 Aug 16 10:38 libdrwxr-xr-x 4 mysql mysql 28 Aug 16 10:37 man-rw-r--r-- 1 mysql mysql 2478 Sep 29 2016 READMEdrwxr-xr-x 28 mysql mysql 4096 Aug 16 10:38 sharedrwxr-xr-x 2 mysql mysql 107 Aug 16 10:38 support-files[root@host_wkq mysql]# cp -a ./support-files/my-default.cnf /etc/my.cnf # 复制mysql配置文件cp: overwrite ‘/etc/my.cnf’? y[root@host_wkq mysql]# cp -a ./support-files/mysql.server /etc/init.d/mysqld # 复制mysql配置文件[root@host_wkq mysql]# cd bin[root@host_wkq bin]# pwd/usr/local/mysql/bin[root@host_wkq bin]# ./mysqld_safe --user=mysql &amp; #启动守护进程[1] 18188[root@host_wkq bin]# 2017-08-16T02:53:55.647112Z mysqld_safe Logging to '/usr/local/mysql/data/host_wkq.err'.2017-08-16T02:53:55.745534Z mysqld_safe Starting mysqld daemon with databases from /usr/local/mysql/data[root@host_wkq bin]# [root@host_wkq bin]# [root@host_wkq bin]# /etc/init.d/mysqld restart #启动mysqlShutting down MySQL..2017-08-16T02:55:43.053119Z mysqld_safe mysqld from pid file /usr/local/mysql/data/host_wkq.pid ended SUCCESS! Starting MySQL.. SUCCESS! [1]+ Done ./mysqld_safe --user=mysql[root@host_wkq bin]# [root@host_wkq bin]# ln -s /usr/local/mysql/bin/mysql /usr/bin #创建软链接，避免启动失败[root@host_wkq bin]# mysql -u root -pEnter password: Welcome to the MySQL monitor. Commands end with ; or \g.Your MySQL connection id is 2Server version: 5.7.16Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.mysql&gt; show databases;ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement.mysql&gt; SET PASSWORD = PASSWORD('mysql_root_password'); #修改密码Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || sys |+--------------------+4 rows in set (0.00 sec)mysql&gt; use mysqlReading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+---------------------------+| Tables_in_mysql |+---------------------------+| columns_priv || db || engine_cost || event || func || general_log || gtid_executed || help_category || help_keyword || help_relation || help_topic || innodb_index_stats || innodb_table_stats || ndb_binlog_index || plugin || proc || procs_priv || proxies_priv || server_cost || servers || slave_master_info || slave_relay_log_info || slave_worker_info || slow_log || tables_priv || time_zone || time_zone_leap_second || time_zone_name || time_zone_transition || time_zone_transition_type || user |+---------------------------+31 rows in set (0.00 sec)mysql&gt; select user,host from user;+-----------+-----------+| user | host |+-----------+-----------+| mysql.sys | localhost || root | localhost |+-----------+-----------+2 rows in set (0.00 sec)mysql&gt; alter user 'root'@'localhost' password expire never; #设置root账户永不过期Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges; #把对数据库的操作同步到内存Query OK, 0 rows affected (0.01 sec)mysql&gt; exitBye[root@host_wkq bin]# expire Windows安装配置MySQLhttps://dev.mysql.com/downloads/ 所有系统所有版本的下载地址 MYSQL=C:\ProfessionalSoftware\MySQL\mysql-5.7.22-winx64%MySQL%\bin References[1] 官方文档[2] server-configuration-defaults[3] option-files[4] Mysql 5.7 Linux安装详细步骤[5] Linux安装MySQL的两种方法[6] linux下mysql的root密码忘记解决方法[7] Linux下MySQL忘记root密码怎么办[8] MySQL 二进制日志(Binary Log)[9] MySQL5.7 开启bin-log功能 [10] MySQL 5.7 开启binary log(binlog)及注意事项]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本分析 应用场景]]></title>
    <url>%2F2017%2F05%2F04%2Ftext-analysis-application-scenarios%2F</url>
    <content type="text"><![CDATA[文本分析的应用场景有哪些 1 传播分析检索对象在传播趋势和传播渠道上的分析。 2 情感分析 情感分析指的是对文本中情感的倾向性和评价对象进行提取的过程。 基于上百万条社交网络平衡语料和数十万条新闻平衡语料的机器学习模型，结合自主开发的半监督学习技术，正负面情感分析准确度达到80%以上，可以轻松的识别网民对于某一检索对象的好恶倾向，最直接的应用就是品牌口碑检测领域。 3 信息分类 文本信息分类将文本按照预设的分类体系进行自动区分。常见的商业应用前景有： 通过社交网络挖掘商业情报和潜在销售机会； 1 企业内文本数据分析； 2 海量数据筛选； 3 资讯分类； 4 自动标签预测等。 4 典型意见提取 典型意见引擎将消费者意见进行单句级别的语义聚合，提取出有代表性的意见。常见的商业应用前景有： 消费者调研； 电商点评分析； 社会热点事件的意见整理。 5 文本聚类 相似文本聚类指的是机器自动对给定的文本进行话题聚类，将语义上相似的内容归为一类。常见的商业应用前景有： 海量文档、资讯的整理； 话题级别的统计分析。 6 关键词抽取 关键词提取引擎从一篇或多篇文本中提取出有代表性的关键词。关键词提取技术综合考虑词语在文本中的频率，和词语在千万级背景数据中的频率，选择出最具有代表性的关键词并给出相应权重。 可以使用户在如恒河沙数的文本数据中提炼出有价值的信息，节省阅读时间。 7 语义网络分析、知识图谱通过将应用数学、图形学、信息可视化技术、信息科学等学科的理论与方法引入文本数据分析领域，并结合共现分析手段，可视化语义网络呈现各类文本/信息之间的内在相关关系，时间维度上的动态关系（传播路径）。 # References[1] 大数据文本分析的应用场景有哪些[2] 【数据运营】在运营中，为什么文本分析远比数值型分析重要？（上）[3] 【数据运营】在运营中，为什么文本分析远比数值型分析重要？一个实际案例，五点分析（下）]]></content>
      <categories>
        <category>nlp</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[neo4j-shell 使用 笔记]]></title>
    <url>%2F2017%2F05%2F03%2Fneo4j-shell%2F</url>
    <content type="text"><![CDATA[Neo4j-shell是Neo4j数据库的命令行操作界面，(类似于MySQL的Command Line) 使用方便，操作简单。 步骤： 首先修改neo4j.conf配置文件，必须要修改配置后可以使用neo4j-shell。 启动Neo4j数据库。 使用 neo4j-shell 1. 修改neo4j.conf配置文件1234567891011# 修改233行，去掉前面的#，允许使用neo4j-shell，类似于mysql 命令行之类的# Enable a remote shell server which Neo4j Shell clients can log in to.dbms.shell.enabled=true# 修改235行，去掉#，设置连接neo4j-shell的端口，一般都是localhost或者127.0.0.1，这样安全，其他地址的话，一般使用https就行# The network interface IP the shell will listen on (use 0.0.0.0 for all interfaces).dbms.shell.host=127.0.0.1# 修改237行，去掉#，设置neo4j-shell端口，端口可以自定义，只要不和其他端口冲突就行# The port the shell will listen on, default is 1337.dbms.shell.port=1337 2 启动Neo4j数据库 用 neo4j start(linux) 或 neo4j console(windows) 启动Neo4j数据库 123456C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\binλ neo4j console2017-05-03 08:04:41.297+0000 INFO Starting...2017-05-03 08:04:43.361+0000 INFO Bolt enabled on 0.0.0.0:7687.2017-05-03 08:04:50.026+0000 INFO Started.2017-05-03 08:04:51.655+0000 INFO Remote interface available at http://0.0.0.0:7474/ 3 使用neo4j-shell 输入 neo4j-shell 命令连接默认的Neo4j，进入neo4j-shell 默认是连接当前操作系统上已经启动的Neo4j数据库， 如果目前的操作系统上启动了两个Neo4j数据库，neo4j-shell会连其中一个(具体我也不知道会选择哪一个，血的教训，因为这个问题，有一次差点删库) 1234567891011121314151617181920212223C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\binλ neo4j-shellWelcome to the Neo4j Shell! Enter &apos;help&apos; for a list of commands. Please note that neo4j-shell is deprecated and to be replaced by cypher-shell.NOTE: Remote Neo4j graph database service &apos;shell&apos; at port 1337neo4j-sh (?)$ helpAvailable commands: alias begin call cd commit create cypher dbinfo drop dump env explain export foreach gsh help index jsh load ls man match merge mknode mkrel mv optional paths planner profile pwd return rm rmnode rmrel rollback runtime schema set start trav unwind using withUse man &lt;command&gt; for info about each command.Please note that neo4j-shell is deprecated and to be replaced by cypher-shell.neo4j-sh (?)$ match (n) return count(n);+----------+| count(n) |+----------+| 5 |+----------+1 row93 msneo4j-sh (?)$# 退出是exitneo4j-sh (?)$ exitC:\ProfessionSofware\Neo4j\neo4j-community-3.3.1\bin 连接指定ip host的Neo4j123456789101112131415161718192021λ neo4j-shell -host 172.16.6.88 -port 1337 -name shellWelcome to the Neo4j Shell! Enter &apos;help&apos; for a list of commands. Please note that neo4j-shell is deprecated and to be replaced by cypher-shell.NOTE: Remote Neo4j graph database service &apos;shell&apos; at port 1337neo4j-sh (?)$ match (n) return count(n);+----------+| count(n) |+----------+| 0 |+----------+1 row68 msneo4j-sh (?)$ match (n) return count(n);+----------+| count(n) |+----------+| 1 |+----------+1 row14 msneo4j-sh (?)$ neo4j-shell帮助信息123456789101112131415161718192021222324C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\bin&gt;neo4j-shell help警告: This command does not appear to be running with administrative rights. Some commands may fail e.g. Start/StopERROR (-v for expanded information): Connection refused -host Domain name or IP of host to connect to (default: localhost) -port Port of host to connect to (default: 1337) -name RMI name, i.e. rmi://&lt;host&gt;:&lt;port&gt;/&lt;name&gt; (default: shell) -pid Process ID to connect to -c Command line to execute. After executing it the shell exits -file File containing commands to execute, or &apos;-&apos; to read from stdin. After executing it the shell exits -readonly Connect in readonly mode (only for connecting with -path) -path Points to a neo4j db path so that a local server can be started there -config Points to a config file when starting a local serverExample arguments for remote: -port 1337 -host 192.168.1.234 -port 1337 -name shell -host localhost -readonly ...or no arguments for default valuesExample arguments for local: -path /path/to/db -path /path/to/db -config /path/to/neo4j.config -path /path/to/db -readonly 可以通过 neo4j-shell -host 172.16.11.123 -port 1337 -name shell 连接指定的数据库 批量执行cypher语句要创建上百个索引，类似于关系型数据库中批量create，想找一个简单的办法 ./bin/neo4j-shell -c &lt; /data/stale/data01/neo4j/create_index.cypher ./bin/neo4j-shell -path /data/stale/data01/neo4j/neo4j-community-3.1.0/data/dbms/ -conf /data/stale/data01/neo4j/neo4j-community-3.1.0/conf/neoo4j.conf -file /data/stale/data01/neo4j/create_index.cypther]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SimHash 算法 笔记]]></title>
    <url>%2F2017%2F05%2F01%2Fsimhash%2F</url>
    <content type="text"><![CDATA[根据《数学之美》里的介绍，相似哈希(SimHash)是Moses S. Charikar 在2002年提出的 《Similarity estimation techniques from rounding algorithms》但真正得到重视是当Google在网页爬虫中使用它，并把它发表在WWW会议上以后。 simhash是google用来处理海量文本去重的算法。 google出品，你懂的。 simhash最牛逼的一点就是将一个文档，最后转换成一个64位的字节，暂且称之为特征字， 然后判断重复只需要判断他们的特征字的距离是不是&lt;n（根据经验这个n一般取值为3），就可以判断两个文档是否相似。 原理simhash值的生成图解如下 大概花三分钟看懂这个图就差不多怎么实现这个simhash算法了。特别简单。谷歌出品嘛，简单实用。 算法过程大概如下： 将Doc进行关键词抽取(其中包括分词和计算权重)，抽取出n个(关键词，权重)对， 即图中的(feature, weight)们。记为 feature_weight_pairs = [fw1, fw2 … fwn]，其中 fwn = (feature_n, weight_n)。 hash_weight_pairs = [ (hash(feature), weight) for feature, weight in feature_weight_pairs ] 生成图中的(hash,weight)们,此时假设hash生成的位数bits_count = 6（如图）; 然后对 hash_weight_pairs 进行位的纵向累加，如果该位是1，则+weight,如果是0，则-weight，最后生成bits_count个数字，如图所示是[13, 108, -22, -5, -32, 55], 这里产生的值和hash函数所用的算法相关。 [13,108,-22,-5,-32,55] -&gt; 110001这个就很简单啦，正1负0。 到此，如何从一个doc到一个simhash值的过程已经讲明白了。 但是还有一个重要的部分没讲，simhash值的海明距离计算 二进制串A 和 二进制串B 的海明距离 就是 A xor B 后二进制中1的个数。 举例如下：123A = 100111;B = 101010;hamming_distance(A, B) = count_1(A xor B) = count_1(001101) = 3; 当我们算出所有doc的simhash值之后，需要计算doc A和doc B之间是否相似的条件是： A和B的海明距离是否小于等于n，这个n值根据经验一般取值为3, simhash本质上是局部敏感性的hash，和md5之类的不一样。 正因为它的局部敏感性，所以我们可以使用海明距离来衡量simhash值的相似度。高效计算二进制序列中1的个数12345678910111213141516/* src/Simhasher.hpp */bool isEqual(uint64_t lhs, uint64_t rhs, unsigned short n = 3)&#123; unsigned short cnt = 0; lhs ^= rhs; while(lhs &amp;&amp; cnt &lt;= n) &#123; lhs &amp;= lhs - 1; cnt++; &#125; if(cnt &lt;= n) &#123; return true; &#125; return false;&#125; 由上式这个函数来计算的话，时间复杂度是 O(n); 这里的n默认取值为3。由此可见还是蛮高效的。 References[1] 《数学之美》 吴军 第16章 信息指纹及其应用[2] 海量数据相似度计算之simhash和海明距离[3] 《Similarity estimation techniques from rounding algorithms》[4] CharikarEstim.pdf[5] Algorithm 使用SimHash进行海量文本去重[6] 海量短文本场景下的去重算法]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>similar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hash]]></title>
    <url>%2F2017%2F05%2F01%2Fhash%2F</url>
    <content type="text"><![CDATA[Hash （哈希，或者散列）函数在计算机领域，尤其是数据快速查找领域，加密领域用的极广。 其作用是将一个大的数据集映射到一个小的数据集上面（这些小的数据集叫做哈希值，或者散列值）。 一个应用是Hash table（散列表，也叫哈希表），是根据哈希值 (Key value) 而直接进行访问的数据结构。 也就是说，它通过把哈希值映射到表中一个位置来访问记录，以加快查找的速度。下面是一个典型的 hash 函数 / 表示意图： 哈希函数有以下两个特点： 如果两个散列值是不相同的（根据同一函数），那么这两个散列值的原始输入也是不相同的。 散列函数的输入和输出不是唯一对应关系的，如果两个散列值相同，两个输入值很可能是相同的。 但也可能不同，这种情况称为 “散列碰撞”（或者 “散列冲突”）。上图中，John Smith和Sandra Dee就存在hash冲突。 Hash一个应用就是对数据集分类，比如上图，Hash值为0的表示可能在A集合中，Hash值为2的表示B集合中，依次类推， 值为15的表示F集合中。但Hash冲突会在这里会导致严重的问题，对于一个未知的新值，其可能不属于上面任何一个集合， 但由于冲突，其Hash值和上面的某一个相同，导致误报（因为事先我们不可能做一个含有无限多项输入的完整的Hash表， 也就是原来的Hash函数不可能是完美的）。并且，hash冲突也会导致查找效率低下。 缺点： 引用吴军博士的《数学之美》中所言，哈希表的空间效率还是不够高。 如果用哈希表存储一亿个垃圾邮件地址，每个email地址 对应 8bytes, 而哈希表的存储效率一般只有50%， 因此一个email地址需要占用16bytes，一亿个email地址占用1.6GB，如果存储几十亿个email address则需要上百GB的内存。 除非是超级计算机，一般的服务器是无法存储的。 References:[1] Hash_function[2] 散列函数[3] hash-and-bloom-filter[4] 布隆过滤器 – 空间效率很高的数据结构]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>similar</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[海量数据处理]]></title>
    <url>%2F2017%2F05%2F01%2Fmassive-data-processing-interview-questions%2F</url>
    <content type="text"><![CDATA[有1000亿条记录，每条记录由url,ip,时间组成，设计一个系统能够快速查询以下内容 1.给定url和时间段（精确到分钟）统计url的访问次数 2.给定ip和时间段（精确到分钟）统计ip的访问次数 References[1] 海量数据处理面试题集锦[2] 教你如何迅速秒杀掉：99%的海量数据处理面试题]]></content>
      <categories>
        <category>data</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>data</tag>
        <tag>interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[位图]]></title>
    <url>%2F2017%2F05%2F01%2Fbitmap%2F</url>
    <content type="text"><![CDATA[在遇到有关联关系 并且 数据量很大时，会遇到一个很头疼的问题，就是要处理的数据量太大，效率很低。 比如用户画像系统、每天的活跃用户、商品适配、人际关系 等。 以用户画像系统为例，如果用户数 &lt; 500万 并且 标签数 也不多，可以用传统的关系型数据库直接查询。 但是当数据数很多 或者 查询比较复杂时，关系型数据会效率会很低。就得想想其它办法。 优化算法 数据压缩 压缩维度展示(类似数据压缩) (1) 优化算法用户画像系统 用户 张三、李四 用户标签 学生、单身、90后、程序员 统计 标签是90后 程序员 的用户 mysql条件查询 统计 标签是90后 或 程序员 的用户个数 mysql 查询 取并集 当 用户数过多 (500万用户) 或 标签过多(10万标签) 时，mysql的效率很低。 假设 user_id int类型，占4字节， 1千万用户 也就是 4 40 000 000 byte = 160M user_name varchar(8) 使用utf-8编码 占24字节 也就是 24 40 000 000 byte = 480M tag_id int类型，占4字节，10万标签 也就是 4 * 100000 byte = 400 000 = 400K 用户 和 标签 是多对多关系，假设有一张 用户标签表， 有1千万行 10万列， 每一行需要 (user_id + 100 000 tag_id ) 4 byte 10 000 000 ≈ 4000G 得换一种办法。 bitmap 一个标签对应多个用户 标签id化，用户id化 (倒排索引) 每个标签 一个bitmap ，这个bitmap存储了用户信息，用户是500万，实际存储需要 2^23 bit = 2^23 / 8 byte ≈ 1MB 有 10万标签 100 000 * 1MB = 100 GB 这个算的是所有的用户和标签，实际上可以只处理需要的。可以过滤掉很多用户和标签。 用户数过滤一部分，其实 一个bitmap 实际存储会变成 2^22 bit = 2^22 / 8 byte ≈ 0.5MB 只处理常用的标签，实际常用的大概1000，1000 * 0.5MB = 5G 根据标签查对应用户，拿到结果后 并集 交集 差集 统计 标签是90后 程序员 的用户 根据标签查对应用户，拿到结果后 交集 统计 标签是90后 或 程序员 的用户个数 根据标签查对应用户，拿到结果后 并集 其它概述 bitmap就是用一个bit位来标记某个元素对应的value，而key即是这个元素。由于采用bit为单位来存储数据，因此在可以大大的节省存储空间 思想 32位机器上，一个整形，比如int a;在内存中占32bit，可以用对应的32个bit位来表示十进制的0-31个数，bitmap算法利用这种思想处理大量数据的排序与查询 优点： 效率高，不许进行比较和移位 占用内存少，比如N=10000000;只需占用内存为N/8 = 1250000Bytes = 1.2M，如果采用int数组存储，则需要38M多 缺点： 无法对存在重复的数据进行排序和查找 示例： 申请一个int型的内存空间，则有4Byte，32bit。输入 4， 2, 1, 3时： 在32位地址上加入4 再加入2 再加入1 再加入3 思想比较简单，关键是十进制和二进制bit位需要一个map映射表，把10进制映射到bit位上 假设需要排序或者查找的总数N=10000000,那么我们需要申请的内存空间为 int a[N/32 + 1].其中a[0]在内存中占32位,依此类推： bitmap表为： a[0] ——&gt; 0 - 31 a[1] ——&gt; 32 - 63 a[2] ——&gt; 64 - 95 a[3] ——&gt; 96 - 127 …… 下面介绍用位移将十进制数转换为对应的bit位 位移转换（1） 求十进制数0-N对应的在数组a中的下标 index_loc = N / 32即可，index_loc即为n对应的数组下标。例如n = 76, 则loc = 76 / 32 = 2,因此76在a[2]中。 （2）求十进制数0-N对应的bit位 bit_loc = N % 32即可，例如 n = 76, bit_loc = 76 % 32 = 12 （3）利用移位0-31使得对应的32bit位为1 References[1] 漫画：什么是Bitmap算法？-程序员小灰]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[布隆过滤器]]></title>
    <url>%2F2017%2F05%2F01%2Fbloom-filter%2F</url>
    <content type="text"><![CDATA[Bloom Filter是由Bloom在1970年提出的一种多哈希函数映射的快速查找算法。通常应用在一些需要快速判断某个元素是否属于集合，但是并不严格要求100%正确的场合。例如 网页URL的去重，垃圾邮件的判别，集合重复元素的判别，查询加速（比如基于key-value的存储系统）等。 (1) 实例 为了说明Bloom Filter存在的重要意义，举一个实例： 假设要你写一个网络蜘蛛（web crawler）。由于网络间的链接错综复杂，蜘蛛在网络间爬行很可能会形成“环”。为了避免形成“环”，就需要知道蜘蛛已经访问过那些URL。给一个URL，怎样知道蜘蛛是否已经访问过呢？稍微想想，就会有如下几种方案： 1. 将访问过的URL保存到数据库。 2. 用HashSet将访问过的URL保存起来。那只需接近O(1)的代价就可以查到一个URL是否被访问过了。 3. URL经过MD5或SHA-1等单向哈希后再保存到HashSet或数据库。 4. Bit-Map方法。建立一个BitSet，将每个URL经过一个哈希函数映射到某一位。 方法1~3都是将访问过的URL完整保存，方法4则只标记URL的一个映射位。 以上方法在数据量较小的情况下都能完美解决问题，但是当数据量变得非常庞大时问题就来了。 方法1的缺点：数据量变得非常庞大后关系型数据库查询的效率会变得很低。而且每来一个URL就启动一次数据库查询是不是太小题大做了？ 方法2的缺点：太消耗内存。随着URL的增多，占用的内存会越来越多。就算只有1亿个URL，每个URL只算50个字符，就需要5GB内存。 方法3：由于字符串经过MD5处理后的信息摘要长度只有128Bit，SHA-1处理后也只有160Bit，因此方法3比方法2节省了好几倍的内存。 方法4消耗内存是相对较少的，但缺点是单一哈希函数发生冲突的概率太高。还记得数据结构课上学过的Hash表冲突的各种解决方法么？若要降低冲突发生的概率到1%，就要将BitSet的长度设置为URL个数的100倍。 实质上上面的算法都忽略了一个重要的隐含条件：允许小概率的出错，不一定要100%准确！也就是说少量url实际上没有没网络蜘蛛访问，而将它们错判为已访问的代价是很小的——大不了少抓几个网页呗。 (2) Bloom Filter的算法 废话说到这里，下面引入本篇的主角——Bloom Filter。其实上面方法4的思想已经很接近Bloom Filter了。方法四的致命缺点是冲突概率高，为了降低冲突的概念，Bloom Filter使用了多个哈希函数，而不是一个。 ### Bloom Filter是1970年由Bloom提出的。它实际上是一个很长的二进制向量和一系列随机映射函数（Hash函数）。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。Bloom Filter广泛的应用于各种需要查询的场合中，如Orocle的数据库，Google的BitTable也用了此技术。 如果想判断一个元素是不是在一个集合里，一般想到的是将所有元素保存起来，然后通过比较确定。链表，树等等数据结构都是这种思路. 但是随着集合中元素的增加，我们需要的存储空间越来越大，检索速度也越来越慢(O(n),O(logn))。 这时候就可以利用哈希表这个数据结构（它可以通过一个Hash函数将一个元素映射成一个位阵列（Bit array）中的一个点）。这样一来，我们只要看看这个点是不是1就知道可以集合中有没有它了。这就是Bloom Filter的基本思想。 但这时，哈希冲突会是一个问题：假设Hash函数是良好的，如果我们的位阵列长度为m个点，那么如果我们想将冲突率降低到例如 1%, 这个散列表就只能容纳m/100个元素。显然这就不叫空间效率了（Space-efficient）了。解决方法也简单，就是使用多个Hash，如果它们有一个说元素不在集合中，那肯定就不在。如果它们都说在，虽然也有一定可能性它们都在说谎，不过直觉上判断这种事情的概率是比较低的。这种多个Hash组成的数据结构就叫Bloom Filter。 一个Bloom Filter是基于一个m位的位向量（b1,…bm），这些位向量的初始值为0。另外，还有一系列的hash函数（h1,…hk），这些hash函数的值域属于1~m。下图是一个bloom filter插入x,y,z并判断某个值w是否在该数据集的示意图： 上图中，m=18，k=3；插入x是，三个hash函数分别得到蓝线对应的三个值，并将对应的位向量改为1，插入y，z时，类似的，分别将红线，紫线对应的位向量改为1。查找时，当查找x时，三个hash值对应的位向量都为1，因此判断x在此数据集中。y，z也是如此。但是查找w时，w有个hash值对应的位向量为0，因此可以判断不在此集合中。但是，假如w的最后那个hash值比上图中的大1，这是就会认为w在此集合中，而事实上，w可能不在此集合中，因此可能出现误报。显然的，插入数据越多，1的位数越多，误报的概率越大。 Wiki的Bloom Filter词条有关于误报的概率的详细分析：Probability of false positives。从分析可以看出，当k比较大时，误报概率还是比较小的，因此这存储还是很空间有效滴。 Bloom Filter有以下几个特点： 不存在漏报（False Negative），即某个元素在某个集合中，肯定能报出来。 可能存在误报（False Positive），即某个元素不在某个集合中，可能也被爆出来。 确定某个元素是否在某个集合中的代价和总的元素数目无关。 优点： 相比于其它的数据结构，Bloom Filter在空间和时间方面都有巨大的优势。Bloom Filter存储空间和插入/查询时间都是常数。另外, Hash函数相互之间没有关系，方便由硬件并行实现。Bloom Filter不需要存储元素本身，在某些对保密要求非常严格的场合有优势。 缺点： 另外，一般情况下不能从Bloom Filter中删除元素. 我们很容易想到把位列阵变成整数数组，每插入一个元素相应的计数器加1, 这样删除元素时将计数器减掉就可以了。然而要保证安全的删除元素并非如此简单。首先我们必须保证删除的元素的确在Bloom Filter里面. 这一点单凭这个过滤器是无法保证的。另外计数器回绕也会造成问题。 Bloom Filter(布隆过滤器)原理 布隆过滤器需要的是一个位数组（这个和位图有点类似）和k个映射函数（和Hash表类似），在初始状态时，对于长度为m的位数组array，它的所有位都被置为0。对于有n个元素的集合S={s1,s2……sn}，通过k个映射函数{f1,f2,……fk}，将集合S中的每个元素sj(1&lt;=j&lt;=n)映射为k个值{g1,g2……gk}，然后再将位数组array中相对应的array[g1],array[g2]……array[gk]置为1；如果要查找某个元素item是否在S中，则通过映射函数{f1,f2…..fk}得到k个值{g1,g2…..gk}，然后再判断array[g1],array[g2]……array[gk]是否都为1，若全为1，则item在S中，否则item不在S中。这个就是布隆过滤器的实现原理。 实例 假定我们存储一亿个URL，我们先建立一个十六亿二进制（比特），即两亿字节的向量，然后将这十六亿个二进制全部设置为零。对于每一个URL，我们用八个不同的随机数产生器（F1,F2, …,F8） 产生八个信息指纹（f1, f2, …, f8）。再用一个随机数产生器 G 把这八个信息指纹映射到 1 到十六亿中的八个自然数 g1, g2, …,g8。现在我们把这八个位置的二进制全部设置为一。当我们对这一亿个URL都进行这样的处理后。一个针对这些URL的布隆过滤器就建成了。 现在，让我们看看如何用布隆过滤器来检测一个URL是否已访问。我们用相同的八个随机数产生器（F1, F2, …, F8）对这个地址产生八个信息指纹 s1,s2,…,s8，然后将这八个指纹对应到布隆过滤器的八个二进制位，分别是 t1,t2,…,t8。如果该URL已访问，显然，t1,t2,..,t8 对应的八个二进制一定是一。这样在遇到任何已访问的URL，我们都能准确地发现。 布隆过滤器决不会漏掉任何一个已访问的URL。但是，它有一条不足之处。也就是它有极小的可能将未访问的URL判定为已访问的URL，因为有可能某个URL正巧对应个八个都被设置成一的二进制位。好在这种可能性很小。我们把它称为误识概率。在上面的例子中，误识概率在万分之一以下。 布隆过滤器的好处在于快速，省空间。但是有一定的误识别率。 常见的补救办法是在建立一个小的白名单，存储那些可能别误判的URL。 (3) Bloom Filter参数选择 (1)哈希函数选择 哈希函数的选择对性能的影响应该是很大的，一个好的哈希函数要能近似等概率的将字符串映射到各个Bit。选择k个不同的哈希函数比较麻烦，一种简单的方法是选择一个哈希函数，然后送入k个不同的参数。 (2)Bit数组大小选择 哈希函数个数k、位数组大小m、加入的字符串数量n的关系可以参考参考文献1。该文献证明了对于给定的m、n，当 k = ln(2)* m/n 时出错的概率是最小的。 同时该文献还给出特定的k，m，n的出错概率。例如：根据参考文献1，哈希函数个数k取10，位数组大小m设为字符串个数n的20倍时，false positive发生的概率是0.0000889 ，这个概率基本能满足网络爬虫的需求了。 (4) 布隆过滤器应用 布隆过滤器在很多场合能发挥很好的效果，比如：网页URL的去重，垃圾邮件的判别，集合重复元素的判别，查询加速（比如基于key-value的存储系统）等。 下面举几个例子： 有两个URL集合A,B，每个集合中大约有1亿个URL，每个URL占64字节，有1G的内存，如何找出两个集合中重复的URL。很显然，直接利用Hash表会超出内存限制的范围。这里给出两种思路： 第一种：如果不允许一定的错误率的话，只有用分治的思想去解决，将A,B两个集合中的URL分别存到若干个文件中{f1,f2…fk}和{g1,g2….gk}中，然后取f1和g1的内容读入内存，将f1的内容存储到hash_map当中，然后再取g1中的url，若有相同的url，则写入到文件中，然后直到g1的内容读取完毕，再取g2…gk。然后再取f2的内容读入内存。。。依次类推，知道找出所有的重复url。 第二种：如果允许一定错误率的话，则可以用布隆过滤器的思想。 (4) Bloom Filter实现代码 下面给出一个简单的Bloom Filter的Java实现代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import java.util.BitSet;/** * Bloom Filter */public class BloomFilter &#123; // BitSet初始分配2^24个bit private static final int DEFAULT_SIZE = 1 &lt;&lt; 24; // 不同哈希函数的种子，一般应取质数 private static final int[] seeds = new int[] &#123; 3, 5, 7, 11, 13, 31, 37, 61 &#125;; // private BitSet bits = new BitSet(DEFAULT_SIZE); // 哈希函数对象 private SimpleHash[] func = new SimpleHash[seeds.length]; public BloomFilter() &#123; int length = seeds.length; for (int i = 0; i &lt; length; i++) &#123; func[i] = new SimpleHash(DEFAULT_SIZE, seeds[i]); &#125; &#125; /** * 将字符串标记到bits中 * * @param value */ public void add(String value) &#123; for (SimpleHash f : func) &#123; bits.set(f.hash(value), true); &#125; &#125; /** * 判断字符串是否已经被bits标记 * * @param value * @return */ public boolean contains(String value) &#123; if (value == null) &#123; return false; &#125; boolean ret = true; for (SimpleHash f : func) &#123; ret = ret &amp;&amp; bits.get(f.hash(value)); &#125; return ret; &#125; /** * 哈希函数类 * */ public class SimpleHash &#123; private int cap; private int seed; public SimpleHash(int cap, int seed) &#123; this.cap = cap; this.seed = seed; &#125; /** * 字符串哈希，选取好的哈希函数很重要&lt;br&gt; * hash函数，采用简单的加权和hash * * @param value * @return */ public int hash(String value) &#123; int result = 0; int len = value.length(); for (int i = 0; i &lt; len; i++) &#123; result = seed * result + value.charAt(i); &#125; return (cap - 1) &amp; result; &#125; &#125;// end class SimpleHash&#125; 参考：[1] Pei Cao. Bloom Filters - the math. http://pages.cs.wisc.edu/~cao/papers/summary-cache/node8.html[2] Wikipedia. Bloom filter. http://en.wikipedia.org/wiki/Bloom_filter[3] http://blog.csdn.net/v_july_v/article/details/6685894[4] http://www.cnblogs.com/heaad/archive/2011/01/02/1924195.html[5] http://blog.csdn.net/dadoneo/article/details/6847481[6] http://blog.csdn.net/jiaomeng/article/details/1495500[7] http://www.cnblogs.com/hxsyl/p/4176280.html[8] http://www.cnblogs.com/KevinYang/archive/2009/02/01/1381803.html]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>big-data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cmder 使用 笔记]]></title>
    <url>%2F2017%2F04%2F28%2Fcmder-notes%2F</url>
    <content type="text"><![CDATA[俗话说，工欲善其事必先利其器。 平常在使用Windows自带的cmd复制粘贴太麻烦，而且cmd在中国用的是gbk编码，有的时候想用UTF-8编码，感觉cmd不太好用，就想找一个替代品，看了powershell，git bash(mingw64) cmder,最后觉得cmder好用。 Cmder1.3.2版本中，可以使用cmd，bash，git-bash。感觉特别方便。 (1) 配置颜色(1) 设置背景为豆沙绿色 如图，0对应的是背景，把0的RGB改为199 237 204，就可以看到背景改为豆沙绿了 7对应的是文字的颜色，我把它调成0 0 0，也就是黑色 感觉git bash的配色特别好看，特别舒服 git bash RGB 用户名 0, 191, 0 绿色 mingw64 191, 0, 191 茄紫色 path 191, 191, 0 黄色 branch dev 0, 191, 191 蓝色 changes to be committed new file 0, 191, 0 绿色 staged for commit modified 191, 0, 0 红色 96, 96, 255 淡紫色 (2) 中文乱码问题 我是在使用 java javac命令 和 git commit –amend命令 的时候发现乱码的，所以想把编码设置成UTF-8 (2.1) v1.3.0及以上版本 在Settings &gt; Startup &gt; Environment里添加： set LANG=zh_CN.UTF8 set LC_ALL=zh_CN.utf8 在cmder v1.3.0以上版本初始创建的cmder/config/user-aliases.cmd文件中已经包含：1ls=ls --show-control-chars -F --color $* 不需要添加其它命令（实际上添加了也没有效果） (2.2) v1.3.0以下版本 把一下几行代码添加到config/aliases文件末尾即可解决中文乱码问题：1234l=ls --show-control-chars la=ls -aF --show-control-chars ll=ls -alF --show-control-charsls=ls --show-control-chars -F (2.3) cmder bash 中文乱码12345#查看编码locale charmap#设置utf8export LANG=zh_CN.utf8export LC_ALL=zh_CN.utf8 以上只在当前窗口生效，设置启动即生效： Settings-&gt;Startup-&gt;Environment 添加 set LANG=zh_CN.UTF-8 set LC_ALL=zh_CN.utf8 保存。重新打开窗口，ls进行测试，成功！ (3) 文字重叠问题Win + Ait + P 唤出设置界面 &gt; mian &gt; font &gt; monospce 的勾勾去掉(点两下). (4) 配置其在win+r中打开把根目录加到系统环境的path变量中即可。 (5) 添加右键在Cmder的目录下执行 Cmder.exe /REGISTER ALL 命令后，点击鼠标右键就可以看到cmder了 123456789101112131415161718192021222324WKQ@WKQ-PC C:\ProfessionSofware\cmder$ dir 驱动器 C 中的卷是 本地磁盘C 卷的序列号是 6A63-8F3E C:\ProfessionSofware\cmder 的目录2017-05-09 16:25 &lt;DIR&gt; .2017-05-09 16:25 &lt;DIR&gt; ..2016-12-02 07:14 &lt;DIR&gt; bin2016-12-02 07:14 73,491 CHANGELOG.md2016-12-02 07:15 130,560 Cmder.exe2017-04-28 17:04 &lt;DIR&gt; config2016-12-02 07:14 1,784 CONTRIBUTING.md2016-12-02 07:14 &lt;DIR&gt; icons2017-05-09 16:25 &lt;DIR&gt; node_modules2016-12-02 07:14 10,039 README.md2016-12-02 07:15 &lt;DIR&gt; vendor2016-12-02 07:15 0 Version v1.3.2 5 个文件 215,874 字节 7 个目录 105,438,040,064 可用字节WKQ@WKQ-PC C:\ProfessionSofware\cmder$ Cmder.exe /REGISTER ALL (6) 修改命令提示符号λ 在Cmder官网最新的版本1.3.2中，cmd使用&gt;，bash中使用$(普通用户)和#(管理员)。 建议升级版本。 如果不想升级版本，按照如下方法： 把cmder\vendor\clink.lua文件中第41行中{lamb}修改为$ 12345修改前：local cmder_prompt = &quot;\x1b[1;32;40m&#123;cwd&#125; &#123;git&#125;&#123;hg&#125; \n\x1b[1;30;40m&#123;lamb&#125; \x1b[0m&quot;修改后：local cmder_prompt = &quot;\x1b[1;32;40m&#123;cwd&#125; &#123;git&#125;&#123;hg&#125; \n\x1b[1;30;40m$ \x1b[0m&quot; (7) 把Tabs放到顶部 Setting(Win+Alt+P) =&gt; Main =&gt; Tab bar，把Tabs on bottom的勾去掉。 (8) 关闭Tab不提示 Setting(Win+Alt+P) =&gt; Main =&gt; Confirm，把Confirm console detach的勾去掉。 (9) 我的配置文件我的配置文件 (10) Command to be executed: “C:\Windows\system32\cmd.exe” /c “C:\ProfessionSofware\cmder\vendor\conemu-maximus5..\git-for-windows\bin\bash –login -i”123456789101112系统找不到指定的路径。Current directory: C:\Users\WKQCommand to be executed:&quot;C:\Windows\system32\cmd.exe&quot; /c &quot;C:\ProfessionSofware\cmder\vendor\conemu-maximus5\..\git-for-windows\bin\bash --login -i&quot;ConEmuC: Root process was alive less than 10 sec, ExitCode=1.Press Enter or Esc to close console... 出现这个问题是因为路径的原因在cmder的完整版里有git，在cmder的mini版里没有git，但是配置文件里的路径时按照完整版的写的，所以后出现这个问题解决办法很简单，改cmder安装目录下的/vendor/conemu-maximus5/ConEmu.xml的配置文件12345678910111213141516171819202122修改配置文件$&#123;cmder_home&#125;/vendor/conemu-maximus5/ConEmu.xml把 %ConEmuDir%\..\git-for-windows 替换成 $&#123;git_home&#125;查找 git-for-windows\bin\bash 能找到两条记录，如下：&lt;!-- 第一条是管理员用户的 bash::bash as Admin --&gt;&lt;value name=&quot;Cmd1&quot; type=&quot;string&quot; data=&quot;*cmd /c &amp;quot;%ConEmuDir%\..\git-for-windows\bin\bash&amp;quot; --login -i -new_console&quot;/&gt;&lt;!-- 第二条是普通用户的 bash::bash --&gt;&lt;value name=&quot;Cmd1&quot; type=&quot;string&quot; data=&quot;cmd /c &amp;quot;%ConEmuDir%\..\git-for-windows\bin\bash&amp;quot; --login -i -new_console&quot;/&gt;我的git安装在C:\ProfessionSofware\Git目录下，$&#123;git_home&#125; = C:\ProfessionSofware\Git 把 %ConEmuDir%\..\git-for-windows 替换成 $&#123;git_home&#125; (因为我的 $&#123;git_home&#125;是C:\ProfessionSofware\Git，所以我改成C:\ProfessionSofware\Git)替换完结果如下，&lt;value name=&quot;Cmd1&quot; type=&quot;string&quot; data=&quot;*cmd /c &amp;quot;C:\ProfessionSofware\Git\bin\bash --login -i&amp;quot; -new_console:d:%USERPROFILE%&quot;/&gt;&lt;value name=&quot;Cmd1&quot; type=&quot;string&quot; data=&quot;cmd /c &amp;quot;C:\ProfessionSofware\Git\bin\bash --login -i&amp;quot; -new_console:d:%USERPROFILE%&quot;/&gt; 关掉cmder，重新打开，再试试，发现cmder的bash是不是能用了 (11) Root process was alive less than 10 sec, ExitCode=1.123456789101112系统找不到指定的路径。Current directory:C:\Users\WKQCommand to be executed:&quot;C:\Windows\system32\cmd.exe&quot; /C C:\ProfessionSofware\cmder\vendor\conemu-maximus5\..\git-for-windows\usr\bin\mintty.exe /bin/bash -lConEmuC: Root process was alive less than 10 sec, ExitCode=1.Press Enter or Esc to close console... 123456789101112131415修改配置文件把 %ConEmuDir%\..\git-for-windows 替换成 $&#123;git_home&#125; &lt;!-- bash::mintty as Admin --&gt;&lt;value name=&quot;Cmd1&quot; type=&quot;string&quot; data=&quot;*%ConEmuDir%\..\git-for-windows\usr\bin\mintty.exe /bin/bash -l -new_console:d:%USERPROFILE%&quot;/&gt;&lt;!-- bash::mintty --&gt;&lt;value name=&quot;Cmd1&quot; type=&quot;string&quot; data=&quot;%ConEmuDir%\..\git-for-windows\usr\bin\mintty.exe /bin/bash -l -new_console:d:%userProfile%&quot;/&gt;我的git安装在C:\ProfessionSofware\Git目录下，$&#123;git_home&#125; = C:\ProfessionSofware\Git 把 %ConEmuDir%\..\git-for-windows 替换成 $&#123;git_home&#125; (因为我的 $&#123;git_home&#125;是C:\ProfessionSofware\Git，所以我改成C:\ProfessionSofware\Git)&lt;value name=&quot;Cmd1&quot; type=&quot;string&quot; data=&quot;*C:\ProfessionSofware\Git\usr\bin\mintty.exe /bin/bash -l -new_console:d:%USERPROFILE%&quot;/&gt;&lt;value name=&quot;Cmd1&quot; type=&quot;string&quot; data=&quot;C:\ProfessionSofware\Git\usr\bin\mintty.exe /bin/bash -l -new_console:d:%userProfile%&quot;/&gt; References[1] Cmder使用说明[2] Win下必备神器之Cmder[3] cmder中文显示相关问题解决方案(1.3以上版本)[4] cmder默认的命令提示符λ改成$[5] cmder bash 中文乱码]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用GIT备份 博客项目 主题项目]]></title>
    <url>%2F2017%2F04%2F27%2Fgit-backup-blog-and-theme%2F</url>
    <content type="text"><![CDATA[总结了很多东西，放在博客上，担心有一天本地的文件损坏或者丢失，就想了一个办法，用Git来备份本地的博客。 有一个问题是博客目录是一个git仓库，博客目录下的themes目录下的一个theme又是一个git仓库，如果git不熟悉，在这儿肯定会犯错。 hexo博客的目录结构 首先说一下hexo 博客的目录结构 12345678910├── .deploy #需要部署的文件├── node_modules #Hexo插件├── public #生成的静态网页文件├── scaffolds #模板├── source #博客正文和其他源文件, 404 favicon CNAME 等都应该放在这里| ├── _drafts #草稿| └── _posts #文章├── themes #主题├── _config.yml #全局配置文件└── package.json 使用Git备份博客因为Git是分布式文件管理工具，我用Git来管理这些目录及文件。我备份的目录及文件有node_modules、scaffolds、source、themes、_config.yml、package.json其中最主要的是source目录 然后把.deploy、public、themes目录放入.gitignore因为有了source目录，你可以很快得到.deploy、public，没有必要再备份.deploy、public因为themes下的一个目录是一个主题，本来就用git管理，没必要再次管理 备份好的如下图 主题的备份主题的备份也很重要，主题已经用Git进行管理了，那我们另开一个分支对主题进行管理就可以了如下图 master分支是从github上克隆下来的dev-wkq分支是从master分支拉过来的，里面保存了我自己的配置 .gitignore配置1234567891011121314151617181920212223242526272829303132333435# Git.git# hexoThumbs.dbdb.json*.logpublic/.deploy*/themes*.class*.log# idea.idea*.iml*.iws*.ipr# eclipse.project.classpath.settings.apt_generated.factorypath.git/target/log**.log*.swp# mac.DS_Store References[1] Hexo的版本控制与持续集成[2] 使用hexo，如果换了电脑怎么更新博客]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 基础]]></title>
    <url>%2F2017%2F04%2F22%2Fmysql-foundation%2F</url>
    <content type="text"><![CDATA[MySQL的一些基础知识。 对比 MySQL Excel 数据库 database 一个excel 表 table 一个sheet 行 row 一个sheet里的一行数据 字段 column 一列 （1） MySQL介绍 MySQL是一个小型关系型数据库管理系统，开发者为瑞典MySql AB公司。2009年被Oracle公司收购。目前MySql被广泛地应用在Internet上的中小型网站中。 (1.1) 什么是数据库 可以认为是一个excel 数据库由一批数据构成的有序集合，这些数据被分门别类地存储放在一些结构化的数据表（table）里，而数据表之间又往往存在交叉引用的关系，这种关系使数据库又被成为关系型数据库 (1.2) 什么是SQL SQL是Structured Query Language（结构化查询语言）的缩写。SQL是专门为数据库而建立的操作命令集，是一种功能齐全的数据语言。 (1.3) DML DCL DDL DDL(Data Definition Language) 数据定义语言 –用来建立数据库、数据库对象和定义其列； 类似设置excel的列的类型(是文本还是数字) CREATE TABLE ALTER TABLE DROP TABLE DML(Data Manipulation Language) 数据操作语言 查询、插入、修改和删除数据库中的数据 类似操作excel SELECT INSERT UPDATE DELETE DCL(Data Control Language数据控制语言) –用来控制存取许可、存储权限等； 类似设置excel谁可以访问 (加密码) GRANT REVOKE 功能函数 日期函数、数学函数、字符函数、系统函数； (1.4) MySql优点 性能快捷、优化Sql语言 容易使用 多线程和可靠性 多用户支持 可移植性和开放源代码 遵循国际标准和国际化支持 为多种编程语言提供API (1.5) MySql不足 不能直接处理XML数据 一些功能上支持的不够完善和成熟 不能提供任何OLAP（实时分析系统）功能 (2) MySql数据类型整形tinyint(m) 1个字节 范围(-128~127)smallint(m) 2个字节 范围(-32768~32767)mediumint(m) 3个字节 范围(-8388608~8388607)int(m) 4个字节 范围(-2147483648~2147483647)bigint(m) 8个字节 范围(+-9.22*10的18次方) 浮点型float(m,d) 单精度浮点型 8位精度(4字节) m总个数，d小数位double(m,d) 双精度浮点型 16位精度(8字节) m总个数，d小数位 字符串char(n) 固定长度，最多255个字符varchar(n) 固定长度，最多65535个字符tinytext 可变长度，最多255个字符text 可变长度，最多65535个字符mediumtext 可变长度，最多2的24次方-1个字符longtext 可变长度，最多2的32次方-1个字符 char和varchar：1.char(n) 若存入字符数小于n，则以空格补于其后，查询之时再将空格去掉。所以char类型 存储的字符串末尾不能有空格，varchar不限于此。2.char(n) 固定长度，char(4)不管是存入几个字符，都将占用4个字节，varchar是存入的实 际字符数+1个字节（n&lt;=255）或2个字节(n&gt;255)，所以varchar(4),存入3个字符将占 用4个字节。3.char类型的字符串检索速度要比varchar类型的快。 二进制数据 1.BLOB和TEXT存储方式不同，TEXT以文本方式存储，英文存储区分大小写，而BLOB是以二进制方式存储，不分大小写。 2.BLOB存储的数据只能整体读出。 3.TEXT可以指定字符集，BLOB不用指定字符集。 日期时间类型 date 日期 ‘2008-12-2’time 时间 ‘12:25:36’datetime 日期时间 ‘2008-12-2 22:06:44’timestamp 自动存储记录修改时间 若定义一个字段为timestamp，这个字段里的时间数据会随其他字段修改的时候自动刷新，所以这个数据类型的字段可以存放这条记录最后被修改的时间。 数据类型的属性NULL 数据列可包含NULL值NOT NULL 数据列不允许包含NULL值DEFAULT 默认值PRIMARY KEY 主键AUTO_INCREMENT 自动递增，适用于整数类型UNSIGNED 无符号CHARACTER SET name 指定一个字符集 (3) DDL 新建数据库 create database db_test; 查看所有数据库 show databases; 使用数据库 use db_test; 删除数据库 drop database db_test ; (3.1) CREATE 创建表1234567891011121314151617181920CREATE TABLE test_user (`id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键id',`create_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '用户记录创建的时间',`update_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '用户资料修改的时间',`user_id` bigint(11) NOT NULL COMMENT '用户id',`username` varchar(45) NOT NULL COMMENT '真实姓名',`email` varchar(30) NOT NULL COMMENT '用户邮箱',`nickname` varchar(45) NOT NULL COMMENT '昵称',`avatar` int(11) NOT NULL COMMENT '头像',`birthday` date NOT NULL COMMENT '生日',`sex` tinyint(4) DEFAULT '0' COMMENT '性别',`short_introduce` varchar(150) DEFAULT NULL COMMENT '一句话介绍自己，最多50个汉字',`user_resume` varchar(300) NOT NULL COMMENT '用户提交的简历存放地址',`user_register_ip` int NOT NULL COMMENT '用户注册时的源ip',`user_review_status` tinyint NOT NULL COMMENT '用户资料审核状态，1为通过，2为审核中，3为未通过，4为还未提交审核',PRIMARY KEY (`id`),UNIQUE KEY `uq_idx_user_id` (`user_id`),KEY `idx_username`(`username`),KEY `idx_create_time`(`create_time`,`user_review_status`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='网站用户基本信息'; 查询所有的表 show tables; 显示表的基本结构 describe employee; 删除表 drop table if exists employee ； (3.2) ALTER表重命名12ALTER TABLE `test_user` rename to `test_user_2` ; 表增加字段123ALTER TABLE `db_test`.`employee` ADD COLUMN `address` varchar(255) NOT NULL DEFAULT '' COMMENT '地址' AFTER `sex` , ADD COLUMN `remark` varchar(255) NOT NULL DEFAULT '' COMMENT '备注' AFTER `address` ; 表更改字段123456ALTER TABLE `employee` CHANGE `remark` `remark_new` VARCHAR(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NOT NULL DEFAULT '' COMMENT '备注' 在ALTER TABLE时，可以使用以下三种方法1234567ALTER TABLE t CHANGE a b BIGINT NOT NULL ;ALTER TABLE t MODIFY b INT NOT NULL ;ALTER TABLE t ALTER COLUMN `is_deleted` DROP DEFAULT ;ALTER TABLE t ALTER COLUMN `is_deleted` SET DEFAULT 'N' ; CHANGE MODIFY ALTER 对比 CHANGE 重命名列并更改定义，简单得说，CHANGE可以改字段类型，还可以改字段名 。功能最强大。 MODIFY 更改列定义 但不能更改其名称，简单地说，MODIFY可以改字段类型，不能改字段名 。 ALTER 仅用于更改列默认值，只能改默认值。 5.7/en/alter-table.html-dev.mysql.com Renaming, Redefining, and Reordering Columns The CHANGE, MODIFY, and ALTER clauses enable the names and definitions of existing columns to be altered. They have these comparative characteristics: CHANGE: Can rename a column and change its definition, or both. Has more capability than MODIFY, but at the expense of convenience for some operations. CHANGE requires naming the column twice if not renaming it. With FIRST or AFTER, can reorder columns. MODIFY: Can change a column definition but not its name. More convenient than CHANGE to change a column definition without renaming it. With FIRST or AFTER, can reorder columns. ALTER: Used only to change a column default value. CHANGE is a MySQL extension to standard SQL. MODIFY is a MySQL extension for Oracle compatibility. To alter a column to change both its name and definition, use CHANGE, specifying the old and new names and the new definition. For example, to rename an INT NOT NULL column from a to b and change its definition to use the BIGINT data type while retaining the NOT NULL attribute, do this: ALTER TABLE t1 CHANGE a b BIGINT NOT NULL; To change a column definition but not its name, use CHANGE or MODIFY. With CHANGE, the syntax requires two column names, so you must specify the same name twice to leave the name unchanged. For example, to change the definition of column b, do this: ALTER TABLE t1 CHANGE b b INT NOT NULL; MODIFY is more convenient to change the definition without changing the name because it requires the column name only once: ALTER TABLE t1 MODIFY b INT NOT NULL; To change a column name but not its definition, use CHANGE. The syntax requires a column definition, so to leave the definition unchanged, you must respecify the definition the column currently has. For example, to rename an INT NOT NULL column from b to a, do this: ALTER TABLE t1 CHANGE b a INT NOT NULL; For column definition changes using CHANGE or MODIFY, the definition must include the data type and all attributes that should apply to the new column, other than index attributes such as PRIMARY KEY or UNIQUE. Attributes present in the original definition but not specified for the new definition are not carried forward. Suppose that a column col1 is defined as INT UNSIGNED DEFAULT 1 COMMENT ‘my column’ and you modify the column as follows, intending to change only INT to BIGINT: ALTER TABLE t1 MODIFY col1 BIGINT; That statement changes the data type from INT to BIGINT, but it also drops the UNSIGNED, DEFAULT, and COMMENT attributes. To retain them, the statement must include them explicitly: ALTER TABLE t1 MODIFY col1 BIGINT UNSIGNED DEFAULT 1 COMMENT &#39;my column&#39;; For data type changes using CHANGE or MODIFY, MySQL tries to convert existing column values to the new type as well as possible. (3.2.3) auto_increment1234SELECT auto_increment FROM information_schema.tables WHERE table_schema = database_test AND table_name = table_test ; 12ALTER TABLE database_test.table_test auto_increment = 100; 表添加外键 add constraint 外建名 foreign key(列名) references 表名(列名); 删除外键 drop foreign key 外建名 可以更改指定列默认值 alter table employee alter is_deleted default &#39;N&#39;; 删除字段 alter table employee drop is_deleted; 更改表的字符集 alter table employee character set UTF8 修改字段名称/类型 alter table employee change 旧字段名 新字段名 新字段的类型; (4) dml插入数据123INSET INTO department ( did, dname ) values (1,‘技术部’) ; 批量插入INSET INTO department ( did, dname ) values (1,’技术部’), (2,’教务部’) ; 只查询表中的某列： select id, emp_id, user_name, sex form employee ; 更新 update employee set user_name = ‘tom’ where id = 1 ; 更新多个字段 update employee set user_name = &#39;tom&#39;, sex=&#39;男&#39; where id = 1 ; 删除 delete from employee where id &lt; 2; 查询所有语法 select * from employee; (4) 连接查询(4.1) 交叉连接(笛卡尔积): cross join SELECT FROM table_1, table_2 ; SELECT FROM table_1 cross join table_2 ; SELECT * FROM table_1 cross join table_2 cross join table_3; “没有任何限制条件的连接方式”称之为”交叉连接”，”交叉连接”后得到的结果跟线性代数中的”笛卡尔乘积”一样。 (2) 内连接：inner join 内连接-等值连接 内连接-不等连接 内连接-自连接 SELECT * FROM table_1 t1, table_2 t2 WHERE t1.id = t2.id ; SELECT * FROM table_1 t1 JOIN table_2 t2 ON t1.id = t2.id ; SELECT * FROM table_1 t1 INNER JOIN table_2 t2 ON t1.id = t2.id ; (内连接 等值连接) SELECT * FROM table_1 t1 INNER JOIN table_2 t2 ON t1.id &gt; t2.id ; (内连接 不等连接) SELECT * FROM table_1 t1 INNER JOIN table_1 t2 ON t1.id t2.user_id ; (内连接 自连接) “内连接”理解成”两张表中同时符合某种条件的数据记录的组合” 注意：inner join 不带条件时就成了 cross join 交叉连接(笛卡尔积) (3) 外连接(3.1) 左连接 left join SELECT * FROM table_1 t1 LEFT OUTER JOIN table_2 t2 ON t1.id = t2.id ; SELECT * FROM table_1 t1 LEFT JOIN table_2 t2 ON t1.id = t2.id ; 左外连接不仅会查询出两表中同时符合条件的记录的组合，同时还会将”left outer join”左侧的表中的不符合条件的记录同时展示出来，由于左侧表中的这一部分记录并不符合连接条件，所以这一部分记录使用”空记录”进行连接。 (3.2) 右连接 right joinSELECT * FROM table_1 t1 RIGHT OUTER JOIN table_2 t2 ON t1.id = t2.id ; SELECT * FROM table_1 t1 RIGHT JOIN table_2 t2 ON t1.id = t2.id ; (4) 联合查询：union 与 union all 当使用union连接两个查询语句时，两个语句查询出的字段数量必须相同，否则无法使用union进行联合查询。 使用union将两个结果集集中显示时，重复的数据会被合并为一条。 SELECT FROM table_1 t1 UNION SELECT FROM table_2 t2 ; 使用union all进行联合查询时，如果两条sql语句存在重复的数据，重复的记录会被展示出来。 SELECT FROM table_1 t1 UNION ALL SELECT FROM table_2 t2 ; (5) 全连接：full join “全连接”的英文原文为full join，但是在mysql中并不支持”全连接”，更准确的说，mysql中不能直接使用”full join”实现全连接，不过，我们可以变相的实现”全连接”,在mysql中，我们可以使用”left join”、”union”、”right join”的组合实现所谓的”全连接”。 SELECT * FROM table_1 t1 LEFT JOIN table_2 t2 ON t1.id = t2.id UNION ALL SELECT * FROM table_1 t1 RIGHT JOIN table_2 t2 ON t1.id = t2.id ; SELECT * FROM table_1 t1 LEFT JOIN table_2 t2 ON t1.id = t2.id WHERE t2.id IS NOT NULL UNION ALL SELECT * FROM table_1 t1 RIGHT JOIN table_2 t2 ON t1.id = t2.id WHERE t1.id IS NOT NULL ; 其它MySQL更改密码 在MySql安装目录下：mysql\bin下输入如下命令：mysqladmin –u root –p旧密码 password 新密码 导出数据库mysqldump -u 用户名 -p 数据库名 &gt; 导出的文件名 例如：在mysql/bin目录下执行 mysqldump -u root -p oa &gt; oa.sql 这个时候会提示要你输入root用户名的密码,输入密码后dataname数据库就成功备份在mysql/bin/目录中，后面的路径可以自己定义，可以使用绝对路径。 导出表语法： mysqldump –u 用户名 –p 数据库名 表名&gt;导出的文件名举例： mysqldump –u root –p oa dep &gt; dep.sql 导入数据进入mysql数据库控制台，如mysql -u root -pmysql&gt;use 数据库然后使用source命令，后面参数为脚本文件（如这里用到的.sql）mysql&gt;source d:\oa.sql 创建用户语法 Grant 操作权限 （select,insert,update,delete或者all） on 数据库名. to 新用户名@访问地址 identified by “密码”;举例：grant all on oa. to xu@localhost identified by ’root’;切换用户 :mysql –u xu -p References[1] MySQL基础语法[2] mysql/mariadb知识点总结（16）：select语句总结之三：多表查询[3] Mysql 多表查询详解[4] 5.7/en/alter-table.html-dev.mysql.com[5] 5.7/en/alter-table-examples]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[隐马尔科夫 模型 笔记]]></title>
    <url>%2F2017%2F04%2F20%2Fimplicit-markov-model-notes%2F</url>
    <content type="text"><![CDATA[隐马尔科夫学习 先来玩一个掷骰子的游戏。 游戏规则：假设我手里有三个不同的骰子。第一个骰子是我们平常见的骰子（称这个骰子为D6），6个面，每个面（1，2，3，4，5，6）出现的概率是1/6。第二个骰子是个四面体（称这个骰子为D4），每个面（1，2，3，4）出现的概率是1/4。第三个骰子有八个面（称这个骰子为D8），每个面（1，2，3，4，5，6，7，8）出现的概率是1/8。假设我们开始掷骰子，我们先从三个骰子里随机挑一个，掷骰子，得到一个数字，然后再随机挑一个骰子，再掷骰子，得到一个数字，重复这个过程10次。我们会依次得到10的点数。 第一种玩法，我告诉你我用了几个骰子(包括骰子的详细信息)，告诉你这十次掷的点数依次是多少，你来猜每次用的是哪个骰子。第二种玩法，我告诉你我用了几个骰子(包括骰子的详细信息)，告诉你这十次掷的点数依次是多少，你来猜依次掷出这十个点的概率。第三种玩法，我告诉你我用了几个骰子(不告诉你骰子的详细信息)，告诉你这十次掷的点数依次是多少，你来猜这几种骰子分别是什么骰子(要说出骰子的详细信息)。 其实，上面这个游戏就可以用隐马尔科夫模型来求解。掷骰子游戏里掷出的这串数字 在隐马尔可夫模型中叫做 可见状态链。 比如 1 6 3 5 2 7 3 5 2 4依次使用哪个骰子 在隐马尔可夫模型中叫做 隐含状态链。 比如 D6 D8 D8 D6 D4 D8 D6 D6 D4 D8 统计学 隐马尔科夫模型 References[1] 《数学之美》 吴军[2] 《统计学习方法》 李航[3] 一文搞懂HMM（隐马尔可夫模型）]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ-IDEA 使用笔记]]></title>
    <url>%2F2017%2F04%2F18%2Fintellij-idea-install-and-config%2F</url>
    <content type="text"><![CDATA[看到好多人说idea好用，自己也想试试。结果发现上手的时候比较难，一旦上手，真的是爱不释手。 比eclipse好用多了。 举一个很简单的例子。 123在main方法里写了下面一行代码 double a; a = args[2]; // idea 会在这行代码下标红线，提示有错误 (1) 下载及安装 http://www.jetbrains.com/idea/download/#section=windows http://www.jetbrains.com/idea/download/download-thanks.html?platform=windowsZip&amp;code=IIC (2) 配置配置编码 全局编码设置 File -&gt; Other Settings -&gt; Default Settings -&gt; Editor -&gt; File Encodings 工程编码设置 File -&gt; Settings -&gt; Editor -&gt; File Encodings 配置主题 File -&gt; Settings -&gt; Appearance &amp; Behavior -&gt; Appeaeance IDEA的主题可以自定义，也可从网上下载 http://www.riaway.com/theme.php 喜欢的主题,保存到本地。 主题是一个jar的包。导入到idea的方法如下： file –&gt; import setttings –&gt;主题jar文件 –&gt; 确认 –&gt; 重启 这样就导入主题了。 同样的，自定义的主题也可以导出保存起来。什么时候想用就再导入。导出的方法如下： file –&gt; Export setttings –&gt; 选中保存路径–&gt; 确认 配置背景豆沙绿 File -&gt; Settings -&gt; Editor -&gt; Color Scheme - &gt; General 配置字体 界面字体 File -&gt; Settings -&gt; Appearance &amp; Behavior -&gt; Appearance 程序字体 File -&gt; Settings -&gt; Editor -&gt; Colors &amp; Fonts -&gt; Font 配置JDK File -&gt; Other Settings -&gt; Default Project Structure 配置Git File -&gt; Default Settings -&gt; Version Control -&gt; Git 或者这样也可以 File -&gt; Settings -&gt; Version Control -&gt; Git 配置maven File -&gt; Other Settings -&gt; Default Settings -&gt; Build, Execution, Deployment -&gt; Build Tools -&gt; Maven 配置代码风格 File -&gt; Settings -&gt;Editor -&gt; Color Scheme - &gt; Java Comments -&gt; JavaDoc -&gt; Tag value 63, 95, 191 #3F5FBF -&gt; Text 63, 95, 191 #3F5FBF -&gt; Line comment 63, 127, 95 #3F7F5F Keyword 127, 0, 85 #7F0055 String -&gt; String text 42, 0, 255 #2A00FF 安装常用插件https://plugins.jetbrains.com/idea File -&gt; Settings -&gt; Plugins -&gt; Browse repositories， 搜索想要的插件 Maven Helper FindBugs-IDEA CheckStyle-IDEA 通过检查对代码编码格式，命名约定，Javadoc，类设计等方面进行代码规范和风格的检查，从而有效约束开发人员更好地遵循代码编写规范。 GsonFormat Java开发中，经常有把json格式的内容转成Object的需求，GsonFormat这款插件可以实现该功能。 配置快捷键 无 常用快捷键1234567891011121314151617181920212223242526272829303132333435363738Alt + Enter 导入包,自动修正 Ctrl + N 查找类 Ctrl + Shift + N 查找文件 Ctrl + Alt + L 格式化代码 Ctrl + Alt + O 优化导入的类和包 Alt + Insert 生成代码(如get,set方法,构造函数等) Ctrl + E 或者 Alt + Shift + C 最近更改的代码 Ctrl + R 替换文本 Ctrl + F 查找文本 Ctrl + Shift + Space 自动补全代码 Ctrl + 空格 代码提示 Ctrl + Alt + Space 类名或接口名提示 Ctrl + P 方法参数提示 Ctrl + Shift + Alt + N 查找类中的方法或变量 Alt + Shift + C 对比最近修改的代码Shift + F6 重构-重命名 Ctrl + X 删除行 Ctrl + D 复制行 Ctrl + / 或 Ctrl + Shift + / 注释（// 或者/…/ ） Ctrl + J 自动代码 Ctrl + E 最近打开的文件 Ctrl + H 显示类结构图 Ctrl + Q 显示注释文档 Alt + F1 查找代码所在位置 Alt + 1 快速打开或隐藏工程面板 Ctrl + Alt + left/right 返回至上次浏览的位置 Alt + left/right 切换代码视图 Alt + Up/Down 在方法间快速移动定位 Ctrl + Shift + Up/Down 代码向上/下移动。 F2 或Shift + F2 高亮错误或警告快速定位代码标签输入完成后，按Tab，生成代码。 选中文本，按Ctrl + Shift + F7 ，高亮显示所有该文本，按Esc高亮消失。 Ctrl + W 选中代码，连续按会有其他效果 选中文本，按Alt + F3 ，逐个往下查找相同文本，并高亮显示。 Ctrl + Up/Down 光标跳转到第一行或最后一行下 Ctrl + B 快速打开光标处的类或方法 (3) 遇到的问题(3.1) Intellij IDEA 代码格式如何与eclipse保持风格一致安装插件File -&gt; Settings -&gt; Plugins -&gt; Browse repositories，搜索Eclipse code formatter即可出现，点击右上角的Download and Install安装。或者可以直接在官网下载安装之后重启Intellij，即可在Preferences-&gt;Eclipse Code Formatter找到配置项。 配置插件使用插件 使用Intellij的格式化快捷键”Ctrl+shift+F”即可进行格式化。如果出错会输出提示到Event Log里， 如果看到’xxx formatted sucessfully by Eclipse code formatter’则表示格式化成功！ (3.2) 输入法输入框不跟随 我用的是 win7 64旗舰版 ideaIU-2017.1.1.win JDK 1.8.0_111 在把JDK升级 idea升级成ideaIU-2017.1.2.win后发现输入汉字的时候，输入法输入框不跟随， 最后发现使用JDK 1.8.0_111 1.8.0_162不会出现上述问题 (3.3) Intellij idea 不能识别 @Slf4j，@Getter ,@Setter注解，编译通过,报红提示：cannot resolve symbol 网上搜了半天，试了很多次，最后在File -&gt; Settings -&gt; Build, Execution,Deployment -&gt; Compoler -&gt; Java Compoler里设置User compiler为Javac后解决了 Lombok Plugin Settings -&gt; Build,Execution,Deployment -&gt; Maven -&gt; Ignored Files (3.4) log cannot be resolved12java.lang.Error: Unresolved compilation problem: log cannot be resolved 发现ideaIU-2017.3.3.win 比较费CPU，然后换成ideaIU-2017.2.2.win，结果发现程序能运行但是idea提示log cannot be resolved，感觉挺难受，就把lombok卸载了，重新装一下，然后接解决了。 (3.5) lombok java找不到符号 logFile -&gt; Settings -&gt; Build,Execution,Deployment -&gt; Annotation Processors 选中 Enable annotation processing (3.6) Failed to create assembly:Error creating assembly archive asm: Problem creating zip:Execution exception1Failed to execute goalorg.apache.maven.plugins:maven-assembly-plugin:2.5.5:single(make-assembly) on project web: Failed to create assembly:Error creating assembly archive asm: Problem creating zip:Execution exception (and the archive is probably corrupt but Icould not delete it): Java heap space -&gt; [Help 1] 原因：maven编译时内存溢出导致 Java heap space 解决办法：1、使用自己安装的maven2、配置IDEA里Maven的配置 Settings -&gt; Build,Execution,Deployment -&gt; Build Tools -&gt; Maven -&gt; Importing VM options for importer -Xmx2048m3、JDK版本设置高一点 References[1] IDEA Settings [2] IDEA 2017.1 EAP[3] Intellij IDEA设置忽略部分类编译错误[4] Intellij使用心得(四) – 导入Eclipse的代码格式化文件[5] IDEA使用–字体、编码和基本设置[6] IDEA右键新建时，选项没有Java class[7] Intellij IDEA运行前不检查其他类的错误 [8] Intellij IDEA 15中文输入框不跟随怎么办[9] 好烦啊，IDEA输入中文时输入法候选词框不跟随光标[10] IntelliJ Idea 实用插件推荐[11] IntelliJ IDEA（2017）安装和破解[12] Intellij idea 不能识别 @Slf4j，@Getter ,@Setter注解，编译通过[13] IDEA项目左边栏只能看到文件看不到项目结构[14] 使用IntelliJ IDEA查看类的继承关系图形[15] 使用lombok 找不到方法[16] idea中字体颜色设置成类似eclipse的方案[17] IDEA的这八条配置你一定要改]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eclipse 安装 配置]]></title>
    <url>%2F2017%2F04%2F18%2Feclipse-install-config%2F</url>
    <content type="text"><![CDATA[1 下载及安装 在Eclipse官网 下载页面选择对应的版本下载，这里提供一个下载链接 下载的是zip安装包，解压完就可以使用 2 配置1.设置JDK Window - Preferences - Java - Installed JREs - Add - Standard VM - 选中JDK安装目录的jdk1.7.0_80文件夹 2. 设置工作空间编码格式 Window - Preferences - General - WorkSpace - TextFileEncoding - Other UTF-8 3. 设置工作空间字体格式 Window - Preferences - General - Appearance - Colors and Fonts - Basic - Text Font Consolas 常规 11 4. 设置其他文件编码例如 把.properties文件设置为UTF-8编码 Window - Preferences - General - Content Type - Text - Java Properties File（选中） - 下面框里面的 *.properties(locked) 5. eclipse内存不够用了 在eclipse 安装目录下 6. eclipse设置背景颜色为豆沙绿，保护视力。Window - Preferences - General - Editors - Text Editors 右边下面找到Background color，选中background color，勾掉System Default，点击&apos;color&apos;，弹出颜色选择面板，然后设置自定义颜色，把色调调成：85 饱和度调成：123 亮度调成205 即可调成豆沙绿色了 然后点确定。 7. 设置tab 键为4个空格 8. 在eclipse的package视图中自动选中相应的类 pachage explorer右边的按钮，“向左，向右”的箭头没，选中它就行了（图中红色框圈住的部分）。 9. eclipse设置JSP页面默认编码为UTF-8Window - Preferences - Web - Jsp Files 10 配置颜色12345678910Java Java keyword (关键字 茄紫色) GBK 127,0,85 String 42,0,255 JavaDoc HTML markup 127,127,159 Links 63,63,191 Others (@param 的value) 63,95,191 Tags (@param) 127,159,191 Common 3. Eclipse快捷键大全123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169Ctrl+1 快速修复(最经典的快捷键,就不用多说了)Ctrl+D: 删除当前行Ctrl+Alt+↓ 复制当前行到下一行(复制增加)Ctrl+Alt+↑ 复制当前行到上一行(复制增加)Alt+↓ 当前行和下面一行交互位置(特别实用,可以省去先剪切,再粘贴了)Alt+↑ 当前行和上面一行交互位置(同上)Alt+← 前一个编辑的页面Alt+→ 下一个编辑的页面(当然是针对上面那条来说了)Alt+Enter 显示当前选择资源(工程,or 文件 or文件)的属性Shift+Enter 在当前行的下一行插入空行(这时鼠标可以在当前行的任一位置,不一定是最后)Shift+Ctrl+Enter 在当前行插入空行(原理同上条)Ctrl+Q 定位到最后编辑的地方Ctrl+L 定位在某行 (对于程序超过100的人就有福音了)Ctrl+M 最大化当前的Edit或View (再按则反之)Ctrl+/ 注释当前行,再按则取消注释Ctrl+O 快速显示 OutLineCtrl+T 快速显示当前类的继承结构Ctrl+W 关闭当前EditerCtrl+K 参照选中的Word快速定位到下一个Ctrl+E 快速显示当前Editer的下拉列表(如果当前页面没有显示的用黑体表示)Ctrl+/(小键盘) 折叠当前类中的所有代码Ctrl+×(小键盘) 展开当前类中的所有代码Ctrl+Space 代码助手完成一些代码的插入(但一般和输入法有冲突,可以修改输入法的热键,也可以暂用Alt+/来代替)Ctrl+Shift+E 显示管理当前打开的所有的View的管理器(可以选择关闭,激活等操作)Ctrl+J 正向增量查找(按下Ctrl+J后,你所输入的每个字母编辑器都提供快速匹配定位到某个单词,如果没有,则在stutes line中显示没有找到了,查一个单词时,特别实用,这个功能Idea两年前就有了)Ctrl+Shift+J 反向增量查找(和上条相同,只不过是从后往前查)Ctrl+Shift+F4 关闭所有打开的EditerCtrl+Shift+X 把当前选中的文本全部变味小写Ctrl+Shift+Y 把当前选中的文本全部变为小写Ctrl+Shift+F 格式化当前代码Ctrl+Shift+P 定位到对于的匹配符(譬如&#123;&#125;) (从前面定位后面时,光标要在匹配符里面,后面到前面,则反之)下面的快捷键是重构里面常用的,本人就自己喜欢且常用的整理一下(注:一般重构的快捷键都是Alt+Shift开头的了)Alt+Shift+R 重命名 (是我自己最爱用的一个了,尤其是变量和类的Rename,比手工方法能节省很多劳动力)Alt+Shift+M 抽取方法 (这是重构里面最常用的方法之一了,尤其是对一大堆泥团代码有用)Alt+Shift+C 修改函数结构(比较实用,有N个函数调用了这个方法,修改一次搞定)Alt+Shift+L 抽取本地变量( 可以直接把一些魔法数字和字符串抽取成一个变量,尤其是多处调用的时候)Alt+Shift+F 把Class中的local变量变为field变量 (比较实用的功能)Alt+Shift+I 合并变量(可能这样说有点不妥Inline)Alt+Shift+V 移动函数和变量(不怎么常用)Alt+Shift+Z 重构的后悔药(Undo)编辑作用域 功能 快捷键全局 查找并替换 Ctrl+F文本编辑器 查找上一个 Ctrl+Shift+K文本编辑器 查找下一个 Ctrl+K全局 撤销 Ctrl+Z全局 复制 Ctrl+C全局 恢复上一个选择 Alt+Shift+↓全局 剪切 Ctrl+X全局 快速修正 Ctrl1+1全局 内容辅助 Alt+/全局 全部选中 Ctrl+A全局 删除 Delete全局 上下文信息 Alt+？Alt+Shift+?Ctrl+Shift+SpaceJava编辑器 显示工具提示描述 F2Java编辑器 选择封装元素 Alt+Shift+↑Java编辑器 选择上一个元素 Alt+Shift+←Java编辑器 选择下一个元素 Alt+Shift+→文本编辑器 增量查找 Ctrl+J文本编辑器 增量逆向查找 Ctrl+Shift+J全局 粘贴 Ctrl+V全局 重做 Ctrl+Y查看作用域 功能 快捷键全局 放大 Ctrl+=全局 缩小 Ctrl+-窗口作用域 功能 快捷键全局 激活编辑器 F12全局 切换编辑器 Ctrl+Shift+W全局 上一个编辑器 Ctrl+Shift+F6全局 上一个视图 Ctrl+Shift+F7全局 上一个透视图 Ctrl+Shift+F8全局 下一个编辑器 Ctrl+F6全局 下一个视图 Ctrl+F7全局 下一个透视图 Ctrl+F8文本编辑器 显示标尺上下文菜单 Ctrl+W全局 显示视图菜单 Ctrl+F10全局 显示系统菜单 Alt+-导航作用域 功能 快捷键Java编辑器 打开结构 Ctrl+F3全局 打开类型 Ctrl+Shift+T全局 打开类型层次结构 F4全局 打开声明 F3全局 打开外部javadoc Shift+F2全局 打开资源 Ctrl+Shift+R全局 后退历史记录 Alt+←全局 前进历史记录 Alt+→全局 上一个 Ctrl+,全局 下一个 Ctrl+.Java编辑器 显示大纲 Ctrl+O全局 在层次结构中打开类型 Ctrl+Shift+H全局 转至匹配的括号 Ctrl+Shift+P全局 转至上一个编辑位置 Ctrl+QJava编辑器 转至上一个成员 Ctrl+Shift+↑Java编辑器 转至下一个成员 Ctrl+Shift+↓文本编辑器 转至行 Ctrl+L搜索作用域 功能 快捷键全局 出现在文件中 Ctrl+Shift+U全局 打开搜索对话框 Ctrl+H全局 工作区中的声明 Ctrl+G全局 工作区中的引用 Ctrl+Shift+G文本编辑作用域 功能 快捷键文本编辑器 改写切换 Insert文本编辑器 上滚行 Ctrl+↑文本编辑器 下滚行 Ctrl+↓文件作用域 功能 快捷键全局 保存 Ctrl+XCtrl+S全局 打印 Ctrl+P全局 关闭 Ctrl+F4全局 全部保存 Ctrl+Shift+S全局 全部关闭 Ctrl+Shift+F4全局 属性 Alt+Enter全局 新建 Ctrl+N项目作用域 功能 快捷键全局 全部构建 Ctrl+B源代码作用域 功能 快捷键Java编辑器 格式化 Ctrl+Shift+FJava编辑器 取消注释 Ctrl+Java编辑器 注释 Ctrl+/Java编辑器 添加导入 Ctrl+Shift+MJava编辑器 组织导入 Ctrl+Shift+OJava编辑器 使用try/catch块来包围 未设置，太常用了，所以在这里列出,建议自己设置。也可以使用Ctrl+1自动修正。运行作用域 功能 快捷键全局 单步返回 F7全局 单步跳过 F6全局 单步跳入 F5全局 单步跳入选择 Ctrl+F5全局 调试上次启动 F11全局 继续 F8全局 使用过滤器单步执行 Shift+F5全局 添加/去除断点 Ctrl+Shift+B全局 显示 Ctrl+D全局 运行上次启动 Ctrl+F11全局 运行至行 Ctrl+R全局 执行 Ctrl+U重构作用域 功能 快捷键全局 撤销重构 Alt+Shift+Z全局 抽取方法 Alt+Shift+M全局 抽取局部变量 Alt+Shift+L全局 内联 Alt+Shift+I全局 移动 Alt+Shift+V全局 重命名 Alt+Shift+R全局 重做 Alt+Shift+Y References[1] http://www.68idc.cn/help/jiabenmake/qita/20150516341321.html[2] http://blog.csdn.net/xiebaochun/article/details/37922571[3] http://blog.csdn.net/angle_birds/article/details/19609011]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j 批量 导入 数据 的 几种方式]]></title>
    <url>%2F2017%2F04%2F14%2Fneo4j-import-data%2F</url>
    <content type="text"><![CDATA[Neo4j是一个比较新的数据库，ETL工具较少，公司一个项目需要导入上百亿数据，想找一个最合适的方案来导入数据。 于是就想测测各种导入方式的效率以及成本 常见数据插入方式概览目前主要有以下几种数据插入方式： Cypher create 语句，为每一条数据写一个create Cypher load csv 语句，将数据转成CSV格式，通过LOAD CSV读取数据。 官方提供的neo4j-import工具，未来将被neo4j-admin import代替 官方提供的Java API - BatchInserter 大牛编写的 batch-import 工具 neo4j-apoc load.csv + apoc.load.relationship 针对实际业务场景，定制化开发 这些工具有什么不同呢？速度如何？适用的场景分别是什么？ just try create语句 load csv语句 neo4j-import BatchInserter batch-import apoc 适用场景 1 ~ 1w 0 ~ 1000w 千万以上 千万以上 千万以上 1 ~ 数千万 速度 很慢 1000/s 一般 5000/s 非常快 x w/s 很快 x w/s 很快x w/s 1w /s 实际测试 无 9.5k/s(节点+关系) 用到了merge，数据量越大，速度越慢 12w/s(节点+关系) 1w/s(节点+关系) 1w/s(节点+关系) 4k/s(1亿数据上增量更新) 1w/s(百万数据上更新) 用到了merge，数据量越大，速度越慢 优点 1.使用方便 2.可实时插入 1.官方ETL工具 2.可以加载本地/远程CSV 3.可实时插入 1.官方工具 2.占用资源少 1.官方API 1.可以增量更新 2.基于BatchInserter 1.官方ETL工具 2.可以增量更新 3.支持在线导入 4.支持动态传Label RelationShip 缺点 1.速度慢 2.处理数据，拼CQL复杂，很少使用 1.导入速度较慢 2.只能导入节点 3.不能动态传Label RelationShip 1.需要脱机导入 停止Neo4j数据库 2.只能用于初始化导入 1.只能在JAVA中使用 2.需要脱机导入 停止Neo4j数据库 1.需要脱机导入 停止Neo4j数据库 1.速度一般 各种导入数据方法测试(1) create语句 个人认为，只有在学习neo4j的时候会用到create，在批量导入数据的时候，一般很少用create 未测 ./neo4j-shell -c &lt; /data/stale/data01/neo4j/neo4j_script.cypther ./bin/neo4j-shell -path ./data/databases/graph.db -conf ./conf/neo4j.conf -file create_index.cypther (2) load csv 语句 服务器配置 CPU 32核，256G内存，10T机械硬盘 数据格式12345uuid,name,Labelb6b0ea842890425588d4d3cfb38139a9,&quot;文烁&quot;,Label15099c4f943d94fa1873165e3f6f3c2fb,&quot;齐贺喜&quot;,Label3c83ed0ae9fb34baa956a42ecf99c8f6e,&quot;李雄&quot;,Label2e62d1142937f4de994854fa1b3f0670a,&quot;房玄龄&quot;,Label 下面是详细测试结果 (2.1) 导入10w数据(仅节点)12345678neo4j-sh (?)$ using periodic commit 10000 load csv with headers from &quot;file:/data/stale/data01/neo4j/node_uuid_10w.csv&quot; as line with line create (:Test &#123;uuid:line.uuid, name:line.name&#125;);+-------------------+| No data returned. |+-------------------+Nodes created: 100000Properties set: 200000Labels added: 1000003412 ms 10万数据(只有节点，没有关系)导入用时3.412s (2.2) 导入1kw数据(仅节点)/data/stale/data01/neo4j/node_uuid_1kw.csv文件加上标题一共10000010条，有10000009条数据 12345678910111213141516neo4j-sh (?)$ load csv from &quot;file:/data/stale/data01/neo4j/node_uuid_1kw.csv&quot; as line return count(*);+----------+| count(*) |+----------+| 10000010 |+----------+1 row7434 msneo4j-sh (?)$ using periodic commit 10000 load csv with headers from &quot;file:/data/stale/data01/neo4j/node_uuid_1kw.csv&quot; as line with line create (:Test &#123;uuid:line.uuid, name:line.name&#125;);+-------------------+| No data returned. |+-------------------+Nodes created: 10000009Properties set: 20000018Labels added: 10000009151498 ms 1千万数据(只有节点，没有关系)导入用时151.498s 导入时CPU利用率在150%左右，RES Memory 5G左右，VIRT Memory 70G左右 (2.3) 导入100w数据(仅关系)123456789101112131415161718192021222324252627282930313233343536neo4j-sh (?)$ match (n) return count(n);+----------+| count(n) |+----------+| 18004002 |+----------+1 row12 msneo4j-sh (?)$ match ()--&gt;() return count(*);+----------+| count(*) |+----------+| 15001999 |+----------+1 row18 msneo4j-sh (?)$ using periodic commit 100000 load csv with headers from &quot;file:/data/stale/data01/neo4j/relathionship_uuid_100w.csv&quot; as line with line merge (n1:Test &#123;uuid:line.uuid1&#125;) merge (n2:Test &#123;uuid:line.uuid2&#125;) with * create (n1)-[r:Relationship]-&gt;(n2);+-------------------+| No data returned. |+-------------------+Relationships created: 100000075737 msneo4j-sh (?)$ match (n) return count(n);+----------+| count(n) |+----------+| 18004002 |+----------+1 row6 msneo4j-sh (?)$ match ()--&gt;() return count(*);+----------+| count(*) |+----------+| 16001999 |+----------+ 创建100w关系用时75.737s 因为我节点已经提前导入了，所以merge的时候节点全部存在，根据结果可以看到，只创建了100w关系，没有创建节点 但是这种方式有一个弊端，关系要写死，在只有一种关系时试用，在有多种关系时，不适用。还有一个不好的地方就是用的merge，uuid是String类型，会随着数据的正常速度变慢。 load csv 的速度我用的是 导入节点时间+导入关系时间导入100w 节点+数据 (Label写死，RelationShip写死，也就是只有一种Label和一种RelationShip) 共花费15.149 + 75.737 = 90.886 。load csv的速度大概在1.1w/s，但这种情况一般很少使用，仅供参考。 (3) neo4j-import (在以后版本会被neo4j-admin import替掉) 服务器配置 CPU 32核，256G内存，10T机械硬盘 空库初始化导入1千万数据(1kw节点 1kw关系 2kw属性，id使用integer，属性中只有数字和英文)花费27s 932ms 空库初始化导入1千万数据(1kw节点 1kw关系 2kw属性，包含中文属性)花费1min 50s 9ms 空库初始化导入1.1亿数据(1.1亿节点 1.1关系 2.2亿属性)花费15min 9s 37ms 数据格式 node.csv12345uuid:ID(users),name:String,:Labelc63bc1e7dc594fd49fbe36dd664ff0a6,&quot;维特&quot;,Label1b52fb5f2266b4edbadc82b5ec4c430b8,&quot;廖二松&quot;,Label2d95d430cfeee47dd95f9bf5e0ec1ae93,&quot;徐青偏&quot;,Label3b2d1fffc8173461fa603d4fbb601b3ee,&quot;杨础维&quot;,Label2 relationship.csv12345uuid:START_ID(users),uuid:END_ID(users),:TYPEc63bc1e7dc594fd49fbe36dd664ff0a6,b2d1fffc8173461fa603d4fbb601b3ee,RelationShip1d95d430cfeee47dd95f9bf5e0ec1ae93,c63bc1e7dc594fd49fbe36dd664ff0a6,RelationShip2b2d1fffc8173461fa603d4fbb601b3ee,b52fb5f2266b4edbadc82b5ec4c430b8,RelationShip3b52fb5f2266b4edbadc82b5ec4c430b8,d95d430cfeee47dd95f9bf5e0ec1ae93,RelationShip1 部分详细日志如下 (3.1) 导入1000w数据(1kw节点 1kw关系 2kw属性，id使用integer，属性中只有数字和英文) 空库初始化导入1千万数据(1kw节点 1kw关系 2kw属性，id使用integer，属性中只有数字和英文)花费27s 932ms 部分日志如下：12345678Done in 779msIMPORT DONE in 27s 932ms.Imported: 10000000 nodes 10000000 relationships 20000000 propertiesPeak memory usage: 209.81 MB (3.2) 导入1000w数据(1kw节点 1kw关系 2kw属性，包含中文属性) 空库初始化导入1千万数据(1kw节点 1kw关系 2kw属性，包含中文属性)花费1min 50s 9ms 部分日志如下：123456IMPORT DONE in 1m 50s 9ms.Imported: 10000000 nodes 10000000 relationships 20000000 propertiesPeak memory usage: 209.81 MB (3.3) 导入1.1亿数据(1.1亿节点 1.1关系 2.2亿属性) 空库初始化导入1.1亿数据(1.1亿节点 1.1关系 2.2亿属性)花费15min 9s 37ms 部分日志如下：1234567IMPORT DONE in 15m 9s 37ms.Imported: 110000010 nodes 110000000 relationships 220000020 propertiesPeak memory usage: 2.27 GBThere were bad entries which were skipped and logged into /data/stale/data01/neo4j/neo4j-community-3.1.0/data/databases/test_uuid_1y_graph.db/bad.log (4) BatchInserter batch-import调的BatchInserter的代码，所以BatchInserter没测，可以认为BatchInster和batch-import速度一样 (5) batch-import 在数据库中已有1kw数据的情况下，导入100w数据(100w节点 100w关系 200w属性，包含中文属性)花费92s node.csv123456uuid:string:users,name:String,:label #官方的文件头uuid:ID(users),name:String,:Label #修改程序后的文件头c63bc1e7dc594fd49fbe36dd664ff0a6,&quot;维特&quot;,Label1b52fb5f2266b4edbadc82b5ec4c430b8,&quot;廖二松&quot;,Label2d95d430cfeee47dd95f9bf5e0ec1ae93,&quot;徐青偏&quot;,Label3b2d1fffc8173461fa603d4fbb601b3ee,&quot;杨础维&quot;,Label2 relationship.csv123456uuid:string:users,uuid:string:users,type #官方的文件头uuid:START_ID(users),uuid:END_ID(users),:TYPE #修改程序后的文件头c63bc1e7dc594fd49fbe36dd664ff0a6,b2d1fffc8173461fa603d4fbb601b3ee,RelationShip1d95d430cfeee47dd95f9bf5e0ec1ae93,c63bc1e7dc594fd49fbe36dd664ff0a6,RelationShip2b2d1fffc8173461fa603d4fbb601b3ee,b52fb5f2266b4edbadc82b5ec4c430b8,RelationShip3b52fb5f2266b4edbadc82b5ec4c430b8,d95d430cfeee47dd95f9bf5e0ec1ae93,RelationShip1 123456789Using: Importer batch.properties /data/stale/data01/neo4j/neo4j-community-3.1.0/data/databases/test_uuid_1000w_graph.db /data/stale/data01/neo4j/node_uuid_100w.csv /data/stale/data01/neo4j/relationship_uuid_100w.csvUsing Existing Configuration File..........Importing 1000000 Nodes took 15 seconds..........Importing 1000000 Relationships took 16 secondsTotal import time: 92 seconds (6) apoc load csv + merge + apoc.create.relationship 1234567891011121314151617neo4j-sh (?)$ using periodic commit 1000000&gt; load csv from &apos;file:/data/stale/data01/neo4j/relathionship_uuid_1kw.csv&apos; as line fieldterminator &apos;,&apos;&gt; merge (n1:Test &#123;uuid: line[0]&#125;)&gt; merge (n2:Test &#123;uuid: line[1]&#125;)&gt; with n1, n2, line&gt; CALL apoc.create.relationship(n1, line[2], &#123;&#125;, n2) YIELD rel&gt; return count(rel) ;+------------+| count(rel) |+------------+| 10000010 |+------------+1 rowNodes created: 8645143Properties set: 8645143Labels added: 86451432395852 ms 在1.1亿数据上增量更新1kw数据花费2395.852sVIRT Memory 90G RES Memory 78G 结论 根据实际情况选用最好的方式 neo4j-import导入速度快，但是要求是空库，导入时要停止neo4j，也就是脱机导入，而且你要提前处理好数据，数据最好不要有重复，如果有重复，可以导入时跳过，然后根据bad.log来查看或者修正这部分数据 batch-import可以增量导入，但是要求导入时停止neo4j数据库(脱机导入)，而且增量更新的数据不会和库里存在的数据对比，所以要求数据全是新的，否则会出现重复数据 load csv比较通用，而且可以在neo4j数据库运行时导入，但是导入速度相对较慢，要提前整理好数据，而且不能动态创建 Label RelationShip apoc挺好用的，可以动态创建RelationShip，但是不能动态创建Label (动态创建Label只能在程序里通过拼接字符串的方法实现) 实际情况中，处理数据比导入数据更花费时间 Neo4j的查询速度为何这么慢？这能商用吗？ 这篇文章中 HackerWhite 写的那部分特别好，可以参考。 References[1] guide-importing-data-and-etl[2] guide-import-csv[3] load-csv[4] import[5] how-to-insert-bulk-data-into-neo4j[6] Neo4j的查询速度为何这么慢？这能商用吗？[7] 使用batch-import工具向neo4j中导入海量数据[8] 如何快速导入网络数据到图数据库Neo4j[9] bulk-data-import-neo4j-3-0[10] neo4j-etl github[11] neo4j google group[12] import-tool[13] Neo4j在并发的load CSV文件的时候出现deadlock问题。怎么有效处理提高并发呢？]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j APOC 初识]]></title>
    <url>%2F2017%2F04%2F14%2Fneo4j-apoc%2F</url>
    <content type="text"><![CDATA[前一段时间有个朋友提了个问题，A和B是朋友关系，经过算法发现，A和C也是朋友关系，在cypher中怎么用A和B的关系来表示A和C的关系。 123456// 旧版本的语句start n1=node(1), n2=node(1725535), n3=node(1725534)match (n1)-[r]-&gt;(n2)with n1 ,n2, n3, type(r) as rtcreate (n1)-[:rt]-&gt;(n3)return n1, n2, n3; 自己测试了半天发现没有办法，Cypher里定义关系时，不支持动态参数。 后来查了半天资料，发现可以用Neo4j APOC解决 最后发现有两种方式可以解决： 用Neo4j APOC，使用apoc.cypher.run可以动态构造cypher 或者 apoc.create.relationship 把一条语句拆分成两条，在第二条语句里动态拼CQL(可能会有CQL注入的风险) References[1] neo4j-apoc github地址[2] neo4j apoc 官方介绍[3] apoc各个版本jar[4] procedures-gallery[5] APOC，Neo4j的开发者宝藏[6] neo4j中文社区apoc]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合之LinkedList源码解读]]></title>
    <url>%2F2017%2F04%2F14%2Fjava-linkedlist%2F</url>
    <content type="text"><![CDATA[LinkedList 是一个继承于AbstractSequentialList的双向链表。它也可以被当作堆栈、队列或双端队列进行操作。 LinkedList 实现 List 接口，能对它进行队列操作。 LinkedList 实现 Deque 接口，即能将LinkedList当作双端队列使用。 LinkedList 实现了Cloneable接口，即覆盖了函数clone()，能克隆。 LinkedList 实现java.io.Serializable接口，这意味着LinkedList支持序列化，能通过序列化去传输。 LinkedList 是非同步的。 References[1] Java 集合系列05之 LinkedList详细介绍(源码解析)和使用示例]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 集合 ArrayList 源码 学习 (JDK-1.8)]]></title>
    <url>%2F2017%2F04%2F14%2Fjava-arraylist-source-code-learning%2F</url>
    <content type="text"><![CDATA[首先看ArrayList的构造方法 123456/** * Constructs an empty list with an initial capacity of ten. */public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125; 1234567891011121314151617/** * Constructs an empty list with the specified initial capacity. * * @param initialCapacity the initial capacity of the list * @throws IllegalArgumentException if the specified initial capacity * is negative */public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); &#125;&#125; 12345678910111213141516171819/** * Constructs a list containing the elements of the specified * collection, in the order they are returned by the collection's * iterator. * * @param c the collection whose elements are to be placed into this list * @throws NullPointerException if the specified collection is null */public ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); if ((size = elementData.length) != 0) &#123; // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; &#125;&#125; 1234567891011121314151617181920212223242526/** * The size of the ArrayList (the number of elements it contains). * * @serial */private int size;/** * Returns the number of elements in this list. * * @return the number of elements in this list */public int size() &#123; return size;&#125;/** * Returns &lt;tt&gt;true&lt;/tt&gt; if this list contains no elements. * * @return &lt;tt&gt;true&lt;/tt&gt; if this list contains no elements */public boolean isEmpty() &#123; return size == 0;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596/** * 存储ArrayList元素的数组缓冲区。 * ArrayList的容量是这个数组缓冲区的长度。 任何 * 使用elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA清空ArrayList * 当第一个元素被添加时，将被扩展为DEFAULT_CAPACITY。 */ /** * The array buffer into which the elements of the ArrayList are stored. * The capacity of the ArrayList is the length of this array buffer. Any * empty ArrayList with elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA * will be expanded to DEFAULT_CAPACITY when the first element is added. */transient Object[] elementData; // non-private to simplify nested class access/** * Shared empty array instance used for default sized empty instances. We * distinguish this from EMPTY_ELEMENTDATA to know how much to inflate when * first element is added. */private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;/** * Default initial capacity. *//** 默认长度是10 */private static final int DEFAULT_CAPACITY = 10;/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &lt;tt&gt;true&lt;/tt&gt; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;/** 确保内部容量 */private void ensureCapacityInternal(int minCapacity) &#123; ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));&#125; /** 计算容量 */private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity;&#125;/** 确保明确的容量 */private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125; /** * Increases the capacity to ensure that it can hold at least the * number of elements specified by the minimum capacity argument. * * @param minCapacity the desired minimum capacity */private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; // 新的容量 = 旧容量 + 旧容量的0.5倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125;private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;&#125; 可以看到在调用add方法时，首先检查内部容量，然后再赋值在扩容时可以看到扩容后是扩容前的1.5倍，原因：int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1);在扩容时进行了数组拷贝，所有扩容时非常费内存 elementData = Arrays.copyOf(elementData, newCapacity); 123456789101112131415161718192021222324252627282930/** * Returns the element at the specified position in this list. * * @param index index of the element to return * @return the element at the specified position in this list * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E get(int index) &#123; rangeCheck(index); return elementData(index);&#125;/** * Checks if the given index is in range. If not, throws an appropriate * runtime exception. This method does *not* check if the index is * negative: It is always used immediately prior to an array access, * which throws an ArrayIndexOutOfBoundsException if index is negative. */private void rangeCheck(int index) &#123; if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125;@SuppressWarnings("unchecked")E elementData(int index) &#123; return (E) elementData[index];&#125; 12345678/** * Returns the number of elements in this list. * * @return the number of elements in this list */public int size() &#123; return size;&#125; 12345678/** * Returns &lt;tt&gt;true&lt;/tt&gt; if this list contains no elements. * * @return &lt;tt&gt;true&lt;/tt&gt; if this list contains no elements */public boolean isEmpty() &#123; return size == 0;&#125; 123456789101112/** * Returns &lt;tt&gt;true&lt;/tt&gt; if this list contains the specified element. * More formally, returns &lt;tt&gt;true&lt;/tt&gt; if and only if this list contains * at least one element &lt;tt&gt;e&lt;/tt&gt; such that * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;e==null&amp;nbsp;:&amp;nbsp;o.equals(e))&lt;/tt&gt;. * * @param o element whose presence in this list is to be tested * @return &lt;tt&gt;true&lt;/tt&gt; if this list contains the specified element */public boolean contains(Object o) &#123; return indexOf(o) &gt;= 0;&#125; 12345678910111213141516171819/** * Returns the index of the first occurrence of the specified element * in this list, or -1 if this list does not contain the element. * More formally, returns the lowest index &lt;tt&gt;i&lt;/tt&gt; such that * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;get(i)==null&amp;nbsp;:&amp;nbsp;o.equals(get(i)))&lt;/tt&gt;, * or -1 if there is no such index. */public int indexOf(Object o) &#123; if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; &#125; return -1;&#125; 12345678910111213141516171819/** * Returns the index of the last occurrence of the specified element * in this list, or -1 if this list does not contain the element. * More formally, returns the highest index &lt;tt&gt;i&lt;/tt&gt; such that * &lt;tt&gt;(o==null&amp;nbsp;?&amp;nbsp;get(i)==null&amp;nbsp;:&amp;nbsp;o.equals(get(i)))&lt;/tt&gt;, * or -1 if there is no such index. */public int lastIndexOf(Object o) &#123; if (o == null) &#123; for (int i = size-1; i &gt;= 0; i--) if (elementData[i]==null) return i; &#125; else &#123; for (int i = size-1; i &gt;= 0; i--) if (o.equals(elementData[i])) return i; &#125; return -1;&#125; 找lastIndexOf的时候倒着找，确实挺优雅的 1234567891011121314151617/** * Returns a shallow copy of this &lt;tt&gt;ArrayList&lt;/tt&gt; instance. (The * elements themselves are not copied.) * * @return a clone of this &lt;tt&gt;ArrayList&lt;/tt&gt; instance */public Object clone() &#123; try &#123; ArrayList&lt;?&gt; v = (ArrayList&lt;?&gt;) super.clone(); v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; &#125; catch (CloneNotSupportedException e) &#123; // this shouldn't happen, since we are Cloneable throw new InternalError(e); &#125;&#125; 12345678910111213141516/** * Replaces the element at the specified position in this list with * the specified element. * * @param index index of the element to replace * @param element element to be stored at the specified position * @return the element previously at the specified position * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E set(int index, E element) &#123; rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; return oldValue;&#125; 123456789101112131415161718/** * Inserts the specified element at the specified position in this * list. Shifts the element currently at that position (if any) and * any subsequent elements to the right (adds one to their indices). * * @param index index at which the specified element is to be inserted * @param element element to be inserted * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; 12345678910111213141516171819202122/** * @param src the source array. * @param srcPos starting position in the source array. * @param dest the destination array. * @param destPos starting position in the destination data. * @param length the number of array elements to be copied. * @exception IndexOutOfBoundsException if copying would cause * access of data outside array bounds. * @exception ArrayStoreException if an element in the &lt;code&gt;src&lt;/code&gt; * array could not be stored into the &lt;code&gt;dest&lt;/code&gt; array * because of a type mismatch. * @exception NullPointerException if either &lt;code&gt;src&lt;/code&gt; or * &lt;code&gt;dest&lt;/code&gt; is &lt;code&gt;null&lt;/code&gt;. */System.arraycopy(Object src, int srcPos, Object dest, int destPos, int length) src 原数组 srcPos 原数组开始复制的位置 dest 目标数组 destPos 目标数组起始位置 length 要复制的数组元素个数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * Copies the specified array, truncating or padding with nulls (if necessary) * so the copy has the specified length. For all indices that are * valid in both the original array and the copy, the two arrays will * contain identical values. For any indices that are valid in the * copy but not the original, the copy will contain &lt;tt&gt;null&lt;/tt&gt;. * Such indices will exist if and only if the specified length * is greater than that of the original array. * The resulting array is of exactly the same class as the original array. * * @param &lt;T&gt; the class of the objects in the array * @param original the array to be copied * @param newLength the length of the copy to be returned * @return a copy of the original array, truncated or padded with nulls * to obtain the specified length * @throws NegativeArraySizeException if &lt;tt&gt;newLength&lt;/tt&gt; is negative * @throws NullPointerException if &lt;tt&gt;original&lt;/tt&gt; is null * @since 1.6 */@SuppressWarnings(&quot;unchecked&quot;)public static &lt;T&gt; T[] copyOf(T[] original, int newLength) &#123; return (T[]) copyOf(original, newLength, original.getClass());&#125;/** * Copies the specified array, truncating or padding with nulls (if necessary) * so the copy has the specified length. For all indices that are * valid in both the original array and the copy, the two arrays will * contain identical values. For any indices that are valid in the * copy but not the original, the copy will contain &lt;tt&gt;null&lt;/tt&gt;. * Such indices will exist if and only if the specified length * is greater than that of the original array. * The resulting array is of the class &lt;tt&gt;newType&lt;/tt&gt;. * * @param &lt;U&gt; the class of the objects in the original array * @param &lt;T&gt; the class of the objects in the returned array * @param original the array to be copied * @param newLength the length of the copy to be returned * @param newType the class of the copy to be returned * @return a copy of the original array, truncated or padded with nulls * to obtain the specified length * @throws NegativeArraySizeException if &lt;tt&gt;newLength&lt;/tt&gt; is negative * @throws NullPointerException if &lt;tt&gt;original&lt;/tt&gt; is null * @throws ArrayStoreException if an element copied from * &lt;tt&gt;original&lt;/tt&gt; is not of a runtime type that can be stored in * an array of class &lt;tt&gt;newType&lt;/tt&gt; * @since 1.6 */public static &lt;T,U&gt; T[] copyOf(U[] original, int newLength, Class&lt;? extends T[]&gt; newType) &#123; @SuppressWarnings(&quot;unchecked&quot;) T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy;&#125; original 要复制的数组 newLength 新数组的长度 newType 新数组的类型 关于对象拷贝有两种方式：浅拷贝(Shallow Copy)和深拷贝(Deep Copy)。顾名思义，浅拷贝，并不拷贝对象本身，仅仅是拷贝指向对象的指针；深拷贝是直接拷贝整个对象内存到另一块内存中。 简单些说：浅拷贝就是指针拷贝；深拷贝就是内容拷贝。 ArrayList总体来说比较简单，不过ArrayList还有以下一些特点： ArrayList自己实现了序列化和反序列化的方法，因为它自己实现了 private void writeObject(java.io.ObjectOutputStream s)和 private void readObject(java.io.ObjectInputStream s) 方法 ArrayList基于数组方式实现，无容量的限制（会扩容） 添加元素时可能要扩容（所以最好预判一下），删除元素时不会减少容量（若希望减少容量，trimToSize()），删除元素时，将删除掉的位置元素置为null，下次gc就会回收这些元素所占的内存空间。 线程不安全 add(int index, E element)：添加元素到数组中指定位置的时候，需要将该位置及其后边所有的元素都整块向后复制一位 get(int index)：获取指定位置上的元素时，可以通过索引直接获取（O(1)） remove(Object o)需要遍历数组 remove(int index)不需要遍历数组，只需判断index是否符合条件即可，效率比remove(Object o)高 contains(E)需要遍历数组 使用iterator遍历可能会引发多线程异常 1、ArrayList的大小是如何自动增加的？ 这是最有技巧性的的一个问题，大多数人都无法回答。事实上，当有人试图在arraylist中增加一个对象的时候，Java会去检查arraylist，以确保已存在的数组中有足够的容量来存储这个新的对象。如果没有足够容量的话，那么就会新建一个长度更长的数组，旧的数组就会使用Arrays.copyOf方法被复制到新的数组中去，现有的数组引用指向了新的数组。看如下的代码段（摘自GrepCode.com中的Java ArrayList Code）： http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/6-b14/java/util/ArrayList.java //ArrayList Add方法：public boolean add(E e){ ensureCapacity(size+1); //Increment modCount!! elementData[size++] = e; return true;} //ensureCapacity方法：处理ArrayList的大小public void ensureCapacity(int minCapacity) { modCount++; int oldCapacity = elementData.length; if (minCapacity &gt; oldCapacity) { Object oldData[] = elementData; int newCapacity = (oldCapacity * 3)/2 + 1; if (newCapacity &lt; minCapacity) newCapacity = minCapacity; // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); }} 请注意这样一个情况：新建了一个数组；新数组长度是旧数组的1.5倍，旧数组的对象被复制到了新的数组中，并且现有的数组指向新的数组。 2、什么情况下会使用ArrayList？什么时候会选择LinkedList？ 多数情况下，当你遇到访问元素比插入或者是删除元素更加频繁的时候，你应该使用ArrayList。 另外一方面，当你在某个特别的索引中，插入或者是删除元素更加频繁，或者你压根就不需要访问元素的时候，会选择LinkedList。 在ArrayList中访问元素的最糟糕的时间复杂度是”1″，而在LinkedList中可能就是”n”了。 在ArrayList中增加或者删除某个元素，通常会调用System.arraycopy方法，这是一种极为消耗资源的操作， 因此，在频繁的插入或者是删除元素的情况下，LinkedList的性能会更加好一点。 3、当传递ArrayList到某个方法中，或者某个方法返回ArrayList，什么时候要考虑安全隐患？如何修复安全违规这个问题呢？ 当array被当做参数传递到某个方法中，如果array在没有被复制的情况下直接被分配给了成员变量，那么就可能发生这种情况，即当原始的数组被调用的方法改变的时候，传递到这个方法中的数组也会改变。下面的这段代码展示的就是安全违规以及如何修复这个问题。 ArrayList被直接赋给成员变量——安全隐患： 修复这个安全隐患： 4、如何复制某个ArrayList到另一个ArrayList中去？ 下面就是把某个ArrayList复制到另一个ArrayList中去的几种技术： 使用clone()方法，比如ArrayList newArray = oldArray.clone(); 使用ArrayList构造方法，比如：ArrayList myObject = new ArrayList(myTempObject); 使用Collection的copy方法。 注意1和2是浅拷贝(shallow copy)。 5、在索引中ArrayList的增加或者删除某个对象的运行过程？效率很低吗？解释一下为什么？ 在ArrayList中增加或者是删除元素，要调用System.arraycopy这种效率很低的操作， 如果遇到了需要频繁插入或者是删除的时候，可以选择其他的Java集合，比如LinkedList。 References[1] ArrayList源码分析（基于JDK8）[2] 浅拷贝(Shallow Copy)与深拷贝(Deep Copy)[3] Java中shallow clone 与deep Clone的区别]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 官方 etl neo4j-import 导入 数据]]></title>
    <url>%2F2017%2F04%2F11%2Fneo4j-import%2F</url>
    <content type="text"><![CDATA[1千万数据(数字 英文，没有中文)导入花费 27s 932ms 1千万数据(包含中文属性)导入花费 1m 50s 9ms 1.1亿数据(1.1亿节点 1.1亿关系 2.2亿属性) 15m 9s 37ms 温馨提示： neo4j-import和neo4j-admin import虽然功能一样，但是参数还有很多区别，neo4j-import的参数更多一些，感觉用起来更方便。 不过官方会在以后的版本里用neo4j-admin import替掉nep4j-import 2017-04-11 导入测试 (1) linux导入1000w数据 node.csv文件 数据样例12345uuid:ID(Person),name:String,:Label2a3e275d9abc4c45913d8e7e619db87a,&quot;张忆耕&quot;,Label1db6ee76baff64db5956b6a5deb80acbf,&quot;傅某评&quot;,Label28d2f4a74e7e7429390b3389d64d77637,&quot;王苏维&quot;,Label34d0a5c3fa89a49e89f81a152f2aa259c,&quot;蓝波&quot;,Label4 relationship.csv文件 数据样例123:START_ID(Person),:END_ID(Person),:TYPE2a3e275d9abc4c45913d8e7e619db87a,db6ee76baff64db5956b6a5deb80acbf,Relationship18d2f4a74e7e7429390b3389d64d77637,4d0a5c3fa89a49e89f81a152f2aa259c,Relationship2 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566[wkq@wkq bin]$ ./neo4j-import --into /home/wkq/neo4j/neo4j-community-3.1.0/data/databases/test_10000000_graph.db --nodes /home/wkq/neo4j/node.csv --relationships /home/wkq/neo4j/relathionship.csv --trim-strings true --input-encoding UTF-8 --id-type INTEGER --stacktrace true --bad-tolerance 0 --skip-bad-relationships true --skip-duplicate-nodes falseWARNING: neo4j-import is deprecated and support for it will be removed in a futureversion of Neo4j; please use neo4j-admin import instead.Neo4j version: 3.1.0Importing the contents of these files into /home/wkq/neo4j/neo4j-community-3.1.0/data/databases/test_10000000_graph.db:Nodes: /home/wkq/neo4j/node.csvRelationships: /home/wkq/neo4j/relathionship.csvAvailable resources: Free machine memory: 28.05 GB Max heap memory : 26.67 GB Processors: 32Nodes[&gt;:23.85 MB/s-|PROPERTIES-----|NODE:68.66 MB---|LABEL SCAN-----|*v:41.50 MB/s-----------------]8.38MDone in 10s 106msPrepare node index[*SPLIT:209.81 MB-----------------------------------------------------------------------------]9.67MDone in 2s 162msCalculate dense nodes[&gt;:115.63 M|TYPE-----------|*PREPARE(32)========================|CALCULATE(2)=================] 5.5MDone in 2s 904msRelationships [:Friend] (1/1)[&gt;:47.30 MB|*PREPARE(10)==================|RECORDS-|P|RELATIONSHIP----------|v:61.33 MB/s-----]5.63MDone in 4s 591msNode --&gt; Relationship [:Friend] (1/1)Done in 10msRelationship --&gt; Relationship [:Friend] (1/1)[&gt;:??----------------------------------|*LINK------------------------------------------------|]3.82MDone in 1s 605msNode --&gt; Relationship Sparse[&gt;------------------------------------|LINK----|*v:140.00 MB/s--------------------------------]9.78MDone in 1s 65msRelationship --&gt; Relationship SparseDone in 1s 503msCount groupsDone in 10msGather[*&gt;:??----------------------------------------------------------------------------------------] 0Done in 1msWriteDone in 11msNode --&gt; GroupDone in 10msNode countsDone in 938msRelationship countsDone in 779msIMPORT DONE in 27s 932ms.Imported: 10000000 nodes 10000000 relationships 20000000 propertiesPeak memory usage: 209.81 MB[wkq@wkq bin]$ (2) windows导入3000条左右节点(仅节点)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\bin&gt;neo4j-import --into test.db --id-type string --nodes:Test C:/User/wdb/2017-04-06_test.csv --stacktrace truektrace true警告: This command does not appear to be running with administrative rights. Some commands may fail e.g. Start/StopWARNING: neo4j-import is deprecated and support for it will be removed in a futureversion of Neo4j; please use neo4j-admin import instead.Neo4j version: 3.1.2Importing the contents of these files into test.db:Nodes: :Test C:\User\wdb\2017-04-06_test.csvAvailable resources: Free machine memory: 3.09 GB Max heap memory : 1.77 GB Processors: 4NodesDone in 441msPrepare node indexDone in 30msCalculate dense nodesDone in 11msNode --&gt; Relationship SparseDone in 11msRelationship --&gt; Relationship SparseDone in 1msCount groups[*&gt;:??----------------------------------------------------------------------------------------] 0Done in 15msGatherDone in 2msWriteDone inNode --&gt; GroupDone inNode countsDone in 96msRelationship countsDone in 3msIMPORT DONE in 2s 234ms.Imported: 3466 nodes 0 relationships 17897 propertiesPeak memory usage: 33.85 kBC:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\bin&gt; (3) windows导入10w数据 (10万节点 10万关系) node.csv文件 数据样例12345uuid:ID(Person),name:String,:Label2a3e275d9abc4c45913d8e7e619db87a,&quot;张忆耕&quot;,Label1db6ee76baff64db5956b6a5deb80acbf,&quot;傅某评&quot;,Label28d2f4a74e7e7429390b3389d64d77637,&quot;王苏维&quot;,Label34d0a5c3fa89a49e89f81a152f2aa259c,&quot;蓝波&quot;,Label4 relationship.csv文件 数据样例123:START_ID(Person),:END_ID(Person),:TYPE2a3e275d9abc4c45913d8e7e619db87a,db6ee76baff64db5956b6a5deb80acbf,Relationship18d2f4a74e7e7429390b3389d64d77637,4d0a5c3fa89a49e89f81a152f2aa259c,Relationship2 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135D:\ProfessionalSoftWare\Neo4j\neo4j-community-3.4.1\binλ neo4j-import --into D:/ProfessionalSoftWare/Neo4j/neo4j-community-3.4.1/data/databases/graph2.db --nodes D:/home/bonc/neo4j/node_uuid_10w.csv --relationships D:/home /bonc/neo4j/relathionship_uuid_10w.csv --trim-strings true --input-encoding UTF-8 --id-type String --stacktrace true --bad-tolerance 10000 --skip-bad-relationships true --skip-duplicate-nodes true警告: This command does not appear to be running with administrative rights. Some commands may fail e.g. Start/StopWARNING: neo4j-import is deprecated and support for it will be removed in a futureversion of Neo4j; please use neo4j-admin import instead.Neo4j version: 3.4.1Importing the contents of these files into D:\ProfessionalSoftWare\Neo4j\neo4j-community-3.4.1\data\databases\graph2.db:Nodes: D:\home\bonc\neo4j\node_uuid_10w.csvRelationships: D:\home\bonc\neo4j\relathionship_uuid_10w.csvAvailable resources: Total machine memory: 7.95 GB Free machine memory: 3.42 GB Max heap memory : 1.77 GB Processors: 4 Configured max memory: 5.56 GB High-IO: falseImport starting 2018-07-10 13:41:28.551+0800 Estimated number of nodes: 112.07 k Estimated number of node properties: 224.14 k Estimated number of relationships: 100.00 k Estimated number of relationship properties: 0.00 Estimated disk space usage: 13.72 MB Estimated required memory usage: 1021.42 MBInteractiveReporterInteractions command list (end with ENTER): c: Print more detailed information about current stage i: Print more detailed information(1/4) Node import 2018-07-10 13:41:28.669+0800 Estimated number of nodes: 112.07 k Estimated disk space usage: 10.47 MB Estimated required memory usage: 1021.42 MB.......... .......... .......... .......... .......... 5%.......... .......... .......... .......... .......... 10%.......... .......... .......... .......... .......... 15%.......... .......... .......... .......... .......... 20%.......... .......... .......... .......... .......... 25%.......... .......... .......... .......... .......... 30%.......... .......... .......... .......... .......... 35%.......... .......... .......... .......... .......... 40%.......... .......... .......... .......... .......... 45%.......... .......... .......... .......... .......... 50%.......... .......... .......... .......... .......... 55%.......... .......... .......... .......... .......... 60%.......... .......... .......... .......... .......... 65%.......... .......... .......... .......... .......... 70%.......... .......... .......... .......... .......... 75%.......... .......... .......... .......... .......... 80%.......... .......... .......... .......... .......... 85%.......... .......... .......... .......... .......... 90%.......... .......... .......... .......... .......... 95%.......... .......... .......... .......... .......... 100%(2/4) Relationship import 2018-07-10 13:41:30.376+0800 Estimated number of relationships: 100.00 k Estimated disk space usage: 3.24 MB Estimated required memory usage: 1.00 GB.......... .......... .......... .......... .......... 5%.......... .......... .......... .......... .......... 10%.......... .......... .......... .......... .......... 15%.......... .......... .......... .......... .......... 20%.......... .......... .......... .......... .......... 25%.......... .......... .......... .......... .......... 30%.......... .......... .......... .......... .......... 35%.......... .......... .......... .......... .......... 40%.......... .......... .......... .......... .......... 45%.......... .......... .......... .......... .......... 50%.......... .......... .......... .......... .......... 55%.......... .......... .......... .......... .......... 60%.......... .......... .......... .......... .......... 65%.......... .......... .......... .......... .......... 70%.......... .......... .......... .......... .......... 75%.......... .......... .......... .......... .......... 80%.......... .......... .......... .......... .......... 85%.......... .......... .......... .......... .......... 90%.......... .......... .......... .......... .......... 95%.......... .......... .......... .......... .......... 100%(3/4) Relationship linking 2018-07-10 13:41:30.787+0800 Estimated required memory usage: 1021.06 MB.......... .......... .......... .......... .......... 5%.......... .......... .......... .......... .......... 10%.......... .......... .......... .......... .......... 15%.......... .......... .......... .......... .......... 20%.......... .......... .......... .......... .......... 25%.......... .......... .......... .......... .......... 30%.......... .......... .......... .......... .......... 35%.......... .......... .......... .......... .......... 40%.......... .......... .......... .......... .......... 45%.......... .......... .......... .......... .......... 50%.......... .......... .......... .......... .......... 55%.......... .......... .......... .......... .......... 60%.......... .......... .......... .......... .......... 65%.......... .......... .......... .......... .......... 70%.......... .......... .......... .......... .......... 75%.......... .......... .......... .......... .......... 80%.......... .......... .......... .......... .......... 85%.......... .......... .......... .......... .......... 90%.......... .......... .......... .......... .......... 95%.......... .......... .......... .......... .......... 100%(4/4) Post processing 2018-07-10 13:41:31.445+0800 Estimated required memory usage: 1020.01 MB.......... .......... .......... .......... .......... 5%.......... .......... .......... .......... .......... 10%.......... .......... .......... .......... .......... 15%.......... .......... .......... .......... .......... 20%.......... .......... .......... .......... .......... 25%.......... .......... .......... .......... .......... 30%.......... .......... .......... .......... .......... 35%.......... .......... .......... .......... .......... 40%.......... .......... .......... .......... .......... 45%.......... .......... .......... .......... .......... 50%.......... .......... .......... .......... .......... 55%.......... .......... .......... .......... .......... 60%.......... .......... .......... .......... .......... 65%.......... .......... .......... .......... .......... 70%.......... .......... .......... .......... .......... 75%.......... .......... .......... .......... .......... 80%.......... .......... .......... .......... .......... 85%.......... .......... .......... .......... .......... 90%.......... .......... .......... .......... .......... 95%.......... .......... .......... .......... .......... 100%IMPORT DONE in 4s 585ms.Imported: 100000 nodes 100000 relationships 200000 propertiesPeak memory usage: 1.00 GB 其他测试如果使用相对路径，导入后数据库在bin目录下neo4j-import --into test.db --id-type string --nodes:Test C:/User/wdb/2017-04-06_test.csv --stacktrace truektrace true 如果使用绝对路径，导入后数据库在指定目录下neo4j-import --into C:/ProfessionSofware/Neo4j/databases/test.db --id-type string --nodes:Test C:/User/wdb/2017-04-06_test.csv --stacktrace truektrace true 2018-04-26 试验winbdows 小数据测试neo4j-import --into C:/ProfessionSofware/Neo4j/neo4j-community-3.1.0/data/databases/test_graph.db --nodes C:/tmp/neo4j/node.csv --relationships C:/tmp/neo4j/relathionship.csv --trim-strings true --input-encoding UTF-8 --id-type INTEGER --stacktrace true --bad-tolerance 0 --skip-bad-relationships true --skip-duplicate-nodes false linux 实际导入1000w数据neo4j-import --into /home/wkq/neo4j/neo4j-community-3.1.0/data/databases/test_10000000_graph.db --nodes /home/wkq/neo4j/node.csv --relationships /home/wkq/neo4j/relathionship.csv --trim-strings true --input-encoding UTF-8 --id-type String --stacktrace true --bad-tolerance 0 --skip-bad-relationships true --skip-duplicate-nodes false 遇到的错误(1) &#39;--nodes&#39; to have at least 1 valid item, but had 0 []1234567891011121314C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\bin&gt;neo4j-import -into test.db --id-type string --nodes [:Customer] customers.csv警告: This command does not appear to be running with administrative rights. Some commands may fail e.g. Start/StopWARNING: neo4j-import is deprecated and support for it will be removed in a futureversion of Neo4j; please use neo4j-admin import instead.Input error: Expected &apos;--nodes&apos; to have at least 1 valid item, but had 0 []Caused by:Expected &apos;--nodes&apos; to have at least 1 valid item, but had 0 []java.lang.IllegalArgumentException: Expected &apos;--nodes&apos; to have at least 1 valid item, but had 0 [] at org.neo4j.kernel.impl.util.Validators.lambda$atLeast$6(Validators.java:125) at org.neo4j.helpers.Args.validated(Args.java:640) at org.neo4j.helpers.Args.interpretOptionsWithMetadata(Args.java:608) at org.neo4j.tooling.ImportTool.extractInputFiles(ImportTool.java:503) at org.neo4j.tooling.ImportTool.main(ImportTool.java:388) at org.neo4j.tooling.ImportTool.main(ImportTool.java:334) 文件头有问题，检查nodes.csv的文件头是否有:ID，relathionship.csv的文件头是否有 :START_ID,:END_ID,:TYPE --nodes 后面的参数有问题，--nodes 直接跟 csv路径 或者 :Label csv绝对路径 仔细检查路径，看路径是否写错，linux和windows路径统一用/作为目录 (2) Error in input data ERROR in input123456789101112131415161718192021222324C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\bin&gt;neo4j-import -into test.db --id-type string --nodes:Test C:/User/wdb/2017-04-06_test.csv --stacktrace truektrace true --skip-duplicate-nodes true警告: This command does not appear to be running with administrative rights. Some commands may fail e.g. Start/StopWARNING: neo4j-import is deprecated and support for it will be removed in a futureversion of Neo4j; please use neo4j-admin import instead.Neo4j version: 3.1.2Importing the contents of these files into test.db:Nodes: :Test C:\User\wdb\2017-04-06_test.csvAvailable resources: Free machine memory: 3.74 GB Max heap memory : 1.77 GB Processors: 4NodesError in input dataCaused by:ERROR in input data source: BufferedCharSeeker[source:C:\User\wdb\2017-04-06_test.csv, position:654, line:11] in field: mobile:int:6 for header: [dep:string, uid:int, name:string, tel:int, fax:string, mobile:int, email:string, id:string] raw field value: 13900001234 original error: Not supported a.t.m csv文件的数据有问题，最后把tel的类型换成string解决了 neo4j-import命令中的参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// 要导入的数据库存放位置，必须是空库--into &lt;store-dir&gt;// 要导入的数据库--database &lt;database-name&gt;// 节点文件，包含头和数据，第一个文件第一行必须是头，多个文件在逻辑上视为一个大文件，文件组必须用引号括起来--nodes[:Label1:Label2] &quot;&lt;file1&gt;,&lt;file2&gt;,...&quot;// 关系文件，包含头和数据，多个文件在逻辑上视为一个大文件，文件组必须用引号括起来--relationships[:RELATIONSHIP_TYPE] &quot;&lt;file1&gt;,&lt;file2&gt;,...&quot;// 数据分隔符，默认是逗号--delimiter &lt;delimiter-character&gt;// 数组分隔符，默认是分号--array-delimiter &lt;array-delimiter-character&gt;// 引用字符 注意转义--quote &lt;quotation-character&gt;// 输入源中的字段是否可以跨越多行，即包含换行符。默认值：false--multiline-fields &lt;true/false&gt;// 是否应该修剪空白字符串。默认值：false--trim-strings &lt;true/false&gt;// csv文件编码，推荐使用UTF-8--input-encoding &lt;character set&gt;// 忽略空字符串，默认是false--ignore-empty-strings &lt;true/false&gt;// id类型，integer string actual, integer会比较快，string比较通用--id-type &lt;id-type&gt;// 最大处理器数量，为了达到最佳性能，这个值不应该大于可用处理器的数量。--processors &lt;max processor count&gt;// 启用错误堆栈跟踪的打印。默认值：false--stacktrace &lt;true/false&gt;// 导入前的错误条目数被视为失败。这种宽容阈值是关于引用缺失节点的关系。格式化错误 输入数据仍被视为错误。默认值：1000--bad-tolerance &lt;max number of bad entries&gt;// 是否跳过导入缺少节点ID的关系，即引用未指定节点的开始或结束节点ID /组 由节点输入数据。跳过的节点将被记录，最多包含数字 由不良容忍指定的实体。默认值：true--skip-bad-relationships &lt;true/false&gt;// 是否跳过导入具有相同ID /组的节点。在事件中在同一组内的多个节点具有相同的ID，第一个遇到的将被导入，而连续的这样的节点将被跳过。跳过的节点将被记录，最多包含由指定的实体数量坏容忍。默认值：false--skip-duplicate-nodes &lt;true/false&gt;// 是否忽略未由标题指定的数据中的额外列。默认值：false--ignore-extra-columns &lt;true/false&gt;// 指定数据库特定配置的文件。--db-config// 指定数据库特定配置的文件--additional-config// 页面大小（以字节为单位）--page-sizeExample: bin/neo4j-import --into retail.db --id-type string --nodes:Customer customers.csv --nodes products.csv --nodes orders_header.csv,orders1.csv,orders2.csv --relationships:CONTAINS order_details.csv --relationships:ORDERED customer_orders_header.csv,orders1.csv,orders2.csv neo4j-3.1.2 neo4j-import help123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109C:\ProfessionSofware\Neo4j\neo4j-community-3.1.2\binλ neo4j-importWARNING: neo4j-import is deprecated and support for it will be removed in a futureversion of Neo4j; please use neo4j-admin import instead.Neo4j Import Tool neo4j-import is used to create a new Neo4j database from data in CSV files. See the chapter &quot;Import Tool&quot; in the Neo4j Manual for details on the CSV file format - a special kind of header is required.Usage:--into &lt;store-dir&gt; Database directory to import into. Must not contain existing database.--database &lt;database-name&gt; Database name to import into. Must not contain existing database.--nodes[:Label1:Label2] &quot;&lt;file1&gt;,&lt;file2&gt;,...&quot; Node CSV header and data. Multiple files will be logically seen as one big file from the perspective of the importer. The first line must contain the header. Multiple data sources like these can be specified in one import, where each data source has its own header. Note that file groups must be enclosed in quotation marks.--relationships[:RELATIONSHIP_TYPE] &quot;&lt;file1&gt;,&lt;file2&gt;,...&quot; Relationship CSV header and data. Multiple files will be logically seen as one big file from the perspective of the importer. The first line must contain the header. Multiple data sources like these can be specified in one import, where each data source has its own header. Note that file groups must be enclosed in quotation marks.--delimiter &lt;delimiter-character&gt; Delimiter character, or &apos;TAB&apos;, between values in CSV data. The default option is ,.--array-delimiter &lt;array-delimiter-character&gt; Delimiter character, or &apos;TAB&apos;, between array elements within a value in CSV data. The default option is ;.--quote &lt;quotation-character&gt; Character to treat as quotation character for values in CSV data. The default option is &quot;. Quotes inside quotes escaped like &quot;&quot;&quot;Go away&quot;&quot;, he said.&quot; and &quot;\&quot;Go away\&quot;, he said.&quot; are supported. If you have set &quot;&apos;&quot; to be used as the quotation character, you could write the previous example like this instead: &apos;&quot;Go away&quot;, he said.&apos;--multiline-fields &lt;true/false&gt; Whether or not fields from input source can span multiple lines, i.e. contain newline characters. Default value: false--trim-strings &lt;true/false&gt; Whether or not strings should be trimmed for whitespaces. Default value: false--input-encoding &lt;character set&gt; Character set that input data is encoded in. Provided value must be one out of the available character sets in the JVM, as provided by Charset#availableCharsets(). If no input encoding is provided, the default character set of the JVM will be used.--ignore-empty-strings &lt;true/false&gt; Whether or not empty string fields, i.e. &quot;&quot; from input source are ignored, i.e. treated as null. Default value: false--id-type &lt;id-type&gt; One out of [STRING, INTEGER, ACTUAL] and specifies how ids in node/relationship input files are treated. STRING: arbitrary strings for identifying nodes. INTEGER: arbitrary integer values for identifying nodes. ACTUAL: (advanced) actual node ids. The default option is STRING. Default value: STRING--processors &lt;max processor count&gt; (advanced) Max number of processors used by the importer. Defaults to the number of available processors reported by the JVM (in your case 4). There is a certain amount of minimum threads needed so for that reason there is no lower bound for this value. For optimal performance this value shouldn&apos;t be greater than the number of available processors.--stacktrace &lt;true/false&gt; Enable printing of error stack traces. Default value: false--bad-tolerance &lt;max number of bad entries&gt; Number of bad entries before the import is considered failed. This tolerance threshold is about relationships refering to missing nodes. Format errors in input data are still treated as errors. Default value: 1000--skip-bad-relationships &lt;true/false&gt; Whether or not to skip importing relationships that refers to missing node ids, i.e. either start or end node id/group referring to node that wasn&apos;t specified by the node input data. Skipped nodes will be logged, containing at most number of entites specified by bad-tolerance. Default value: true--skip-duplicate-nodes &lt;true/false&gt; Whether or not to skip importing nodes that have the same id/group. In the event of multiple nodes within the same group having the same id, the first encountered will be imported whereas consecutive such nodes will be skipped. Skipped nodes will be logged, containing at most number of entities specified by bad-tolerance. Default value: false--ignore-extra-columns &lt;true/false&gt; Whether or not to ignore extra columns in the data not specified by the header. Skipped columns will be logged, containing at most number of entities specified by bad-tolerance. Default value: false--db-config &lt;path/to/neo4j.conf&gt; (advanced) File specifying database-specific configuration. For more information consult manual about available configuration options for a neo4j configuration file. Only configuration affecting store at time of creation will be read. Examples of supported config are: dbms.relationship_grouping_threshold unsupported.dbms.block_size.strings unsupported.dbms.block_size.array_properties--additional-config &lt;path/to/neo4j.conf&gt; (advanced) File specifying database-specific configuration. For more information consult manual about available configuration options for a neo4j configuration file. Only configuration affecting store at time of creation will be read. Examples of supported config are: dbms.relationship_grouping_threshold unsupported.dbms.block_size.strings unsupported.dbms.block_size.array_properties--legacy-style-quoting &lt;true/false&gt; Whether or not backslash-escaped quote e.g. \&quot; is interpreted as inner quote. Default value: trueExample: bin/neo4j-import --into retail.db --id-type string --nodes:Customer customers.csv --nodes products.csv --nodes orders_header.csv,orders1.csv,orders2.csv --relationships:CONTAINS order_details.csv --relationships:ORDERED customer_orders_header.csv,orders1.csv,orders2.csv 官方文档 10.1. ImportThis chapter covers importing data into Neo4j. The import tool is used to create a new Neo4j database from data in CSV files. This chapter explains how to use the tool and format the input data. For in-depth examples of using the import tool, see Section B.4, “Use the Import tool”. These are some things you will need to keep in mind when creating your input files: Fields are comma separated by default but a different delimiter can be specified. All files must use the same delimiter. Multiple data sources can be used for both nodes and relationships. A data source can optionally be provided using multiple files. A header which provides information on the data fields must be on the first row of each data source. Fields without corresponding information in the header will not be read. UTF-8 encoding is used. Indexes are not created during the import. Instead, you will need to add indexes afterwards (see Developer Manual → Indexes). Data cannot be imported into an existing database using this tool. If you want to load small to medium sized CSV files use LOAD CSV (see Developer Manual → LOAD CSV). 10.1.1. CSV file header format This section explains the header format of CSV files when using the Neo4j import tool. The header row of each data source specifies how the fields should be interpreted. The same delimiter is used for the header row as for the rest of the data. The header contains information for each field, with the format: :&lt;field_type&gt;. The is used as the property key for values, and ignored in other cases. The following &lt;field_type&gt; settings can be used for both nodes and relationships: Property value Use one of int, long, float, double, boolean, byte, short, char, string to designate the data type. If no data type is given, this defaults to string. To define an array type, append [] to the type. By default, array values are separated by ;. A different delimiter can be specified with –array-delimiter.IGNORE Ignore this field completely. See below for the specifics of node and relationship data source headers. 10.1.1.1. Nodes The following field types do additionally apply to node data sources: ID Each node must have a unique id which is used during the import. The ids are used to find the correct nodes when creating relationships. Note that the id has to be unique across all nodes in the import, even nodes with different labels.LABEL Read one or more labels from this field. Like array values, multiple labels are separated by ;, or by the character specified with –array-delimiter. 10.1.1.2. Relationships For relationship data sources, there are three mandatory fields: TYPE The relationship type to use for the relationship.START_ID The id of the start node of the relationship to create.END_ID The id of the end node of the relationship to create. 10.1.1.3. ID spaces The import tool assumes that node identifiers are unique across node files. If this is not the case then we can define an id space. Id spaces are defined in the ID field of node files. For example, to specify the Person id space we would use the field type ID(Person) in our persons node file. We also need to reference that id space in our relationships file i.e. START_ID(Person) or END_ID(Person). 10.1.2. Command line usage This section covers how to use the Neo4j import tool from the command line. 10.1.2.1. Linux Under Unix/Linux/OSX, the command is named neo4j-import. Depending on the installation type, the tool is either available globally, or used by executing ./bin/neo4j-import from inside the installation directory.10.1.2.2. Windows Under Windows, used by executing bin\neo4j-import from inside the installation directory. For help with running the import tool under Windows, see the reference in Windows.10.1.2.3. Options –into Database directory to import into. Must not contain existing database. –nodes[:Label1:Label2] “,,…​” Node CSV header and data. Multiple files will be logically seen as one big file from the perspective of the importer. The first line must contain the header. Multiple data sources like these can be specified in one import, where each data source has its own header. Note that file groups must be enclosed in quotation marks. –relationships[:RELATIONSHIP_TYPE] “,,…​” Relationship CSV header and data. Multiple files will be logically seen as one big file from the perspective of the importer. The first line must contain the header. Multiple data sources like these can be specified in one import, where each data source has its own header. Note that file groups must be enclosed in quotation marks. –delimiter Delimiter character, or TAB, between values in CSV data. The default option is ,. –array-delimiter Delimiter character, or TAB, between array elements within a value in CSV data. The default option is ;. –quote Character to treat as quotation character for values in CSV data. The default option is “. Quotes inside quotes escaped like “””Go away””, he said.” and “\”Go away\”, he said.” are supported. If you have set ‘ to be used as the quotation character, you could write the previous example like this instead: ‘“Go away”, he said.’ –multiline-fields &lt;true/false&gt; Whether or not fields from input source can span multiple lines, i.e. contain newline characters. Default value: false –input-encoding Character set that input data is encoded in. Provided value must be one out of the available character sets in the JVM, as provided by Charset#availableCharsets(). If no input encoding is provided, the default character set of the JVM will be used. –ignore-empty-strings &lt;true/false&gt; Whether or not empty string fields (“”) from input source are ignored, i.e. treated as null. Default value: false –id-type One out of [STRING, INTEGER, ACTUAL] and specifies how ids in node/relationship input files are treated. STRING: arbitrary strings for identifying nodes. INTEGER: arbitrary integer values for identifying nodes. ACTUAL: (advanced) actual node ids. Default value: STRING –processors (advanced) Max number of processors used by the importer. Defaults to the number of available processors reported by the JVM. There is a certain amount of minimum threads needed so for that reason there is no lower bound for this value. For optimal performance this value shouldn’t be greater than the number of available processors. –stacktrace &lt;true/false&gt; Enable printing of error stack traces. –bad-tolerance Number of bad entries before the import is considered failed. This tolerance threshold is about relationships referring to missing nodes. Format errors in input data are still treated as errors. Default value: 1000 –skip-bad-relationships &lt;true/false&gt; Whether or not to skip importing relationships that refers to missing node ids, i.e. either start or end node id/group referring to node that wasn’t specified by the node input data. Skipped nodes will be logged, containing at most number of entities specified by bad-tolerance. Default value: true –skip-duplicate-nodes &lt;true/false&gt; Whether or not to skip importing nodes that have the same id/group. In the event of multiple nodes within the same group having the same id, the first encountered will be imported whereas consecutive such nodes will be skipped. Skipped nodes will be logged, containing at most number of entities specified by bad-tolerance. Default value: false –ignore-extra-columns &lt;true/false&gt; Whether or not to ignore extra columns in the data not specified by the header. Skipped columns will be logged, containing at most number of entities specified by bad-tolerance. Default value: false –db-config &lt;path/to/neo4j.conf&gt; (advanced) File specifying database-specific configuration. For more information consult manual about available configuration options for a neo4j configuration file. Only configuration affecting store at time of creation will be read. Examples of supported config are: dbms.relationship_grouping_threshold unsupported.dbms.block_size.strings unsupported.dbms.block_size.array_properties 10.1.2.4. Verbose error information In some cases if an unexpected error occurs it might be useful to supply the command line option –stacktrace to the import (and rerun the import to actually see the additional information). This will have the error printed with additional debug information, useful for both developers and issue reporting. 10.1.2.5. Output and statistics While an import is running through its different stages, some statistics and figures are printed in the console. The general interpretation of that output is to look at the horizontal line, which is divided up into sections, each section representing one type of work going on in parallel with the other sections. The wider a section is, the more time is spent there relative to the other sections, the widest being the bottleneck, also marked with *. If a section has a double line, instead of just a single line, it means that multiple threads are executing the work in that section. To the far right a number is displayed telling how many entities (nodes or relationships) have been processed by that stage. As an example: [*&gt;:20,25 MB/s——————|PREPARE(3)====================|RELATIONSHIP(2)===============] 16M Would be interpreted as: &gt; data being read, and perhaps parsed, at 20,25 MB/s, data that is being passed on to …​ PREPARE preparing the data for …​ RELATIONSHIP creating actual relationship records and …​ v writing the relationships to the store. This step is not visible in this example, because it is so cheap compared to the other sections. Observing the section sizes can give hints about where performance can be improved. In the example above, the bottleneck is the data read section (marked with &gt;), which might indicate that the disk is being slow, or is poorly handling simultaneous read and write operations (since the last section often revolves around writing to disk). References[1] https://neo4j.com/developer/guide-import-csv/[2] http://neo4j.com/docs/operations-manual/current/tools/import/[3] http://neo4j.com/docs/operations-manual/current/tools/import/command-line-usage/[4] http://neo4j.com/docs/operations-manual/current/tutorial/import-tool/]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[neo4j 使用 官方 ETL load-csv 导入 数据]]></title>
    <url>%2F2017%2F04%2F11%2Fneo4j-load-csv%2F</url>
    <content type="text"><![CDATA[Neo4j load csv 使用 牛刀小试 下面的例子是 把 http://data.neo4j.com/examples/person.csv 的csv文件导入到neo4j数据库里，指定的csv文件分隔符是,， 只导入第一个字段不为null的数据，根据id为唯一主键导入，导入时设置name为第二个字段，每10000条提交一次。 1234567USING PERIODIC COMMIT 10000LOAD CSV FROM &quot;http://data.neo4j.com/examples/person.csv&quot; AS linefieldterminator &apos;,&apos;WHERE line[0] IS NOT NULLMERGE (n:Person &#123;id: toInt(line[0])&#125;)SET n.name = line[1]RETURN n 1、 LOAD CSV FROM &quot;http://data.neo4j.com/examples/person.csv&quot; 是读取文件，文件路径可以是url、可以是本地相对路径、本地绝对路径2、 fieldterminator 是设置分隔符3、 MERGE 是插入节点 (不存在则插入，存在则更新) 导入步骤 1、处理数据，确保数据是UTF8无BOM编码，确保数据不存在字符转义问题，确保不存在脏数据2、检查数据一共有多少行。 如果行数和实际的有差别，检查数据或者cypher语句3、抽样检查数据。 返回前10条。看格式是否正确，字段是否对应。是否有乱码。4、导入数据。 如果特别熟悉了，可以直接进行第4步导入数据，如果不太确定，建议还是1-4步都执行一遍。 (1) 导入前注意事项1、 csv一定要用 UTF-8无BOM编码，否则入库时可能乱码2、 默认使用,作为分隔符，如果想自定义，可以通过 fieldterminator 设置3、 导入数据前，一定要校验数据，都是坑呀，总结出的经验4、 建议使用绝对路径，使用绝对路径时需要把 neo4j.conf配置文件里的dbms.directories.import=import注释掉(在前面加个#号) (2) 检查csv一共多少行 注意文件路径 (2.1) windows下相对路径方式 Test.csv 放在 ${neo4j_home}/import/Test.csv 123LOAD CSV FROM &quot;file:/Test.csv&quot; AS line RETURN COUNT(*); (2.2) windows下绝对路径方式 文件放在 C:/User/wdb/2017-04-06_test.csv 123LOAD CSV FROM &quot;file:///C:/User/wdb/2017-04-06_test.csv&quot; AS line RETURN COUNT(*); (2.3) linux下相对路径格式 Test.csv 放在 ${neo4j_home}/import/Test.csv 123LOAD CSV FROM &quot;file:/2017-04-06_test.csv&quot; AS line RETURN COUNT(*); (2.4) linux下绝对路径格式123LOAD CSV FROM &quot;file:/home/wkq/databases/data/2017-04-06_test.csv&quot; AS line RETURN COUNT(*); (3) 校验数据 注意文件路径 文件使用UTF-8编码，否则可能导致乱码 (3.1) 检查数据 不带标题12345LOAD CSV FROM &quot;file:///C:/User/wdb/Test.csv&quot; AS line WITH line RETURN line LIMIT 5 (3.2) 检查数据 带标题12345LOAD CSV WITH HEADERS FROM &quot;file:///C:/User/wdb/Test.csv&quot; AS line WITH line RETURN line LIMIT 5 (4) 真正导入数据 (4.1) 导入数据 csv文件带文件头 注意WITH HEADERS的写法和不用HEADERS的写法，一个用的是 line.name 一个用的是 line[0] 导入时可以使用函数 toInt(‘1’) toFloat(‘1.0’) toInteger(), toFloat(), split() 1234LOAD CSV WITH HEADERS FROM &quot;file:///C:/User/wdb/Test.csv&quot; AS line WITH line CREATE (:Person &#123;name:line.name, number:toInt(line.number), tel:toInt(line.tel), sex:line.sex, class:line.classmate&#125;) ; (4.2) 导入数据 csv文件不带文件头1234LOAD CSV FROM &quot;file:///C:/User/wdb/Test.csv&quot; AS line WITH line CREATE (:Person &#123;name:line[0], number:toInt(line[1]), tel:toInt(line[2]), sex:line[3], class:line[4]&#125;) ; (5) 注事事项(5.1) 批量提交 数据量大了以后可以使用批量提交 using periodic commit 10000 表示 每10000行进行一次事务提交 123456USING PERIODIC COMMIT 500LOAD CSV WITH HEADERS FROM &apos;file:///data.csv&apos; AS rowMERGE (pet:Pet &#123;petId: row.PetId&#125;)MERGE (owner:Owner &#123;ownerId: row.OwnerId&#125;) ON CREATE SET owner.name = row.OwnerNameMERGE (pet)-[r:OWNED_BY]-&gt;(owner) (5.2) 处理空值 处理空值12345678910111213//skip null valuesLOAD CSV WITH HEADERS FROM &apos;file:///data.csv&apos; AS rowWITH row WHERE row.Company IS NOT NULLMERGE (c:Company &#123;companyId: row.Id&#125;)//set default for null valuesLOAD CSV WITH HEADERS FROM &apos;file:///data.csv&apos; AS rowMERGE (c:Company &#123;companyId: row.Id, hqLocation: coalesce(row.Location, &quot;Unknown&quot;)&#125;)//change empty strings to null values (not stored)LOAD CSV WITH HEADERS FROM &apos;file:///data.csv&apos; AS rowMERGE (c:Company &#123;companyId: row.Id&#125;)SET c.emailAddress = CASE trim(row.Email) WHEN &quot;&quot; THEN null ELSE row.Email END (5.3) 使用split()123456//split string of employee skills into separate nodesLOAD CSV FROM &apos;file:///data.csv&apos; AS rowMERGE (e:Employee &#123;employeeId: row.Id&#125;)UNWIND split(row.skills, &apos;,&apos;) AS skillMERGE (s:Skill &#123;name: skill&#125;)MERGE (e)-[r:HAS_EXPERIENCE]-&gt;(s); (5.4) 使用CASE1234567891011//set businessType property based on shortened value in CSVLOAD CSV WITH HEADERS FROM &apos;file:///data.csv&apos; AS rowWITH row,(CASE row.BusinessType WHEN &apos;P&apos; THEN &apos;Public&apos; WHEN &apos;R&apos; THEN &apos;Private&apos; WHEN &apos;G&apos; THEN &apos;Government&apos; ELSE &apos;Other&apos; END) AS typeMERGE (c:Company &#123;companyId: row.CompanyId&#125;)SET c.businessType = typeRETURN * (5.5) MERGE relation 为获得最佳性能，请始终在具有索引主键属性的单个标签上使用MATCH和MERGE。 您还应该将节点和关系创建分离为单独的语句。 以下两种写法都没问题，第一种相对第二种更占内存，第二种在内存有限的情况下性能会好一点 123MERGE (e:Employee &#123;employeeId: row.employeeId&#125;)MERGE (c:Company &#123;companyId: row.companyId&#125;)MERGE (e)-[r:WORKS_FOR]-&gt;(c) 官方推荐在具体的Label的属性上建立索引使用MATCH和MERGE，并且把创建节点和创建关系分开写 12345678910111213LOAD CSV WITH HEADERS FROM &apos;file:///data.csv&apos; AS rowMERGE (e:Employee &#123;employeeId: row.employeeId&#125;)RETURN count(e);LOAD CSV WITH HEADERS FROM &apos;file:///data.csv&apos; AS rowMERGE (c:Company &#123;companyId: row.companyId&#125;)RETURN count(c);LOAD CSV WITH HEADERS FROM &apos;file:///data.csv&apos; AS rowMATCH (e:Employee &#123;employeeId: row.employeeId&#125;)MATCH (c:Company &#123;companyId: row.companyId&#125;)MERGE (e)-[r:WORKS_FOR]-&gt;(c)RETURN count(*); 官方原文 To improve inserting or updating unique entities into your graph (using MERGE or MATCH with updates), you can create indexes and constraints declared for each of the labels and properties you plan to merge or match on. For best performance, always MATCH and MERGE on a single label with the indexed primary-key property.You should also separate node and relationship creation into separate statements. This way, the load is only doing one piece of the import at a time and can move through large amounts of data quickly and efficiently, reducing heavy processing. When the amount of data being loaded is too much to fit into memory, there are a couple of different approaches you can use to combat running out of memory during the data load. (6) 性能测试测试112345678neo4j-sh (?)$ using periodic commit 10000 load csv with headers from &quot;file:/data/stale/data01/neo4j/node_uuid_10w.csv&quot; as line with line create (:Test &#123;uuid:line.uuid, name:line.name&#125;);+-------------------+| No data returned. |+-------------------+Nodes created: 100000Properties set: 200000Labels added: 1000003412 ms 测试2/data/stale/data01/neo4j/node_uuid_1kw.csv文件加上标题一共10000010条，有10000009条数据 12345678910111213141516neo4j-sh (?)$ load csv from &quot;file:/data/stale/data01/neo4j/node_uuid_1kw.csv&quot; as line return count(*);+----------+| count(*) |+----------+| 10000010 |+----------+1 row7434 msneo4j-sh (?)$ using periodic commit 10000 load csv with headers from &quot;file:/data/stale/data01/neo4j/node_uuid_1kw.csv&quot; as line with line create (:Test &#123;uuid:line.uuid, name:line.name&#125;);+-------------------+| No data returned. |+-------------------+Nodes created: 10000009Properties set: 20000018Labels added: 10000009151498 ms 服务器32核，256G内存，1T机械硬盘导入时CPU利用率在150%左右，RES Memory 5G左右，VIRT Memory 70G左右 (7) 可能遇到的问题(7.1) Neo.DatabaseError.General.UnknownErrorNeo.DatabaseError.General.UnknownError: At /home/usdp/databases/data/news.csv:172393 - there&#39;s a field starting with a quote and whereas it ends that quote there seems to be characters in that field after that ending quote. That isn&#39;t supported. This is what I read: &#39;最后一公里&quot;&#39;字符转义的问题，换一个分隔符或者把那条数据去掉 在修改语句后 load csv from &quot;file:/home/usdp/databases/data/news.csv&quot; as line fieldterminator &#39;;&#39; return count(*); 数据格式问题在生成数据或者处理数据这步没做好，没有好的办法，只能把有问题的数据处理掉 (7.2) Couldn’t load the external resourceWARNING: Couldn&#39;t load the external resource at: file:/home/usdp/databases/data/news.csv路径输错了或者没有权限 (7.3) WARNING: Invalid inputWARNING: Invalid input &#39;)&#39;: expected whitespace, &#39;.&#39;, node labels, &#39;[&#39;, &quot;=~&quot;, IN, STARTS, ENDS, CONTAINS, IS, &#39;^&#39;, &#39;*&#39;, &#39;/&#39;, &#39;%&#39;, &#39;+&#39;, &#39;-&#39;, &#39;=&#39;, &quot;&lt;&gt;&quot;, &quot;!=&quot;, &#39;&lt;&#39;, &#39;&gt;&#39;, &quot;&lt;=&quot;, &quot;&gt;=&quot;, AND, XOR, OR, &#39;,&#39; or &#39;}&#39;cypher语句写的有问题 (7.4) WARNING: Variable n not definedcypher语句写的有问题，找不见变量n (7.5) WARNING: Expected 0 to be a java.lang.String, but it was a java.lang.Longload csv时使用header就在cypher里用header的属性或者load csv时不使用header，然后用line[0], line[1]否则会出现上面的错误 (8) 官网英文资料 LOAD CSV非常适合导入中小型数据，大概1000 0000 左右数据。 对于大数据集，即在100B记录范围内，我们可以访问专门的批量导入程序。 LOAD CSV is great for importing small – medium sized data, i.e. up to the 10M records range. For large data sets, i.e. in the 100B records range, we have access to a specialized bulk importer. CSV Data QualityReal World Data Considerations Real world data is messy. Don’t assume what someone told you is in a CSV file is actually, in there, don’t rely on format descriptions, consistency or correct quoting. Only trust data validity that you checked yourself.Common Pitfalls BOM byte order mark (2 UTF-8) bytes at the beginning of a file ← remove themBinary zeros or other non-text-characters dispersed throughout the file ← remove themInconsisent line breaks – mixed Windows and Unix linebreaks ← make sure they are consistent, best choose Unix styleHeader inconsistent with data (missing, too many columns, different delimiter in header) ← fix headersSpecial character in non-quoted text ← make sure unusual text is always quotedUnexpected newlines in quoted and unquoted text-fields ← either quote text or remove newlinesstray quotes – standalone double or single quote in the middle of non-quoted text, or non-escaped quotes in quoted text ← escape or remove stray quotes // assert correct line countLOAD CSV FROM “file-url” AS lineRETURN count(*); // check first few raw linesLOAD CSV FROM “file-url” AS line WITH lineRETURN lineLIMIT 5; // check first 5 line-sample with header-mappingLOAD CSV WITH HEADERS FROM “file-url” AS line WITH lineRETURN lineLIMIT 5; LOAD CSV for Medium Sized DatasetsThe real secret of LOAD CSV. It is not just your basic data ingestion mechanism, but actually an ETL Power Tool. Why? It combines multiple aspects in a single operation: supports loading / ingesting CSV data from an URIdirect mapping of input data into complex graph/domain structuredata conversionsupports complex computationscreate or merge data, relationships and structure Important Tips for LOAD CSV Always use the latest version of Neo4j, it will most probably be faster than earlier ones. Data Quality and Conversion See the data quality section aboveEmpty fields have to be skipped or replaced with default values during LOAD CSVAll data from the CSV file is read as a string, you have to use toInt, toFloat, split or similar functions to convertSplit arrays in a cell by delimiter using split (combine with extract for conversions)Check your Cypher import statement for typos: labels, property names and relationship-types are case-sensitiveConditional conversions can be achieved with CASE Indexing and Performance Make sure to have indexes and constraints declared and ONLINE for entities you want to MATCH or MERGE onAlways MATCH and MERGE on a single label and the indexed primary-key propertyPrefix your load statements with USING PERIODIC COMMIT 10000If possible, separate node creation from relationship creation into different statementsIf your import is slow or runs into memory issues, see Mark’s blog post on Eager loading. Memory Config Make sure to have enough memory (at least 4G heap in neo4j-wrapper.conf) reserved for your Neo4j-Server or Neo4j-Shell (export JAVA_OPTS=”-Xmx4G”)Configure the memory mapping settings (Neo4j 2.1) according to your expected file sizes(Neo4j 2.1 neo4j.properties keep these ratios: nodestore=100M, relationshipstore=2G, propertystore=500M, stringstore=500M) File-URLs and Neo4j-Shell Make sure to use the right URLs esp. file URLs.+ On OSX and Unix use file:///path/to/data.csv, on Windows, please use file:c:/path/to/data.csvUse the bin/neo4j-shell instead of the browser for better error messages and controlby default it connects to a running server, but you can also use bin/neo4j-shell -path import.db -config conf/neo4j.properties for direct database directory access (when no server is running with that db). Step by Step Example for LOAD CSV In our guide on ETL import from a relational database we explain how to import CSV data step by step, from data modeling, creating indexes to writing the individual LOAD CSV statements.Webinar “LOAD CSV in the Real World” In this very hands-on webinar Nicole White, Neo Technology’s Data Scientist, shows how to use LOAD CSV to import a real world dataset (consumer complaints from consumerfinance.gov) into Neo4j. After a quick modeling discussion she walks through the steps of preparing indexes and constraints and then imports one part of the dataset at a time into Neo4j. Super Fast Batch Importer For Huge Datasets LOAD CSV is great for importing small – medium sized data, i.e. up to the 10M records range. For large data sets, i.e. in the 100B records range, we have access to a specialized bulk importer. References[1] neo4j office guide-import-csv[2] neo4j-cypher-avoiding-the-eager[3] using-load-csv-to-import-git-history-into-neo4j[4] load-csv-into-neo4j-quickly-and-successfully[5] how-to-use-a-csv-field-to-define-the-node-label-in-a-load-statement[6] how-to-load-csv-files-into-spss-variable-and-value-labels]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谷歌搜索方法]]></title>
    <url>%2F2017%2F04%2F09%2Fgoogle-search%2F</url>
    <content type="text"><![CDATA[因为平时需要看一些技术性的资料，所以有时候需要使用谷歌搜索，个人比较喜欢google搜索英文版。 今天在换了一个VPN后使用谷歌，竟然使用的是其他语言，想用谷歌英文版和谷歌中文版，最后在网上找到了答案。 http://www.google.com/ncr 谷歌搜索中文版 http://www.google.com/webhp?hl=en&amp;gl=us 谷歌搜索英文版 (1) 谷歌搜索使用指定语言 google隐藏了一个秘密，你想看英文版而不被跳转，请使用谷歌浏览器输入 http://www.google.com/ncr http://www.google.com/webhp?hl=en&amp;gl=us https://www.google.com/ncr https://www.google.com/webhp?hl=en&amp;gl=us https://www.google.com.hk/webhp?hl=zh-CN&amp;sourceid=cnhp&amp;gws_rd=ssl 道理很简单：ncr后缀是no country redirect的英文缩写，在网址中指定了这个要求后， 谷歌就自动屏蔽了根据IP定位当地站点的功能，而直接指向了美国谷歌站。 (2) 谷歌是如何判断搜索语言的 根据http请求里的”Accept-Language”: “zh-CN,zh;q=0.9,zh-TW;q=0.8” 谷歌有你所登录的账号的设置与当前文本所属语言 ip地址 根据ip辅助判断 谷歌维护了词表，触发Query Rewrite 12345678910111213141516&#123; &quot;args&quot;: &#123;&#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate, br&quot;, &quot;Accept-Language&quot;: &quot;zh-CN,zh;q=0.9,zh-TW;q=0.8&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;Sec-Fetch-Mode&quot;: &quot;navigate&quot;, &quot;Sec-Fetch-Site&quot;: &quot;none&quot;, &quot;Sec-Fetch-User&quot;: &quot;?1&quot;, &quot;Upgrade-Insecure-Requests&quot;: &quot;1&quot;, &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.70 Safari/537.36&quot; &#125;, &quot;origin&quot;: &quot;106.38.115.25, 106.38.115.25&quot;, &quot;url&quot;: &quot;https://httpbin.org/get&quot;&#125; 12345678910111213&#123; &quot;args&quot;: &#123;&#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate, br&quot;, &quot;Accept-Language&quot;: &quot;zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;Upgrade-Insecure-Requests&quot;: &quot;1&quot;, &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:70.0) Gecko/20100101 Firefox/70.0&quot; &#125;, &quot;origin&quot;: &quot;106.38.115.25, 106.38.115.25&quot;, &quot;url&quot;: &quot;https://httpbin.org/get&quot;&#125; 123456789101112&#123; &quot;args&quot;: &#123;&#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&quot;, &quot;Accept-Encoding&quot;: &quot;br, gzip, deflate&quot;, &quot;Accept-Language&quot;: &quot;en-us&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1.2 Safari/605.1.15&quot; &#125;, &quot;origin&quot;: &quot;106.38.115.25, 106.38.115.25&quot;, &quot;url&quot;: &quot;https://httpbin.org/get&quot;&#125; (3) 被墙时的临时办法 谷歌学术搜索 http://scholar.hedasudi.com/ 代理 https://ww.tw.53yu.com/ 代理 https://guge.hk.bban.fun/ References[1] 谷歌无限制搜索方法[2] 谷歌是如何判断搜索语言的？]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>google</tag>
        <tag>search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[neo4j 配置]]></title>
    <url>%2F2017%2F04%2F05%2Fneo4j-config%2F</url>
    <content type="text"><![CDATA[neo4j使用里的内容有点杂，把配置择出来。 本文的主要讲述是neo4j-community-3.3.1 neo4j.conf的配置 2017-04-05 总结、翻译 neo4j-community-3.1.2的配置 2017-11-09 升级配置，使用neo4j-community-3.2.5 2017-12-03 更新博客，3.3.1和3.2.5的配置一样，结合实际使用，添加新的注释。 3.2.5 比 3.1.2 新增了 SSL system configuration 的配置 我的neo4j配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# 修改第9行，去掉#，修改数据库名dbms.active_database=wkq_graph.db# 修改第12行，去掉#。修改路径，改成绝对路径dbms.directories.data=C:/WorkSpaces/Neo4j/# 修改9行和12行后，neo4j数据存放在 C:/WorkSpaces/Neo4j/databases/wkq_graph.db，没错，多了一个databases# 修改第22行，在前面加个#，load csv时允许从任意路径读取文件#dbms.directories.import=import# 修改第26行，删除#后，连接数据库不需要密码#dbms.security.auth_enabled=false# 修改35行和36行，设置JVM初始堆内存和JVM最大堆内存# 这是在我自己电脑上测试用，生产环境自己慢慢调# 建议生产环境给的JVM最大堆内存越大越好，但是要小于机器的物理内存dbms.memory.heap.initial_size=2048mdbms.memory.heap.max_size=6144m# 修改46行，可以认为这个是缓存，如果机器配置高，这个越大越好dbms.memory.pagecache.size=10g# 修改54行，去掉改行的#，可以远程通过ip访问neo4j数据库dbms.connectors.default_listen_address=0.0.0.0# 默认 bolt端口是7687，http端口是7474，https关口是7473，不修改下面3项也可以# 修改71行，去掉#，设置http端口为7474，端口可以自定义，只要不和其他端口冲突就行#dbms.connector.bolt.listen_address=:7687# 修改75行，去掉#，设置http端口为7474，端口可以自定义，只要不和其他端口冲突就行dbms.connector.http.listen_address=:7474# 修改79行，去掉#，设置http端口为7473，端口可以自定义，只要不和其他端口冲突就行dbms.connector.https.listen_address=:7473# 修改227行，去掉#，允许从远程url来load csvdbms.security.allow_csv_import_from_file_urls=true# 修改233行，允许使用neo4j-shell，类似于mysql 命令行之类的dbms.shell.enabled=true# 修改235行，去掉#，设置连接neo4j-shell的端口，一般都是localhost或者127.0.0.1，这样安全，其他地址的话，一般使用https就行dbms.shell.host=127.0.0.1# 修改237行，去掉#，设置neo4j-shell端口，端口可以自定义，只要不和其他端口冲突就行dbms.shell.port=1337# 修改241行，设置neo4j可读可写dbms.read_only=false# CTT - Asia/Shanghaidbms.db.timezone=SYSTEM# dbms.logs.timezone has been replaced with dbms.db.timezone.#dbms.logs.timezone=SYSTEM#db.temporal.timezone=Asia/Shanghai# disable udcdbms.udc.enabled=false# apoc#dbms.security.procedures.whitelist=apoc.load.*dbms.security.procedures.unrestricted=apoc.*apoc.import.file.enabled=trueapoc.export.file.enabled=trueapoc.trigger.enabled=trueapoc.ttl.enabled=false neo4j.conf汉化版123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177For more details and a complete list of settings, please see https://neo4j.com/docs/operations-manual/current/reference/configuration-settings/# 如果想自定义neo4j数据库数据的存储路径，要同时修改dbms.active_database 和 dbms.directories.data 两项配置，# 修改配置后，数据会存放在$&#123;dbms.directories.data&#125;/databases/$&#123;dbms.active_database&#125; 目录下# 安装的数据库的名称，默认使用$&#123;NEO4J_HOME&#125;/data/databases/graph.db目录# The name of the database to mount #dbms.active_database=graph.db#安装Neo4j数据库的各个配置路径，默认使用$NEO4J_HOME下的路径#Paths of directories in the installation.# 数据路径#dbms.directories.data=data # 插件路径#dbms.directories.plugins=plugins #dbms.directories.certificates=certificates 证书路径#dbms.directories.logs=logs 日志路径#dbms.directories.lib=lib jar包路径#dbms.directories.run=run 运行路径#默认情况下想load csv文件，只能把csv文件放到$&#123;NEO4J_HOME&#125;/import目录下，把下面的#删除后，可以在load csv时使用绝对路径，这样可能不安全#This setting constrains all `LOAD CSV` import files to be under the `import` directory. Remove or comment it out to allow files to be loaded from anywhere in the filesystem; this introduces possible security problems. See the `LOAD CSV` section of the manual for details. #此设置将所有“LOAD CSV”导入文件限制在`import`目录下。删除注释允许从文件系统的任何地方加载文件;这引入了可能的安全问题。dbms.directories.import=import#把下面这行的#删掉后，连接neo4j数据库时就不用输密码了#Whether requests to Neo4j are authenticated. 是否对Neo4j的请求进行了身份验证。#To disable authentication, uncomment this line 要禁用身份验证，请取消注释此行。#dbms.security.auth_enabled=false#Enable this to be able to upgrade a store from an older version. 是否兼容以前版本的数据dbms.allow_format_migration=true#Java Heap Size: by default the Java heap size is dynamically calculated based on available system resources. Java堆大小：默认情况下，Java堆大小是动态地根据可用的系统资源计算。#Uncomment these lines to set specific initial and maximum heap size. 取消注释这些行以设置特定的初始值和最大值#dbms.memory.heap.initial_size=512m#dbms.memory.heap.max_size=512m#The amount of memory to use for mapping the store files, in bytes (or kilobytes with the &apos;k&apos; suffix, megabytes with &apos;m&apos; and gigabytes with &apos;g&apos;). 用于映射存储文件的内存量（以字节为单位）千字节带有&apos;k&apos;后缀，兆字节带有&apos;m&apos;，千兆字节带有&apos;g&apos;）。#If Neo4j is running on a dedicated server, then it is generally recommended to leave about 2-4 gigabytes for the operating system, give the JVM enough heap to hold all your transaction state and query context, and then leave the rest for the page cache. 如果Neo4j在专用服务器上运行，那么通常建议为操作系统保留大约2-4千兆字节，为JVM提供足够的堆来保存所有的事务状态和查询上下文，然后保留其余的页面缓存 。#The default page cache memory assumes the machine is dedicated to running Neo4j, and is heuristically set to 50% of RAM minus the max Java heap size. 默认页面缓存存储器假定机器专用于运行Neo4j，并且试探性地设置为RAM的50％减去最大Java堆大小。#dbms.memory.pagecache.size=10g### Network connector configuration#With default configuration Neo4j only accepts local connections. Neo4j默认只接受本地连接(localhost)#To accept non-local connections, uncomment this line: 要接受非本地连接，请取消注释此行dbms.connectors.default_listen_address=0.0.0.0 (这是删除#后的配置，可以通过ip访问)#You can also choose a specific network interface, and configure a non-default port for each connector, by setting their individual listen_address. 还可以选择特定的网络接口，并配置非默认值端口，设置它们各自的listen_address#The address at which this server can be reached by its clients. This may be the server&apos;s IP address or DNS name, or it may be the address of a reverse proxy which sits in front of the server. This setting may be overridden for individual connectors below. 客户端可以访问此服务器的地址。这可以是服务器的IP地址或DNS名称，或者可以是位于服务器前面的反向代理的地址。此设置可能会覆盖以下各个连接器。#dbms.connectors.default_advertised_address=localhost#You can also choose a specific advertised hostname or IP address, and configure an advertised port for each connector, by setting their individual advertised_address. 您还可以选择特定广播主机名或IP地址，为每个连接器配置通告的端口，通过设置它们独特的advertised_address。#Bolt connector 使用Bolt协议dbms.connector.bolt.enabled=truedbms.connector.bolt.tls_level=OPTIONALdbms.connector.bolt.listen_address=:7687#HTTP Connector. There must be exactly one HTTP connector. 使用http协议dbms.connector.http.enabled=truedbms.connector.http.listen_address=:7474#HTTPS Connector. There can be zero or one HTTPS connectors. 使用https协议dbms.connector.https.enabled=truedbms.connector.https.listen_address=:7473#Number of Neo4j worker threads. Neo4j线程数#dbms.threads.worker_count=#Logging configuration 日志配置#To enable HTTP logging, uncomment this line 要启用HTTP日志记录，请取消注释此行dbms.logs.http.enabled=true#Number of HTTP logs to keep. 要保留的HTTP日志数#dbms.logs.http.rotation.keep_number=5#Size of each HTTP log that is kept. 每个HTTP日志文件的大小dbms.logs.http.rotation.size=20m#To enable GC Logging, uncomment this line 要启用GC日志记录，请取消注释此行#dbms.logs.gc.enabled=true#GC Logging Options see http://docs.oracle.com/cd/E19957-01/819-0084-10/pt_tuningjava.html#wp57013 for more information. GC日志记录选项 有关详细信息，请参见http://docs.oracle.com/cd/E19957-01/819-0084-10/pt_tuningjava.html#wp57013#dbms.logs.gc.options=-XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintPromotionFailure -XX:+PrintTenuringDistribution#Number of GC logs to keep. 要保留的GC日志数#dbms.logs.gc.rotation.keep_number=5#Size of each GC log that is kept. 保留的每个GC日志文件的大小#dbms.logs.gc.rotation.size=20m#Size threshold for rotation of the debug log. If set to zero then no rotation will occur. Accepts a binary suffix &quot;k&quot;, &quot;m&quot; or &quot;g&quot;. 调试日志旋转的大小阈值。如果设置为零，则不会发生滚动(达到指定大小后切割日志文件)。接受二进制后缀“k”，“m”或“g”。#dbms.logs.debug.rotation.size=20m#Maximum number of history files for the internal log. 最多保存几个日志文件#dbms.logs.debug.rotation.keep_number=7### Miscellaneous configuration 其他配置#Enable this to specify a parser other than the default one. 启用此选项可指定除默认解析器之外的解析器#cypher.default_language_version=3.0#Determines if Cypher will allow using file URLs when loading data using `LOAD CSV`. Setting this value to `false` will cause Neo4j to fail `LOAD CSV` clauses that load data from the file system. 确定当使用加载数据时，Cypher是否允许使用文件URL `LOAD CSV`。将此值设置为`false`将导致Neo4j不能通过互联网上的URL导入数据，`LOAD CSV` 会从文件系统加载数据。dbms.security.allow_csv_import_from_file_urls=true#Retention policy for transaction logs needed to perform recovery and backups. 执行恢复和备份所需的事务日志的保留策略#dbms.tx_log.rotation.retention_policy=7 days#Enable a remote shell server which Neo4j Shell clients can log in to. 启用Neo4j Shell客户端可以登录的远程shell服务器dbms.shell.enabled=true#The network interface IP the shell will listen on (use 0.0.0.0 for all interfaces).dbms.shell.host=127.0.0.1#The port the shell will listen on, default is 1337.dbms.shell.port=1337#Only allow read operations from this Neo4j instance. This mode still requires write access to the directory for lock purposes. 只允许从Neo4j实例读取操作。此模式仍然需要对目录的写访问以用于锁定目的。#dbms.read_only=false#Comma separated list of JAX-RS packages containing JAX-RS resources, one package name for each mountpoint. The listed package names will be loaded under the mountpoints specified. Uncomment this line to mount the org.neo4j.examples.server.unmanaged.HelloWorldResource.java from neo4j-server-examples under /examples/unmanaged, resulting in a final URL of http://localhost:7474/examples/unmanaged/helloworld/&#123;nodeId&#125; 包含JAX-RS资源的JAX-RS软件包的逗号分隔列表，每个安装点一个软件包名称。所列出的软件包名称将在指定的安装点下加载。取消注释此行以装载org.neo4j.examples.server.unmanaged.HelloWorldResource.java neo4j-server-examples下/ examples / unmanaged，最终的URL为http//localhost7474/examples/unmanaged/helloworld/&#123;nodeId&#125;#dbms.unmanaged_extension_classes=org.neo4j.examples.server.unmanaged=/examples/unmanaged#JVM Parameters JVM参数#G1GC generally strikes a good balance between throughput and tail latency, without too much tuning. G1GC通常在吞吐量和尾部延迟之间达到很好的平衡，而没有太多的调整。dbms.jvm.additional=-XX:+UseG1GC#Have common exceptions keep producing stack traces, so they can be debugged regardless of how often logs are rotated. 有共同的异常保持生成堆栈跟踪，所以他们可以被调试，无论日志被旋转的频率dbms.jvm.additional=-XX:-OmitStackTraceInFastThrow#Make sure that `initmemory` is not only allocated, but committed to the process, before starting the database. This reduces memory fragmentation, increasing the effectiveness of transparent huge pages. It also reduces the possibility of seeing performance drop due to heap-growing GC events, where a decrease in available page cache leads to an increase in mean IO response time. Try reducing the heap memory, if this flag degrades performance. 确保在启动数据库之前，“initmemory”不仅被分配，而且被提交到进程。这减少了内存碎片，增加了透明大页面的有效性。它还减少了由于堆增长的GC事件而导致性能下降的可能性，其中可用页面缓存的减少导致平均IO响应时间的增加。如果此标志降低性能，请减少堆内存。 dbms.jvm.additional=-XX:+AlwaysPreTouch#Trust that non-static final fields are really final. This allows more optimizations and improves overall performance. NOTE: Disable this if you use embedded mode, or have extensions or dependencies that may use reflection or serialization to change the value of final fields! 信任非静态final字段真的是final。这允许更多的优化和提高整体性能。注意：如果使用嵌入模式，或者有可能使用反射或序列化更改最终字段的值的扩展或依赖关系，请禁用此选项！dbms.jvm.additional=-XX:+UnlockExperimentalVMOptionsdbms.jvm.additional=-XX:+TrustFinalNonStaticFields#Disable explicit garbage collection, which is occasionally invoked by the JDK itself. 禁用显式垃圾回收，这是偶尔由JDK本身调用。dbms.jvm.additional=-XX:+DisableExplicitGC#Remote JMX monitoring, uncomment and adjust the following lines as needed. Absolute paths to jmx.access and jmx.password files are required. 远程JMX监视，取消注释并根据需要调整以下行。需要jmx.access和jmx.password文件的绝对路径。#Also make sure to update the jmx.access and jmx.password files with appropriate permission roles and passwords, the shipped configuration contains only a read only role called &apos;monitor&apos; with password &apos;Neo4j&apos;. 还要确保使用适当的权限角色和密码更新jmx.access和jmx.password文件，所配置的配置只包含名为“monitor”的只读角色，密码为“Neo4j”。#For more details, see: http://download.oracle.com/javase/8/docs/technotes/guides/management/agent.html On Unix based systems the jmx.password file needs to be owned by the user that will run the server, and have permissions set to 0600. Unix系统，有关详情，请参阅：http：//download.oracle.com/javase/8/docs/technotes/guides/management/agent.html，jmx.password文件需要由运行服务器的用户拥有，并且权限设置为0600。#For details on setting these file permissions on Windows see: http://docs.oracle.com/javase/8/docs/technotes/guides/management/security-windows.html Windows系统 有关在设置这些文件权限的详细信息，请参阅：http://docs.oracle.com/javase/8/docs/technotes/guides/management/security-windows.html#dbms.jvm.additional=-Dcom.sun.management.jmxremote.port=3637#dbms.jvm.additional=-Dcom.sun.management.jmxremote.authenticate=true#dbms.jvm.additional=-Dcom.sun.management.jmxremote.ssl=false#dbms.jvm.additional=-Dcom.sun.management.jmxremote.password.file=/absolute/path/to/conf/jmx.password#dbms.jvm.additional=-Dcom.sun.management.jmxremote.access.file=/absolute/path/to/conf/jmx.access#Some systems cannot discover host name automatically, and need this line configured: 某些系统无法自动发现主机名，需要配置以下行：#dbms.jvm.additional=-Djava.rmi.server.hostname=$THE_NEO4J_SERVER_HOSTNAME#Expand Diffie Hellman (DH) key size from default 1024 to 2048 for DH-RSA cipher suites used in server TLS handshakes. 对于服务器TLS握手中使用的DH-RSA密码套件，将Diffie Hellman（DH）密钥大小从默认1024展开到2048。#This is to protect the server from any potential passive eavesdropping. 这是为了保护服务器免受任何潜在的被动窃听。dbms.jvm.additional=-Djdk.tls.ephemeralDHKeySize=2048### Wrapper Windows NT/2000/XP Service Properties 包装器Windows NT / 2000 / XP服务属性包装器Windows NT / 2000 / XP服务属性#WARNING - Do not modify any of these properties when an application using this configuration file has been installed as a service. WARNING - 当使用此配置文件的应用程序已作为服务安装时，不要修改任何这些属性。#Please uninstall the service before modifying this section. The service can then be reinstalled. 请在修改此部分之前卸载服务。 然后可以重新安装该服务。#Name of the service 服务的名称dbms.windows_service_name=neo4j### Other Neo4j system properties 其他Neo4j系统属性dbms.jvm.additional=-Dunsupported.dbms.udc.source=zip neo4j-community-3.3.1/conf/neo4j.conf文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318#*****************************************************************# Neo4j configuration## For more details and a complete list of settings, please see# https://neo4j.com/docs/operations-manual/current/reference/configuration-settings/#*****************************************************************# The name of the database to mount#dbms.active_database=graph.db# Paths of directories in the installation.#dbms.directories.data=data#dbms.directories.plugins=plugins#dbms.directories.certificates=certificates#dbms.directories.logs=logs#dbms.directories.lib=lib#dbms.directories.run=run# This setting constrains all `LOAD CSV` import files to be under the `import` directory. Remove or comment it out to# allow files to be loaded from anywhere in the filesystem; this introduces possible security problems. See the# `LOAD CSV` section of the manual for details.dbms.directories.import=import# Whether requests to Neo4j are authenticated.# To disable authentication, uncomment this line#dbms.security.auth_enabled=false# Enable this to be able to upgrade a store from an older version.#dbms.allow_upgrade=true# Java Heap Size: by default the Java heap size is dynamically# calculated based on available system resources.# Uncomment these lines to set specific initial and maximum# heap size.#dbms.memory.heap.initial_size=512m#dbms.memory.heap.max_size=512m# The amount of memory to use for mapping the store files, in bytes (or# kilobytes with the &apos;k&apos; suffix, megabytes with &apos;m&apos; and gigabytes with &apos;g&apos;).# If Neo4j is running on a dedicated server, then it is generally recommended# to leave about 2-4 gigabytes for the operating system, give the JVM enough# heap to hold all your transaction state and query context, and then leave the# rest for the page cache.# The default page cache memory assumes the machine is dedicated to running# Neo4j, and is heuristically set to 50% of RAM minus the max Java heap size.#dbms.memory.pagecache.size=10g#*****************************************************************# Network connector configuration#*****************************************************************# With default configuration Neo4j only accepts local connections.# To accept non-local connections, uncomment this line:#dbms.connectors.default_listen_address=0.0.0.0# You can also choose a specific network interface, and configure a non-default# port for each connector, by setting their individual listen_address.# The address at which this server can be reached by its clients. This may be the server&apos;s IP address or DNS name, or# it may be the address of a reverse proxy which sits in front of the server. This setting may be overridden for# individual connectors below.#dbms.connectors.default_advertised_address=localhost# You can also choose a specific advertised hostname or IP address, and# configure an advertised port for each connector, by setting their# individual advertised_address.# Bolt connectordbms.connector.bolt.enabled=true#dbms.connector.bolt.tls_level=OPTIONAL#dbms.connector.bolt.listen_address=:7687# HTTP Connector. There must be exactly one HTTP connector.dbms.connector.http.enabled=true#dbms.connector.http.listen_address=:7474# HTTPS Connector. There can be zero or one HTTPS connectors.dbms.connector.https.enabled=true#dbms.connector.https.listen_address=:7473# Number of Neo4j worker threads.#dbms.threads.worker_count=#*****************************************************************# SSL system configuration#*****************************************************************# Names of the SSL policies to be used for the respective components.# The legacy policy is a special policy which is not defined in# the policy configuration section, but rather derives from# dbms.directories.certificates and associated files# (by default: neo4j.key and neo4j.cert). Its use will be deprecated.# The policies to be used for connectors.## N.B: Note that a connector must be configured to support/require# SSL/TLS for the policy to actually be utilized.## see: dbms.connector.*.tls_level#bolt.ssl_policy=legacy#https.ssl_policy=legacy#*****************************************************************# SSL policy configuration#*****************************************************************# Each policy is configured under a separate namespace, e.g.# dbms.ssl.policy.&lt;policyname&gt;.*## The example settings below are for a new policy named &apos;default&apos;.# The base directory for cryptographic objects. Each policy will by# default look for its associated objects (keys, certificates, ...)# under the base directory.## Every such setting can be overriden using a full path to# the respective object, but every policy will by default look# for cryptographic objects in its base location.## Mandatory setting#dbms.ssl.policy.default.base_directory=certificates/default# Allows the generation of a fresh private key and a self-signed# certificate if none are found in the expected locations. It is# recommended to turn this off again after keys have been generated.## Keys should in general be generated and distributed offline# by a trusted certificate authority (CA) and not by utilizing# this mode.#dbms.ssl.policy.default.allow_key_generation=false# Enabling this makes it so that this policy ignores the contents# of the trusted_dir and simply resorts to trusting everything.## Use of this mode is discouraged. It would offer encryption but no security.#dbms.ssl.policy.default.trust_all=false# The private key for the default SSL policy. By default a file# named private.key is expected under the base directory of the policy.# It is mandatory that a key can be found or generated.#dbms.ssl.policy.default.private_key=# The private key for the default SSL policy. By default a file# named public.crt is expected under the base directory of the policy.# It is mandatory that a certificate can be found or generated.#dbms.ssl.policy.default.public_certificate=# The certificates of trusted parties. By default a directory named# &apos;trusted&apos; is expected under the base directory of the policy. It is# mandatory to create the directory so that it exists, because it cannot# be auto-created (for security purposes).## To enforce client authentication client_auth must be set to &apos;require&apos;!#dbms.ssl.policy.default.trusted_dir=# Client authentication setting. Values: none, optional, require# The default is to require client authentication.## Servers are always authenticated unless explicitly overridden# using the trust_all setting. In a mutual authentication setup this# should be kept at the default of require and trusted certificates# must be installed in the trusted_dir.#dbms.ssl.policy.default.client_auth=require# A comma-separated list of allowed TLS versions.# By default TLSv1, TLSv1.1 and TLSv1.2 are allowed.#dbms.ssl.policy.default.tls_versions=# A comma-separated list of allowed ciphers.# The default ciphers are the defaults of the JVM platform.#dbms.ssl.policy.default.ciphers=#*****************************************************************# Logging configuration#*****************************************************************# To enable HTTP logging, uncomment this line#dbms.logs.http.enabled=true# Number of HTTP logs to keep.#dbms.logs.http.rotation.keep_number=5# Size of each HTTP log that is kept.#dbms.logs.http.rotation.size=20m# To enable GC Logging, uncomment this line#dbms.logs.gc.enabled=true# GC Logging Options# see http://docs.oracle.com/cd/E19957-01/819-0084-10/pt_tuningjava.html#wp57013 for more information.#dbms.logs.gc.options=-XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintPromotionFailure -XX:+PrintTenuringDistribution# Number of GC logs to keep.#dbms.logs.gc.rotation.keep_number=5# Size of each GC log that is kept.#dbms.logs.gc.rotation.size=20m# Size threshold for rotation of the debug log. If set to zero then no rotation will occur. Accepts a binary suffix &quot;k&quot;,# &quot;m&quot; or &quot;g&quot;.#dbms.logs.debug.rotation.size=20m# Maximum number of history files for the internal log.#dbms.logs.debug.rotation.keep_number=7#*****************************************************************# Miscellaneous configuration#*****************************************************************# Enable this to specify a parser other than the default one.#cypher.default_language_version=3.0# Determines if Cypher will allow using file URLs when loading data using# `LOAD CSV`. Setting this value to `false` will cause Neo4j to fail `LOAD CSV`# clauses that load data from the file system.#dbms.security.allow_csv_import_from_file_urls=true# Retention policy for transaction logs needed to perform recovery and backups.dbms.tx_log.rotation.retention_policy=1 days# Enable a remote shell server which Neo4j Shell clients can log in to.#dbms.shell.enabled=true# The network interface IP the shell will listen on (use 0.0.0.0 for all interfaces).#dbms.shell.host=127.0.0.1# The port the shell will listen on, default is 1337.#dbms.shell.port=1337# Only allow read operations from this Neo4j instance. This mode still requires# write access to the directory for lock purposes.#dbms.read_only=false# Comma separated list of JAX-RS packages containing JAX-RS resources, one# package name for each mountpoint. The listed package names will be loaded# under the mountpoints specified. Uncomment this line to mount the# org.neo4j.examples.server.unmanaged.HelloWorldResource.java from# neo4j-server-examples under /examples/unmanaged, resulting in a final URL of# http://localhost:7474/examples/unmanaged/helloworld/&#123;nodeId&#125;#dbms.unmanaged_extension_classes=org.neo4j.examples.server.unmanaged=/examples/unmanaged#********************************************************************# JVM Parameters#********************************************************************# G1GC generally strikes a good balance between throughput and tail# latency, without too much tuning.dbms.jvm.additional=-XX:+UseG1GC# Have common exceptions keep producing stack traces, so they can be# debugged regardless of how often logs are rotated.dbms.jvm.additional=-XX:-OmitStackTraceInFastThrow# Make sure that `initmemory` is not only allocated, but committed to# the process, before starting the database. This reduces memory# fragmentation, increasing the effectiveness of transparent huge# pages. It also reduces the possibility of seeing performance drop# due to heap-growing GC events, where a decrease in available page# cache leads to an increase in mean IO response time.# Try reducing the heap memory, if this flag degrades performance.dbms.jvm.additional=-XX:+AlwaysPreTouch# Trust that non-static final fields are really final.# This allows more optimizations and improves overall performance.# NOTE: Disable this if you use embedded mode, or have extensions or dependencies that may use reflection or# serialization to change the value of final fields!dbms.jvm.additional=-XX:+UnlockExperimentalVMOptionsdbms.jvm.additional=-XX:+TrustFinalNonStaticFields# Disable explicit garbage collection, which is occasionally invoked by the JDK itself.dbms.jvm.additional=-XX:+DisableExplicitGC# Remote JMX monitoring, uncomment and adjust the following lines as needed. Absolute paths to jmx.access and# jmx.password files are required.# Also make sure to update the jmx.access and jmx.password files with appropriate permission roles and passwords,# the shipped configuration contains only a read only role called &apos;monitor&apos; with password &apos;Neo4j&apos;.# For more details, see: http://download.oracle.com/javase/8/docs/technotes/guides/management/agent.html# On Unix based systems the jmx.password file needs to be owned by the user that will run the server,# and have permissions set to 0600.# For details on setting these file permissions on Windows see:# http://docs.oracle.com/javase/8/docs/technotes/guides/management/security-windows.html#dbms.jvm.additional=-Dcom.sun.management.jmxremote.port=3637#dbms.jvm.additional=-Dcom.sun.management.jmxremote.authenticate=true#dbms.jvm.additional=-Dcom.sun.management.jmxremote.ssl=false#dbms.jvm.additional=-Dcom.sun.management.jmxremote.password.file=/absolute/path/to/conf/jmx.password#dbms.jvm.additional=-Dcom.sun.management.jmxremote.access.file=/absolute/path/to/conf/jmx.access# Some systems cannot discover host name automatically, and need this line configured:#dbms.jvm.additional=-Djava.rmi.server.hostname=$THE_NEO4J_SERVER_HOSTNAME# Expand Diffie Hellman (DH) key size from default 1024 to 2048 for DH-RSA cipher suites used in server TLS handshakes.# This is to protect the server from any potential passive eavesdropping.dbms.jvm.additional=-Djdk.tls.ephemeralDHKeySize=2048#********************************************************************# Wrapper Windows NT/2000/XP Service Properties#********************************************************************# WARNING - Do not modify any of these properties when an application# using this configuration file has been installed as a service.# Please uninstall the service before modifying this section. The# service can then be reinstalled.# Name of the servicedbms.windows_service_name=neo4j#********************************************************************# Other Neo4j system properties#********************************************************************dbms.jvm.additional=-Dunsupported.dbms.udc.source=zip References[1] https://docs.oracle.com/javase/8/docs/technotes/guides/management/agent.html[2] http://www.markhneedham.com/blog/2013/10/20/neo4j-accessing-jmx-beans-via-http/[3] https://dzone.com/articles/using-jmx-profile-neo4j-jdk]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 集合 HashMap 源码 学习 (JDK-1.8)]]></title>
    <url>%2F2017%2F04%2F02%2Fjava-hashmap-source-code-learning%2F</url>
    <content type="text"><![CDATA[小组开小讲堂，在小讲堂上听了HashMap这部分内容，决定看源码再巩固一下。顺便分享给大家。 我参考的是的是JDK 1.8 HashMap的源码 在这之前，先问大家几个问题 new一个HashMap的默认长度是多少 默认加载因子 HashMap的底层存储 为什么HashMap的大小必须是2的幂次方 key的Hash方法 hashCode碰撞时怎么处理 扩容时怎么处理 HashMap的最大长度 首先看一下HashMap的常量和方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * 默认加载因子0.75 * The load factor used when none specified in constructor. */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * 默认容量 16 * The default initial capacity - MUST be a power of two. */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * 默认构造方法 * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the default initial capacity * (16) and the default load factor (0.75). */public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125;/** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the specified initial * capacity and the default load factor (0.75). * * @param initialCapacity the initial capacity. * @throws IllegalArgumentException if the initial capacity is negative. */public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;/** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the specified initial * capacity and load factor. * * @param initialCapacity the initial capacity * @param loadFactor the load factor * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125; 123456789101112131415161718// 默认的初始容量是16 - 2的幂。static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16// 最大容量，必须是2的幂且小于2的30次方，传入容量过大将被这个值替换// 1 &lt;&lt; 30，也就是2的30次方 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;// static final float DEFAULT_LOAD_FACTOR = 0.75f;//static final int TREEIFY_THRESHOLD = 8;//static final int UNTREEIFY_THRESHOLD = 6;//static final int MIN_TREEIFY_CAPACITY = 64; 一个静态内部类Node static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; 内部类KeySet final class KeySet extends AbstractSet 内部类Values final class Values extends AbstractCollection 内部类EntrySet final class EntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; 抽象类HashIterator abstract class HashIterator 内部类KeyIterator final class KeyIterator extends HashIterator implements Iterator final class ValueIterator extends HashIterator implements Iterator … // 构造方法，传入初始容量和加载因子 // 方法会对初始容量进行校验，大于0，小于2的30次方 // 方法会对加载因子进行校验，大于0，并且部位NaN public HashMap(int initialCapacity, float loadFactor) // 构造方法，传入初始容量 // 同上 // 使用默认加载因子0.75 public HashMap(int initialCapacity) // 构造方法 // 同上 // 使用默认初始容量16，默认加载因子0.75 public HashMap() // 构造方法 // 传入map // 使用默认加载因子0.75 public HashMap(Map&lt;? extends K, ? extends V&gt; m) // 根据key计算hash值 static final int hash(Object key) // static Class&lt;?&gt; comparableClassFor(Object x) // static int compareComparables(Class&lt;?&gt; kc, Object k, Object x) // Returns a power of two size for the given target capacity. static final int tableSizeFor(int cap) // Implements Map.putAll and Map constructor final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) // Returns the number of key-value mappings in this map. public int size() // Returns true if this map contains no key-value mappings. public boolean isEmpty() // Returns the value to which the specified key is mapped, // or null if this map contains no mapping for the key. public V get(Object key) // Implements Map.get and related methods final Node&lt;K,V&gt; getNode(int hash, Object key) // Returns true if this map contains a mapping for the specified key. public boolean containsKey(Object key) // Associates the specified value with the specified key in this map. // If the map previously contained a mapping for the key, the old // value is replaced. public V put(K key, V value) // Implements Map.put and related methods final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) Hashmap继承于AbstractMap，实现了Map、Cloneable、java.io.Serializable接口。 它的key、value都可以为null，映射不是有序的。 Hashmap不是同步的，如果想要线程安全的HashMap，可以通过Collections类的静态方法synchronizedMap获得线程安全的HashMap。Map map = Collections.synchronizedMap(new HashMap()); HashMap 中两个影响其性能的重要参数：“初始容量” 和 “加载因子”。容量： 是哈希表中桶的数量，初始容量 只是哈希表在创建时的容量加载因子： 是哈希表在其容量自动增加之前可以达到多满的一种尺度（默认0.75）。 当哈希表中的条目数超出了加载因子与当前容量的乘积时，则要对该哈希表进行 rehash 操作（扩容，即重建内部数据结构，桶数Ｘ２）。加载因子越大,填满的元素越多,好处是,空间利用率高了,但:冲突的机会加大了.反之,加载因子越小,填满的元素越少,好处是:冲突的机会减小了,但:空间浪费多了. HashMap数据结构 Hashmap本质是数组加链表。 通过key的hashCode来计算hash值的，只要hashCode相同，计算出来的hash值就一样，然后再计算出数组下标， 如果多个key对应到同一个下标，就用链表串起来，新插入的在前面。 长度必须是2的幂次方这里的h是”int hash = hash(key.hashCode());”, 也就是根据key的hashCode再次进行一次hash操作计算出来的 .length是Entry数组的长度 .一般我们利用hash码, 计算出在一个数组的索引, 常用方式是”h % length”, 也就是求余的方式 .可能是这种方式效率不高, SUN大师们发现, “当容量一定，是2^n时，h &amp; (length - 1) == h % length” . 按位运算特别快 .123456/** * Returns index for hash code h. */ static int indexFor(int h, int length) &#123; return h &amp; (length-1); &#125; 对于length = 16, 对应二进制”1 0000”, length-1=”0 1111”假设此时h = 17 .(1) 使用”h % length”, 也就是”17 % 16”, 结果是1 .(2) 使用”h &amp; (length - 1)”, 也就是 “1 0001 &amp; 0 1111”, 结果也是1 .我们会发现, 因为”0 1111”低位都是”1”, 进行”&amp;”操作, 就能成功保留”1 0001”对应的低位, 将高位的都丢弃, 低位是多少, 最后结果就是多少 .刚好低位的范围是”0~15”, 刚好是长度为length=16的所有索引 . 线程安全Map m = Collections.synchronizeMap(hashMap); ConcurrentHashMap References[1] 深入解析HashMap、HashTable[2] HashMap和Hashtable的区别[3] JAVA中HashMap和Hashtable区别[4] Differences between HashMap and Hashtable? [5] HashMap的工作原理[6] Java集合之HashMap源码解析 | gyl-coder[7] Java集合之HashMap源码解析[8] HashMap的长度为什么设置为2的n次方[8] HashMap深度解析(一)[9] HashMap深度解析(二)[10] Java集合框架源码解析之HashMap]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven 笔记]]></title>
    <url>%2F2017%2F03%2F25%2Fmaven-notes%2F</url>
    <content type="text"><![CDATA[有关maven的使用 (1) 安装配置(1.1) 安装 在apache官网下载maven压缩包，解压完即可使用 下载地址 https://maven.apache.org/download.cgi (1.2) 配置 需要配置环境变量 MAVEN_HOME 安装路径 ( C:\ProfessionSofware\Maven\apache-maven-3.3.9 ) 注意：配置M2_HOME 或者 MAVEN_HOME 都可以，两个功能上是一样的 在环境变量path里添加 %MAVEN_HOME%\bin 注意：不要在环境变量的最后加;否则以后会遇到问题 Windows下配置 MAVEN_HOME = C:\ProfessionSofware\Maven\apache-maven-3.3.9 Mac下配置 12export MAVEN_HOME=/Users/weikeqin/SoftWare/apache-maven-3.6.1export PATH=$PATH:$MAVEN_HOME/bin 验证是否安装配置成功 配置完重新打开命令行 输入 windows 1234567mvn -versionApache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)Maven home: C:\ProfessionSofware\Maven\apache-maven-3.3.9Java version: 1.8.0_111, vendor: Oracle CorporationJava home: C:\ProfessionSofware\Java\jdk1.8.0_111\jreDefault locale: zh_CN, platform encoding: GBKOS name: &quot;windows 7&quot;, version: &quot;6.1&quot;, arch: &quot;amd64&quot;, family: &quot;dos&quot; mac 123456$ mvn -versionApache Maven 3.6.1 (d66c9c0b3152b2e69ee9bac180bb8fcc8e6af555; 2019-04-05T03:00:29+08:00)Maven home: /Users/weikeqin1/SoftWare/apache-maven-3.6.1Java version: 1.8.0_211, vendor: Oracle Corporation, runtime: /Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home/jreDefault locale: en_CN, platform encoding: UTF-8OS name: &quot;mac os x&quot;, version: &quot;10.13.6&quot;, arch: &quot;x86_64&quot;, family: &quot;mac&quot; 出现以上信息说明配置成功 (2) 配置maven仓库 maven作为一个项目管理工具确实非常好用，但是在国内这个网络条件下实在很慢。 maven仓库国内镜像。 maven仓库 修改 ${maven_home}/conf/setting.xml 文件 在maven的 settings.xml 文件里的 mirrors 节点，添加如下子节点：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;!-- add aliyun mirror--&gt;&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;/mirror&gt; &lt;!-- 中央仓库1 --&gt;&lt;mirror&gt; &lt;id&gt;repo1&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://repo1.maven.org/maven2/&lt;/url&gt;&lt;/mirror&gt;&lt;!-- 中央仓库2 --&gt;&lt;mirror&gt; &lt;id&gt;repo2&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://repo2.maven.org/maven2/&lt;/url&gt;&lt;/mirror&gt;&lt;!-- mvnrepository镜像，常用的maven中央仓库jar查询站点，可直接当maven镜像使用 --&gt; &lt;mirror&gt; &lt;id&gt;mvn&lt;/id&gt; &lt;mirrorOf&gt;mvnrepository&lt;/mirrorOf&gt; &lt;url&gt;http://mvnrepository.com/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;ui&lt;/id&gt; &lt;name&gt;Mirror from UK&lt;/name&gt; &lt;url&gt;http://uk.maven.org/maven2/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;&lt;/mirror&gt;&lt;mirror&gt; &lt;id&gt;jboss-public-repository-group&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;JBoss Public Repository Group&lt;/name&gt; &lt;url&gt;http://repository.jboss.org/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt;&lt;!-- spring的libs-release镜像，存放spring项目及其子项目的jar包，以及相关的依赖jar --&gt; &lt;mirror&gt; &lt;id&gt;libs-release&lt;/id&gt; &lt;mirrorOf&gt;repo1&lt;/mirrorOf&gt; &lt;url&gt;https://repo.spring.io/libs-release&lt;/url&gt; &lt;/mirror&gt; &lt;!-- spring的milestone镜像，存放着spring项目及其子项目的里程碑版本jar包 --&gt; &lt;mirror&gt; &lt;id&gt;milestone&lt;/id&gt; &lt;mirrorOf&gt;repo2&lt;/mirrorOf&gt; &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt; &lt;/mirror&gt; &lt;!-- spring的snapshot镜像，存放着spring项目及其子项目的预览版本jar包 --&gt; &lt;mirror&gt; &lt;id&gt;snapshot&lt;/id&gt; &lt;mirrorOf&gt;repo3&lt;/mirrorOf&gt; &lt;url&gt;https://repo.spring.io/snapshot&lt;/url&gt; &lt;/mirror&gt; 或者在pom.xml文件里添加repositories节点，代码如下 123456789101112&lt;repository&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt;&lt;/repository&gt; (3) maven常用命令(3.1) mvn clean mvn clean 命令把详细参数全打印出来如下1/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home/bin/java -Dmaven.multiModuleProjectDirectory=/Users/weikeqin1/WorkSpaces/java/java-test -Dmaven.home=/Users/weikeqin1/SoftWare/apache-maven-3.6.1 -Dclassworlds.conf=/Users/weikeqin1/SoftWare/apache-maven-3.6.1/bin/m2.conf -Dfile.encoding=UTF-8 -classpath /Users/weikeqin1/SoftWare/apache-maven-3.6.1/boot/plexus-classworlds-2.6.0.jar org.codehaus.classworlds.Launcher -s /Users/weikeqin1/.m2/settings.xml -DskipTests=true clean -P local 123456789/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home/bin/java -Dmaven.multiModuleProjectDirectory=/Users/weikeqin1/WorkSpaces/java/java-test-Dmaven.home=/Users/weikeqin1/SoftWare/apache-maven-3.6.1-Dclassworlds.conf=/Users/weikeqin1/SoftWare/apache-maven-3.6.1/bin/m2.conf-Dfile.encoding=UTF-8 -classpath /Users/weikeqin1/SoftWare/apache-maven-3.6.1/boot/plexus-classworlds-2.6.0.jar-s /Users/weikeqin1/.m2/settings.xml-DskipTests=true-P local -s /Users/weikeqin1/.m2/settings.xml 指使用指定的 settings.xml文件 --settings /Users/weikeqin1/.m2/settings.xml (3.2) mvn clean package -Dmaven.test.skip=true -P local1/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home/bin/java -Dmaven.multiModuleProjectDirectory=/Users/weikeqin1/WorkSpaces/java/java-test -Dmaven.home=/Users/weikeqin1/SoftWare/apache-maven-3.6.1 -Dclassworlds.conf=/Users/weikeqin1/SoftWare/apache-maven-3.6.1/bin/m2.conf -Dfile.encoding=UTF-8 -classpath /Users/weikeqin1/SoftWare/apache-maven-3.6.1/boot/plexus-classworlds-2.6.0.jar org.codehaus.classworlds.Launcher -s /Users/weikeqin1/.m2/settings.xml clean package -Dmaven.test.skip=true -P local pom文件解释12 (5) 常见错误1. maven项目 错误: 找不到或无法加载主类 报错信息为：Missing artifact com.company.air:air-client:jar:1.0.1 到当前用户的.m2目录下查看，jar文件已经正常下载了。 解决方法： 1 到报错的.m2的对应目录下，检查发现目录下是否存在以如下结尾的文件： -not-available .lastUpdated 将这两个文件删掉，重新build，如果问题解决，应该是之前未下载成功产生了这两个文件，影响了maven正常更新 2 删掉之后重新下载，下载完可能出现一种情况，本地maven仓库有了，Eclipse还提示没有，这是IDE的问题， 这个时候 选中项目 右键 maven update 记得选中force update，更新一下就好了 3 如果上述办法无效，到eclipse-help-install new software-available software sites下， 找之前安装m2eclipse插件的地址，如果是，将其卸载， 按如下地址重新安装m2eclipse插件：http://m2eclipse.sonatype.org/sites/m2e 4 如果上述方法仍无效，可尝试在eclipse中先用Close Project关掉出问题的工程， 然后再Open Project打开；或用Project-Clean重新build该工程 2. git + mavenhttp://www.blogjava.net/youxia/archive/2013/12/29/408182.html 3. maven可选依赖（Optional Dependencies）和依赖排除（Dependency Exclusions）http://www.tuicool.com/articles/yaeIV3123456&lt;exclusions&gt; &lt;exclusion&gt; &lt;!-- declare the exclusion here --&gt; &lt;groupId&gt;sample.ProjectC&lt;/groupId&gt; &lt;artifactId&gt;Project-C&lt;/artifactId&gt; &lt;/exclusion&gt;&lt;/exclusions&gt; 4. Missing artifact jdk.tools:jdk.tools:jar:1.71234567&lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.7&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt; &lt;/dependency&gt; 5. -source 1.5 中不支持 diamond 运算符原因:在pom.xml里没有指定JDK版本12345678910111213141516171819&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;java.version&gt;1.7&lt;/java.version&gt;&lt;/properties&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;$&#123;java.version&#125;&lt;/source&gt; &lt;target&gt;$&#123;java.version&#125;&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 6. The type org.nlpcn.commons.lang.tire.domain.Forest cannot be resolved. It is indirectly referenced from required .class files 1 JDK 版本 2 Jdk.tools 3 少nlp-lang的包12345678910111213&lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;$&#123;java.version&#125;&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;$&#123;JAVA_HOME&#125;/lib/tools.jar&lt;/systemPath&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.nlpcn&lt;/groupId&gt; &lt;artifactId&gt;nlp-lang&lt;/artifactId&gt; &lt;version&gt;1.7&lt;/version&gt;&lt;/dependency&gt; 6. There are test failures. BUILD FAILURE There are test failures. 1mvn clean install -Dmaven.test.skip=true References[1] maven常用命令[2] 在Eclipse中创建Maven多模块工程的例子[3] pom文件提示：Missing artifact[4] maven项目 错误: 找不到或无法加载主类[5] 快使用阿里云的maven仓库[6] maven 编译时跳过单元测试[7] Maven命令行窗口指定settings.xml[8] maven-command-line-how-to-point-to-a-specific-settings-xml-for-a-single-command[9] maven-command-to-determine-which-settings-xml-file-maven-is-using]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j 设置时间]]></title>
    <url>%2F2017%2F03%2F23%2Fneo4j-set-local-time%2F</url>
    <content type="text"><![CDATA[把Neo4j数据库日志的时间改成本地时间 在 neo4j.conf 里加上 dbms.db.timezone=SYSTEM 即可 下面的方法弃用 这个方法只能把写到文件的日志里的时间转成自己想要的时区的时间，原理是用的pl脚本 首先需要启动neo4j数据库，等数据库启动后，执行utc.pl脚本，然后就可以看到日志里的时间转成想要的时区的时间了， 缺点是只是把文件里的时间改变了，并没有改变neo4j里的时间 时区，所以控制台输出的还是原来没转化的。 脚本和方法如下 1. 首先新建utc.pl脚本，并用chmod设置权限 2. 启动neo4j数据库 3. 执行 utc.pl 脚本 ./utc.pl logs/debug.log &gt; logs/debug.utc.log 4. 查看logs目录下的debug.utc.log，看是否起作用 utc.pl脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/usr/bin/perl -wuse strict;use Time::Local; #needed for timegm()my $file = $ARGV[0] or die "USAGE: $0 &lt;filename&gt;\n";open(my $data, '&lt;', $file) or die "Could not open '$file' $!\n";while (my $line = &lt;$data&gt;) &#123; # where a line might start as # 2017-01-11 23:22:28.372+0000 INFO ... .... .... chomp $line; # check to make sure the line begins with a YYYY-MM-DD HH if ( $line =~ /\d\d\d\d-\d\d-\d\d \d\d/ ) &#123; my $newstring = UTC2LocalString($line); print "$newstring\n"; &#125; else &#123; print "$line\n"; &#125;&#125;sub UTC2LocalString&#123; # below attributed to Marshall at http://www.perlmonks.org/?node_id=873435 my $t = shift; my ($datehour, $rest) = split(/:/,$t,2); # $datehour will represent YYYY-MM-DD HH (i.e. 2017-01-14 12) # $rest represents the rest of the line after # and this will reassemble and return $datehour (adjusted) + $rest my ($year, $month, $day, $hour) = $datehour =~ /(\d+)-(\d\d)-(\d\d)\s+(\d\d)/; # proto: $time = timegm($sec,$min,$hour,$mday,$mon,$year); my $epoch = timegm (0,0,$hour,$day,$month-1,$year); # proto: ($sec,$min,$hour,$mday,$mon,$year,$wday,$yday,$isdst) = # localtime(time); my ($lyear,$lmonth,$lday,$lhour,$isdst) = (localtime($epoch))[5,4,3,2,-1]; $lyear += 1900; # year is 1900 based $lmonth++; # month number is zero based #print "isdst: $isdst\n"; #debug flag day-light-savings time return ( sprintf("%04d-%02d-%02d %02d:%s", $lyear,$lmonth,$lday,$lhour,$rest) );&#125; 详细操作过程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252[wkq@wkq neo4j-community-3.4.1]$ lltotal 288drwxr-xr-x. 3 wkq wkq 4096 Jun 11 23:11 bindrwxrwxr-x. 2 wkq wkq 51 Jul 4 16:31 certificatesdrwxr-xr-x. 2 wkq wkq 31 Jul 4 16:53 confdrwxr-xr-x. 4 wkq wkq 45 Jul 4 16:31 datadrwxr-xr-x. 2 wkq wkq 10 Jun 11 22:57 importdrwxrwxr-x. 2 wkq wkq 4096 Jul 4 16:31 lib-rw-r--r--. 1 wkq wkq 84668 Jun 11 22:57 LICENSES.txt-rw-r--r--. 1 wkq wkq 36005 Jun 11 22:57 LICENSE.txtdrwxr-xr-x. 2 wkq wkq 4096 Jul 4 16:55 logs-rw-r--r--. 1 wkq wkq 5983 Jun 11 22:57 NOTICE.txtdrwxr-xr-x. 2 wkq wkq 10 Jun 11 22:57 plugins-rw-r--r--. 1 wkq wkq 1596 Jun 11 22:57 README.txtdrwxr-xr-x. 2 wkq wkq 10 Jul 4 16:58 run-rw-r--r--. 1 wkq wkq 96 Jun 11 22:57 UPGRADE.txt-rwxrwxrwx. 1 wkq wkq 1538 Jul 4 16:37 utc.pl[wkq@wkq neo4j-community-3.4.1]$ ./bin/neo4j consoleActive database: graph.dbDirectories in use: home: /data/stale/data01/neo4j/neo4j-community-3.4.1 config: /data/stale/data01/neo4j/neo4j-community-3.4.1/conf logs: /data/stale/data01/neo4j/neo4j-community-3.4.1/logs plugins: /data/stale/data01/neo4j/neo4j-community-3.4.1/plugins import: NOT SET data: /data/stale/data01/neo4j/neo4j-community-3.4.1/data certificates: /data/stale/data01/neo4j/neo4j-community-3.4.1/certificates run: /data/stale/data01/neo4j/neo4j-community-3.4.1/runStarting Neo4j.2018-07-04 08:59:51.364+0000 INFO ======== Neo4j 3.4.1 ========2018-07-04 08:59:51.392+0000 INFO Starting...2018-07-04 08:59:53.202+0000 INFO Bolt enabled on 0.0.0.0:7687.2018-07-04 08:59:54.849+0000 INFO Started.2018-07-04 08:59:55.631+0000 WARN Low configured threads: (max=&#123;&#125; - required=&#123;&#125;)=&#123;&#125; &lt; warnAt=&#123;&#125; for &#123;&#125;2018-07-04 08:59:55.649+0000 INFO Remote interface available at http://localhost:7474/[wkq@wkq neo4j-community-3.4.1]$ ./utc.pl logs/debug.log &gt; logs/debug.utc.log[wkq@wkq neo4j-community-3.4.1]$ cd logs/[wkq@wkq logs]$ lltotal 140-rw-rw-r--. 1 wkq wkq 48884 Jul 4 17:02 debug.log-rw-rw-r--. 1 wkq wkq 48884 Jul 4 17:03 debug.utc.log-rw-rw-r--. 1 wkq wkq 40654 Jul 4 17:02 gc.log.0.current-rw-rw-r--. 1 wkq wkq 850 Jul 4 17:02 http.log[wkq@wkq logs]$ tail -100 debug.log2018-07-04 08:59:53.285+0000 INFO [o.n.k.i.DiagnosticsManager] Id usage:2018-07-04 08:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] ArrayPropertyStore: used=1 high=02018-07-04 08:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] NodeStore: used=0 high=-12018-07-04 08:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] StringPropertyStore: used=1 high=02018-07-04 08:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] PropertyIndexStore: used=0 high=-12018-07-04 08:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] StringPropertyStore: used=1 high=02018-07-04 08:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] ArrayPropertyStore: used=1 high=02018-07-04 08:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] PropertyStore: used=0 high=-12018-07-04 08:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] RelationshipStore: used=0 high=-12018-07-04 08:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] StringPropertyStore: used=1 high=02018-07-04 08:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] RelationshipTypeStore: used=0 high=-12018-07-04 08:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] StringPropertyStore: used=1 high=02018-07-04 08:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] LabelTokenStore: used=0 high=-12018-07-04 08:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] SchemaStore: used=1 high=02018-07-04 08:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] RelationshipGroupStore: used=1 high=02018-07-04 08:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] NeoStore: used=15 high=142018-07-04 08:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] --- STARTED diagnostics for NEO_STORE_ID_USAGE END ---2018-07-04 08:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] --- STARTED diagnostics for NEO_STORE_RECORDS START ---2018-07-04 08:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] Neostore records:2018-07-04 08:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] TIME (Creation time): 15306931028992018-07-04 08:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] RANDOM_NUMBER (Random number for store id): 89274000696936539012018-07-04 08:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] LOG_VERSION (Current log version): 02018-07-04 08:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] LAST_TRANSACTION_ID (Last committed transaction): 12018-07-04 08:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] STORE_VERSION (Store format version): 160949311551872062018-07-04 08:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] FIRST_GRAPH_PROPERTY (First property record containing graph properties): -12018-07-04 08:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] LAST_CONSTRAINT_TRANSACTION (Last committed transaction containing constraint changes): 02018-07-04 08:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] UPGRADE_TRANSACTION_ID (Transaction id most recent upgrade was performed at): 12018-07-04 08:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] UPGRADE_TIME (Time of last upgrade): 15306931028992018-07-04 08:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] LAST_TRANSACTION_CHECKSUM (Checksum of last committed transaction): 02018-07-04 08:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] UPGRADE_TRANSACTION_CHECKSUM (Checksum of transaction id the most recent upgrade was performed at): 02018-07-04 08:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] LAST_CLOSED_TRANSACTION_LOG_VERSION (Log version where the last transaction commit entry has been written into): 02018-07-04 08:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] LAST_CLOSED_TRANSACTION_LOG_BYTE_OFFSET (Byte offset in the log file where the last transaction commit entry has been written into): 162018-07-04 08:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] LAST_TRANSACTION_COMMIT_TIMESTAMP (Commit time timestamp for last committed transaction): 02018-07-04 08:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] UPGRADE_TRANSACTION_COMMIT_TIMESTAMP (Commit timestamp of transaction the most recent upgrade was performed at): 02018-07-04 08:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] --- STARTED diagnostics for NEO_STORE_RECORDS END ---2018-07-04 08:59:53.289+0000 INFO [o.n.k.i.DiagnosticsManager] --- STARTED diagnostics for TRANSACTION_RANGE START ---2018-07-04 08:59:53.289+0000 INFO [o.n.k.i.DiagnosticsManager] Transaction log:2018-07-04 08:59:53.290+0000 INFO [o.n.k.i.DiagnosticsManager] Oldest transaction 2 found in log with version 02018-07-04 08:59:53.290+0000 INFO [o.n.k.i.DiagnosticsManager] --- STARTED diagnostics for TRANSACTION_RANGE END ---2018-07-04 08:59:53.290+0000 INFO [o.n.k.i.DiagnosticsManager] --- STARTED diagnostics for KernelDiagnostics:StoreFiles START ---2018-07-04 08:59:53.290+0000 INFO [o.n.k.i.DiagnosticsManager] Disk space on partition (Total / Free / Free %): 5995761762304 / 5630701182976 / 932018-07-04 08:59:53.290+0000 INFO [o.n.k.i.DiagnosticsManager] Storage files: (filename : modification date - size)2018-07-04 08:59:53.291+0000 INFO [o.n.k.i.DiagnosticsManager] index:2018-07-04 08:59:53.292+0000 INFO [o.n.k.i.DiagnosticsManager] - Total: 2018-07-04T16:31:43+0800 - 0.00 B2018-07-04 08:59:53.293+0000 INFO [o.n.k.i.DiagnosticsManager] neostore: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 08:59:53.293+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.counts.db.a: 2018-07-04T16:31:43+0800 - 96.00 B2018-07-04 08:59:53.293+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 08:59:53.293+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.labelscanstore.db: 2018-07-04T16:59:52+0800 - 40.00 kB2018-07-04 08:59:53.294+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.labeltokenstore.db: 2018-07-04T16:31:42+0800 - 0.00 B2018-07-04 08:59:53.294+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.labeltokenstore.db.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 08:59:53.294+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.labeltokenstore.db.names: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 08:59:53.294+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.labeltokenstore.db.names.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 08:59:53.295+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.nodestore.db: 2018-07-04T16:31:42+0800 - 0.00 B2018-07-04 08:59:53.295+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.nodestore.db.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 08:59:53.295+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.nodestore.db.labels: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 08:59:53.295+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.nodestore.db.labels.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 08:59:53.296+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db: 2018-07-04T16:31:42+0800 - 0.00 B2018-07-04 08:59:53.296+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.arrays: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 08:59:53.296+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.arrays.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 08:59:53.296+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 08:59:53.296+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.index: 2018-07-04T16:31:42+0800 - 0.00 B2018-07-04 08:59:53.297+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.index.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 08:59:53.297+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.index.keys: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 08:59:53.297+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.index.keys.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 08:59:53.297+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.strings: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 08:59:53.297+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.strings.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 08:59:53.297+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.relationshipgroupstore.db: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 08:59:53.298+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.relationshipgroupstore.db.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 08:59:53.298+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.relationshipstore.db: 2018-07-04T16:31:42+0800 - 0.00 B2018-07-04 08:59:53.298+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.relationshipstore.db.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 08:59:53.298+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.relationshiptypestore.db: 2018-07-04T16:31:42+0800 - 0.00 B2018-07-04 08:59:53.298+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.relationshiptypestore.db.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 08:59:53.299+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.relationshiptypestore.db.names: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 08:59:53.299+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.relationshiptypestore.db.names.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 08:59:53.299+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.schemastore.db: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 08:59:53.299+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.schemastore.db.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 08:59:53.299+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.transaction.db.0: 2018-07-04T16:58:15+0800 - 88.00 B2018-07-04 08:59:53.299+0000 INFO [o.n.k.i.DiagnosticsManager] store_lock: 2018-07-04T16:31:42+0800 - 0.00 B2018-07-04 08:59:53.300+0000 INFO [o.n.k.i.DiagnosticsManager] Storage summary:2018-07-04 08:59:53.300+0000 INFO [o.n.k.i.DiagnosticsManager] Total size of store: 112.31 kB2018-07-04 08:59:53.300+0000 INFO [o.n.k.i.DiagnosticsManager] Total size of mapped files: 112.00 kB2018-07-04 08:59:53.300+0000 INFO [o.n.k.i.DiagnosticsManager] --- STARTED diagnostics for KernelDiagnostics:StoreFiles END ---2018-07-04 08:59:54.850+0000 INFO [o.n.k.i.DiagnosticsManager] --- SERVER STARTED START ---2018-07-04 08:59:55.649+0000 INFO [o.n.k.i.DiagnosticsManager] --- SERVER STARTED END ---2018-07-04 09:02:17.230+0000 ERROR [o.n.b.r.DefaultBoltConnection] Unexpected error detected in bolt session &apos;1402ecfffe7d5b64-00001f8c-00000001-a464075b467c9fad-e3b5926d&apos;. The client is unauthorized due to authentication failure.org.neo4j.bolt.v1.runtime.BoltConnectionFatality: The client is unauthorized due to authentication failure. at org.neo4j.bolt.v1.runtime.BoltStateMachine.handleFailure(BoltStateMachine.java:742) at org.neo4j.bolt.v1.runtime.BoltStateMachine.handleFailure(BoltStateMachine.java:728) at org.neo4j.bolt.v1.runtime.BoltStateMachine.access$500(BoltStateMachine.java:62) at org.neo4j.bolt.v1.runtime.BoltStateMachine$State$1.init(BoltStateMachine.java:435) at org.neo4j.bolt.v1.runtime.BoltStateMachine.init(BoltStateMachine.java:145) at org.neo4j.bolt.v1.messaging.BoltMessageRouter.lambda$onInit$0(BoltMessageRouter.java:70) at org.neo4j.bolt.runtime.DefaultBoltConnection.processNextBatch(DefaultBoltConnection.java:195) at org.neo4j.bolt.runtime.DefaultBoltConnection.processNextBatch(DefaultBoltConnection.java:143) at org.neo4j.bolt.runtime.ExecutorBoltScheduler.executeBatch(ExecutorBoltScheduler.java:170) at org.neo4j.bolt.runtime.ExecutorBoltScheduler.lambda$scheduleBatchOrHandleError$2(ExecutorBoltScheduler.java:153) at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)[wkq@wkq logs]$ tail -100 debug.utc.log2018-07-04 16:59:53.285+0000 INFO [o.n.k.i.DiagnosticsManager] Id usage:2018-07-04 16:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] ArrayPropertyStore: used=1 high=02018-07-04 16:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] NodeStore: used=0 high=-12018-07-04 16:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] StringPropertyStore: used=1 high=02018-07-04 16:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] PropertyIndexStore: used=0 high=-12018-07-04 16:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] StringPropertyStore: used=1 high=02018-07-04 16:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] ArrayPropertyStore: used=1 high=02018-07-04 16:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] PropertyStore: used=0 high=-12018-07-04 16:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] RelationshipStore: used=0 high=-12018-07-04 16:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] StringPropertyStore: used=1 high=02018-07-04 16:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] RelationshipTypeStore: used=0 high=-12018-07-04 16:59:53.286+0000 INFO [o.n.k.i.DiagnosticsManager] StringPropertyStore: used=1 high=02018-07-04 16:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] LabelTokenStore: used=0 high=-12018-07-04 16:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] SchemaStore: used=1 high=02018-07-04 16:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] RelationshipGroupStore: used=1 high=02018-07-04 16:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] NeoStore: used=15 high=142018-07-04 16:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] --- STARTED diagnostics for NEO_STORE_ID_USAGE END ---2018-07-04 16:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] --- STARTED diagnostics for NEO_STORE_RECORDS START ---2018-07-04 16:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] Neostore records:2018-07-04 16:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] TIME (Creation time): 15306931028992018-07-04 16:59:53.287+0000 INFO [o.n.k.i.DiagnosticsManager] RANDOM_NUMBER (Random number for store id): 89274000696936539012018-07-04 16:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] LOG_VERSION (Current log version): 02018-07-04 16:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] LAST_TRANSACTION_ID (Last committed transaction): 12018-07-04 16:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] STORE_VERSION (Store format version): 160949311551872062018-07-04 16:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] FIRST_GRAPH_PROPERTY (First property record containing graph properties): -12018-07-04 16:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] LAST_CONSTRAINT_TRANSACTION (Last committed transaction containing constraint changes): 02018-07-04 16:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] UPGRADE_TRANSACTION_ID (Transaction id most recent upgrade was performed at): 12018-07-04 16:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] UPGRADE_TIME (Time of last upgrade): 15306931028992018-07-04 16:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] LAST_TRANSACTION_CHECKSUM (Checksum of last committed transaction): 02018-07-04 16:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] UPGRADE_TRANSACTION_CHECKSUM (Checksum of transaction id the most recent upgrade was performed at): 02018-07-04 16:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] LAST_CLOSED_TRANSACTION_LOG_VERSION (Log version where the last transaction commit entry has been written into): 02018-07-04 16:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] LAST_CLOSED_TRANSACTION_LOG_BYTE_OFFSET (Byte offset in the log file where the last transaction commit entry has been written into): 162018-07-04 16:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] LAST_TRANSACTION_COMMIT_TIMESTAMP (Commit time timestamp for last committed transaction): 02018-07-04 16:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] UPGRADE_TRANSACTION_COMMIT_TIMESTAMP (Commit timestamp of transaction the most recent upgrade was performed at): 02018-07-04 16:59:53.288+0000 INFO [o.n.k.i.DiagnosticsManager] --- STARTED diagnostics for NEO_STORE_RECORDS END ---2018-07-04 16:59:53.289+0000 INFO [o.n.k.i.DiagnosticsManager] --- STARTED diagnostics for TRANSACTION_RANGE START ---2018-07-04 16:59:53.289+0000 INFO [o.n.k.i.DiagnosticsManager] Transaction log:2018-07-04 16:59:53.290+0000 INFO [o.n.k.i.DiagnosticsManager] Oldest transaction 2 found in log with version 02018-07-04 16:59:53.290+0000 INFO [o.n.k.i.DiagnosticsManager] --- STARTED diagnostics for TRANSACTION_RANGE END ---2018-07-04 16:59:53.290+0000 INFO [o.n.k.i.DiagnosticsManager] --- STARTED diagnostics for KernelDiagnostics:StoreFiles START ---2018-07-04 16:59:53.290+0000 INFO [o.n.k.i.DiagnosticsManager] Disk space on partition (Total / Free / Free %): 5995761762304 / 5630701182976 / 932018-07-04 16:59:53.290+0000 INFO [o.n.k.i.DiagnosticsManager] Storage files: (filename : modification date - size)2018-07-04 16:59:53.291+0000 INFO [o.n.k.i.DiagnosticsManager] index:2018-07-04 16:59:53.292+0000 INFO [o.n.k.i.DiagnosticsManager] - Total: 2018-07-04T16:31:43+0800 - 0.00 B2018-07-04 16:59:53.293+0000 INFO [o.n.k.i.DiagnosticsManager] neostore: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 16:59:53.293+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.counts.db.a: 2018-07-04T16:31:43+0800 - 96.00 B2018-07-04 16:59:53.293+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 16:59:53.293+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.labelscanstore.db: 2018-07-04T16:59:52+0800 - 40.00 kB2018-07-04 16:59:53.294+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.labeltokenstore.db: 2018-07-04T16:31:42+0800 - 0.00 B2018-07-04 16:59:53.294+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.labeltokenstore.db.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 16:59:53.294+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.labeltokenstore.db.names: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 16:59:53.294+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.labeltokenstore.db.names.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 16:59:53.295+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.nodestore.db: 2018-07-04T16:31:42+0800 - 0.00 B2018-07-04 16:59:53.295+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.nodestore.db.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 16:59:53.295+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.nodestore.db.labels: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 16:59:53.295+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.nodestore.db.labels.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 16:59:53.296+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db: 2018-07-04T16:31:42+0800 - 0.00 B2018-07-04 16:59:53.296+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.arrays: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 16:59:53.296+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.arrays.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 16:59:53.296+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 16:59:53.296+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.index: 2018-07-04T16:31:42+0800 - 0.00 B2018-07-04 16:59:53.297+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.index.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 16:59:53.297+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.index.keys: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 16:59:53.297+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.index.keys.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 16:59:53.297+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.strings: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 16:59:53.297+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.propertystore.db.strings.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 16:59:53.297+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.relationshipgroupstore.db: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 16:59:53.298+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.relationshipgroupstore.db.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 16:59:53.298+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.relationshipstore.db: 2018-07-04T16:31:42+0800 - 0.00 B2018-07-04 16:59:53.298+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.relationshipstore.db.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 16:59:53.298+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.relationshiptypestore.db: 2018-07-04T16:31:42+0800 - 0.00 B2018-07-04 16:59:53.298+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.relationshiptypestore.db.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 16:59:53.299+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.relationshiptypestore.db.names: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 16:59:53.299+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.relationshiptypestore.db.names.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 16:59:53.299+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.schemastore.db: 2018-07-04T16:31:42+0800 - 8.00 kB2018-07-04 16:59:53.299+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.schemastore.db.id: 2018-07-04T16:59:52+0800 - 9.00 B2018-07-04 16:59:53.299+0000 INFO [o.n.k.i.DiagnosticsManager] neostore.transaction.db.0: 2018-07-04T16:58:15+0800 - 88.00 B2018-07-04 16:59:53.299+0000 INFO [o.n.k.i.DiagnosticsManager] store_lock: 2018-07-04T16:31:42+0800 - 0.00 B2018-07-04 16:59:53.300+0000 INFO [o.n.k.i.DiagnosticsManager] Storage summary:2018-07-04 16:59:53.300+0000 INFO [o.n.k.i.DiagnosticsManager] Total size of store: 112.31 kB2018-07-04 16:59:53.300+0000 INFO [o.n.k.i.DiagnosticsManager] Total size of mapped files: 112.00 kB2018-07-04 16:59:53.300+0000 INFO [o.n.k.i.DiagnosticsManager] --- STARTED diagnostics for KernelDiagnostics:StoreFiles END ---2018-07-04 16:59:54.850+0000 INFO [o.n.k.i.DiagnosticsManager] --- SERVER STARTED START ---2018-07-04 16:59:55.649+0000 INFO [o.n.k.i.DiagnosticsManager] --- SERVER STARTED END ---2018-07-04 17:02:17.230+0000 ERROR [o.n.b.r.DefaultBoltConnection] Unexpected error detected in bolt session &apos;1402ecfffe7d5b64-00001f8c-00000001-a464075b467c9fad-e3b5926d&apos;. The client is unauthorized due to authentication failure.org.neo4j.bolt.v1.runtime.BoltConnectionFatality: The client is unauthorized due to authentication failure. at org.neo4j.bolt.v1.runtime.BoltStateMachine.handleFailure(BoltStateMachine.java:742) at org.neo4j.bolt.v1.runtime.BoltStateMachine.handleFailure(BoltStateMachine.java:728) at org.neo4j.bolt.v1.runtime.BoltStateMachine.access$500(BoltStateMachine.java:62) at org.neo4j.bolt.v1.runtime.BoltStateMachine$State$1.init(BoltStateMachine.java:435) at org.neo4j.bolt.v1.runtime.BoltStateMachine.init(BoltStateMachine.java:145) at org.neo4j.bolt.v1.messaging.BoltMessageRouter.lambda$onInit$0(BoltMessageRouter.java:70) at org.neo4j.bolt.runtime.DefaultBoltConnection.processNextBatch(DefaultBoltConnection.java:195) at org.neo4j.bolt.runtime.DefaultBoltConnection.processNextBatch(DefaultBoltConnection.java:143) at org.neo4j.bolt.runtime.ExecutorBoltScheduler.executeBatch(ExecutorBoltScheduler.java:170) at org.neo4j.bolt.runtime.ExecutorBoltScheduler.lambda$scheduleBatchOrHandleError$2(ExecutorBoltScheduler.java:153) at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) References：[1] config_dbms.db.timezone[2] timezone-setting-for-neo4j-log[3] how-do-i-convert-neo4j-logs-from-base-utc-to-local-timezone[4] neo4j 日志的时区问题]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown 使用 笔记]]></title>
    <url>%2F2017%2F03%2F22%2Fmarkdown-notes%2F</url>
    <content type="text"><![CDATA[markdown使用笔记 Welcome to MarkdownPad 2MarkdownPad is a full-featured Markdown editor for Windows. Built exclusively for MarkdownEnjoy first-class Markdown support with easy access to Markdown syntax and convenient keyboard shortcuts. Give them a try: Bold (Ctrl+B) and Italic (Ctrl+I) Quotes (Ctrl+Q) Code blocks (Ctrl+K) Headings 1, 2, 3 (Ctrl+1, Ctrl+2, Ctrl+3) Lists (Ctrl+U and Ctrl+Shift+O) See your changes instantly with LivePreviewDon’t guess if your hyperlink syntax is correct; LivePreview will show you exactly what your document looks like every time you press a key. Make it your ownFonts, color schemes, layouts and stylesheets are all 100% customizable so you can turn MarkdownPad into your perfect editor. A robust editor for advanced Markdown usersMarkdownPad supports multiple Markdown processing engines, including standard Markdown, Markdown Extra (with Table support) and GitHub Flavored Markdown. With a tabbed document interface, PDF export, a built-in image uploader, session management, spell check, auto-save, syntax highlighting and a built-in CSS management interface, there’s no limit to what you can do with MarkdownPad. 表格的使用使用 | 来分隔不同的单元格，使用 - 来分隔表头和其他行： 1234name | age---- | ---LiSi | 12LiHua | 32 name age LiSi 12 LiHua 32 为了美观，可以使用空格对齐不同行的单元格，并在左右两侧都使用 | 来标记单元格边界：1234| name | age || ------- | --- || LiSi | 12 || LiHua | 32 | name age LiSi 12 LiHua 32 123:--- 代表左对齐:--: 代表居中对齐---: 代表右对齐 名称 关键字 调用的js 说明AppleScript applescript shBrushAppleScript.jsActionScript 3.0 actionscript3 , as3 shBrushAS3.jsShell bash , shell shBrushBash.jsColdFusion coldfusion , cf shBrushColdFusion.jsC cpp , c shBrushCpp.jsC# c# , c-sharp , csharp shBrushCSharp.jsCSS css shBrushCss.jsDelphi delphi , pascal , pas shBrushDelphi.jsdiff&amp;patch diff patch shBrushDiff.js 用代码版本库时,遇到代码冲突,其语法就是这个.Erlang erl , erlang shBrushErlang.jsGroovy groovy shBrushGroovy.jsJava java shBrushJava.jsJavaFX jfx , javafx shBrushJavaFX.jsJavaScript js , jscript , javascript shBrushJScript.jsPerl perl , pl , Perl shBrushPerl.jsPHP php shBrushPhp.jstext text , plain shBrushPlain.js 就是普通文本.Python py , python shBrushPython.jsRuby ruby , rails , ror , rb shBrushRuby.jsSASS&amp;SCSS sass , scss shBrushSass.jsScala scala shBrushScala.jsSQL sql shBrushSql.jsVisual Basic vb , vbnet shBrushVb.jsXML xml , xhtml , xslt , html shBrushXml.jsObjective C objc , obj-c shBrushObjectiveC.jsF# f# f-sharp , fsharp shBrushFSharp.jsxpp , dynamics-xpp shBrushDynamics.jsR r , s , splus shBrushR.jsmatlab matlab shBrushMatlab.jsswift swift shBrushSwift.jsGO go , golang shBrushGo.js 1 References[1] writing[2] Learning-Markdown (Markdown 入门参考)[3] 博客园markdown代码块支持的语言[4] 注释（Comment）[5] 复杂excel转html[6] excel csv markdown url html 转各种格式[7] Markdown 语法介绍[8] Markdown: 语法]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j 底层 架构 学习]]></title>
    <url>%2F2017%2F03%2F21%2Fneo4j-internals-notes%2F</url>
    <content type="text"><![CDATA[今天领导问我Neo4j底层架构，调优等问题。平时只顾着优化cypther，这部分内容就没怎么看，决定深入了解Neo4j数据库。 上图 Neo4j底层总体架构 如图，Neo4j 包括Traversals Core API Cypther Node/Relationship Object cache Thread loacl diffs FS Cache HA Record files Transaction log Disk(s) Neo4j的硬盘存储 Neo4j Storage Record Layout 节点 关系这幅图其实也解释了 Neo4j里为什么一个节点可以有多个Label，但是一个关系只能有一个type 反证法：假如一个关系可以有多个type，张三和李四相互认识，张三和王五相互认识，李四和王五不认识。 那么如何表示张三、李四、王五 三个节点间的关系如果一个关系有多个type表示上述例子中的关系反而很复杂了。 References[1] 《Neo4j-Internals》[2] Understanding Database Growth[3] Neo4j的存储结构[4] Neo4j 底层存储结构分析]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字符编码 笔记]]></title>
    <url>%2F2017%2F03%2F18%2Fcharacte-encoding%2F</url>
    <content type="text"><![CDATA[有没有在生活中看到 锟斤拷 这几个字，有没有想过这几个字是怎么写出来来或者怎么发生的？ 工作中经常会因为字符编码问题而头疼，不同编码间的转化也是是比较头疼的一件事 本文介绍了字符编码的原理，并提供了一个Java工具类用于判断字符编码 (1) 字符编码的原理(1.1) ASCII码我们知道，在计算机内部，所有的信息最终都表示为一个二进制的字符串。每一个二进制位(bit)有0和 1两种状态，因此八个二进制位就可以组合出256种状态，这被称为一个字节(byte)。也就是说，一个字节一共可以用来表示256种不同的状态，每一个状态对应一个符号，就是256个符号，从0000000到11111111。 上个世纪60年代，美国制定了一套字符编码，对英语字符与二进制位之间的关系，做了统一规定。这被称为ASCII码，一直沿用至今。 ASCII码一共规定了128个字符的编码，比如空格“SPACE”是32(二进制00100000)，大写的字母A是65(二进制01000001)。这128个符号(包括32个不能打印出来的控制符号)，只占用了一个字节的后面7位，最前面的1位统一规定为0。 (1.2) 非ASCII编码英语用128个符号编码就够了，但是用来表示其他语言，128个符号是不够的。比如，在法语中，字母上方有注音符号，它就无法用ASCII码表示。于是，一些欧洲国家就决定，利用字节中闲置的最高位编入新的符号。比如，法语中的é的编码为130(二进制 10000010)。这样一来，这些欧洲国家使用的编码体系，可以表示最多256个符号。 但是，这里又出现了新的问题。不同的国家有不同的字母，因此，哪怕它们都使用256个符号的编码方式，代表的字母却不一样。比如，130在法语编码中代表了é，在希伯来语编码中却代表了字母Gimel ()，在俄语编码中又会代表另一个符号。但是不管怎样，所有这些编码方式中，0—127表示的符号是一样的，不一样的只是128—255的这一段。 至于亚洲国家的文字，使用的符号就更多了，汉字就多达10万左右。一个字节只能表示256种符号，肯定是不够的，就必须使用多个字节表达一个符号。比如，简体中文常见的编码方式是GB2312，使用两个字节表示一个汉字，所以理论上最多可以表示 256x256=65536个符号。 中文编码的问题需要专文讨论，这篇笔记不涉及。这里只指出，虽然都是用多个字节表示一个符号，但是GB类的汉字编码与后文的Unicode和UTF-8是毫无关系的。 (1.3) UnicodeUnicode字符集(简称为UCS),国际标准组织于1984年4月成立ISO/IEC JTC1/SC2/WG2工作组，针对各国文字、符号进行统一性编码。1991年美国跨国公司成立Unicode Consortium，并于1991年10月与WG2达成协议，采用同一编码字集。目前Unicode是采用16位编码体系，其字符集内容与 ISO10646的BMP(Basic Multilingual Plane)相同。Unicode于1992年6月通过DIS(Draf International Standard)，目前版本V2.0于1996公布，内容包含符号6811个，汉字20902个，韩文拼音11172个，造字区6400个，保留 20249个，共计65534个。Unicode编码后的大小是一样的.例如一个英文字母 “a” 和 一个汉字 “好”，编码后都是占用的空间大小是一样的，都是两个字节！ Unicode可以用来表示所有语言的字符，而且是定长双字节(也有四字节的)编码，包括英文字母在内。所以可以说它是不兼容iso8859-1编码的，也不兼容任何编码。不过，相对于iso8859-1编码来说，uniocode编码只是在前面增加了一个0字节，比如字母’a’为”00 61”。 需要说明的是，定长编码便于计算机处理(注意GB2312/GBK不是定长编码)，而unicode又可以用来表示所有字符，所以在很多软件内部是使用unicode编码来处理的，比如java。 Unicode当然是一个很大的集合，现在的规模可以容纳100多万个符号。每个符号的编码都不一样，比如，U+0639表示阿拉伯字母Ain，U+0041表示英语的大写字母A，U+4E25表示汉字“严”。具体的符号对应表，可以查询 unicode.org，或者专门的汉字对应表。 http://www.chi2ko.com/tool/CJK.htm (1.4) Unicode的问题需要注意的是，Unicode只是一个符号集，它只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储。 比如，汉字“严”的unicode是十六进制数4E25，转换成二进制数足足有15位(100111000100101)，也就是说这个符号的表示至少需要2个字节。表示其他更大的符号，可能需要3个字节或者4个字节，甚至更多。 这里就有两个严重的问题，第一个问题是，如何才能区别unicode和ascii？计算机怎么知道三个字节表示一个符号，而不是分别表示三个符号呢？第二个问题是，我们已经知道，英文字母只用一个字节表示就够了，如果unicode统一规定，每个符号用三个或四个字节表示，那么每个英文字母前都必然有二到三个字节是0，这对于存储来说是极大的浪费，文本文件的大小会因此大出二三倍，这是无法接受的。 它们造成的结果是：1)出现了unicode的多种存储方式，也就是说有许多种不同的二进制格式，可以用来表示unicode。2)unicode在很长一段时间内无法推广，直到互联网的出现。 (1.5) UTF-8互联网的普及，强烈要求出现一种统一的编码方式。UTF-8就是在互联网上使用最广的一种unicode的实现方式。其他实现方式还包括UTF-16和UTF-32，不过在互联网上基本不用。重复一遍，这里的关系是，UTF-8是Unicode的实现方式之一。 UTF-8最大的一个特点，就是它是一种变长的编码方式。它可以使用1~4个字节表示一个符号，根据不同的符号而变化字节长度。 UTF-8的编码规则很简单，只有二条：1)对于单字节的符号，字节的第一位设为0，后面7位为这个符号的unicode码。因此对于英语字母，UTF-8编码和ASCII码是相同的。2)对于n字节的符号(n&gt;1)，第一个字节的前n位都设为1，第n+1位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的unicode码。 下表总结了编码规则，字母x表示可用编码的位。Unicode符号范围 | UTF-8编码方式(十六进制) | (二进制)——————–+———————————————0000 0000-0000 007F | 0xxxxxxx0000 0080-0000 07FF | 110xxxxx 10xxxxxx0000 0800-0000 FFFF | 1110xxxx 10xxxxxx 10xxxxxx0001 0000-0010 FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 下面，还是以汉字“严”为例，演示如何实现UTF-8编码。已知“严”的unicode是4E25(100111000100101)，根据上表，可以发现 4E25处在第三行的范围内(0000 0800-0000 FFFF)，因此“严”的UTF-8编码需要三个字节，即格式是“1110xxxx 10xxxxxx 10xxxxxx”。然后，从“严”的最后一个二进制位开始，依次从后向前填入格式中的x，多出的位补0。这样就得到了，“严”的UTF-8编码是 “11100100 10111000 10100101”，转换成十六进制就是E4B8A5。 (1.6) Unicode与UTF-8之间的转换通过上一节的例子，可以看到“严”的Unicode码是4E25，UTF-8编码是E4B8A5，两者是不一样的。它们之间的转换可以通过程序实现。 在Windows平台下，有一个最简单的转化方法，就是使用内置的记事本小程序Notepad.exe。打开文件后，点击“文件”菜单中的“另存为”命令，会跳出一个对话框，在最底部有一个“编码”的下拉条。 里面有四个选项：ANSI，Unicode，Unicode big endian 和 UTF-8。1)ANSI是默认的编码方式。对于英文文件是ASCII编码，对于简体中文文件是GB2312编码(只针对Windows简体中文版，如果是繁体中文版会采用Big5码)。2)Unicode编码指的是UCS-2编码方式，即直接用两个字节存入字符的Unicode码。这个选项用的little endian格式。3)Unicode big endian编码与上一个选项相对应。我在下一节会解释little endian和big endian的涵义。4)UTF-8编码，也就是上一节谈到的编码方法。 选择完”编码方式“后，点击”保存“按钮，文件的编码方式就立刻转换好了。 (1.7) Little endian和Big endian上一节已经提到，Unicode码可以采用UCS-2格式直接存储。以汉字”严“为例，Unicode 码是4E25，需要用两个字节存储，一个字节是4E，另一个字节是25。存储的时候，4E在前，25在后，就是Big endian方式；25在前，4E在后，就是Little endian方式。 这两个古怪的名称来自英国作家斯威夫特的《格列佛游记》。在该书中，小人国里爆发了内战，战争起因是人们争论，吃鸡蛋时究竟是从大头(Big-Endian)敲开还是从小头(Little-Endian)敲开。为了这件事情，前后爆发了六次战争，一个皇帝送了命，另一个皇帝丢了王位。 因此，第一个字节在前，就是”大头方式“(Big endian)，第二个字节在前就是”小头方式“(Little endian)。 那么很自然的，就会出现一个问题：计算机怎么知道某一个文件到底采用哪一种方式编码？ Unicode规范中定义，每一个文件的最前面分别加入一个表示编码顺序的字符，这个字符的名字叫做”零宽度非换行空格“(ZERO WIDTH NO-BREAK SPACE)，用FEFF表示。这正好是两个字节，而且FF比FE大1。 如果一个文本文件的头两个字节是FE FF，就表示该文件采用大头方式；如果头两个字节是FF FE，就表示该文件采用小头方式。 (1.8) 实例下面，举一个实例。 打开”记事本“程序Notepad.exe，新建一个文本文件，内容就是一个”严“字，依次采用ANSI，Unicode，Unicode big endian 和 UTF-8编码方式保存。 然后，用文本编辑软件UltraEdit中的”十六进制功能“，观察该文件的内部编码方式。 1)ANSI：文件的编码就是两个字节“D1 CF”，这正是“严”的GB2312编码，这也暗示GB2312是采用大头方式存储的。2)Unicode：编码是四个字节“FF FE 25 4E”，其中“FF FE”表明是小头方式存储，真正的编码是4E25。3)Unicode big endian：编码是四个字节“FE FF 4E 25”，其中“FE FF”表明是大头方式存储。4)UTF-8：编码是六个字节“EF BB BF E4 B8 A5”，前三个字节“EF BB BF”表示这是UTF-8编码，后三个“E4B8A5”就是“严”的具体编码，它的存储顺序与编码顺序是一致的。 (1.9) 国标(1.9.1) GB码 全称是GB2312-80《信息交换用汉字编码字符集基本集》，1980年发布，是中文信息处理的国家标准，在大陆及海外使用简体中文的地区(如新加坡等)是强制使用的唯一中文编码。P-Windows3.2和苹果OS就是以GB2312为基本汉字编码， Windows 95/98则以GBK为基本汉字编码、但兼容支持GB2312。 双字节编码范围：A1A1~FEFEA1-A9：符号区，包含682个符号B0-F7：汉字区，包含6763个汉字 (1.9.2) GB2312 GB2312(1980年)一共收录了7445个字符，包括6763个汉字和682个其它符号。汉字区的内码范围高字节从B0-F7，低字节从 A1-FE，占用的码位是72*94=6768。其中有5个空位是D7FA-D7FE。GB2312-80中共收录了7545个字符，用两个字节编码一个字符。每个字符最高位为0。GB2312-80编码简称国标码。 GB2312支持的汉字太少。1995年的汉字扩展规范GBK1.0收录了21886个符号，它分为汉字区和图形符号区。汉字区包括21003个字符。 (1.9.3) GB12345-901990年制定了繁体字的编码标准GB12345-90《信息交换用汉字编码字符集第一辅助集》，目的在于规范必须使用繁体字的各种场合，以及古籍整理等。该标准共收录6866个汉字(比GB2312多103个字，其它厂商的字库大多不包括这些字)，纯繁体的字大概有2200余个。双字节编码范围：A1A1~FEFEA1-A9：符号区，增加竖排符号B0-F9：汉字区，包含6866个汉字 (1.9.4) GBK GBK编码(Chinese Internal Code Specification)是中国大陆制订的、等同于UCS的新的中文编码扩展国家标准。gbk编码能够用来同时表示繁体字和简体字，而gb2312只能表示简体字，gbk是兼容gb2312编码的。GBK工作小组于1995年10月，同年12月完成GBK规范。该编码标准兼容GB2312，共收录汉字 21003个、符号883个，并提供1894个造字码位，简、繁体字融于一库。Windows95/98简体中文版的字库表层编码就采用的是GBK，通过 GBK与UCS之间一一对应的码表与底层字库联系。英文名：Chinese Internal Code Specification中文名：汉字内码扩展规范1.0版双字节编码，GB2312-80的扩充，在码位上和GB2312-80兼容范围：8140~FEFE(剔除xx7F)共23940个码位包含21003个汉字，包含了ISO/IEC 10646-1中的全部中日韩汉字 (2) 字符编码的工具类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import java.io.BufferedInputStream;import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStream;/** * 字符工具类 */public class CharsetUtil &#123; /** * * @param file * @return * @throws IOException */ public static String getFilecharset(File file) throws IOException &#123; return getInputStreamCharset(new FileInputStream(file)); &#125; /** * * @param inputStream * @return * @throws IOException */ public static String getInputStreamCharset(InputStream inputStream) throws IOException &#123; String charset = "GBK"; byte[] first3Bytes = new byte[3]; boolean checked = false; BufferedInputStream bis = new BufferedInputStream(inputStream); bis.mark(0); int read = bis.read(first3Bytes, 0, 3); if (read == -1) &#123; return charset; // 文件编码为 ANSI，在中国，windows中文版使用GBK编码 &#125; else if (first3Bytes[0] == (byte) 0xFF &amp;&amp; first3Bytes[1] == (byte) 0xFE) &#123; charset = "UTF-16LE"; // 文件编码为 Unicode checked = true; &#125; else if (first3Bytes[0] == (byte) 0xFE &amp;&amp; first3Bytes[1] == (byte) 0xFF) &#123; charset = "UTF-16BE"; // 文件编码为 Unicode big endian checked = true; &#125; else if (first3Bytes[0] == (byte) 0xEF &amp;&amp; first3Bytes[1] == (byte) 0xBB &amp;&amp; first3Bytes[2] == (byte) 0xBF) &#123; charset = "UTF-8"; // 文件编码为 UTF-8 checked = true; &#125; bis.reset(); if (!checked) &#123; while ((read = bis.read()) != -1) &#123; if (read &gt;= 0xF0) break; if (0x80 &lt;= read &amp;&amp; read &lt;= 0xBF) // 单独出现BF以下的，也算是GBK break; if (0xC0 &lt;= read &amp;&amp; read &lt;= 0xDF) &#123; read = bis.read(); if (0x80 &lt;= read &amp;&amp; read &lt;= 0xBF) // 双字节 (0xC0 - 0xDF) // (0x80 // - 0xBF),也可能在GB编码内 continue; else break; &#125; else if (0xE0 &lt;= read &amp;&amp; read &lt;= 0xEF) &#123;// 也有可能出错，但是几率较小 read = bis.read(); if (0x80 &lt;= read &amp;&amp; read &lt;= 0xBF) &#123; read = bis.read(); if (0x80 &lt;= read &amp;&amp; read &lt;= 0xBF) &#123; charset = "UTF-8"; break; &#125; else break; &#125; else break; &#125; &#125; &#125; bis.close(); return charset; &#125;&#125; (3) 根据Unicode区分中英文及标点符号等(常用的unicode编码范围)123456789101112Unicode编码表中基本拉丁字符 表示范围是 0020 - 007F 其中 0-9 表示范围是 0030 - 0039 其中 A-Z 表示范围是 0041 - 005A 其中 a-z 表示范围是 0061 - 007A 其中 英文标点符号 范围是 0020 - 002F 003A - 0040 005B - 0060 007B - 007F 中日韩统一表意文字 表示范围 4E00 - 9FFF 其中汉字的编码范围是 4E00 - 9FA5 (0x是十六进制) 1234567891011121314// 去掉不是汉字的字符// 把不是汉字的替换为空str.replaceAll(&quot;[^\u4E00-\u9FFF]&quot;, &quot;&quot;);//str.replaceAll(&quot;[^\u0030-\u0039|^\u0041-\u005A|^\u0061-\u007A|^\u2E80-\u2FFF|^\u3040-\u9FFF]&quot;, &quot; &quot;)// 去掉不是数字的字符// 把不是数字的替换为空 str.replaceAll(&quot;[^\u0030-\u0039]&quot;,&quot;&quot;);// 以中日韩开头的str.matches(&quot;^[\u2E80-\u2FFF|\u3040-\u9FFF].+&quot;) References[1] Unicode®字符百科[2] 关于各种编码格式[3] java读取文件字符集示例方法_java[4] 字体编辑用中日韩汉字Unicode编码表[5] java自动探测文件的字符编码[6] cpdetector]]></content>
      <categories>
        <category>computer</category>
      </categories>
      <tags>
        <tag>encoding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j 笔记]]></title>
    <url>%2F2017%2F03%2F17%2Fneo4j-notes%2F</url>
    <content type="text"><![CDATA[Neo4j使用笔记 1. Neo4j介绍 Neo4j是一种NoSQL数据库，原理是数学里的图论。 常见的SQL数据库有MySQL Oracle等 常见的NoSQL数据库有Redis ES Mongdb Neo4j等 近几年比较流行的的图数据库有Neo4j Titan OrientDB Sparksee Virtuso ArangoDb Airaph GraphDB GraphBase等 Neo4j数据库比较适合处理关系，比如人和人之间的社交关系。 比较成功的应用有 领英 FaceBook Twitter 2. Neo4j下载、安装、配置 Neo4j是开源免费的图数据库，社区版代码开源，企业版代码除集群相关的代码，其余代码全部开源 还有一点，社区版免费，企业版收费。 社区版最多允许 2^35 个节点，2^35 个关系，2^36 个属性 2.1 下载 Neo4j所有版本的下载地址 linux unix 推荐使用 .tar.gz包，windows推荐使用.zip包 这样做的好处就是解压完就可以使用，对目录结构、配置等比较了解 windows上使用.exe安装的用户经常找不见配置，load csv以及其他操作时经常找不见目录 2.2 安装使用2.2.1 linux下安装使用 tar -zxvf neo4j-community-3.1.2.tar.gz 进入bin目录，使用 neo4j console 启动Neo4j数据库，如果想让Neo4j后台运行，使用 neo4j start 启动 在浏览器里输入http://localhost:7474来访问数据库，默认用户名、密码: neo4j/neo4j 如果想自定义配置，可以在${NEO4J_HOME}/conf/neo4j.conf修改对应配置 linux用户注意:使用超级用户修改 /etc/security/limits.conf 文件，允许当前用户(neo4j)打开40000个文件 12neo4j soft nofile 40000neo4j hard nofile 40000 修改 /etc/pam.d/su 文件 1session required pam_limits.so 设置neo4j开机启动 vim /etc/rc.d/rc.local 在文件最后添加如下命令：/usr/share/neo4j/bin/neo4j start 其中 /usr/share/neo4j/bin/ 是Neo4j的安装路径 2.2.2 windows下安装使用 下载的压缩包neo4j-community-3.1.2.zip，解压完就可以用。 解压完进入bin目录，输入 neo4j console 就可以看到neo4j数据库启动了 在浏览器里输入http://localhost:7474来访问数据库，默认用户名、密码: neo4j/neo4j 如果想自定义配置，可以在${NEO4J_HOME}/conf/neo4j.conf修改对应配置 2.3 配置常用配置123456#设置可以通过ip当问Neo4j数据库dbms.connectors.default_listen_address=0.0.0.0 #历史版本请修改dbms.connector.http.address=0.0.0.0:7474org.neo4j.server.webserver.address=0.0.0.0 neo4j数据库设置初始密码1bin/neo4j-admin set-initial-password yourpassword 3. Neo4j使用 Neo4j里非常重要的一些思想，一个节点、一条边就是一个对象 节点可以有多个Label、边只能有一个RelationShip Neo4j是no schema的数据库，导入数据前不需要定义结构 不要用关系型数据库的思维来对待Neo4j A row is a node A table name is a label name Properties Both nodes and relationships can have properties. Properties are named values where the name is a string. The supported property values are: • Numeric values, • String values, • Boolean values, • Collections of any other type of value. Labels have an id space of an int, meaning the maximum number of labels the database can contain is roughly 2 billion. PathsA path is one or more nodes with connecting relationships, typically retrieved as a query or traversal result Neo4j is a schema-optional graph database You can use Neo4j without any schema. Optionally you can introduce it in order to gain performance or modeling benefits. This allows a way of working where the schema does not get in your way until you are at a stage where you want to reap the benefits of having one. IndexsPerformance is gained by creating indexes, which improve the speed of looking up nodes in the database. Neo4j启动后动态修改配置123456789//CALL dbms.setConfigValue(&apos;dbms.logs.query.enabled&apos;, &apos;true&apos;)dbms.checkpoint.iops.limit Limit the number of IOs the background checkpoint process will consume per second.dbms.logs.query.enabled Log executed queries that take longer than the configured threshold, dbms.logs.query.threshold.dbms.logs.query.rotation.keep_number Maximum number of history files for the query log.dbms.logs.query.rotation.size The file size in bytes at which the query log will auto-rotate.dbms.logs.query.threshold If the execution of query takes more time than this threshold, the query is logged - provided query logging is enabled.dbms.transaction.timeout The maximum time interval of a transaction within which it should be completed. Cypher1234567891011121314151617181920212223242526272829:help 帮助页面:schema 查看数据库结构:schema ls -l :Person:server change-password // 修改密码CALL dbms.changePassword(&quot;newpassword&quot;) // (旧版本)修改密码:server connect 连接:play sysinfo 查看系统信息// List node labels 查询所有的labelCALL db.labels()// List relationship types 查询所有的typeCALL db.relationshipTypes()// What is related, and how 查询数据里的节点和关系 类似于 SQL的descCALL db.schema()// List functionsCALL dbms.functions()// List proceduresCALL dbms.procedures()CALL dbms.listQueries() ;CALL dbms.killQuery(queryId); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485// delete single node// 删除id=1的节点match (n:DictEntity) where id(n) = 1 delete n;// delete a node and connected relationships// 删除节点和关系match (n:DictEntity &#123;name: &apos;zhangsan&apos;&#125;)-[r]-() delete r,n// delete all nodes and relationships// 删除所有节点和关系match (n) OPTIONAL match (n)-[r]-() delete n,r// delete all nodes and relationships// 删除所有节点和关系// 容易内存溢出match (n) detach delete n;match (n)-[r]-() where n.name = &apos;词典1&apos; delete r 删除关系 match (n:DictEntity) where n.name=&quot;词典1&quot; delete n 删除节点match (n:DictEntity &#123;name:&quot;词典1&quot;&#125;) delete n// Count all nodes// 查询一共有多少节点match (n) RETURN count(n)// Count all relationships// 查询一共有多少关系 // 不带方向的话结果是2倍match ()--&gt;() RETURN count(*);match ()-[r]-&gt;() return count(r);// What kind of nodes exist// Sample some nodes, reporting on property and relationship counts per node.match (n) where rand() &lt;= 0.1RETURNDISTINCT labels(n),count(*) AS SampleSize,avg(size(keys(n))) as Avg_PropertyCount,min(size(keys(n))) as Min_PropertyCount,max(size(keys(n))) as Max_PropertyCount,avg(size( (n)-[]-() ) ) as Avg_RelationshipCount,min(size( (n)-[]-() ) ) as Min_RelationshipCount,max(size( (n)-[]-() ) ) as Max_RelationshipCount// What is related, and how// Sample the graph, reporting the patterns of connected labels,// with min, max, avg degrees and associated node and relationship properties.match (n) where rand() &lt;= 0.1match (n)-[r]-&gt;(m)WITH n, type(r) as via, mRETURN labels(n) as from,reduce(keys = [], keys_n in collect(keys(n)) | keys + filter(k in keys_n where NOT k IN keys)) as props_from,via,labels(m) as to,reduce(keys = [], keys_m in collect(keys(m)) | keys + filter(k in keys_m where NOT k IN keys)) as props_to,count(*) as freq// 在用户结点的用户名属性上创建索引 (除了结点名和字段名，cypther不区分大小写)create index on :Person(name); // 删除索引drop index on :Person(name);create constraint on (p:Person) assert p.name is unique; //CREATE CONSTRAINT ON (book:Book) ASSERT book.isbn IS UNIQUE; // 在Book对象isbn属性上创建唯一性约束// 删除Person类别上的name属性 索引drop constraint on (p:Person) assert p.name is unique;// 删除isbn属性上的唯一性约束DROP CONSTRAINT ON (book:Book) ASSERT book.isbn IS UNIQUE //DROP CONSTRAINT ON (book:Book) ASSERT exists(book.isbn)PROFILE 在查询前加上prifile可以查看数据库查询的详细流程match (a:Person), (b:Person) where a.name = &apos;zhangsan&apos; and b.name = &apos;lisi&apos;merge (a)-[r:RELTYPE]-&gt;(b) return r 1234567891011121314151617181920212223242526272829303132// 模糊匹配match (n:Person) where n.name =~ '张.*' return n// 包含match (n:Person) where n.name contains '张' return n;// 去重match (n:Person) with n.name as name return distinct name;// Count all nodes // 查询一共有多少节点match (n) RETURN count(n)// Count all relationships// 查询一共有多少关系match ()--&gt;() RETURN count(*);// Get some data 随便查一些数据match (n1)-[r]-&gt;(n2) RETURN r, n1, n2 LIMIT 25// 查询一共有多少种节点call db.labels();match (n) return distinct label(n);// 查询一共有多少关系call db.relationshipTypes()// 查询数据库里的所有属性match (n) unwind keys(n) as allkeys return distinct allkeys; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 查询关系最多的节点// 实际使用时，最好对n加个范围，要不然 全图扫描// 使用with 和 别名，能减少一次count(*)的查询match (n:Movie)--() with n.title as title, count(*) as count return title, count order by count desc limit 1;match (n:Movie)-[r]-() with n.tile as title, count(r) as count return title, count order by count desc limit 1;// 查询孤立节点match (n) where not (n)--() return id(n);// 查询有3条关系的节点match (n:Test)--&gt;(m) with count(*) as count, n as result where count =3 return result limit 1;// 查询有3度关系的节点match (n:Test)-[r:RelationShip*3]-&gt;(m) return n limit 1;// 查询有1到3度关系的节点match (n:Test)-[r:RelationShip*1..3]-&gt;(m) return n limit 10;match (p:Person) where id(p) &gt; 184 set p.number=p.序号, p.name=p.姓名, p.class = p.班级, p.sex = p.性别 remove p.序号,p.姓名,p.班级,p.性别 return p;// Person对象有个sex属性，因为业务需要想改成gender属性match (p:Person &#123;name:'张三'&#125;) set p.gender = p.sex remove p.sex return p// 关系r的名字叫IsFriend，因为业务需要改成汉语的名字match (a)-[r:IsFrend]-&gt;(b) create (a)-[r2:朋友]-&gt;(b) set r2.id = r.id delete rmatch (p1:Person)-[r:isFrend*1..6 &#123;friend:1&#125;]-&gt;(p2:Person) return p1,r,p2match (a:A)-[r1:AB]-(b:B)-[r2:BC]-(c:C) where not (a)-[:AC]-(c) return a,r1,b,r2,cmatch (n:Person &#123;name:'lisi'&#125;) with n skip 1 delete n ;(a:Person &#123;name:'a'&#125;)-[r:RelationShip &#123;date:'2017-06-22 12:00:00'&#125;]-&gt;(c:Person &#123;name:'c'&#125;)&lt;-[r2:RelationShip &#123;date:'2017-06-22 12:30:00'&#125;]-(b:Person &#123;name:'b'&#125;)(d:Person &#123;name:'d'&#125;)-[r:RelationShip &#123;date:'2017-06-22 12:00:00'&#125;]-&gt;(e:Person &#123;name:'e'&#125;)想删除a, d, e// 创建节点merge (a:Person &#123;name:'a'&#125;)-[r:RelationShip &#123;date:'2017-06-22 12:00:00'&#125;]-&gt;(c:Person &#123;name:'c'&#125;)&lt;-[r2:RelationShip &#123;date:'2017-06-22 12:30:00'&#125;]-(b:Person &#123;name:'b'&#125;) return r,r2;merge (d:Person &#123;name:'d'&#125;)-[r:RelationShip &#123;date:'2017-06-22 12:00:00'&#125;]-&gt;(e:Person &#123;name:'e'&#125;) return r// 分三次执行match (n:Person)-[r]-&gt;(m:Person) where (r.date &gt;='2017-06-22 11:00:00' and r.date&lt;='2017-06-22 12:29:59') set n.flag = '1' , m.flag = '1' delete r ;match (n:Person)-[r]-(m:Person) with count(r) as count , n where count &gt; 0 set n.flag = '0' return count,n;match (n:Person) where n.flag = '1' delete n; As the indexes are created after the nodes are inserted, their population happens asynchronously, so we use schema await (a shell command) to block until they are populated.schema await [r:Person*3..4] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354neo4j-sh (?)$ CALL dbms.procedures();+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| name | signature | description |+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| &quot;db.awaitIndex&quot; | &quot;db.awaitIndex(index :: STRING?, timeOutSeconds = 300 :: INTEGER?) :: VOID&quot; | &quot;Wait for an index to come online (for example: CALL db.awaitIndex(&quot;:Person(name)&quot;)).&quot; || &quot;db.awaitIndexes&quot; | &quot;db.awaitIndexes(timeOutSeconds = 300 :: INTEGER?) :: VOID&quot; | &quot;Wait for all indexes to come online (for example: CALL db.awaitIndexes(&quot;500&quot;)).&quot; || &quot;db.constraints&quot; | &quot;db.constraints() :: (description :: STRING?)&quot; | &quot;List all constraints in the database.&quot; || &quot;db.createLabel&quot; | &quot;db.createLabel(newLabel :: STRING?) :: VOID&quot; | &quot;Create a label&quot; || &quot;db.createProperty&quot; | &quot;db.createProperty(newProperty :: STRING?) :: VOID&quot; | &quot;Create a Property&quot; || &quot;db.createRelationshipType&quot; | &quot;db.createRelationshipType(newRelationshipType :: STRING?) :: VOID&quot; | &quot;Create a RelationshipType&quot; || &quot;db.index.explicit.addNode&quot; | &quot;db.index.explicit.addNode(indexName :: STRING?, node :: NODE?, key :: STRING?, value :: ANY?) :: (success :: BOOLEAN?)&quot; | &quot;Add a node to an explicit index based on a specified key and value&quot; || &quot;db.index.explicit.addRelationship&quot; | &quot;db.index.explicit.addRelationship(indexName :: STRING?, relationship :: RELATIONSHIP?, key :: STRING?, value :: ANY?) :: (success :: BOOLEAN?)&quot; | &quot;Add a relationship to an explicit index based on a specified key and value&quot; || &quot;db.index.explicit.auto.searchNodes&quot; | &quot;db.index.explicit.auto.searchNodes(query :: ANY?) :: (node :: NODE?, weight :: FLOAT?)&quot; | &quot;Search nodes in explicit automatic index. Replaces `START n=node:node_auto_index(&apos;key:foo*&apos;)`&quot; || &quot;db.index.explicit.auto.searchRelationships&quot; | &quot;db.index.explicit.auto.searchRelationships(query :: ANY?) :: (relationship :: RELATIONSHIP?, weight :: FLOAT?)&quot; | &quot;Search relationship in explicit automatic index. Replaces `START r=relationship:relationship_auto_index(&apos;key:foo*&apos;)`&quot; || &quot;db.index.explicit.auto.seekNodes&quot; | &quot;db.index.explicit.auto.seekNodes(key :: STRING?, value :: ANY?) :: (node :: NODE?)&quot; | &quot;Get node from explicit automatic index. Replaces `START n=node:node_auto_index(key = &apos;A&apos;)`&quot; || &quot;db.index.explicit.auto.seekRelationships&quot; | &quot;db.index.explicit.auto.seekRelationships(key :: STRING?, value :: ANY?) :: (relationship :: RELATIONSHIP?)&quot; | &quot;Get relationship from explicit automatic index. Replaces `START r=relationship:relationship_auto_index(key = &apos;A&apos;)`&quot; || &quot;db.index.explicit.drop&quot; | &quot;db.index.explicit.drop(indexName :: STRING?) :: (type :: STRING?, name :: STRING?, config :: MAP?)&quot; | &quot;Remove an explicit index - YIELD type,name,config&quot; || &quot;db.index.explicit.existsForNodes&quot; | &quot;db.index.explicit.existsForNodes(indexName :: STRING?) :: (success :: BOOLEAN?)&quot; | &quot;Check if a node explicit index exists&quot; || &quot;db.index.explicit.existsForRelationships&quot; | &quot;db.index.explicit.existsForRelationships(indexName :: STRING?) :: (success :: BOOLEAN?)&quot; | &quot;Check if a relationship explicit index exists&quot; || &quot;db.index.explicit.forNodes&quot; | &quot;db.index.explicit.forNodes(indexName :: STRING?) :: (type :: STRING?, name :: STRING?, config :: MAP?)&quot; | &quot;Get or create a node explicit index - YIELD type,name,config&quot; || &quot;db.index.explicit.forRelationships&quot; | &quot;db.index.explicit.forRelationships(indexName :: STRING?) :: (type :: STRING?, name :: STRING?, config :: MAP?)&quot; | &quot;Get or create a relationship explicit index - YIELD type,name,config&quot; || &quot;db.index.explicit.list&quot; | &quot;db.index.explicit.list() :: (type :: STRING?, name :: STRING?, config :: MAP?)&quot; | &quot;List all explicit indexes - YIELD type,name,config&quot; || &quot;db.index.explicit.removeNode&quot; | &quot;db.index.explicit.removeNode(indexName :: STRING?, node :: NODE?, key :: STRING?) :: (success :: BOOLEAN?)&quot; | &quot;Remove a node from an explicit index with an optional key&quot; || &quot;db.index.explicit.removeRelationship&quot; | &quot;db.index.explicit.removeRelationship(indexName :: STRING?, relationship :: RELATIONSHIP?, key :: STRING?) :: (success :: BOOLEAN?)&quot; | &quot;Remove a relationship from an explicit index with an optional key&quot; || &quot;db.index.explicit.searchNodes&quot; | &quot;db.index.explicit.searchNodes(indexName :: STRING?, query :: ANY?) :: (node :: NODE?, weight :: FLOAT?)&quot; | &quot;Search nodes in explicit index. Replaces `START n=node:nodes(&apos;key:foo*&apos;)`&quot; || &quot;db.index.explicit.searchRelationships&quot; | &quot;db.index.explicit.searchRelationships(indexName :: STRING?, query :: ANY?) :: (relationship :: RELATIONSHIP?, weight :: FLOAT?)&quot; | &quot;Search relationship in explicit index. Replaces `START r=relationship:relIndex(&apos;key:foo*&apos;)`&quot; || &quot;db.index.explicit.searchRelationshipsBetween&quot; | &quot;db.index.explicit.searchRelationshipsBetween(indexName :: STRING?, in :: NODE?, out :: NODE?, query :: ANY?) :: (relationship :: RELATIONSHIP?, weight :: FLOAT?)&quot; | &quot;Search relationship in explicit index, starting at the node &apos;in&apos; and ending at &apos;out&apos;.&quot; || &quot;db.index.explicit.searchRelationshipsIn&quot; | &quot;db.index.explicit.searchRelationshipsIn(indexName :: STRING?, in :: NODE?, query :: ANY?) :: (relationship :: RELATIONSHIP?, weight :: FLOAT?)&quot; | &quot;Search relationship in explicit index, starting at the node &apos;in&apos;.&quot; || &quot;db.index.explicit.searchRelationshipsOut&quot; | &quot;db.index.explicit.searchRelationshipsOut(indexName :: STRING?, out :: NODE?, query :: ANY?) :: (relationship :: RELATIONSHIP?, weight :: FLOAT?)&quot; | &quot;Search relationship in explicit index, ending at the node &apos;out&apos;.&quot; || &quot;db.index.explicit.seekNodes&quot; | &quot;db.index.explicit.seekNodes(indexName :: STRING?, key :: STRING?, value :: ANY?) :: (node :: NODE?)&quot; | &quot;Get node from explicit index. Replaces `START n=node:nodes(key = &apos;A&apos;)`&quot; || &quot;db.index.explicit.seekRelationships&quot; | &quot;db.index.explicit.seekRelationships(indexName :: STRING?, key :: STRING?, value :: ANY?) :: (relationship :: RELATIONSHIP?)&quot; | &quot;Get relationship from explicit index. Replaces `START r=relationship:relIndex(key = &apos;A&apos;)`&quot; || &quot;db.indexes&quot; | &quot;db.indexes() :: (description :: STRING?, label :: STRING?, properties :: LIST? OF STRING?, state :: STRING?, type :: STRING?, provider :: MAP?)&quot; | &quot;List all indexes in the database.&quot; || &quot;db.labels&quot; | &quot;db.labels() :: (label :: STRING?)&quot; | &quot;List all labels in the database.&quot; || &quot;db.propertyKeys&quot; | &quot;db.propertyKeys() :: (propertyKey :: STRING?)&quot; | &quot;List all property keys in the database.&quot; || &quot;db.relationshipTypes&quot; | &quot;db.relationshipTypes() :: (relationshipType :: STRING?)&quot; | &quot;List all relationship types in the database.&quot; || &quot;db.resampleIndex&quot; | &quot;db.resampleIndex(index :: STRING?) :: VOID&quot; | &quot;Schedule resampling of an index (for example: CALL db.resampleIndex(&quot;:Person(name)&quot;)).&quot; || &quot;db.resampleOutdatedIndexes&quot; | &quot;db.resampleOutdatedIndexes() :: VOID&quot; | &quot;Schedule resampling of all outdated indexes.&quot; || &quot;db.schema&quot; | &quot;db.schema() :: (nodes :: LIST? OF NODE?, relationships :: LIST? OF RELATIONSHIP?)&quot; | &quot;Show the schema of the data.&quot; || &quot;dbms.changePassword&quot; | &quot;dbms.changePassword(password :: STRING?) :: VOID&quot; | &quot;Change the current user&apos;s password. Deprecated by dbms.security.changePassword.&quot; || &quot;dbms.components&quot; | &quot;dbms.components() :: (name :: STRING?, versions :: LIST? OF STRING?, edition :: STRING?)&quot; | &quot;List DBMS components and their versions.&quot; || &quot;dbms.functions&quot; | &quot;dbms.functions() :: (name :: STRING?, signature :: STRING?, description :: STRING?)&quot; | &quot;List all user functions in the DBMS.&quot; || &quot;dbms.listConfig&quot; | &quot;dbms.listConfig(searchString = :: STRING?) :: (name :: STRING?, description :: STRING?, value :: STRING?)&quot; | &quot;List the currently active config of Neo4j.&quot; || &quot;dbms.procedures&quot; | &quot;dbms.procedures() :: (name :: STRING?, signature :: STRING?, description :: STRING?)&quot; | &quot;List all procedures in the DBMS.&quot; || &quot;dbms.queryJmx&quot; | &quot;dbms.queryJmx(query :: STRING?) :: (name :: STRING?, description :: STRING?, attributes :: MAP?)&quot; | &quot;Query JMX management data by domain and name. For instance, &quot;org.neo4j:*&quot;&quot; || &quot;dbms.security.changePassword&quot; | &quot;dbms.security.changePassword(password :: STRING?) :: VOID&quot; | &quot;Change the current user&apos;s password.&quot; || &quot;dbms.security.createUser&quot; | &quot;dbms.security.createUser(username :: STRING?, password :: STRING?, requirePasswordChange = true :: BOOLEAN?) :: VOID&quot; | &quot;Create a new user.&quot; || &quot;dbms.security.deleteUser&quot; | &quot;dbms.security.deleteUser(username :: STRING?) :: VOID&quot; | &quot;Delete the specified user.&quot; || &quot;dbms.security.listUsers&quot; | &quot;dbms.security.listUsers() :: (username :: STRING?, flags :: LIST? OF STRING?)&quot; | &quot;List all local users.&quot; || &quot;dbms.security.showCurrentUser&quot; | &quot;dbms.security.showCurrentUser() :: (username :: STRING?, flags :: LIST? OF STRING?)&quot; | &quot;Show the current user. Deprecated by dbms.showCurrentUser.&quot; || &quot;dbms.showCurrentUser&quot; | &quot;dbms.showCurrentUser() :: (username :: STRING?, flags :: LIST? OF STRING?)&quot; | &quot;Show the current user.&quot; |+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+46 rows96 msneo4j-sh (?)$ github例子 https://github.com/neo4j-examples/movies-java-spring-data-neo4j-4 批量执行cpyther语句要创建上百个索引，想找一个简单的办法1./neo4j-shell -c &lt; /data/stale/data01/neo4j/create_index.cypther 热启动命令1MATCH (n)OPTIONAL MATCH (n)-[r]-&gt;()RETURN count(n.prop) + count(r.prop); 1.1亿节点1234567891011121314151617181920212223242526neo4j-sh (?)$ match (n) optional match (n)-[r]-&gt;() return count(n.prop), count(r.prop);+-------------------------------+| count(n.prop) + count(r.prop) |+-------------------------------+| 0 |+-------------------------------+1 row556974 msneo4j-sh (?)$ match (n) optional match (n)-[r]-&gt;() return count(n)+count(r);+-------------------+| count(n)+count(r) |+-------------------+| 262701611 |+-------------------+1 row172145 msneo4j-sh (?)$ match (n) optional match (n)-[r]-&gt;() return count(n.uuid), count(r);+---------------------------+| count(n.uuid) | count(r) |+---------------------------+| 1233398 | 111001002 |+---------------------------+1 row260481 ms 热启动命令2CALL apoc.warmup.run(); 12345678neo4j-sh (?)$ CALL apoc.warmup.run();+--------------------------------------------------------------------------------------------------------------------------+| pageSize | nodesPerPage | nodesTotal | nodePages | nodesTime | relsPerPage | relsTotal | relPages | relsTime | totalTime |+--------------------------------------------------------------------------------------------------------------------------+| 8192 | 546 | 110000000 | 201466 | 0 | 240 | 110000000 | 458334 | 0 | 1 |+--------------------------------------------------------------------------------------------------------------------------+1 row1416 ms 社区版和企业版有什么区别。其实他们在功能上没有本质区别。主要区别在如下几点： 1、容量：社区版最多支持 320 亿个节点、320 亿个关系和 640 亿个属性，而企业版没有这个限制；2、并发：社区版只能部署成单实例，不能做集群。而企业版可以部署成高可用集群或因果集群，从而可以解决高并发量的问题；3、容灾：由于企业版支持集群，部分实例出故障不会影响整个系统正常运行；4、热备：社区版只支持冷备份，即需要停止服务后才能进行备份，而企业版支持热备，第一次是全量备份，后续是增量备份；5、性能：社区版最多用到 4 个内核，而企业能用到全部内核，且对性能做了精心的优化；6、支持：企业版客户能得到 5X10 电话支持（Neo4j 美国电话、邮件，微云数聚电话、微信、邮件）； References[1] http://neo4j.com/docs/operations-manual/3.1/[2] https://neo4j.com/docs/developer-manual/3.1/[3] http://neo4j.com/docs/2.2.9/query-delete.html[4] https://neo4j.com/docs/developer-manual/3.1/cypher/[5] https://neo4j.com/blog/neo4j-3-1-ga-release/?ref=home[6] https://neo4j.com/docs/developer-manual/3.1/cypher/clauses/set/[7] https://neo4j.com/docs/operations-manual/3.2/installation/linux/debian/#multiple-java-versions[8] https://neo4j.com/docs/operations-manual/current/installation/windows/[9] http://neo4j.com/docs/developer-manual/current/extending-neo4j/procedures/[10] https://neo4j.com/developer/guide-importing-data-and-etl/ 使用ETL方式导入Neo4j[11] https://neo4j.com/developer/guide-import-csv/ 使用csv文件方式导入Neo4j[12] https://neo4j.com/docs/[13] https://neo4j.com/blog/neo4j-3-0-massive-scale-developer-productivity/#capabilities-data-size neo4j支持节点个数[14] https://neo4j.com/developer/kb/warm-the-cache-to-improve-performance-from-cold-start/ https://stackoverflow.com/questions/41762487/neo4j-bulk-import-and-indexinghttps://neo4j.com/blog/batchinsert-auto-indexing-and-friend-recommendation-with-neo4j/http://grokbase.com/t/gg/neo4j/146xv2wa77/how-i-can-create-index-schema-legacy-after-importing-data-by-batch-inserter-from-neo4j-shell https://stackoverflow.com/questions/44996896/how-to-execute-cypher-file-using-neo4j-3-1-4-not-through-zip-filehttps://github.com/neo4j/cypher-shell/issues/96 https://stackoverflow.com/questions/15161221/neo4j-script-file-format-is-there-any]]></content>
      <categories>
        <category>neo4j</category>
      </categories>
      <tags>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git 笔记]]></title>
    <url>%2F2017%2F03%2F16%2Fgit-notes%2F</url>
    <content type="text"><![CDATA[有关Git使用的笔记 (1) Git是什么 Git是目前世界上最先进的分布式版本控制系统(没有之一) (1.1) 配置Git 安装完git建议配置的选项 配置用户名 git config --global user.name &quot;wkq&quot; 配置邮箱 git config --global user.email &quot;weikeqin.cn@gmail.com&quot; 不忽略大小写 (文件名区分大小写) git config --global core.ignorecase false 不适用路径转义 (可以认为类似使用UTF-8) git config --global core.quotepath false (1.2) 生成ssh ssh-keygen -t rsa -C &quot;weikeqin.cn@gmail.com&quot; 123456789ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/Users/schacon/.ssh/id_rsa):Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /Users/schacon/.ssh/id_rsa.Your public key has been saved in /Users/schacon/.ssh/id_rsa.pub.The key fingerprint is:43:c5:5b:5f:b1:f1:50:43:ax:20:a6:92:6a:1x:9x:3x schacon@agadorlaptop.local 输入以上命令，然后一直回车，最后就可以生成ssh了 把生成的ssh公钥配置到git服务器上，比如github git.oschina 以后提交代码的时候可以用ssh协议，速度快，还不用输密码 (1.3) 牛刀小试 使用git把项目上传到git服务器 git init 初始化一个仓库 git add * 把所有文件标记问添加状态 git commit -m &quot;2017-05-21 21:14 init &quot; 提交 git add remote origin master git@github.com:wkq278276130/weikeqin.github.io.git 添加远程仓库(地址改成自己的) git fetch origin 把远程仓库的修改拉到本地 (如果远程仓库没有文件，这一步可以可以省略) git push origin --all 推送所有分支 git push origin --tags 推送所有标签 (1.2) 牛刀小试212345git clone git@git.oschina.net:wkq278276130/git_test.git # 从远程仓库克隆到本地vim readme.txt # 新建或修改readme.txtgit add readme.txt # 把readme文件添加到暂存区git commit -m &quot;修改readme文件&quot; # 把修改提交，把修改保存到本地仓库git push origin master # 把修改推送到远程仓库 (1.3) 迁移仓库 其实很简单，把仓库克隆下来，并传到另一个远程仓库 比如从github的远程仓库克隆到本地，然后传到/推送到gitee远程仓库 git clone --bare git://github.com/username/project.git 从原地址克隆一份裸版本库 –bare 创建的克隆版本库都不包含工作区，直接就是版本库的内容，这样的版本库称为裸版本库。 git push --mirror git@gitee.com/username/newproject.git 以镜像推送的方式上传代码到 gitee 服务器上。 (1.4) 工作区和暂存区 如图，我们修改后的文件都保存在工作区 把Git当成一个软件，它需要我们告诉它保存哪些东西 git add操作就是告诉Git要保存哪些修改，使用git add命令后保存的修改就放到暂存区了 (注意:是保存的修改放到暂存区，没有保存的修改还在工作区) git commit操作是告诉Git把保存的修改提交，git commit后就保存的修改就提交到到本地仓库了 (2) 常用命令(2.1) 生成Git仓库 git init 进入一个目录下，使用git init 命令，会把这个目录变成Git可以管理的仓库 (2.2) 添加修改 git add filename 把文件标记为添加状态 git add readme.txt (2.3) 保存修改 git commit -m &quot;message&quot; 提交到本地仓库，并包含提交时更新的信息。 git commit -m &quot;2016-05-03 07:40 add lib and jars&quot; (2.3.1) 修改最后依次提交的注释 git commit --amend 注意，Git默认使用UTF-8编码，在windows cmd命令行可能乱码，推荐使用Git Bash (2.3.2) 提交部分修改 git cherry-pick &lt;commit id&gt; 把已经提交的commit,从6.0分支放到1.0分支 先切换到一个分支，然后 git cherry-pick (2.4) 添加远程仓库 git remote add origin(仓库别名，默认是origin，可以自己设置) 地址 git remote add origin https://git.oschina.net/wkq278276130/java_test_all.git 使用的https协议 git remote add origin git@git.oschina.net:wkq278276130/java_test_all.git 使用ssh协议 (2.5) 更新代码 git pull origin master (2.5.1) pull和fetch的区别 git fetch 从远程获取最新版本到本地 ` git fetch origin master ` 拉取远程仓库master分支的更新 git merge origin/master 把远程master分支的更新合并到本地master分支 git pull = git fetch + git merge 以上命令的含义： 首先从远程的origin的master主分支下载最新的版本到origin/master分支上 然后比较本地的master分支和origin/master分支的差别 最后进行合并 在实际使用中，git fetch更安全一些 因为在merge前，我们可以查看更新情况，然后再决定是否合并 (2.6) 提交修改 git push origin master 把本地仓库推送到远程仓库（实际上是把master分支推送到远程） 远程仓库git默认叫origin git push origin master --force 强制推送(有危险) git push -f -u origin master 第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命。 git push -u origin master 第一次推送要加 -u (2.7) 查看仓库当前状态 git status (2.8) 查看不同 git diff 查看文件修改前和修改后的不同 git diff readme.txt git diff --cached #查看已经暂存区文件 git diff --staged #查看和上次提交时快照之间的差异 git diff commit_id1 commit_id2 #查看两个快照之间的差异 git diff branch1 branch2 --stat #查看两个分支所有差异的文件列表 git diff branch1 branch2 文件名(路径，推荐使用绝对路径) #查看同一文件在不同分支上的差异 git diff branch1 branch2 #查看所有有差异文件的详细信息 git log -p filename #查看文件的每一个详细的历史修改，如果没有-p选项，只显示提交记录，不显示文件内容修改。 git log -p -3 filename 显示最近的3次提交。 git log --pretty=oneline filename #每一行显示一个提交，先显示哈希码，再显示提交说明 git blame filename #查看文件的每一行是哪个提交最后修改的 (2.9) 暂存修改 git stash 把修改的内容先保存起来 git stash save &quot;保存2017-04-12 用户板块的修改&quot; 暂存修改并加备注 save 可以加备注 git stash list 查看工作区 git stash pop 把保存的内容，同时把stash删除 git stash apply stash@{1} 将你指定版本号为stash@{1}的工作取出来 git stash pop --index stash@{0} 恢复编号为0的进度的工作区和暂存区 git stash --help for more info (2.10) git log git log 显示从最近到最远的提交日志 git log --stat 显示提交历史，并简要显示受影响的文件和行数 git log -p -2 详细显示最近两次提交的内容 git log -n 1 -p 查看最近一次提交的详细内容 git reflog 记录你的每一次修改的日志 (2.11) 撤销修改 版本回退(2.11.1) 还没有提交，撤销修改 git checkout readme.txt 把工作区的readme.txt文件的修改全部撤销 git reset HEAD readme.txt 取消已经暂存的文件 (2.11.2) 已经提交，撤销修改(回退/回滚)本地版本回退 git reset HEAD &lt;file&gt; 取消暂存文件，把暂存区的修改撤销 git reset -- readme.txt ` git reset --hard commit_id ` 回退到某一个版本 ` git reset --hard 6db2eef ` 回退到指定版本 git reset -- hard HEAD~i 6db2eefcad16a8b9e85a1e995f697fd76ef76559 这是我的git中的一个日志id HEAD~ 后面的 ~ 是键盘上1前面的~，不是6那个，不要弄错了 git reset -- hard HEAD~1 git reset --hard HEAD^ 回退到上一个版本 git reset --hard HEAD^^ 回退到上上一个版本 ` git reset --hard 91e7b75 ` 把本地的版本回退(本次回退相当于一次新的提交)，再用git push把本地的修改提交 git revert commit_id 备注：用于回退一次提交，不能用于回退多次提交，不能用于merge的回退 A -&gt; B -&gt; C -&gt; D 如果想把B，C，D都给revert，除了一个一个revert之外，还可以使用range revert git revert -n B^..D 这样就把B,C,D都给revert了，变成： A-&gt; B -&gt;C -&gt; D -&gt; D&apos;-&gt; C&apos; -&gt; B&apos; git revert -n old_commit^..new_commit 用于回退连续多次提交，不能用于merge的回退 git revert -n 8c49615^..bdbb007 git revert -m 1 xxxx 用于merge的回退 (2.11.2.1) git reset和git revert的区别 reset是回朔到指定的commit版本（指定commit版本之后的操作都消失了）。 revert是删除指定的commit操作的内容（指定的版本内容消失，之前和之后commit版本内的操作都保留），但是这个操作也会做了一个commit提交版本。 举个例子，lisi提交了3次，分别是1,2,3 现在项目经理想回退到2， 假如用git reset就相当于直接回退到2，仓库还剩两次提交1,2 (打游戏打到第三关，回退到第二关) 用git revert也回退到2的内容了，只不过产生了一个新的提交4，git revert完成后仓库有4个提交1,2,3,4 (打游戏打到第三关，想玩第二关了，再开一关和第二关一模一样) 推荐使用git revert，假如用git reset，如果zhangsan的本地仓库有3这次commit，lisi把本地仓库回退到2，并且强制推送到远程仓库，假如zhangsan不知道，直接用push，就会又把3提交到远程仓库 如果使用git revert就不用担心这种事情。 (2.12) 克隆仓库 从远程仓库克隆到本地 git clone 仓库地址 git clone git@github.com:wkq278276130/weikeqin.github.io.git (2.13) 补丁(2.13.1) diff 补丁打补丁 在commit以后, 使用git diff xxx &gt; xxx_path生成补丁 git diff master &gt; 2017-04-27_测试补丁_patch git diff 9609398 &gt; 2017-04-27_测试补丁2_patch （9609398是commit_id） 应用补丁 别人给你一个补丁，你怎么使用 如果你收到了一个使用 git diff 或 Unix diff 命令（不推荐使用这种方式，具体见下一节）创建的补丁， 可以使用 git apply 命令来应用。 假设你将补丁保存在了 /tmp/patch-ruby-client.patch 中，可以这样应用补丁： git apply C:/WorkSpaces/git/java_test_all/2017-04-27_测试补丁_patch 在实际应用补丁前，你还可以使用 git apply 来检查补丁是否可以顺利应用——即对补丁运行 git apply –check 命令：123$ git apply --check 0001-seeing-if-this-helps-the-gem.patcherror: patch failed: ticgit.gemspec:1error: ticgit.gemspec: patch does not apply (2.13.2) format-patch补丁使用 am 命令应用补丁12$ git am 0001-limit-log-function.patchApplying: add limit to log function 你会看到补丁被顺利地应用，并且为你自动创建了一个新的提交。 其中的作者信息来自于 电子邮件头部的 From 和 Date 字段，提交消息则取自 Subject 和邮件正文中补丁之前的内容。 (2.14) git cleangit clean 是删除没有track过的文件(就是没有被add过的文件)相当于批量git checkout的操作 git clean -n 是一次clean的演习, 告诉你哪些文件会被删除. 记住他不会真正的删除文件, 只是一个提醒 git clean -f 删除当前目录下所有没有track过的文件. 他不会删除.gitignore文件里面指定的文件夹和文件, 不管这些文件有没有被track过 git clean -f &lt;path&gt; 删除指定路径下的没有被track过的文件 git clean -df 删除当前目录下没有被track过的文件和文件夹 git clean -xf 删除当前目录下所有没有track过的文件. 不管他是否是.gitignore文件里面指定的文件夹和文件 (3) 分支管理 git branch 查看当前分支 git branch -a 查看分支(包括远程的分支，远程分支会用红色表示出来) git branch dev 创建分支 git branch branch-name tag-name 根据tag创建分支 git checkout -b branch-name tag-name 根据tag创建并切换分支 git checkout dev 切换分支 git checkout -b dev 表示创建并切换，相当于以上两条命令 git merge dev 把dev分支的工作成果合并到当前分支上 git merge 用于合并指定分支到当前分支 git merge –no-ff -m “2016-10-09 18:26:00 不使用快速合并，修改XX内容” dev git merge dev --squash 多次commit合并成一个 git branch -d &lt;name&gt; 删除分支 git branch -D &lt;name&gt; 强制删除分支 git push [远程仓库别名] [本地分支]:[远程分支] 如果省略 [本地分支]，那就等于删除远程分支 git push origin :branch-name 删除远程分支 git push origin --delete branch-name 删除远程分支 git checkout -b branch-name origin/branch-name 在本地创建和远程分支对应的分支，本地和远程分支的名称最好一致 git fetch origin 远程分支名:本地分支名 git branch -m old-branch-name new-branch-name 分支重命名 git branch --set-upstream branch-name origin/branch-name 建立本地分支和远程分支的关联 git log --graph 查看分支合并图 通常，合并分支时，如果可能，Git会用Fast forward模式，但这种模式下，删除分支后，会丢掉分支信息。 git merge --no-ff -m &quot;merge with no-ff&quot; dev 准备合并dev分支，请注意–no-ff参数，表示禁用Fast forward (4) 标签管理 git tag v1.0 新建一个标签 git tag 查看所有标签 git tag name commit-id 对某次提交打标签 git tag v0.9 6224937 git tag -d tagName 删除标签 git tag -d v0.1 删除0.1版本的标签 git push origin tagName 推送某个标签到远程 git push origin --tags 一次性推送全部尚未推送到远程的本地标签 标签已经推送到远程，要删除远程标签就麻烦一点，先从本地删除，然后，从远程删除。删除命令也是push git tag -d v0.9 git push origin :refs/tags/v0.9 (5) .gitignore文件配置 .gitignore 配置文件用于配置不需要加入版本管理的文件，配置好该文件可以为我们的版本管理带来很大的便利。 以下是配置 .gitignore 的一些配置。 1、配置语法： 斜杠/表示目录 星号*通配多个字符 问号?通配单个字符 方括号[]包含单个字符的匹配列表； 叹号!表示不忽略(跟踪)匹配到的文件或目录； 此外，git 对于 .ignore 配置文件是按行从上到下进行规则匹配的，意味着如果前面的规则匹配的范围更大，则后面的规则将不会生效。 2、示例： (1) logs/* 说明：忽略目录logs下的全部内容，注意，不管是根目录下的 /logs/ 目录，还是某个子目录 /child/logs/ 目录，都会被忽略。 (2) /logs/* 说明：忽略根目录下的 /logs/ 目录的全部内容 (3) /* !.gitignore !/fw/bin/ !/fw/sf/ 说明：忽略全部内容，但是不忽略 .gitignore 文件、根目录下的/fw/bin/ 和 /fw/sf/目录 .gitignore文件配置12345678910111213141516171819202122232425262728293031323334353637383940# git.git# java*.class*.jar*lib*# log*.log/log/logs# idea.idea*.iml*.iws*.ipr/out/bin# eclipse.project.classpath.settings.apt_generated.factorypath/target# mac.DS_Store#svn.svn*svn*# other*.swp*.txt (6) 常见问题(6.1) 使用git命令时推荐使用Git Bash 使用git命令时推荐使用Git Bash，这样会有很多好处，不用担心乱码 (6.2) 在命令行无法使用git 因为你的环境变量没配置，系统识别不了git命令，所以会报错， 在环境变量path里添加git软件的bin路径就可以了（类似于JDK） 我的git安装在F:\Program Files\Git，所以我在path里添加F:\Program Files\Git\bin就可以在命令行使用了 (6.3) 中文不能正确地显示 在默认设置下，中文文件名在工作区状态输出，查看历史更改概要，以及在补丁文件中，文件名的中文不能正确地显示，而是显示为八进制的字符编码，示例如下： 12345678$ git status -s?? &quot;\350\257\264\346\230\216.txt\n$ printf &quot;\350\257\264\346\230\216.txt\n&quot;说明.txt通过将Git配置变量 core.quotepath 设置为false，就可以解决中文文件名称在这些Git命令输出中的显示问题，示例：$ git config --global core.quotepath false （在命令行输入这个配置）$ git status -s?? 说明.txt (6.4) 命令行提示 Your console font probably doesn’t support Unicode. Your console font probably doesn’t support Unicode. If you experience strange characters in the output, consider switching to a TrueType font such as Consolas! 修改命令行的字体，使用新宋体。 设置git的参数，按照如下设置 12git config --global core.quotepath false # 全局(一个电脑上)设定git config core.quotepath false # 只在某一个项目里设定 (6.5) git命令行每次都要输入用户名密码，能不能只输入一次永久使用 #保存输入的密码 “store” 模式会将凭证用明文的形式存放在磁盘中，并且永不过期。 git config –global credential.helper store #保存输入的密码 “cache” 模式会将凭证存放在内存中一段时间。 密码永远不会被存储在磁盘中，并且在15分钟后从内存中清除。 git config –global credential.helper cache (6.6) 设置别名12345git config --global alias.lg &quot;log --color --graph --pretty=format:&apos;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&apos; --abbrev-commit&quot; git config --global alias.lg2 &quot;log --color --graph --pretty=format:&apos;%Cred%h%Creset -%C(yellow)%d%Creset %s %C(blue)(%cr) %Cgreen&lt;%an&gt;%Creset %C(bold blue)(%ci)&apos; --abbrev-commit --date-order&quot;git config --global alias.lge &quot;log --color --graph --pretty=format:&apos;%Cred%h%Creset -%C(yellow)%d%Creset %s %C(blue)(%cr) %Cgreen&lt;%an&gt;%Creset %C(bold blue)(%ci) %Cred(%ae)&apos; --abbrev-commit --date-order&quot; 123456789%h: 缩写提交哈希 abbreviated commit hash%d: ref名称 ref names, like the --decorate option of git-log(1)%s: 提交的注释 subject %cr: 相对于现在提交多长时间 committer date, relative%an: 作者姓名 author name%cn: 提交者姓名 committer name%ci: 提交者日期，类似ISO 8601的格式 committer date, ISO 8601-like format 2018-01-10 21:58:02 +0800%ae: 作者邮箱 author email%ce: 提交电子邮件 committer email (6.7) 改写历史，永久删除git库的物理文件 WEB-INF/lib 留存有很多jar文件，使用maven管理项目后不需要这些jar文件了，想在git里删除 通过 git help filter-branch 找到改变历史的办法，具体操作如下：记得在git bash里执行，cmd了没法执行。 12345678910git filter-branch --tree-filter &apos;rm -f src/test/java/com/xxxx/xxxx/crawler/test/company/tianyuancha/TianYanChaTest.java&apos; --tag-name-filter cat -- --allRewrite 85c3264950f054e351a1b03a9ebaa039e3a991f6 (82/82) (118 seconds passed, remaining 0 predicted)Ref &apos;refs/heads/master&apos; was rewrittenRef &apos;refs/remotes/origin/master&apos; was rewrittenRef &apos;refs/stash&apos; was rewritten#一定要强推git push origin --all --forcegit push origin --tags --force 如果不强推，会出现如下错误123456789101112131415161718192021222324252627git push origin masterTo ssh://code.xxx.com.cn/xxx/xxx.git ! [rejected] master -&gt; master (non-fast-forward)error: failed to push some refs to &apos;ssh://git@code.xxx.com.cn/xxx/xxx.git&apos;hint: Updates were rejected because the tip of your current branch is behindhint: its remote counterpart. Integrate the remote changes (e.g.hint: &apos;git pull ...&apos;) before pushing again.hint: See the &apos;Note about fast-forwards&apos; in &apos;git push --help&apos; for details.git fetch origin masterFrom ssh://code.xxx.com.cn/xxx/xxx * branch master -&gt; FETCH_HEAD + 7fd80b5...11bb05e master -&gt; origin/master (forced update)git push origin master --forceCounting objects: 1708, done.Delta compression using up to 4 threads.Compressing objects: 100% (1199/1199), done.Writing objects: 100% (1708/1708), 751.75 KiB | 0 bytes/s, done.Total 1708 (delta 588), reused 0 (delta 0)remote: 处理 delta 中: 100% (588/588), 完成 32 个本地对象.To ssh://code.xxxx.com.cn/xxx/xxx.git + 11bb05e...0c364e9 master -&gt; master (forced update) 1234git clone git@github.com:jfinal/jfinal.gitgit filter-branch --tree-filter &apos;rm -f WebRoot/WEB-INF/lib/*.jar&apos; --tag-name-filter cat -- --allgit push origin --tags --forcegit push origin --all --force 删完后再 git clone 整个文件大小缩减到 940K，git clone 秒秒钟搞定，即便是 github 也是极速，打完收工 http://my.oschina.net/jfinal/blog/215624 (6.8) 使用git log命令乱码 推荐使用Git Bash，使用cmd可能会出现乱码，因为windows中文版的cmd默认使用GBK编码 12345678λ git lg2* 64784dd - (HEAD -&gt; master, oschina/master) 2017-09-07 &lt;E5&gt;&lt;90&gt;&lt;88&gt;&lt;E5&gt;&lt;B9&gt;&lt;B6&gt;&lt;E4&gt;&lt;B8&gt;&lt;A4&gt;&lt;E4&gt;&lt;B8&gt;&lt;AA&gt;&lt;E5&gt;&lt;88&gt;&lt;86&gt;&lt;E6&gt;&lt;94&gt;&lt;AF&gt;&lt;E7&gt;&lt;9A&gt;&lt;84&gt;&lt;E4&gt;&lt;BF&gt;&lt;A1&gt;&lt;E6&gt;&lt;81&gt;&lt;AF&gt; (2 minutes ago) &lt;WeiKeQin&gt; (2017-09-07 11:52:14 +0800)* 33a456e - Merge branch &apos;master&apos; of git.oschina.net:wkq278276130/test (4 minutes ago) &lt;WeiKeQin&gt; (2017-09-07 11:50:22 +0800)|\| * d05b779 - 2017-08-29 init (9 days ago) &lt;usdptext&gt; (2017-08-29 10:58:46 +0800)* 4cf57ca - 2017-09-07 &lt;E5&gt;&lt;90&gt;&lt;88&gt;&lt;E5&gt;&lt;B9&gt;&lt;B6&gt;&lt;E4&gt;&lt;B8&gt;&lt;A4&gt;&lt;E4&gt;&lt;B8&gt;&lt;AA&gt;&lt;E4&gt;&lt;BB&gt;&lt;93&gt;&lt;E5&gt;&lt;BA&gt;&lt;93&gt;&lt;E7&gt;&lt;9A&gt;&lt;84&gt;&lt;E9&gt;&lt;85&gt;&lt;8D&gt;&lt;E7&gt;&lt;BD&gt;&lt;AE&gt; (4 minutes ago) &lt;WeiKeQin&gt; (2017-09-07 11:50:00 +0800)* 5df1a38 - 2017-09-07 &lt;E6&gt;&lt;B7&gt;&lt;BB&gt;&lt;E5&gt;&lt;8A&gt;&lt;A0&gt;&lt;E8&gt;&lt;B4&gt;&lt;A2&gt;&lt;E7&gt;&lt;BB&gt;&lt;8F&gt;&lt;E7&gt;&lt;BD&gt;&lt;91&gt;&lt;E7&gt;&lt;AB&gt;&lt;99&gt; (13 minutes ago) &lt;WeiKeQin&gt; (2017-09-07 11:41:44 +0800)* e0ad97e - 2017-08-30 init for backup (8 days ago) &lt;WeiKeQin&gt; (2017-08-30 16:24:26 +0800) 12345678# 通过将Git配置变量 core.quotepath 设置为false，就可以解决中文文件名称在这些Git命令输出中的显示问题 git config --global core.quotepath false 如果设置后还有乱码，使用git config --list查看配置 撤销不必要的配置 git config --global --unset i18n.commitencoding git config --global --unset i18n.logoutputencoding git config --global --unset i18n.logoutputenconding (6.9) warning: user.name has multiple values1git config --global --replace-all user.name &quot;WeiKeQin&quot; (6.10) 误删文件怎么办原理和版本回退的原理一样1234567891. 刚误删，还没提交 git reset head git checkout test.txt2. 已经提交了 2.1 提交到本地仓库，没有推送到远程仓库 git reset 2.2 提交到本地仓库，并且已经推送到远程仓库 git revert (6.11) 想提交其他分支的个别提交1git rebase -i &lt;earlier SHA&gt; 原理: -i 参数让 rebase 进入“交互模式”。它开始类似于前面讨论的 rebase，但在重新进行任何提交之前， 它会暂停下来并允许你详细地修改每个提交。 (6.12) 停止追踪一个文件场景: 你偶然把 application.log 加到代码库里了，现在每次你运行应用，Git 都会报告在 application.log 里 有未提交的修改。你把 *.login 放到了 .gitignore 文件里，可文件还是在代码库里 问题：你怎么才能告诉 Git “撤销” 对这个文件的追踪呢？ 方法: git rm –cached application.log 原理: 虽然 .gitignore 会阻止 Git 追踪文件的修改，甚至不关注文件是否存在， 但这只是针对那些以前从来没有追踪过的文件。一旦有个文件被加入并提交了，Git 就会持续关注该文件的改变。 类似地，如果你利用 git add -f 来强制或覆盖了 .gitignore， Git 还会持续追踪改变的情况。 之后你就不必用-f 来添加这个文件了。 如果你希望从 Git 的追踪对象中删除那个本应忽略的文件， git rm –cached 会从追踪对象中删除它， 但让文件在磁盘上保持原封不动。因为现在它已经被忽略了，你在 git status 里就不会再看见这个文件， 也不会再偶然提交该文件的修改了。 (6.13) Auto packing the repository in background for optimum performance.123Auto packing the repository in background for optimum performance.See &quot;git help gc&quot; for manual housekeeping.Counting objects: 383927, done. 运行git gc命令，会比较耗时 1234567&gt; git gcCounting objects: 383930, done.Delta compression using up to 4 threads.Compressing objects: 100% (383307/383307), done.Writing objects: 100% (383930/383930), done.Total 383930 (delta 12057), reused 3 (delta 0)Removing duplicate objects: 100% (256/256), done. (6.14) src refspec test-v1.3 matches more than one.123&gt; git push origin test-v1.3error: src refspec test-v1.3 matches more than one.error: failed to push some refs to &apos;git@git.oschina.net:wkq278276130/test.git&apos; 原因：原因是 branch和tag的名字重复了，git不知道推送branch还是tag，所以会出错。我本地有一个branch叫test-v1.3，有一个tag叫test-v1.3，推送的时候就报这个错。 解决办法：删除taggit tag -d test-v1.3然后推送就可以了 建议tag命名的时候用成test-v1.3_tag在分支名后加个_tag，这样能避免重复 (6.15) Connection reset123Connection reset by 120.55.226.24 port 22fatal: The remote end hung up unexpectedlyfatal: The remote end hung up unexpectedly 我连的WiFi变了，ip地址变了，导致这个问题 (6.16) remote: GitLab: You are not allowed to change existing tags on this project.1remote: GitLab: You are not allowed to change existing tags on this project. https:#stackoverflow.com/questions/44019963/how-to-delete-remote-server-tag (6.17) ssh_dispatch_run_fatal1234ssh_dispatch_run_fatal: Connection to 192.30.255.112 port 22: message authentication code incorrectfatal: The remote end hung up unexpectedlyfatal: early EOFfatal: index-pack failed 重新克隆 (6.18) Git 合并时将其当作一个新的提交而不是记录你合并时的分支的历史记录1squashed merge (6.19) Git没有共同祖先的两个分支如何合并git merge your_branch --allow-unrelated-histories (6.20) 统计每个开发者提交多少次 git shortlog --numbered --summary (6.21) git 默认对文件名大小写不敏感 (不区分文件名大小写) 在更新博客的时候发现文件名重命名了后重新部署，本地没有问题，但是服务器上找不到，提示404，刚开始以为是next主题的问题，后来才发现是git的锅 是因为git默认文件名大小写不敏感，也就是不区分文件名大小写，导致我把博客的文件名改成小写后，url里还是大写的博客名，导致几十篇博客不能访问。 解决办法：配置 git config --global core.ignorecase false 这样修改后，发现远程仓库还是大写的，有三个办法解决 文件少的话，用 git mv README.md readme.md 命令来删除缓存head 如果有文件夹，可以用 git rm --cached src/java/ -r 来删除缓存head 把涉及到的文件放另一个目录，然后提交一次，再把文件移回来，在提交一次。 () 把git仓库删了，重新git init一下，然后重新提交 (6.30) fatal: refusing to merge unrelated histories造成这个问题的原因是远程仓库里有内容，但是提交者不知道，自己新建了一个仓库，并且使用已有仓库的url，提交的时候强制推送到远程仓库，造成unrelated histories，下面是解决办法 git merge origin/master --allow-unrelated-histories 12345678910111213141516git 查看最近或某一次提交修改的文件列表相关命令整理。git log --name-status 每次修改的文件列表, 显示状态(新增还是修改)git log --name-only 每次修改的文件列表git log --stat 每次修改的文件列表, 及文件修改的统计git whatchanged 每次修改的文件列表git whatchanged --stat 每次修改的文件列表, 及文件修改的统计git show 显示最后一次的文件改变的具体内容git show -5 显示最后 5 次的文件改变的具体内容git show commitid 显示某个 commitid 改变的具体内容 git lg212345678910111213141516WKQ@WKQ-PC MINGW64 /d/WorkSpaces/data/996.ICU (master)$ git lg2* e7ae2c2 - (HEAD -&gt; master, origin/master, origin/HEAD) Merge pull request #25551 from Johnny-Wish/master (4 hours ago) &lt;996icu&gt; (2019-04-14 18:27:14 +0800)|\| * dff20c3 - Merge branch &apos;master&apos; into master (4 hours ago) &lt;996icu&gt; (2019-04-14 18:27:06 +0800)| |\| |/|/|* | 311757c - Merge pull request #25548 from liyujiang-gzu/master (4 hours ago) &lt;996icu&gt; (2019-04-14 18:26:20 +0800)|\ \| * | 29684d8 - 有老铁（#25532）想要把华为移除黑名单，特此添加“华为杭州研发基地+华为外包”加班的打卡记录证据，华为正式员工和华为外包员工一起工作，作息是一样的： (6 hours ago) &lt;gzu-liyujiang&gt; (2019-04-14 16:22:17 +0800)* | | cf24662 - Merge pull request #25547 from KyleStore/master (4 hours ago) &lt;996icu&gt; (2019-04-14 18:26:05 +0800)|\ \ \| * | | 8339a63 - add Diary: a web app (7 hours ago) &lt;KyleStore&gt; (2019-04-14 15:51:05 +0800)| |/ / git log –name-status1234567891011121314151617181920212223242526WKQ@WKQ-PC MINGW64 /d/WorkSpaces/data/996.ICU (master)$ git log --name-statuscommit e7ae2c2640d47591f420b2c9392933fb1e366327 (HEAD -&gt; master, origin/master, origin/HEAD)Merge: 311757c dff20c3Author: 996icu &lt;48942249+996icu@users.noreply.github.com&gt;Date: Sun Apr 14 18:27:14 2019 +0800 Merge pull request #25551 from Johnny-Wish/master 增加工人日报报导 : 马云谈996时，可曾知道8小时工作制是这样争取来的！commit dff20c318502f3cc42d3a7990a88fbd609b30940Merge: 3418ba3 311757cAuthor: 996icu &lt;48942249+996icu@users.noreply.github.com&gt;Date: Sun Apr 14 18:27:06 2019 +0800 Merge branch &apos;master&apos; into mastercommit 311757c4b7057fa47a5857c1973ec8313d92b2f3Merge: cf24662 29684d8Author: 996icu &lt;48942249+996icu@users.noreply.github.com&gt;Date: Sun Apr 14 18:26:20 2019 +0800 Merge pull request #25548 from liyujiang-gzu/master 添加“华为杭州研发基地+华为外包”加班的打卡记录证据 11234567891011121314151617181920212223242526WKQ@WKQ-PC MINGW64 /d/WorkSpaces/data/996.ICU (master)$ git log --name-onlycommit e7ae2c2640d47591f420b2c9392933fb1e366327 (HEAD -&gt; master, origin/master, origin/HEAD)Merge: 311757c dff20c3Author: 996icu &lt;48942249+996icu@users.noreply.github.com&gt;Date: Sun Apr 14 18:27:14 2019 +0800 Merge pull request #25551 from Johnny-Wish/master 增加工人日报报导 : 马云谈996时，可曾知道8小时工作制是这样争取来的！commit dff20c318502f3cc42d3a7990a88fbd609b30940Merge: 3418ba3 311757cAuthor: 996icu &lt;48942249+996icu@users.noreply.github.com&gt;Date: Sun Apr 14 18:27:06 2019 +0800 Merge branch &apos;master&apos; into mastercommit 311757c4b7057fa47a5857c1973ec8313d92b2f3Merge: cf24662 29684d8Author: 996icu &lt;48942249+996icu@users.noreply.github.com&gt;Date: Sun Apr 14 18:26:20 2019 +0800 Merge pull request #25548 from liyujiang-gzu/master 添加“华为杭州研发基地+华为外包”加班的打卡记录证据 1234567891011121314151617181920212223242526WKQ@WKQ-PC MINGW64 /d/WorkSpaces/data/996.ICU (master)$ git log --statcommit e7ae2c2640d47591f420b2c9392933fb1e366327 (HEAD -&gt; master, origin/master, origin/HEAD)Merge: 311757c dff20c3Author: 996icu &lt;48942249+996icu@users.noreply.github.com&gt;Date: Sun Apr 14 18:27:14 2019 +0800 Merge pull request #25551 from Johnny-Wish/master 增加工人日报报导 : 马云谈996时，可曾知道8小时工作制是这样争取来的！commit dff20c318502f3cc42d3a7990a88fbd609b30940Merge: 3418ba3 311757cAuthor: 996icu &lt;48942249+996icu@users.noreply.github.com&gt;Date: Sun Apr 14 18:27:06 2019 +0800 Merge branch &apos;master&apos; into mastercommit 311757c4b7057fa47a5857c1973ec8313d92b2f3Merge: cf24662 29684d8Author: 996icu &lt;48942249+996icu@users.noreply.github.com&gt;Date: Sun Apr 14 18:26:20 2019 +0800 Merge pull request #25548 from liyujiang-gzu/master 添加“华为杭州研发基地+华为外包”加班的打卡记录证据 1234567891011121314151617181920212223242526WKQ@WKQ-PC MINGW64 /d/WorkSpaces/data/996.ICU (master)$ git whatchangedcommit 3418ba36ec2f5b66fa2256bdd1811d70e237ae6bAuthor: Shuheng Johnny Liu &lt;wish1104@outlook.com&gt;Date: Sun Apr 14 18:00:14 2019 +0800 add new report by 工人日报 : 马云谈996时，可曾知道8小时工作制是这样争取来的！:100644 100644 9f74c1f a9d70b6 M externals/news.mdcommit 0abff86f5a2621967c39fcd2fc64dba9a4c98d41Author: kwonghinho &lt;jiangyanhao1991@gmail.com&gt;Date: Sun Apr 14 17:03:38 2019 +0800 添加《不能给反对996的员工贴“混日子”标签》:100644 100644 9f74c1f d9f509a M externals/news.mdcommit 29684d8c8692ee5208af1dfbad861c52641913bcAuthor: gzu-liyujiang &lt;admin@qqtheme.cn&gt;Date: Sun Apr 14 16:22:17 2019 +0800 有老铁（#25532）想要把华为移除黑名单，特此添加“华为杭州研发基地+华为外包”加班的打卡记录证据，华为正式员工和华为外包员工一起工作，作息是一样的：:100644 100644 a52d394 a53b6e5 M blacklist/README.md:000000 100644 0000000 2d8ff70 A blacklist/img/huawei_hangzhou_timing.png 1234567891011121314151617181920212223242526272829WKQ@WKQ-PC MINGW64 /d/WorkSpaces/data/996.ICU (master)$ git whatchanged --statcommit 3418ba36ec2f5b66fa2256bdd1811d70e237ae6bAuthor: Shuheng Johnny Liu &lt;wish1104@outlook.com&gt;Date: Sun Apr 14 18:00:14 2019 +0800 add new report by 工人日报 : 马云谈996时，可曾知道8小时工作制是这样争取来的！ externals/news.md | 2 ++ 1 file changed, 2 insertions(+)commit 0abff86f5a2621967c39fcd2fc64dba9a4c98d41Author: kwonghinho &lt;jiangyanhao1991@gmail.com&gt;Date: Sun Apr 14 17:03:38 2019 +0800 添加《不能给反对996的员工贴“混日子”标签》 externals/news.md | 2 ++ 1 file changed, 2 insertions(+)commit 29684d8c8692ee5208af1dfbad861c52641913bcAuthor: gzu-liyujiang &lt;admin@qqtheme.cn&gt;Date: Sun Apr 14 16:22:17 2019 +0800 有老铁（#25532）想要把华为移除黑名单，特此添加“华为杭州研发基地+华为外包”加班的打卡记录证据，华为正式员工和华为外包员工一起工作，作息是一样的： blacklist/README.md | 2 +- blacklist/img/huawei_hangzhou_timing.png | Bin 0 -&gt; 65678 bytes 2 files changed, 1 insertion(+), 1 deletion(-) 12345678910WKQ@WKQ-PC MINGW64 /d/WorkSpaces/data/996.ICU (master)$ git showcommit e7ae2c2640d47591f420b2c9392933fb1e366327 (HEAD -&gt; master, origin/master, origin/HEAD)Merge: 311757c dff20c3Author: 996icu &lt;48942249+996icu@users.noreply.github.com&gt;Date: Sun Apr 14 18:27:14 2019 +0800 Merge pull request #25551 from Johnny-Wish/master 增加工人日报报导 : 马云谈996时，可曾知道8小时工作制是这样争取来的！ 1234567891011121314151617181920212223242526272829303132333435363738394041424344WKQ@WKQ-PC MINGW64 /d/WorkSpaces/data/996.ICU (master)$ git show -5commit e7ae2c2640d47591f420b2c9392933fb1e366327 (HEAD -&gt; master, origin/master, origin/HEAD)Merge: 311757c dff20c3Author: 996icu &lt;48942249+996icu@users.noreply.github.com&gt;Date: Sun Apr 14 18:27:14 2019 +0800 Merge pull request #25551 from Johnny-Wish/master 增加工人日报报导 : 马云谈996时，可曾知道8小时工作制是这样争取来的！commit dff20c318502f3cc42d3a7990a88fbd609b30940Merge: 3418ba3 311757cAuthor: 996icu &lt;48942249+996icu@users.noreply.github.com&gt;Date: Sun Apr 14 18:27:06 2019 +0800 Merge branch &apos;master&apos; into masterdiff --cc externals/news.mdindex a9d70b6,147a7ce..8207483--- a/externals/news.md+++ b/externals/news.md@@@ -12,6 -14,6 +14,8 @@@ - 人民网：[“996工作制”是谁的如意算盘？](http://opinion.people.com.cn/n1/2019/0402/c119388-31009768.html)（[Wayback Machine](https://web.archive.org/web/20190409022535/http://opinion.people.com.cn/n1/2019/0402/c119388-31009768.html)）++- 工人日报：[马云谈996时，可曾知道8小时工作制是这样争取来的！](https://mp.weixin.qq.com/s/tp0u0wI0gHEjlV5ZpdDoDQ)++ - 工人日报：[“工作996，生病ICU”该引起重视了](http://news.workercn.cn/32845/201904/07/190407041525409.shtml)（[Wayback Machine](https://web.archive.org/web/20190409022551/http://news.workercn.cn/32845/201904/07/190407041525409.shtml)） - 央广网：[风波再起 40余家互联网公司被指实行“996工作制”](http://china.cnr.cn/xwwgf/20190405/t20190405_524568985.shtml)（[Wayback Machine](https://web.archive.org/web/20190409022701/http://china.cnr.cn/xwwgf/20190405/t20190405_524568985.shtml)）commit 311757c4b7057fa47a5857c1973ec8313d92b2f3Merge: cf24662 29684d8Author: 996icu &lt;48942249+996icu@users.noreply.github.com&gt;Date: Sun Apr 14 18:26:20 2019 +0800 Merge pull request #25548 from liyujiang-gzu/master 添加“华为杭州研发基地+华为外包”加班的打卡记录证据 12345678910111213141516171819202122232425262728293031323334WKQ@WKQ-PC MINGW64 /d/WorkSpaces/data/996.ICU (master)$ git show dff20c3 e7ae2c2commit dff20c318502f3cc42d3a7990a88fbd609b30940Merge: 3418ba3 311757cAuthor: 996icu &lt;48942249+996icu@users.noreply.github.com&gt;Date: Sun Apr 14 18:27:06 2019 +0800 Merge branch &apos;master&apos; into masterdiff --cc externals/news.mdindex a9d70b6,147a7ce..8207483--- a/externals/news.md+++ b/externals/news.md@@@ -12,6 -14,6 +14,8 @@@ - 人民网：[“996工作制”是谁的如意算盘？](http://opinion.people.com.cn/n1/2019/0402/c119388-31009768.html)（[Wayback Machine](https://web.archive.org/web/20190409022535/http://opinion.people.com.cn/n1/2019/0402/c119388-31009768.html)）++- 工人日报：[马云谈996时，可曾知道8小时工作制是这样争取来的！](https://mp.weixin.qq.com/s/tp0u0wI0gHEjlV5ZpdDoDQ)++ - 工人日报：[“工作996，生病ICU”该引起重视了](http://news.workercn.cn/32845/201904/07/190407041525409.shtml)（[Wayback Machine](https://web.archive.org/web/20190409022551/http://news.workercn.cn/32845/201904/07/190407041525409.shtml)） - 央广网：[风波再起 40余家互联网公司被指实行“996工作制”](http://china.cnr.cn/xwwgf/20190405/t20190405_524568985.shtml)（[Wayback Machine](https://web.archive.org/web/20190409022701/http://china.cnr.cn/xwwgf/20190405/t20190405_524568985.shtml)）commit e7ae2c2640d47591f420b2c9392933fb1e366327 (HEAD -&gt; master, origin/master, origin/HEAD)Merge: 311757c dff20c3Author: 996icu &lt;48942249+996icu@users.noreply.github.com&gt;Date: Sun Apr 14 18:27:14 2019 +0800 Merge pull request #25551 from Johnny-Wish/master 增加工人日报报导 : 马云谈996时，可曾知道8小时工作制是这样争取来的！ References[1] git官网 [2] git book 中文版[3] git doc 英文版[4] git-log[5] 廖雪峰的git教程 [6] git入门[7] 将本地项目推送到Git[8] git中文资料[6] git rebase简介(基本篇)[9] 在commit之前撤销git add操作 [10] Git fetch和git pull的区别[11] Git的中文支持[12] git 中文支持配置[13] git中reset和revert用法和区别[14] 代码回滚：git reset、git checkout和git revert区别和联系[15] git book [16] 如何在 Git 里撤销(几乎)任何操作[17] git 远程分支版本回退[18] Git Community Book 中文版 rebase [19] 「Git」合并多个 Commit [20] Svn和Git的一次详细对比]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java调用javascript]]></title>
    <url>%2F2017%2F03%2F16%2Fjava-call-javascript%2F</url>
    <content type="text"><![CDATA[Java调用JavaScript123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import org.slf4j.Logger;import org.slf4j.LoggerFactory;import javax.script.Invocable;import javax.script.ScriptEngine;import javax.script.ScriptEngineManager;import javax.script.ScriptException;import java.io.BufferedReader;import java.io.FileNotFoundException;import java.io.FileReader;/** * @version V1.0 * @date 2017-09-21 20:58 */public class JsTest &#123; private static final Logger logger = LoggerFactory.getLogger(JsTest.class); public static void main(String[] args) &#123; String url = "//flights.ctrip.com/domesticsearch/search/SearchFirstRouteFlights?DCity1=BJS&amp;ACity1=KMG&amp;SearchType=S&amp;DDate1=2017-09-22&amp;IsNearAirportRecommond=0&amp;LogToken=1bd69145999c49d391102da28ded88fe&amp;CK=1E1CDC6F399D26F328ACAB7254823FA5" + "&amp;rk=" + Math.random() * 10 + "204001"; String r = "0.4229086476791374857311"; ScriptEngineManager manager = new ScriptEngineManager(); // nashorn javascript ScriptEngine engine = manager.getEngineByName("javascript"); String jsFileName = "doc/html/xiecheng/ajaxRequest.js"; // 读取js文件 // 执行指定脚本 BufferedReader br = null; Invocable invoke = null; try &#123; br = new BufferedReader(new FileReader(jsFileName)); engine.eval(br); invoke = (Invocable) engine; &#125; catch (FileNotFoundException e) &#123; logger.error("", e); &#125; catch (ScriptException e) &#123; logger.error("", e); &#125; Object o = null; try &#123; // 调用方法，并传入两个参数 o = invoke.invokeFunction("ajaxRequest", url, r); &#125; catch (ScriptException e) &#123; logger.error("", e); &#125; catch (NoSuchMethodException e) &#123; logger.error("", e); &#125; logger.info("执行js后的结果：\r\n&#123;&#125;", o); &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>javascript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java-jdbc]]></title>
    <url>%2F2017%2F03%2F16%2Fjava-jdbc%2F</url>
    <content type="text"><![CDATA[JDBC1234567891011121314151617181920212223242526272829303132333435import java.sql.Connection;import java.sql.DriverManager;import java.sql.SQLException;import java.sql.PreparedStatement;import java.sql.ResultSet;import java.sql.SQLException;String driverClassName = "com.mysql.jdbc.Driver";String url = "jdbc:mysql://127.0.0.1:3306/test?useUnicode=true&amp;characterEncoding=UTF-8&amp;autoReconnect=true&amp;useSSL=false";String user = "user01";String password = "user01"; Class.forName(driverClassName).newInstance();Connection con = DriverManager.getConnection(url, user, password);// 插入 String sql = "insert into user(name, password, age) values (?, ?, ?) ;";PreparedStatement pst = con.prepareStatement(sql);pst.setString(1, name);pst.setString(2, password);pst.setInt(3, age);num = pst.executeUpdate();pst.close();con.close(); // 查询String sql = "select count(*) as count from table_user ;";PreparedStatement pst = secondCon.prepareStatement(sql);ResultSet rs = pst.executeQuery();while (rs.next()) &#123; // TODO&#125; rs.close();pst.close();con.close(); jdbc-poolTomcat 在 7.0 以前的版本都是使用 commons-dbcp 做为连接池的实现，但是 dbcp 饱受诟病，原因有：12345dbcp 是单线程的，为了保证线程安全会锁整个连接池dbcp 性能不佳dbcp 太复杂，超过 60 个类dbcp 使用静态接口，在 JDK 1.6 编译有问题dbcp 发展滞后 因此很多人会选择一些第三方的连接池组件，例如 c3p0 , bonecp, druid (@wenshao ) 等。 为此，Tomcat 从 7.0 开始引入一个新的模块：Tomcat jdbc pool 123456789tomcat jdbc pool 近乎兼容 dbcp ，性能更高异步方式获取连接tomcat jdbc pool 是 tomcat 的一个模块，基于 tomcat JULI，使用 Tomcat 的日志框架使用 javax.sql.PooledConnection 接口获取连接支持高并发应用环境超简单，核心文件只有8个，比 c3p0 还更好的空闲连接处理机制支持 JMX支持 XA Connection tomcat jdbc pool 的优点远不止这些，详情请看 官网。 tomcat jdbc pool 可在 Tomcat 中直接使用，也可以在独立的应用中使用。 在独立的应用中使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import java.sql.Connection;import java.sql.ResultSet;import java.sql.Statement;import org.apache.tomcat.jdbc.pool.DataSource;import org.apache.tomcat.jdbc.pool.PoolProperties;public class SimplePOJOExample &#123; public static void main(String[] args) throws Exception &#123; PoolProperties p = new PoolProperties(); p.setUrl("jdbc:mysql://localhost:3306/mysql"); p.setDriverClassName("com.mysql.jdbc.Driver"); p.setUsername("root"); p.setPassword("password"); p.setJmxEnabled(true); p.setTestWhileIdle(false); p.setTestOnBorrow(true); p.setValidationQuery("SELECT 1"); p.setTestOnReturn(false); p.setValidationInterval(30000); p.setTimeBetweenEvictionRunsMillis(30000); p.setMaxActive(100); p.setInitialSize(10); p.setMaxWait(10000); p.setRemoveAbandonedTimeout(60); p.setMinEvictableIdleTimeMillis(30000); p.setMinIdle(10); p.setLogAbandoned(true); p.setRemoveAbandoned(true); p.setJdbcInterceptors( "org.apache.tomcat.jdbc.pool.interceptor.ConnectionState;"+ "org.apache.tomcat.jdbc.pool.interceptor.StatementFinalizer"); DataSource datasource = new DataSource(); datasource.setPoolProperties(p); Connection con = null; try &#123; con = datasource.getConnection(); Statement st = con.createStatement(); ResultSet rs = st.executeQuery("select * from user"); int cnt = 1; while (rs.next()) &#123; System.out.println((cnt++)+". Host:" +rs.getString("Host")+ " User:"+rs.getString("User")+" Password:"+rs.getString("Password")); &#125; rs.close(); st.close(); &#125; finally &#123; if (con!=null) try &#123;con.close();&#125;catch (Exception ignore) &#123;&#125; &#125; &#125;&#125; 异步获取连接1234567891011121314Connection con = null;try &#123; Future&lt;Connection&gt; future = datasource.getConnectionAsync(); while (!future.isDone()) &#123; System.out.println("Connection is not yet available. Do some background work"); try &#123; Thread.sleep(100); //simulate work &#125;catch (InterruptedException x) &#123; Thread.currentThread().interrupted(); &#125; &#125; con = future.get(); //should return instantly Statement st = con.createStatement(); ResultSet rs = st.executeQuery("select * from user"); 12345678910111213141516171819202122&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat&lt;/groupId&gt; &lt;artifactId&gt;tomcat-jdbc&lt;/artifactId&gt; &lt;version&gt;7.0.29&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat&lt;/groupId&gt; &lt;artifactId&gt;tomcat-juli&lt;/artifactId&gt; &lt;version&gt;7.0.29&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.tomcat/tomcat-jdbc --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat&lt;/groupId&gt; &lt;artifactId&gt;tomcat-jdbc&lt;/artifactId&gt; &lt;version&gt;8.5.15&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.tomcat/tomcat-juli --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat&lt;/groupId&gt; &lt;artifactId&gt;tomcat-juli&lt;/artifactId&gt; &lt;version&gt;8.5.15&lt;/version&gt;&lt;/dependency&gt; ReferencesJDBC[1] http://www.yiibai.com/jdbc/jdbc_quick_guide.html[2] http://www.cnblogs.com/DreamDrive/p/5757693.html[3] http://blog.csdn.net/whucyl/article/details/20838079[4] http://javastudyeye.iteye.com/blog/835448 jdbc-pool[1] https://stackoverflow.com/questions/24559468/neo4j-jdbc-connection-pool[2] http://tomcat.apache.org/tomcat-7.0-doc/jdbc-pool.html[3] https://www.oschina.net/question/12_36910[4] https://github.com/alibaba/druid]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jdbc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java解析xml]]></title>
    <url>%2F2017%2F03%2F16%2Fjava-parser-xml%2F</url>
    <content type="text"><![CDATA[Java解析xml 遇到一个问题，要解析一个xml，网上搜了搜，都说有4中方式，试了试dom解析，感觉解析的时候开发效率太低，忽然想到Jsoup，然后就用了第5种方式Jsoup解析XML。 用Jsoup解析XML，开发效率确实是高，但是运行效率太低了。解析一个10K左右的xml要0.2s左右。300万的xml文件要解析到什么时候呀。 然后试了试Dom解析xml，效率提高了不少，解析一个10K左右的xml 0.05s左右，效率提高的不少。 当然，还有其他3种方式解析。知道Sax解析时占用内存小，可能会快一点，但是着急处理文件，暂时没有测试。 DOM SAX JDOM DOM4J Jsoup 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359import javax.xml.parsers.DocumentBuilderFactory;import java.io.*;import java.util.ArrayList;import java.util.Collections;import java.util.Comparator;import java.util.List;import javax.xml.parsers.DocumentBuilder;import javax.xml.parsers.ParserConfigurationException;import com.xxxx.usdp.odk.common.file.FileUtil;import com.xxxx.usdp.xxxx.poc.yuyin.entity.XmlEntity;import org.junit.Test;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.w3c.dom.*;import org.xml.sax.SAXException;/** * DOM方式解析xml * * @version V1.0 */public class DomParserXml &#123; private static final Logger log = LoggerFactory.getLogger(DomParserXml.class); /** * 测试解析 */ @Test public void testPaser()&#123; String xmlFilePath = "src/main/resources/2018010109013384362390728_1522286730680.xml"; xmlFilePath = "C:\\home\\user1\\xxxx\\2018010109000672062385406"; //xmlFilePath = "src/main/resources/test.xml"; XmlEntity xmlEntity = parserXml(xmlFilePath); log.info("xml数据：\r\n&#123;&#125;", xmlEntity); &#125; /** * 批量解析 */ @Test public void batchParser()&#123; String filePath = "D:\\data\\210_1\\210_test"; String outDirPath = "D:\\data\\210_1\\210_201801_result"; File outDir = new File(outDirPath); if(!outDir.exists())&#123; outDir.mkdirs(); &#125; File dir = new File(filePath); File[] files = dir.listFiles(); int length = files.length; for(int i = 0; i &lt; 1000; i++)&#123; File f = files[i]; XmlEntity xmlEntity = parserXml(f.getAbsolutePath()); try &#123; FileUtil.writeStringToFile(xmlEntity.getMix(), outDirPath+"/"+xmlEntity.getFileName()+".txt"); &#125; catch (IOException e) &#123; log.error("写文件出错 &#123;&#125;", e.toString()); &#125; &#125; &#125; /** * 把xml解析成对话格式 * * @param xmlFilePath */ public static XmlEntity parserXml(String xmlFilePath) &#123; return parserXml(new File(xmlFilePath)); &#125; /** * 把xml解析成对话格式 * * @param f */ public static XmlEntity parserXml(File f) &#123; long t1 = System.currentTimeMillis(); long t2 = 0; //1、创建一个DocumentBuilderFactory的对象 DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance(); //2、创建一个DocumentBuilder的对象 Document document = null; XmlEntity xmlEntity = new XmlEntity(); try &#123; //创建DocumentBuilder对象 DocumentBuilder db = dbf.newDocumentBuilder(); //3、通过DocumentBuilder对象的parser方法加载books.xml文件到当前项目下 /*注意导入Document对象时，要导入org.w3c.dom.Document包下的*/ //传入文件名可以是相对路径也可以是绝对路径 //document = db.parse(xmlFilePath); document = db.parse(f); xmlEntity.setFileName(f.getName().replace(".xml", "")); t2 = System.currentTimeMillis(); log.info("读文件用时&#123;&#125;s", 1.0*(t2-t1)/1000); &#125; catch (ParserConfigurationException e) &#123; log.error("Dom解析Xml出错 &#123;&#125;", e.toString()); &#125; catch (SAXException e) &#123; log.error("Dom解析Xml出错 &#123;&#125;", e.toString()); &#125; catch (IOException e) &#123; log.error("Dom解析Xml出错 &#123;&#125;", e.toString()); &#125; Element instance = (Element) document.getElementsByTagName("instance").item(0); // 文件保存地址 String waveuri = instance.getAttribute("waveuri"); log.debug("waveuri:&#123;&#125;",waveuri); xmlEntity.setWaveuri(waveuri); String duration = instance.getAttribute("duration"); log.debug("duration:&#123;&#125;",duration); xmlEntity.setDuration(duration); NodeList subjectNodes = document.getElementsByTagName("subject"); if(subjectNodes == null || subjectNodes.getLength() &lt; 2)&#123; log.error("文件格式错误，subject节点个数小于2个"); return null; &#125; log.debug("subject节点个数：&#123;&#125;", subjectNodes.getLength()); /** 处理正文和时间片 */ Element subject1 = (Element) subjectNodes.item(1); NodeList channels = subject1.getElementsByTagName("channel"); log.debug("channels 节点个数:&#123;&#125;", channels.getLength()); // channel0 n0 Element c1 = (Element) channels.item(0); String tagname = c1.getTagName(); log.debug("tagname:&#123;&#125;" ,tagname); Element textElementA = (Element) c1.getElementsByTagName("text").item(0); Element timeElementA = (Element) c1.getElementsByTagName("time").item(0); String textA = textElementA.getTextContent().trim(); log.debug("textA:|&#123;&#125;|", textA); xmlEntity.setN0(textA); String timeA = timeElementA.getTextContent().trim(); log.debug("timeA:|&#123;&#125;|", timeA); String[] textArrayA = textA.split(" "); String[] timeArrayA = timeA.split(" "); int textLengthA = textArrayA.length; log.debug("textLengthA:&#123;&#125;", textLengthA); // channel1 n1 Element c2 = (Element) channels.item(1); String tagname2 = c2.getTagName(); log.debug("tagname2:&#123;&#125;" ,tagname2); Element textElementB = (Element) c2.getElementsByTagName("text").item(0); Element timeElementB = (Element) c2.getElementsByTagName("time").item(0); String textB = textElementB.getTextContent().trim(); log.debug("textB:|&#123;&#125;|", textB); xmlEntity.setN1(textB); String timeB = timeElementB.getTextContent().trim(); log.debug("timeB:|&#123;&#125;|", timeB); String[] textArrayB = textB.split(" "); String[] timeArrayB = timeB.split(" "); int textLengthB = textArrayB.length; log.debug("textLengthB:&#123;&#125;", textLengthB); String n0 = "n0"; String n1 = "n1"; List&lt;TimeTextEntity&gt; timeTextList = new ArrayList&lt;&gt;(textLengthA +textLengthB); if(textLengthA &gt; 1)&#123; // A for(int i = 0; i &lt; textLengthA; i++)&#123; // 一个词语 String oneTerm = textArrayA[i]; // 时间片 String oneTime = timeArrayA[i]; String[] timeArraySub = oneTime.split(","); int start = Integer.parseInt(timeArraySub[0]); int end = Integer.parseInt(timeArraySub[1]); TimeTextEntity t = new TimeTextEntity(start, end, oneTerm, n0); timeTextList.add(t); &#125; &#125; if(textLengthB &gt;1)&#123; // B for(int i =0; i &lt;textLengthB; i++)&#123; // 一个词语 String oneTerm = textArrayB[i]; // 时间片 String oneTime = timeArrayB[i]; String[] timeArraySub = oneTime.split(","); int start = Integer.parseInt(timeArraySub[0]); int end = Integer.parseInt(timeArraySub[1]); TimeTextEntity t = new TimeTextEntity(start, end, oneTerm, n1); timeTextList.add(t); &#125; &#125; long t4 = System.currentTimeMillis(); // 升序 Collections.sort(timeTextList, new Comparator&lt;TimeTextEntity&gt;() &#123; @Override public int compare(TimeTextEntity o1, TimeTextEntity o2) &#123; return new Integer(o1.getStart()).compareTo(o2.getStart()); &#125; &#125;); long t5 = System.currentTimeMillis(); log.info("排序用时&#123;&#125;s", 1.0*(t5-t4)/1000); int allCount = timeTextList.size(); StringBuilder sb = new StringBuilder(); String flag = null; for(int i =0; i &lt; allCount; i++)&#123; log.debug("&#123;&#125; &#123;&#125;", i, timeTextList.get(i)); TimeTextEntity entity = timeTextList.get(i); String who = entity.getWho(); if(who.equals(flag))&#123; sb.append(entity.getText()); sb.append(" "); &#125;else&#123; sb.append("\r\n"); flag = who; sb.append(flag); sb.append(" : "); sb.append(entity.getText()); sb.append(" "); &#125; &#125; // end for xmlEntity.setMix(sb.toString()); long t3 = System.currentTimeMillis(); log.info("解析用时&#123;&#125;s", 1.0*(t3-t2)/1000); log.info("总共用时&#123;&#125;s", 1.0*(t3-t1)/1000); log.debug("对话：&#123;&#125;", sb); return xmlEntity; &#125; &#125;/** * 时间段对象&lt;br&gt; */class TimeEntity&#123; private int start; private int end; public TimeEntity()&#123; &#125; public TimeEntity(int start, int end)&#123; this.start = start; this.end = end; &#125; public int getStart() &#123; return start; &#125; public void setStart(int start) &#123; this.start = start; &#125; public int getEnd() &#123; return end; &#125; public void setEnd(int end) &#123; this.end = end; &#125; @Override public String toString() &#123; return "TimeEntity&#123;" + "start='" + start + '\'' + ", end='" + end + '\'' + '&#125;'; &#125; &#125;/** * */class TimeTextEntity&#123; private int start; private int end; private String text; /** n0 n1 */ private String who; public TimeTextEntity()&#123; &#125; public TimeTextEntity(int start, int end, String text, String who)&#123; this.start = start; this.end = end; this.text = text; this.who = who; &#125; public int getStart() &#123; return start; &#125; public void setStart(int start) &#123; this.start = start; &#125; public int getEnd() &#123; return end; &#125; public void setEnd(int end) &#123; this.end = end; &#125; public String getText() &#123; return text; &#125; public void setText(String text) &#123; this.text = text; &#125; public String getWho() &#123; return who; &#125; public void setWho(String who) &#123; this.who = who; &#125; @Override public String toString() &#123; return "TimeTextEntity&#123;" + "start=" + start + ", end=" + end + ", " + "text='" + text + '\'' + ", who='" + who + '\'' + '&#125;'; &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>xml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java]]></title>
    <url>%2F2017%2F03%2F16%2Fjava%2F</url>
    <content type="text"><![CDATA[java学习 123java -cp %classpath; HelloWorld-Dfile.encoding=UTF-8 12345678910WKQ@WKQ-PC C:\Users\WKQ&gt; chcp活动代码页: 936WKQ@WKQ-PC C:\Users\WKQ&gt; chcp 65001Active code page: 65001java -Dfile.encoding=UTF-8 -jar test.jar 123InputStream in = DictsLoader.class.getClassLoader().getResourceAsStream(&quot;application.properties&quot;);InputStream in = DictsLoader.class.getClassLoader().getResourceAsStream(&quot;dictionary/segDicts/stopwords.txt&quot;); Premature end of file3 字节的 UTF-8 序列的字节 2 无效com.sun.org.apache.xerces.internal.impl.io.MalformedByteSequenceException: 3 字节的 UTF-8 序列的字节 2 无效。编码原因，使用UTF-8编码 Java有关身份证的操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109/** * 有关身份证的操作&lt;br&gt; * * http://www.cnblogs.com/10158wsj/p/7050736.html * http://www.stats.gov.cn/tjsj/tjbz/xzqhdm/ 行政区划代码 * * &lt;pre&gt; * 要进行身份证号码的验证，首先需要了解我国身份证号码的编码规则。 * 我国身份证号码多由若干位数字或者数字与字母混合组成。 * 早期身份证由15位数字构成，这主要是在1980年以前发放的身份证， * 后来考虑到千年虫问题， * 因为15位的身份证号码只能为1900年1月1日到1999年12月31日出生的人编号， * 所以又增加了18位身份证号码编号规则。 * * 15位身份证号 * 3 7 0 9 8 6 8 9 0 6 2 3 2 1 2 * A A A A A A Y Y M M D D N N S * * 18位身份证号 * 3 7 0 9 8 6 1 9 8 9 0 6 2 3 2 1 2 3 * A A A A A A Y Y Y Y M M D D N N S J * * 前六位AAAAAA是身份证编码对象的所在地（出生地）的编码， * 该号码可由国家统计局公布的相关标准中得到。 * YY表示出生年的后两位， * MM和DD表示出生月和日，不足两位的高位补0， * NNS为顺序号，无法确定。 * S为性别识别码，男性为奇数，女性为偶数。 * * 了解了身份证号码的规则后，我们就可以推断出，身份证的15位转化位需要两步。 * 首先把15位身份证号补全为17位，然后再补全最后一位。 * 但是最后一位是数字还是字母X？这里又出现了问题。 * 我们知道，身份证的最后一位为校验位，那么最后一位是怎么得到的呢？ * 原来，最后一位是由数字1-9组成，超过9的比如11就用字母X表示，否则号码就变成了19位。 * 了解了这些，经过整理得出身份证补全算法实现思想如下： * step1 将15位身份证号码加入出生年变为17位 * step2 将step1得到的身份证17位数分别乘以不同的系数。从第1位到第17位的系数分别为：7-9-10-5-8-4-2-1-6-3-7-9-10-5-8-4-2. * step3 将这17位数字和系数相乘的结果相加 * step4 将step3的结果除以11，得出余数 * 由于数字的特殊性，这些余数只可能是0-10这11个数字，身份证最后一位的对应数字为1-0-X-9-8-7-6-5-4-3-2.。 * 例上面的余数结果为3那么对应身份证号码的最后一位就是9，如果是10，身份证最后一位便是2。 * * &lt;/pre&gt; * * http://www.cnblogs.com/10158wsj/p/7050736.html * */public class IDCard &#123; public static void main(String[] args) &#123; System.out.println(transIDCard15to18("370986890623212")); System.out.println(transIDCard15to18("370725881105149")); &#125; /** * 把15位身份证转化为18位标准证件号 * * @param IdCardNO * @return */ public static String transIDCard15to18(String IdCardNO) &#123; String cardNo = null; if (null != IdCardNO &amp;&amp; IdCardNO.trim().length() == 15) &#123; IdCardNO = IdCardNO.trim(); StringBuffer sb = new StringBuffer(IdCardNO); sb.insert(6, "19"); // 19xx年，15位的身份证省略了19 sb.append(transCardLastNo(sb.toString())); cardNo = sb.toString(); &#125; return cardNo; &#125; /** * 15位补全‘19’位后的身份证号码 * */ private static String transCardLastNo(String newCardId) &#123; char[] ch = newCardId.toCharArray(); int m = 0; int[] co = &#123; 7, 9, 10, 5, 8, 4, 2, 1, 6, 3, 7, 9, 10, 5, 8, 4, 2 &#125;; char[] verCode = new char[] &#123; '1', '0', 'X', '9', '8', '7', '6', '5', '4', '3', '2' &#125;; for (int i = 0; i &lt; newCardId.length(); i++) &#123; m += (ch[i] - '0') * co[i]; &#125; int residue = m % 11; return String.valueOf(verCode[residue]); &#125;&#125; Java-Web-errorjunit.framework.AssertionFailedError: Test method isn’t public: test1junit.framework.AssertionFailedError: Test method isn&apos;t public: test java.lang.NoClassDefFoundError: javax/servlet/http/HttpServletRequest123456782017-07-02 20:58:46.771 [main] [ERROR] [org.springframework.boot.SpringApplication] [821] - Application startup failedjava.lang.NoClassDefFoundError: javax/servlet/http/HttpServletRequest at java.lang.Class.getDeclaredMethods0(Native Method) Caused by: java.lang.ClassNotFoundException: javax.servlet.http.HttpServletRequest at java.net.URLClassLoader.findClass(URLClassLoader.java:381) 2017-07-02 20:58:46.774 [main] [INFO ] [o.s.b.logging.ClasspathLoggingApplicationListener] [57] - Application failed to start 在初次使用 IntelliJ IDEA 中，当你使用javax.servlet包下的类时(例:javax.servlet.http.HttpServlet), 在你会发现在IntelliJ IDEA里无法成功编译这个程序。报错如下： java.lang.ClassNotFoundException:javax.el.ELResolver 为什么呢？因为IntelliJ IDEA 没有导入 servlet-api.jar 这个架包，需要你手动导入支持。 解决方案如下：1、选中项目（在IntelliJ中称为Module）；2、点击右键，选择open modual settings（或者直接按F4）；3、在弹出的窗口左端选择Libraries；4、点击顶端的一个类似加号“+”的图标；5、在右端选择第一项（Attach Classes…）；6、在弹出的窗口中选择tomcat所在的目录，进入里面的lib目录，寻找servlet-api.jar这个jar包（如果JSP页面也有相关的JavaWeb对象，则还要寻找jsp-api.jar；如果只有Servlet，则只选择servlet-api.jar）；7、选中上述jar包，依次点击OK。 Unable to start embedded container; nested exception is org.springframework.context.ApplicationContextException: Unable to start EmbeddedWebApplicationContext due to missing EmbeddedServletContainerFactory bean.123456782017-07-02 21:09:54.505 [main] [ERROR] [org.springframework.boot.SpringApplication] [821] - Application startup failedorg.springframework.context.ApplicationContextException: Unable to start embedded container; nested exception is org.springframework.context.ApplicationContextException: Unable to start EmbeddedWebApplicationContext due to missing EmbeddedServletContainerFactory bean. at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.onRefresh(EmbeddedWebApplicationContext.java:133) Caused by: org.springframework.context.ApplicationContextException: Unable to start EmbeddedWebApplicationContext due to missing EmbeddedServletContainerFactory bean. at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.getEmbeddedServletContainerFactory(EmbeddedWebApplicationContext.java:185) 2017-07-02 21:09:54.507 [main] [INFO ] [o.s.b.logging.ClasspathLoggingApplicationListener] [57] - Application failed to start com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry ‘????(????)’ for key12Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry &apos;????(????)&apos; for key &apos;UK_ijdlfwufomeirxn4yddpckkui&apos; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_121] 编码问题修改数据库编码，表编码，字段编码如果数据库是空的，可以删除数据库，重新建一个 The main resource set specified [/war/test-0.1] is not valid123456717-Aug-2017 10:54:49.821 INFO [main] org.apache.catalina.core.StandardEngine.startInternal Starting Servlet Engine: Apache Tomcat/8.5.417-Aug-2017 10:54:49.839 SEVERE [Catalina-startStop-1] org.apache.catalina.core.ContainerBase.startInternal A child container failed during start java.util.concurrent.ExecutionException: org.apache.catalina.LifecycleException: Failed to start component [StandardEngine[Catalina].StandardHost[localhost].StandardContext[/test]] at java.util.concurrent.FutureTask.report(FutureTask.java:122) Caused by: java.lang.IllegalArgumentException: The main resource set specified [/home/admin/software/test/war/test-0.1] is not valid at org.apache.catalina.webresources.StandardRoot.createMainResourceSet(StandardRoot.java:725) /home/admin/software/test/war/test-0.1 这么目录路径写错了，导致tomcat访问不到，路径该了就好了。 值类型是存储在内存中的堆栈（以后简称栈），而引用类型的变量在栈中仅仅是存储引用类型变量的地址，而其本身则存储在堆中。==操作比较的是两个变量的值是否相等，对于引用型变量表示的是两个变量在堆中存储的地址是否相同，即栈中的内容是否相同。equals操作表示的两个变量是否是对同一个对象的引用，即堆中的内容是否相同。 jar creation failed see details additional information1、选中项目右键=&gt;properties=&gt;Java Compiler=&gt;JDK Compliance level栏换一个JDK，最好版本换成1.6以上的2、正常打包流程，一步步点，如果总是不成功，可以尝试将以下两个选项的勾去掉3、选择Project——&gt;Clean 然后选择你的项目，clean一下（由于某种原因，缓存导致）4、如果以上三步都不行的话， 这个时候就看看你项目有没有引用外部jar包，而且这个jar包路径不存在 右键项目——&gt;build Path 看看项目的jar包，如果有不存在的jar包，要么去掉，要么换个路径 Could not find or load main class1、检查JDK环境，环境变量是不是配置有问题2、包名和文件路径不对应，去掉包名。 Java面试题1 Switch能否用string做参数？ a.在 Java 7 之前, switch 只能支持byte,short,char,int 或者其对应的封装类以及 Enum 类型。在JAVA 7中,String 支持被加上了。 2 equals与==的区别： a.==是判断两个变量或实例是不是指向同一个内存空间 equals是判断两个变量或实例所指向的内存空间的值是不是相同 3 Object有哪些公用方法？ a.方法equals测试的是两个对象是否相等 b.方法clone进行对象拷贝 c.方法getClass返回和当前对象相关的Class对象 d.方法notify,notifyall,wait都是用来对给定对象进行线程同步的 4 Java的四种引用，强弱软虚，用到的场景 a.利用软引用和弱引用解决OOM问题：用一个HashMap来保存图片的路径和相应图片对象关联的软引用之间的映射关系，在内存不足时，JVM会自动回收这些缓存图片对象所占用的空间，从而有效地避免了OOM的问题 b.通过软可及对象重获方法实现Java对象的高速缓存:比如我们创建了一Employee的类，如果每次需要查询一个雇员的信息。哪怕是几秒中之前刚刚查询过的，都要重新构建一个实例，这是需要消耗很多时间的。我们可以通过软引用和 HashMap 的结合，先是保存引用方面：以软引用的方式对一个Employee对象的实例进行引用并保存该引用到HashMap 上，key 为此雇员的 id，value为这个对象的软引用，另一方面是取出引用，缓存中是否有该Employee实例的软引用，如果有，从软引用中取得。如果没有软引用，或者从软引用中得到的实例是null，重新构建一个实例，并保存对这个新建实例的软引用。 c.强引用：如果一个对象具有强引用，它就不会被垃圾回收器回收。即使当前内存空间不足，JVM也不会回收它，而是抛出 OutOfMemoryError 错误，使程序异常终止。如果想中断强引用和某个对象之间的关联，可以显式地将引用赋值为null，这样一来的话，JVM在合适的时间就会回收该对象 d.软引用：在使用软引用时，如果内存的空间足够，软引用就能继续被使用，而不会被垃圾回收器回收，只有在内存不足时，软引用才会被垃圾回收器回收。 e..弱引用：具有弱引用的对象拥有的生命周期更短暂。因为当 JVM 进行垃圾回收，一旦发现弱引用对象，无论当前内存空间是否充足，都会将弱引用回收。不过由于垃圾回收器是一个优先级较低的线程，所以并不一定能迅速发现弱引用对象 f.虚引用：顾名思义，就是形同虚设，如果一个对象仅持有虚引用，那么它相当于没有引用，在任何时候都可能被垃圾回收器回收。 g.使用场景： 5 Hashcode的作用，与 equal 有什么区别 a.同样用于鉴定2个对象是否相等的，java集合中有 list 和 set 两类，其中 set不允许元素重复实现，那个这个不允许重复实现的方法，如果用 equal 去比较的话，如果存在1000个元素，你 new 一个新的元素出来，需要去调用1000次 equal 去逐个和他们比较是否是同一个对象，这样会大大降低效率。hashcode实际上是返回对象的存储地址，如果这个位置上没有元素，就把元素直接存储在上面，如果这个位置上已经存在元素，这个时候才去调用equal方法与新元素进行比较，相同的话就不存了，散列到其他地址上 6 String、StringBuffer与StringBuilder的区别 a.String 类型和 StringBuffer 类型的主要性能区别其实在于 String 是不可变的对象 b.StringBuffer和StringBuilder底层是 char[]数组实现的 c.StringBuffer是线程安全的，而StringBuilder是线程不安全的 7 Override和Overload的含义去区别 a.Overload顾名思义是重新加载，它可以表现类的多态性，可以是函数里面可以有相同的函数名但是参数名、返回值、类型不能相同；或者说可以改变参数、类型、返回值但是函数名字依然不变。 b.就是ride(重写)的意思，在子类继承父类的时候子类中可以定义某方法与其父类有相同的名称和参数，当子类在调用这一函数时自动调用子类的方法，而父类相当于被覆盖（重写）了。 8 抽象类和接口的区别 a.一个类只能继承单个类，但是可以实现多个接口 b.接口强调特定功能的实现，而抽象类强调所属关系 c.抽象类中的所有方法并不一定要是抽象的，你可以选择在抽象类中实现一些基本的方法。而接口要求所有的方法都必须是抽象的 9 解析XML的几种方式的原理与特点：DOM、SAX、PULL a.DOM：消耗内存：先把xml文档都读到内存中，然后再用DOM API来访问树形结构，并获取数据。这个写起来很简单，但是很消耗内存。要是数据过大，手机不够牛逼，可能手机直接死机 b.SAX：解析效率高，占用内存少，基于事件驱动的：更加简单地说就是对文档进行顺序扫描，当扫描到文档(document)开始与结束、元素(element)开始与结束、文档(document)结束等地方时通知事件处理函数，由事件处理函数做相应动作，然后继续同样的扫描，直至文档结束。 c.SAX：与 SAX 类似，也是基于事件驱动，我们可以调用它的next（）方法，来获取下一个解析事件（就是开始文档，结束文档，开始标签，结束标签），当处于某个元素时可以调用XmlPullParser的getAttributte()方法来获取属性的值，也可调用它的nextText()获取本节点的值。 10 wait()和sleep()的区别 sleep来自Thread类，和wait来自Object类 调用sleep()方法的过程中，线程不会释放对象锁。而 调用 wait 方法线程会释放对象锁 sleep睡眠后不出让系统资源，wait让出系统资源其他线程可以占用CPU sleep(milliseconds)需要指定一个睡眠时间，时间一到会自动唤醒 11 JAVA 中堆和栈的区别，说下java 的内存机制 a.基本数据类型比变量和对象的引用都是在栈分配的 b.堆内存用来存放由new创建的对象和数组 c.类变量（static修饰的变量），程序在一加载的时候就在堆中为类变量分配内存，堆中的内存地址存放在栈中 d.实例变量：当你使用java关键字new的时候，系统在堆中开辟并不一定是连续的空间分配给变量，是根据零散的堆内存地址，通过哈希算法换算为一长串数字以表征这个变量在堆中的”物理位置”,实例变量的生命周期–当实例变量的引用丢失后，将被GC（垃圾回收器）列入可回收“名单”中，但并不是马上就释放堆中内存 e.局部变量: 由声明在某方法，或某代码段里（比如for循环），执行到它的时候在栈中开辟内存，当局部变量一但脱离作用域，内存立即释放 12 JAVA多态的实现原理 a.抽象的来讲，多态的意思就是同一消息可以根据发送对象的不同而采用多种不同的行为方式。（发送消息就是函数调用） b.实现的原理是动态绑定，程序调用的方法在运行期才动态绑定，追溯源码可以发现，JVM 通过参数的自动转型来找到合适的办法。 13 JAVA 垃圾回收机制 a.标记回收法：遍历对象图并且记录可到达的对象，以便删除不可到达的对象，一般使用单线程工作并且可能产生内存碎片 b.标记-压缩回收法：前期与第一种方法相同，只是多了一步，将所有的存活对象压缩到内存的一端，这样内存碎片就可以合成一大块可再利用的内存区域，提高了内存利用率 c.复制回收法：把现有内存空间分成两部分，gc运行时，它把可到达对象复制到另一半空间，再清空正在使用的空间的全部对象。这种方法适用于短生存期的对象，持续复制长生存期的对象则导致效率降低。 d.分代回收发：把内存空间分为两个或者多个域，如年轻代和老年代，年轻代的特点是对象会很快被回收，因此在年轻代使用效率比较高的算法。当一个对象经过几次回收后依然存活，对象就会被放入称为老年的内存空间，老年代则采取标记-压缩算法 e.引用计数（最简单古老的方法）：指将资源（可以是对象、内存或磁盘空间等等）的被引用次数保存起来，当被引用次数变为零时就将其释放的过程 f.对象引用遍历（现在大多数 jvm 使用的方法）：对象引用遍历从一组对象开始，沿着整个对象图上的每条链接，递归确定可到达（reachable）的对象。如果某对象不能从这些根对象的一个（至少一个）到达，则将它作为垃圾收集 g.什么是垃圾回收机：释放那些不再持有引用的对象的内存 h.怎么判断一个对象是否需要收集？ i.几种垃圾回收机制 14 讲讲 Java 中的集合有多少种，区别是什么？ a.HashTable比较老，是基于Dictionary 类实现的，HashTable 则是基于 Map接口实现的 b.HashTable 是线程安全的， HashMap 则是线程不安全的 c.HashMap可以让你将空值作为一个表的条目的key或value d.ArrayList、LinkedList、Vector的区别：ArrayList 和Vector底层是采用数组方式存储数据，Vector由于使用了synchronized方法（线程安全）所以性能上比ArrayList要差，LinkedList使用双向链表实现存储，随机存取比较慢 e.HashMap的底层源码实现：当我们往HashMap中put元素的时候，先根据key的hashCode重新计算hash值，根据hash值得到这个元素在数组中的位置（即下标），如果数组该位置上已经存放有其他元素了，那么在这个位置上的元素将以链表的形式存放，新加入的放在链头，最先加入的放在链尾。如果数组该位置上没有元素，就直接将该元素放到此数组中的该位置上。 f.Fail-Fast机制:在使用迭代器的过程中有其他线程修改了map，那么将抛出ConcurrentModificationException，这就是所谓fail-fast机制。这一机制在源码中的实现是通过modCount域，modCount顾名思义就是修改次数，对HashMap内容的修改都将增加这个值，那么在迭代器初始化过程中会将这个值赋给迭代器的expectedModCount。在迭代过程中，判断modCount跟expectedModCount是否相等，如果不相等就表示已经有其他线程修改了Map. References[1] https://www.java.com/zh_CN/about/[2] https://docs.oracle.com/javase/tutorial/ Java 教程[3] http://www.oracle.com/technetwork/java/javamagazine/index.html[4] http://www.oracle.com/technetwork/topics/newtojava/youngdevelopers/index.html[5] http://www.oracle.com/technetwork/topics/newtojava/overview/index.html[6] http://www.cnblogs.com/xrq730/p/4826691.html Java虚拟机1：什么是Java[7] http://www.importnew.com/ ImportNew - 专注Java &amp; Android 技术分享[8] http://www.cnblogs.com/zengkefu/p/5633342.html[9] http://blog.csdn.net/moneyshi/article/details/43056321[10] http://blog.csdn.net/whorus1/article/details/51518139[11] https://mp.weixin.qq.com/s/ENxjkPMsBTwImD-sBTNeKw 46张PPT讲述JVM体系结构、GC算法和调优[12] https://mp.weixin.qq.com/s/sHJoPmbpcGDe1XyyI4qKTg 10种简单的Java性能优化[13] https://yq.aliyun.com/articles/73581 2016年Java面试题整理[14] https://yq.aliyun.com/articles/73584 Java集合类操作优化经验总结[15] http://www.toutiao.com/a6409910051989258498/ 图解Java面试题——JVM[16] http://www.toutiao.com/a6409986995707871489/ 2017年终BAT的JAVA面试题聚集[17] http://www.tuicool.com/articles/7RRrIr Java 资源大全中文版 java-web error[1] https://my.oschina.net/fdblog/blog/161305[2] http://blog.csdn.net/zhang168/article/details/51423905[3] https://stackoverflow.com/questions/21783391/spring-boot-unable-to-start-embeddedwebapplicationcontext-due-to-missing-embedd 一点解决版本冲突的应急思路、怎样在所有jar包文件中搜索冲突的方法？]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树 笔记]]></title>
    <url>%2F2017%2F03%2F15%2Fdecision-tree%2F</url>
    <content type="text"><![CDATA[对决策树的学习 以及 决策树在文本分类中的应用。 (1) 生活实例 通俗来说，决策树分类的思想类似于找对象。现想象一个女孩的母亲要给这个女孩介绍男朋友，于是有了下面的对话： 女儿：多大年纪了？ 母亲：26。 女儿：长的帅不帅？ 母亲：挺帅的。 女儿：收入高不？ 母亲：不算很高，中等情况。 女儿：是公务员不？ 母亲：是，在税务局上班呢。 女儿：那好，我去见见。 这个女孩的决策过程就是典型的分类树决策。相当于通过年龄、长相、收入和是否公务员 对将男人分为两个类别：见和不见。 上图完整表达了这个女孩决定是否见一个约会对象的策略，其中绿色节点表示判断条件， 橙色节点表示决策结果，箭头表示在一个判断条件在不同情况下的决策路径， 图中红色箭头表示了上面例子中女孩的决策过程。 这幅图基本可以算是一颗决策树，说它“基本可以算”是因为图中的判定条件没有量化，如收入高中低等等， 还不能算是严格意义上的决策树，如果将所有条件量化，则就变成真正的决策树了。 (2) 定义 决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试， 每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始， 测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。 (3) 决策树分类步骤 使用决策树进行文本分类时，通常包含3个步骤：特征选择、决策树生成、决策树的修剪。以下分别介绍这3个步骤。 (3.1) 特征选择 特征选择的常用方法就不说了，直接说结果。 通过参考论文和测试，发现使用CHI进行特征选择选择出的特征效果较好。本案例中采用的是CHI改进版。 CHI的主要思想是认为特征和类别关系符合X2分布，X2统计量的值越高，就认为特征和类别间的差异性越小， 则两者的相关性就越高，就可以认为特征对类别的贡献度越大。 X2公式： n表示特征频数总和 n11 表示特征i在类别j中出现的频数 n12 表示特征i在除类别j外的其它类别出现的频数 n21 表示除特征i外其他特征在类别j中出现的频数 n22 表示除特征i外的其他特征在除类别j外的其他类别出现的频数 但是CHI有一个缺点，在统计时只关系否出现特征，却不管特征在该文档中出现了几次，这会使得他对低频特征 有所偏袒（因为它夸大了低频特征的作用）。为了解决这个问题，对CHI公式进行改进。 X2改进版公式： 从公式可以看到，X2改进版不仅可以看出特征对类别的贡献度，还可以看到特征和类别的相关性。 在改进的X2统计量的值上规定特征i的CHI值为： s为文本类别的数量 |X2|是X2的绝对值 使用X2改进版进行特征提取后，特征个数较多，假设为M个，为了进一步降维，同时为了各个特征对 各个类别的贡献是否一致，必须将每个特征特征的X2统计量统一处理到一个固定的区间，为[-1, 1]，按公式 其中，max_i min_i 为特征i对各个类别X2统计的最大值和最小值。 这样会得到新的特征个数，假设为L个，L要远小于M (3.2) 决策树生成 决策树生成是决策树中比较重要的一步。决策树生成的算法有ID3, C45, CART。 (3.2.1) ID3 ID3算法的核心思想就是以信息增益度量属性选择，选择分裂后信息增益最大的属性进行分裂。 从信息论知识中我们知道，期望信息越小，信息增益越大，从而纯度越高。 3.2.1.1 熵 在信息论与概率统计中，熵(entropy)是随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为 P(X=xi) = pi i=1,2,…,n 则随机变量X的熵定义为 熵越大，随机变量的不确定性就越大。 3.2.1.2 条件熵 设有随机变量(X,Y),其联合概率分布为 P(X=xi, Y=yi)=pij, i=1,2,…n; j=1,2,…m 条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量C给定条件下 随机变量Y的条件熵(conditional entropy) H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望 pi = P(X=xi), i=1,2,…,n 3.2.1.3 信息增益 特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即 g(D,A) = H(D) - H(D|A) 3.2.1.4 ID3原理及步骤 ID3算法使用信息增益来分裂训练数据集，选择信息增益值最大的特征，根据该特征的取值来划分数据集。被划分的数据集重复此过程，直到信息增益很小或没有特征选择为止。 H(D)表示数据集D的不确定性 pi表示第i个类别在整个训练数据集(语料)D中出现的概率 Di表示类别i的文档个数 D代表所有文档个数 n代表训练数据集(语料)D的类别个数 H(D|A)表示加入特征A后，数据集D的不确定性 n为特征X的取值个数 Di为特征A取值为i对应的值时的文档个数 D代表所有文档个数 g(D,A) = H(D) - H(D|A) g(D,A)表示引入特征A后，数据集D减少的不确定程度 ID3算法步骤： 输入：训练数据集D，特征集A，阀值f 输出：决策树T (1) 若D中所有实例属于同一类Ck，则T为单结点树，并将类Ck作为该结点的类标记，返回T； (2) 若A为空集，则T为单结点数，并将D中实例数最大的类Ck作为该结点的类标记，返回T； (3) 否则，计算A中各特征对D的信息增益，选择信息增益最大的特征Ag； (4) 如果Ag的信息增益小于阀值f，则T为单结点树，并将D中实例数最大的类Ck作为该节点的类标记，返回T； (5) 否则，对Ag的每一可能值ai，依Ag=ai将D分割为若干非空子集Di，将Di中实例数最大的类作为标记， 构建子结点，由结点及其子结点构成树T，返回T； (6) 对第i个子结点，以Di为训练集，以A - {Ag}为特征集，递归调用步骤(1) - 步骤(5),得到子树T，返回Ti。 (3.2.2) C4.5 C45算法与ID3算法相似，C45算法对ID3算法进行了改进，C45在生成的过程中，用信息增益比来选择特征。 3.2.2.1 信息增益比 信息增益比的大小是相对于训练数据集而言的，并没有绝对意义。在分类问题困难时，也就是训练数据集 的经验熵大的时候，信息增益值会偏大。反之，信息增益值会变小。使用信息增益值会对这一问题进行校正。 特征A对训练数据集D的信息增益比gr(D,A)定义为其信息增益g(D,A)与训练数据集D的经验熵H(D)之比: gr(D, A) = g(D, A) / H(D) gr(D,A)表示引入特征A后使数据集D减少的不确定程度 相比 数据集D的不确定性 的值。 这么做避免了在部分数据集上某些特征信息增益较大的情况，使所有特征的信息增益在整个数据集上做比较，分类效果更好。 3.2.2.2 C45原理及步骤 C45算法使用信息增益比来分裂训练数据集，选择信息增益比最大的特征，根据该特征的取值来划分数据集。 被划分的数据集重复此过程，直到信息增益比很小或没有特征选择为止。 C45算法步骤： 输入：训练数据集D，特征集A，阀值f 输出：决策树T (1) 若D中所有实例属于同一类Ck，则T为单结点树，并将类Ck作为该结点的类标记，返回T； (2) 若A为空集，则T为单结点数，并将D中实例数最大的类Ck作为该结点的类标记，返回T； (3) 否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征Ag； (4) 如果Ag的信息增益比小于阀值f，则T为单结点树，并将D中实例数最大的类Ck作为该节点的类标记，返回T； (5) 否则，对Ag的每一可能值ai，依Ag=ai将D分割为若干非空子集Di，将Di中实例数最大的类作为标记， 构建子结点，由结点及其子结点构成树T，返回T； (6) 对第i个子结点，以Di为训练集，以A - {Ag}为特征集，递归调用步骤(1) - 步骤(5),得到子树T，返回Ti。 (3.3) 决策树的剪枝 决策树生成算法递归地产生决策树，直到不能继续下去为止，这样产生的树往往对训练数据的分类很准确， 但是对未知测试数据的分类却没有那么准确，即出现过拟合现象。过拟合的原因在于学习时过多地考虑 如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法是考虑决策树的复杂度， 对已生成的决策树进行简化。 决策树很容易发生过拟合，过拟合的原因在于学习的时候过多地考虑如何提高对训练数据的正确分类， 从而构建出过于复杂的决策树。解决这个问题的办法就是简化已生成的决策树，也就是剪枝。 决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。 设决策树T的叶节点有|T|个， t是某个叶节点， t有Nt个样本点， 其中归入k类的样本点有Ntk个， Ht(T)为叶节点t上的经验熵， α≥0为参数，则损失函数可以定义为： 其中经验熵为 如果剪枝后的损失函数的值小于剪枝前的损失函数的值，则可以剪枝。 C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度， |T|表示模型复杂度，参数α≥0控制两者之间的影响，α越大，模型越简单，α=0表示不考虑复杂度。 剪枝，就是当α确定时，选择损失函数最小的模型。 子树越大C(T)越小，但是α|T|越大，损失函数反映的是两者的平衡。 决策树的生成过程只考虑了信息增益或信息增益比，只考虑更好地拟合训练数据， 而剪枝过程则考虑了减小复杂度。前者是局部学习，后者是整体学习。 References决策树[1]《统计学习方法》 李航[2] df.pdf[3] 码农场 决策树介绍[4] decision-tree[5] 我们为什么需要信息增益比，而不是信息增益[6] IBM 决策树算法介绍及应用[7] decisiontree[8] 决策树[9] Learning to Classify Text[10] 决策树+文本分类 决策树 文本分类[11] 江苏科技大学2011年硕士论文[12] 东北大学2008年硕士论文[13] 博客 决策树 特征提取:[14] 基于类内词频改进互信息特征选择算法[15] 文本挖掘之特征选择[16] 特征选择[17] 特征选择]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>classifier</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 笔记]]></title>
    <url>%2F2017%2F03%2F14%2Fmysql-notes%2F</url>
    <content type="text"><![CDATA[本文主要介绍有关MySQL的内容，包括一些常用配置，常见问题。根据个人使用经验总结，希望可以帮到大家。 MySQL配置下面是我的配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586[client]port=3306# utf8mb4 is a superset of utf8default-character-set = utf8mb4[mysql]# utf8mb4 is a superset of utf8default-character-set = utf8mb4# SERVER SECTION# ----------------------------------------------------------------------## The following options will be read by the MySQL Server. Make sure that# you have installed the server correctly (see above) so it reads this # file.#[mysqld]# utf8mb4 is a superset of utf8character-set-server=utf8mb4#collation-server=utf8mb4_unicode_ci #collation-server=utf8_general_ci#character-set-client-handshake = FALSE#init_connect=&apos;SET NAMES utf8mb4&apos;# mkdir for every databaseinnodb_file_per_table=1# ignore lowercaselower_case_table_names=1# all import biggest 1024M file to mysqlmax_allowed_packet=1024M# The TCP/IP Port the MySQL Server will listen onport=3306#log# Binary Loglog-bin=mysql-binbinlog-format=ROW server_id=1# if query_time &gt; 1s sql will loglong_query_time=1# if query is slow, query will log version 5.6slow-query-log=1slow-query-log-file = /usr/local/mysql/log/slow_query.log# slow-query-log-file=c:/professionsofware/mysql/log/slow_query.log# version5.0 log-slow-queries=c:/professionsofware/mysql/log/slow_query.log# log all query version5.6general_log=ONgeneral_log_file = /usr/local/mysql/log/all_query.log#general_log_file=c:/ProfessionSofware/MySQL/log/all_query.log#version5.0 log=c:/ProfessionSofware/MySQL/log/all_query.log# log error #log-error=c:/professionsofware/mysql/log/mysql_error.log#Path to installation directory. All paths are usually resolved relative to this.#basedir=&quot;C:/ProfessionSofware/MySQL/MySQLServer5.6/&quot;basedir=/usr/local/mysql #Path to the database root#datadir=&quot;C:/ProgramData/MySQL/MySQL Server 5.6/Data/&quot;datadir=/usr/local/mysql/data# The default character set that will be used when a new schema or table is# created and no character set is defined#character-set-server=gbk# The default storage engine that will be used when create new tables whendefault-storage-engine=INNODB# The default storage engine that will be used for temporary tablesdefault-tmp-storage-engine=INNODB MySQL编码1234567891011121314mysql&gt; show variables like "%char%";+--------------------------+---------------------------------------------------------------+| Variable_name | Value |+--------------------------+---------------------------------------------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/local/mysql-5.7.16-linux-glibc2.5-x86_64/share/charsets/ |+--------------------------+---------------------------------------------------------------+8 rows in set (0.01 sec) 123456789--如果仍有编码不是utf8的，请检查配置文件，也可使用mysql命令设置：set character_set_client = utf8;set character_set_server = utf8;set character_set_connection = utf8;set character_set_database = utf8;set character_set_results = utf8;set collation_connection = utf8_general_ci;set collation_database = utf8_general_ci;set collation_server = utf8_general_ci; show variables like &#39;collation_%&#39;; show variables like &#39;character_set_%&#39;; MySQL日志MySQL默认是不开启那些日志的，如：二进制日志，错误日志，慢查询日志，查询日志等 MySQL有以下几种日志： 错误日志：-log-err 查询日志：-log 慢查询日志: -log-slow-queries 更新日志:-log-update 二进制日志：-log-bin 查看是否启用了日志 show variables like &#39;log_%&#39;; 12345678910111213141516mysql&gt; show variables like 'log_%';+----------------------------------------+------------------------------------------------------------+| Variable_name | Value |+----------------------------------------+------------------------------------------------------------+| log_bin | ON || log_bin_basename | C:\ProgramData\MySQL\MySQL Server 5.6\Data\mysql-bin || log_bin_index | C:\ProgramData\MySQL\MySQL Server 5.6\Data\mysql-bin.index || log_bin_trust_function_creators | OFF || log_error | C:\ProgramData\MySQL\MySQL Server 5.6\Data\WKQ-PC.err || log_output | FILE || log_queries_not_using_indexes | OFF || log_slave_updates | OFF || log_throttle_queries_not_using_indexes | 0 || log_warnings | 1 |+----------------------------------------+------------------------------------------------------------+10 rows in set (0.00 sec) 开启二进制日志开启二进制日志有两种办法 修改配置文件修改MySQL安装目录下的 my.ini (windows环境) 123456[mysqld]# Binary Loglog-bin=mysql-bin#在[mysqld]下添加以上两行，添加完保存文件，重启MySQL服务就可以看到二进制日志启用了 通过命令修改 显示二进制日志数目 show master logs; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556mysql&gt; show master logs;+------------------+-----------+| Log_name | File_size |+------------------+-----------+| mysql-bin.000001 | 139 || mysql-bin.000002 | 5004253 || mysql-bin.000003 | 139 || mysql-bin.000004 | 139 || mysql-bin.000005 | 139 || mysql-bin.000006 | 139 |中间的一部分省略| mysql-bin.000158 | 139 || mysql-bin.000159 | 120 |+------------------+-----------+159 rows in set (0.04 sec)``` ### 查看二进制日志 bin-log因为是二进制文件，不能通过记事本等编辑器直接打开查看，mysql提供两种方式查看方式```sqlmysql&gt; show binlog events in 'mysql-bin.000001';+------------------+-----+-------------+-----------+-------------+-----------------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+------------------+-----+-------------+-----------+-------------+-----------------------------------------+| mysql-bin.000001 | 4 | Format_desc | 1 | 120 | Server ver: 5.6.5-m8-log, Binlog ver: 4 || mysql-bin.000001 | 120 | Stop | 1 | 139 | |+------------------+-----+-------------+-----------+-------------+-----------------------------------------+2 rows in set (0.00 sec)Log_name:此条log存在那个文件中，从上面可以看出这2条log皆存在与mysql_bin.000001文件中。Pos:log在bin-log中的开始位置Event_type:log的类型信息Server_id:可以查看配置中的server_id,表示log是那个服务器产生End_log_pos：log在bin-log中的结束位置Info:log的一些备注信息，可以直观的看出进行了什么操作C:\ProgramData\MySQL\MySQL Server 5.6\data&gt;mysqlbinlog mysql-bin.000001/*!40019 SET @@session.max_insert_delayed_threads=0*/;/*!50003 SET @OLD_COMPLETION_TYPE=@@COMPLETION_TYPE,COMPLETION_TYPE=0*/;DELIMITER /*!*/;#at 4#170121 21:19:46 server id 1 end_log_pos 120 Start: binlog v 4, server v 5.6.5-m8-log created 170121 21:19:46 at startupROLLBACK/*!*/;BINLOG '8l+DWA8BAAAAdAAAAHgAAAAAAAQANS42LjUtbTgtbG9nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADyX4NYEzgNAAgAEgAEBAQEEgAAXAAEGggAAAAICAgCAAAAAAAAGRkAAH+qGBU='/*!*/;#at 120#170121 23:23:22 server id 1 end_log_pos 139 StopDELIMITER ;#End of log fileROLLBACK /* added by mysqlbinlog */;/*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/; 删除二进制日志12345678910// 删除编号000003之前的所有日志purge binary logs to 'mysql-bin.000003';// 删除2017-05-11 22:00:00之前的所有日志mysql&gt; purge master logs before '2017-05-11 22:00:00';Query OK, 0 rows affected (0.53 sec)// 设置参数—expire_logs_days=#(days)，此参数的含义是设置日志的过期天数，过来指定的天数后日志将会被自动删除，这样将有利于减少DBA管理日志的工作量// 这样，3天前的日志都会被删除，系统自动删除--expire_logs_days=3 用户管理添加用户MySQL创建用户的方法分成三种：INSERT USER表的方法、CREATE USER的方法、GRANT的方法。 第一种 insert into mysql.user(Host,User,Password) values(&quot;%&quot;,&quot;test&quot;,password(&quot;123abc&quot;)); 第二种 create user &#39;user1&#39;@&#39;%&#39; identified by &#39;123abc&#39;; create user &#39;user1&#39;@&#39;localhost&#39; identified by &#39;123abc&#39;; create user &#39;user1&#39;@&#39;192.168.189.*&#39; identified by &#39;123abc&#39;; 第三种 grant all privileges on *.* to &#39;user1&#39;@&#39;%&#39; identified by &#39;123abc&#39; with grant option; grant all privileges on test.* to &#39;user1&#39;@&#39;192.168.10.1&#39; identified by &#39;123abc&#39;; 创建完记得 flush privileges; SQL语句1.查询结果去重User表里name有重复的，想要不重复的name的用户的全部信息。 select *, count(distinct name) from user group by name; 删除数据表中重复记录，只保留一条 delete from 表名 where 字段ID in (select * from (select max(字段ID) from 表名 group by 重复的字段 having count(重复的字段) &gt; 1) as b); delete from un_visited_url where id not in ( select * from (select max(id) from un_visited_url group by unique_key having count(unique_key)&gt;1 ) as b ); mysql分页查询123456789mysql&gt; SELECT * FROM table LIMIT 5,10; // 检索记录行 6-15//为了检索从某一个偏移量到记录集的结束所有的记录行，可以指定第二个参数为 -1： mysql&gt; SELECT * FROM table LIMIT 95,-1; // 检索记录行 96-last.//如果只给定一个参数，它表示返回最大的记录行数目： mysql&gt; SELECT * FROM table LIMIT 5; //检索前 5 个记录行//换句话说，LIMIT n 等价于 LIMIT 0,n。 查询最后一条数据 select * from user order by id desc limit 1; mysql中如何查看某个数据库或表占用的磁盘空间1234567-- 查整个库的状态：select concat(truncate(sum(data_length)/1024/1024,2),'MB') as data_size, concat(truncate(sum(max_data_length)/1024/1024,2),'MB') as max_data_size, concat(truncate(sum(data_free)/1024/1024,2),'MB') as data_free, concat(truncate(sum(index_length)/1024/1024,2),'MB') as index_size from information_schema.tables where TABLE_SCHEMA = 'databasename';-- 注意是**TABLE_SCHEMA** databasename 1234567查单表：select concat(truncate(sum(data_length)/1024/1024,2),'MB') as data_size, concat(truncate(sum(max_data_length)/1024/1024,2),'MB') as max_data_size, concat(truncate(sum(data_free)/1024/1024,2),'MB') as data_free, concat(truncate(sum(index_length)/1024/1024,2),'MB') as index_size from information_schema.tables where TABLE_NAME = 'tablename';-- 注意是**TABLE_NAME** 1234--删除user表truncate table user ;delete * from user ; 12345show variables like 'character_set_database'; # 数据库编码show variables like 'collation_%';show variables like 'character_set_%'; alter table user modify column sex enum(&#39;0&#39;,&#39;1&#39;); 优化1234567891011121314151617181920212223242526mysql&gt; show engines;+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+| Engine | Support | Comment | Transactions | XA | Savepoints |+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+| MyISAM | YES | MyISAM storage engine | NO | NO | NO || CSV | YES | CSV storage engine | NO | NO | NO || PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO || BLACKHOLE | YES | /dev/null storage engine (anything you write to it disappears) | NO | NO | NO || MRG_MYISAM | YES | Collection of identical MyISAM tables | NO | NO | NO || InnoDB | DEFAULT | Supports transactions, row-level locking, and foreign keys | YES | YES | YES || ARCHIVE | YES | Archive storage engine | NO | NO | NO || MEMORY | YES | Hash based, stored in memory, useful for temporary tables | NO | NO | NO || FEDERATED | NO | Federated MySQL storage engine | NULL | NULL | NULL |+--------------------+---------+----------------------------------------------------------------+--------------+------+------------+9 rows in set (0.00 sec)mysql&gt; show variables like '%storage_engine%';+----------------------------------+--------+| Variable_name | Value |+----------------------------------+--------+| default_storage_engine | InnoDB || default_tmp_storage_engine | InnoDB || disabled_storage_engines | || internal_tmp_disk_storage_engine | InnoDB |+----------------------------------+--------+4 rows in set (0.00 sec) References[1] https://dev.mysql.com/doc/refman/5.7/en/preface.html[2] https://dev.mysql.com/doc/refman/5.7/en/create-user.html[3] https://yq.aliyun.com/topic/100[4] http://blog.csdn.net/a351945755/article/details/28239819 version5[5] http://blog.itpub.net/29510932/viewspace-1668784/ version5.6[6] http://blog.csdn.net/zqtsx/article/details/24381583[7] http://www.jb51.net/article/25360.htm mysql[8] http://www.cnblogs.com/dapeng111/archive/2013/01/02/2842106.html mysql注释[9] http://blog.csdn.net/cto_51/article/details/8791200 mysql中如何查看某个数据库或表占用的磁盘空间[10] http://www.cnblogs.com/paul8339/p/5649151.html Mysql删除数据后，磁盘空间未释放的解决办法[11] http://flyer0126.iteye.com/blog/497359[12] http://www.cnblogs.com/hustcat/archive/2009/12/19/1627525.html[13] http://www.cnblogs.com/xwdreamer/archive/2012/07/11/2585993.html Server Collation介绍及其变更对数据的影响[14] https://dev.mysql.com/doc/refman/5.7/en/charset-database.html[15] https://dev.mysql.com/doc/refman/5.7/en/charset-unicode-conversion.html[16] http://seanlook.com/2016/10/23/mysql-utf8mb4/[17] http://blog.chinaunix.net/uid-23916356-id-5765908.html[18] MySQL 超新手入门教程系列[19] http://blog.okbase.net/haobao/archive/1213.html[20] MySQL经典题目[21] http://www.iswoole.com/article/2054 全面的MySQL优化参考[22] http://www.iswoole.com/article/2053 mysql索引最左匹配原则的理解[23] http://www.cnblogs.com/echo-something/archive/2012/08/26/mysql_int.html 详解mysql int类型的长度值问题[24] http://imysql.com[25] http://blog.codinglabs.org/articles/theory-of-mysql-index.html[26] http://blog.codinglabs.org/articles/index-condition-pushdown.html]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo搭建博客]]></title>
    <url>%2F2017%2F03%2F13%2Fhexo-build-blog%2F</url>
    <content type="text"><![CDATA[原来在学会一些东西的时候总喜欢总结，做一些笔记。刚开始用txt做笔记。使用Git来管理版本。后来很多需要图片的，发现txt不方便，就开始用word。但是在用git提交的时候就发现word的不方便了。而且word不是纯文本文件，在其他电脑或者手机上打开不太方便。后来发现了markdown，就是用markdown来写博客。 在写博客前，在网上搜了好多写博客的方法。最后发现使用markdown格式的比较多。而且最近大家都用github + hexo来搭建自己的博客，教程也比较多，所以自己也想搭一个试试。 (1) 安装软件(1.1) 安装git 在Git官网下载，或者可以在百度下载。下载完以后双击运行。一直点next，直到完成安装。 通过 git --version 验证是否安装正确以及查看版本。 (1.2) 安装Node.js 在Node.js官网下载，或者在百度下载。推荐使用zip包，解压完配置一下就能用。 下载完解压到一个目录，解压文件到 D:\ProfessionalSoftWare\Node , 并在解压后的目录下建立 node_global和node_cache (node_global: npm全局安装路径 node_cache: npm全局缓存路径) 新建环境变量 NODE_PATH = D:\ProfessionalSoftWare\Node\node-v10.16.0-win-x64 修改环境变量 PATH 增加 %NODE_PATH%;%NODE_PATH%\node_global; 通过 node -v 验证是否安装正确以及查看版本。 注意：npm其实是Node.js的包管理工具（package manager），刚开始一直不知道nmp和Node.js是什么关系，晕了半天 (1.3) 配置使用 在所有程序里打开CMD，通过CMD来配置 配置git git config --global use.name &quot;wkq&quot; git config --global use.email &quot;weikeqin.cn@gmail.com&quot; 大家估计都有被“墙”的经历，安装hexo为了避免出现类似情况，我使用淘宝NPM镜像,输入以下命令等待安装完成 npm install -g cnpm --registry=https://registry.npm.taobao.org 使用淘宝NPM安装Hexo cnpm install -g hexo-cli 出现WARN不用管，继续输入一下命令 cnpm install hexo --save 安装完成后，通过 hexo -v，验证是否安装正确 在cmd里进入任意目录，在该目录下创建博客，目录名可以自定义，这儿我使用blog作为博客名， 创建完该目录里就保存你博客的主要信息 hexo init blog 生成静态页面至public目录 hexo generate 部署并使用5000端口，执行完后在浏览器输入 http://localhost:5000 就可以访问博客了，端口可以自定义 hexo server -p 5000 (2) hexo其他配置(2.1) RSS和站点地图 npm install hexo-generator-feed --save npm install hexo-generator-sitemap --save npm install hexo-generator-baidu-sitemap --save rss、通用搜索引擎sitemap、百度爬虫sitemap 之后重启博客，访问/atom.xml和/sitemap.xml，会发现已经生成了。可以把sitemap提交到搜索引擎的站长平台来增加收录 npm install hexo-generator-search --save npm install hexo-generator-searchdb --save npm install hexo-renderer-stylus --save (2.2) 添加tags页面和categories页面 tags页面是标签云 categories页面是分类 12345678910111213141516# 新建tags页面和categories页面 hexo new page tags hexo new page categories# 修改对应的index.mdedit source/tags/index.mdadd line: layout: &quot;tags&quot;edit source/categories/index.mdadd line: layout: &quot;categories&quot;delete db.jsonhexo cleanhexo generate (2.3) 添加字数统计 npm install hexo-wordcount --save wordcount可以实现字数统计，阅读时常还有总字数的统计功能 只需要 npm install hexo-wordcount --save 就可以安装wordcount插件， 主要功能 字数统计:WordCount 阅读时长预计:Min2Read 总字数统计: TotalCount 安装完插件之后在主题的配置文件中开启该功能就可以~ (2.4) 集成Algolia搜索 npm install hexo-algolia --save npm install hexo-algoliasearch --save 在站点的_config.yml中加入如下配置:12345678910algolia: appId: &apos;appId&apos; apiKey: &apos;apiKey&apos; adminApiKey: &apos;adminApiKey&apos; indexName: &apos;indexName&apos; chunkSize: 5000 fields: - title - slug - content:strip 配置环境变量HEXO_ALGOLIA_INDEXING_KEY，值为获取的apiKey，如果不知道用哪个，可以设置为 Admin API Key (3) 域名解析hexo blog 托管到github和coding这么做有几个目的 增加流量，百度爬不到github的内容，但可以爬到coding的内容 国内加速 互为镜像，即使一个不能用了，还有另一个 1234 CNAME www 默认 pages.coding.meCNAME @ 默认 pages.coding.me A www 海外 151.101.xxx.xxx (对应username.github.io的ip地址) A @ 海外 151.101.xxx.xxx 域名解析参数含义：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172(1)记录类型 A记录： 将域名指向一个IPv4地址（例如：10.10.10.10），需要增加A记录 CNAME记录： 如果将域名指向一个域名，实现与被指向域名相同的访问效果，需要增加CNAME记录 MX记录： 建立电子邮箱服务，将指向邮件服务器地址，需要设置MX记录 NS记录： 域名解析服务器记录，如果要将子域名指定某个域名服务器来解析，需要设置NS记录 TXT记录： 可任意填写（可为空），通常用做SPF记录（反垃圾邮件）使用 AAAA记录： 将主机名（或域名）指向一个IPv6地址（例如：ff03:0:0:0:0:0:0:c1），需要添加AAAA记录 SRV记录： 记录了哪台计算机提供了哪个服务。格式为：服务的名字.协议的类型（例如：_example-server._tcp） 显性URL： 将域名指向一个http（s)协议地址，访问域名时，自动跳转至目标地址（例如：将www.net.cn显性转发到www.hichina.com后，访问www.net.cn时，地址栏显示的地址为：www.hichina.com）。 隐性URL： 与显性URL类似，但隐性转发会隐藏真实的目标地址（例如：将www.net.cn隐性转发到www.hichina.com后，访问www.net.cn时，地址栏显示的地址仍然为：www.net.cn）。温馨提示： 搭建网站：要将域名指向主机服务商提供的IP地址，请选择「A记录」；要将域名指向主机服务商提供的另一个域名，请选择「CNAME记录」。 建立邮箱：需要设置「MX记录」，根据邮箱服务商提供的MX记录填写。(2)主机记录 主机记录就是域名前缀，常见用法有： www ： 将域名解析为www.example.com，填写www； @ ： 将域名解析为example.com（不带www），填写@或者不填写； mail ： 将域名解析为mail.example.com，通常用于解析邮箱服务器； * ： 泛解析，所有子域名均被解析到统一地址（除单独设置的子域名解析）； 二级域名 ： 如：mail.example.com或abc.example.com，填写mail或abc； 手机网站 ： 如：m.example.com，填写m。 温馨提示： 要将域名example.com解析为www.example.com，在主机记录(RR)处填写www即可。(3)解析线路(运营商) 默认 联通 电信 移动 教育网 海外 搜索引擎(4) 记录值温馨提示： A记录值请填写您的服务器IP地址（必须为IPv4地址，例如：202.106.0.20），若不清楚IP，请您咨询您的空间服务商。 如果IP地址的格式中带有端口，如：202.106.0.20:8080，则只添加202.106.0.20即可。(5) TTL 10分钟 30分钟 1小时 12小时 24小时 一般都选第一个，10分钟(6) 我的配置 CNAME www 默认 pages.coding.me CNAME @ 默认 pages.coding.me A www 海外 151.101.xxx.xxx (对应username.github.io的ip地址) A @ 海外 151.101.xxx.xxx (4) hexo升级 升级需谨慎 升级没有好办法，升级hexo 要换一个目录重新init，可以_config.yml可能增加了一些配置，需要自己改 source/_post下的文件可以拷过来 插件需要自己安装 可能会遇到很多意想不到的错误，不要急，慢慢解决就行 如果没有必须升级的必要，建议不要升级。 12345npm install hexo-cli -ghexo init blogcd blognpm installhexo server (5) 常用命令12345678910111213141516171819202122232425262728293031323334353637 npm install hexo -g #安装hexo npm update hexo -g #升级 hexo init #初始化 hexo init 创建目录 hexo new &quot;postName&quot; # 新建文章 hexo new page &quot;pageName&quot; # 新建页面 hexo generate # 生成静态页面至public目录 hexo server # 开启预览访问端口（默认端口4000，&apos;ctrl + c&apos;关闭server） hexo deploy # 将.deploy目录部署到GitHub hexo generate #使用 Hexo 生成静态文件快速而且简单 hexo generate --watch #监视文件变动 hexo server # Hexo会监视文件变动并自动更新，您无须重启服务器。 hexo server -s # 静态模式 hexo server -p 5000 # 部署并使用5000端口 hexo server -i 192.168.1.1 # 自定义 IP hexo server -g #生成加预览 hexo deploy -g #生成加部署完成后部署两个命令的作用是相同的 hexo generate --deploy hexo deploy --generate hexo new [layout] &lt;title&gt; hexo new photo &quot;My Gallery&quot; hexo new &quot;Hello World&quot; --lang tw hexo help # 查看帮助 hexo version #查看Hexo的版本 草稿 私密博客 草稿相当于很多博客都有的“私密文章”功能。 $ hexo new draft “draft” 会在source/_drafts目录下生成一个new-draft.md文件。但是这个文件不被显示在页面上，链接也访问不到。 也就是说如果你想把某一篇文章移除显示，又不舍得删除，可以把它移动到_drafts目录之中。 (6) 遇到的错误(6.1) ERROR Deployer not found: gitnpm install hexo-deployer-git --save 改了之后执行hexo deploy就可以了 (6.2) Cannot set property ‘lastIndex’ of undefined1234567891011121314151617WKQ@WKQ-PC C:\blog&gt; hexo algolia --debug13:34:28.423 DEBUG Hexo version: 3.3.113:34:28.426 DEBUG Working directory: C:\blog\13:34:28.601 DEBUG Config loaded: C:\blog\_config.yml...13:34:38.088 INFO Error has occurred during collecting posts : TypeError: Cannot set property &apos;lastIndex&apos; of undefined13:34:38.089 FATAL Something&apos;s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlTypeError: Cannot set property &apos;lastIndex&apos; of undefined at highlight (C:\blog\node_modules\highlight.js\lib\highlight.js:511:35)13:34:39.040 DEBUG Database saved13:34:39.042 FATAL Cannot set property &apos;lastIndex&apos; of undefinedTypeError: Cannot set property &apos;lastIndex&apos; of undefined at highlight (C:\blog\node_modules\highlight.js\lib\highlight.js:511:35) 在博客的_config.yml中auto_detect设为false 遇到这种情况，首先重新clean generate 一次如果重新clean generate后还出现这个问题，可能是哪篇博客里的内容有问题，可以用排除法解决如果hexo clean generate时报错信息变了，那就是另一个问题了 (6.3) Please provide an application ID. Usage: algoliasearch(applicationID, apiKey, opts)123456789101112131415WKQ@WKQ-PC C:\blog&gt; hexo algolia --debug14:54:55.858 DEBUG Hexo version: 3.3.114:54:55.862 DEBUG Working directory: C:\blog\14:54:56.037 DEBUG Config loaded: C:\blog\_config.yml...C:\blog\node_modules\bluebird\js\release\async.js:61 fn = function () &#123; throw arg; &#125;; ^AlgoliaSearchError: Please provide an application ID. Usage: algoliasearch(applicationID, apiKey, opts) at AlgoliaSearchNodeJS.AlgoliaSearchCore (C:\blog\node_modules\algoliasearch\src\AlgoliaSearchCore.js:51:11) at processImmediate [as _immediateCallback] (timers.js:596:5) algolia在新版本中没有使用apiKey和adminApiKey，而是使用环境变量的方式来获取apikey，需要配置一个环境变量HEXO_ALGOLIA_INDEXING_KEY=’you apiKey’ 参考next主题有关algolia的Issues 和 algolia官方文档 后发现algolia在新版本中没有使用apiKey和adminApiKey，而是使用环境变量的方式来获取apikey，所以要本地配置一个环境变量，123windows下 计算机 右键 属性 高级系统设置 环境变量 新建 HEXO_ALGOLIA_INDEXING_KEY xxx(对应的apikey) 或者可以使用dos命令行 set HEXO_ALGOLIA_INDEXING_KEY=xxx(对应的apikey)linux下 export HEXO_ALGOLIA_INDEXING_KEY=xxx(对应的apikey) 在https://www.algolia.com/apps/${your count}/api-keys/restricted页面修改配置 (6.4) Invalid Application-ID or API key123AlgoliaSearchError: Invalid Application-ID or API key at success (C:\WorkSpaces\blog_test\node_modules\algoliasearch\src\AlgoliaSearchCore.js:351:32) at process._tickCallback (internal/process/next_tick.js:103:7) 首先看key对不对，applicationID 有的版本里是appId然后看value对不对，这个是自己申请的，复制就可以 (6.5) AlgoliaSearchError: Record at the position 84 objectID=xxxx is too big size=416696 bytes.1234567AlgoliaSearchError: Record at the position 84 objectID=xxxx is too big size=416696 bytes. Contact us if you need an extended quota at success (C:\blog\node_modules\algoliasearch\src\AlgoliaSearchCore.js:335:32) at process._tickCallback (internal/process/next_tick.js:103:7)AlgoliaSearchError: Record at the position 14 objectID=ad9c6969914d2c69736bd053a3d977846ffd3ac9 is too big size=159686 bytes. Contact us if you need an extended quota at success (C:\WorkSpaces\blog_test\node_modules\algoliasearch\src\AlgoliaSearchCore.js:351:32) at process._tickCallback (internal/process/next_tick.js:103:7) 博客的字数太多，文件太大，把文件大小大于41K(我测试的博客文件大小为41K时更新algolia成功的最大大小)的博客移动位置或者删除 (6.6) AlgoliaSearchRequestTimeoutError: Request timedout before getting a response12345AlgoliaSearchRequestTimeoutError: Request timedout before getting a response at Timeout.timeout [as _onTimeout] (C:\WorkSpaces\blog_test\node_modules\algoliasearch\src\server\builds\node.js:234:14) at ontimeout (timers.js:365:14) at tryOnTimeout (timers.js:237:5) at Timer.listOnTimeout (timers.js:207:5) 最后终于成功了1234567WKQ@WKQ-PC C:\WorkSpaces\blog_test&gt; hexo algolia --debug14:33:23.642 DEBUG Hexo version: 3.3.814:33:25.162 INFO [Algolia] Identified 1 posts to index.14:33:25.164 INFO [Algolia] Start indexing...14:33:28.255 INFO [Algolia] Indexing done. (6.7) ERROR Deployer not found: git hexo 更新到3.0之后，deploy的type 的github需要改成git npm install hexo-deployer-git --save ## (6.8) 给博客添加feed 安装hexo-generator-feed npm install hexo-generator-feed --save 配置到站点配置文件_config.yml 12345678910#Extensions##Plugins: http://hexo.io/plugins/#RSS订阅plugin:- hexo-generator-feed#Feed Atomfeed:type: atompath: atom.xmllimit: 20 最后，在你next主题下的_config.yml下，添加RSS订阅链接即可： rss: /atom.xml (6.9) 给博客生成一个站点地图 安装hexo-generator-seo-friendly-sitemap $ npm install hexo-generator-seo-friendly-sitemap --save 在站点配置文件_config.yml 中添加 12sitemap: path: sitemap.xml (6.10) 遇到的问题12npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@^1.0.0 (node_modules\chokidar\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.1.1: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;) 这不是错误，忽略就行 参考文章 It is a warning that fseventsd.its not an error.its a MAC OS Specified, running on windows or Linux show this warning,this the only reason it is showing this warning, you can skip it.. (6.11) bad indentation of a mapping entry1234YAMLException: bad indentation of a mapping entry at line 9, column 4: - xiecheng ^ at generateError (C:\blog\node_modules\js-yaml\lib\js-yaml\loader.js:165:10) (6.12) npm ERR! Unexpected end of JSON input while parsing near ‘…ttp”:”~0.0.2”},”_hasS’ npm cache clean --force (6.13) 设置Hexo不渲染.md或者.html 自己写的一些html或者谷歌百度认证的html不想让hexo编译，可以通过设置 skip_render 禁止hexo编译 在 source 文件夹下的所有 md 文件或者 html 文件都会被渲染，有时候我们不想这些文件被渲染怎么办？比如很多时候我们想要写一个 README.md 或者一些自定义的页面。比如百度或者谷歌在验证站长权限的时候，通常都会要求在主目录下添加一个 html 文件。 不渲染 md 文件 使用上面的办法虽然不会渲染 md 文件，但是还是将 md 文件转化成了 html 文件，如果想保留原 md 文件后缀要怎么做呢？这就需要在 站点配置文件 _config.yml 中配置，找到 skip_render 参数，开始匹配的位置是基于你的 source_dir 的，一般来说，是你的 source 文件夹下。下面我分别列举几种常见的情况进行说明： 修改博客根目录下 _config.yml ，在 skip_render 后添加以下内容。 注意：1. _config.yml 里有 skip_render，不需要再自己添加，否则会报 hexo skip_render YAMLException: duplicated mapping key 2. html里引的js文件也设置一下，禁止编译，否则会出现意想不到的错误。 3. html js 等 推荐使用UTF-8编码 12345678910skip_render: README.md# 单个文件夹下指定类型文件 - `test/*.md`# 单个文件夹下全部文件 - `test1/*.html`# 单个文件夹下全部文件以及子目录 - `test2/**` - &quot;README.md&quot; - &quot;file/**&quot; - &quot;*.html&quot; 在不想被渲染的 html 文件最上面添加如下代码123---layout: false--- (7) 调优 最近这几次执行hexo generate的时候发现特别慢，特别费时间，有一次generate用了20多分钟。 感觉不正常，今天正好有时间来找一下原因。 使用debug来调试，如下 hexo generate --debug 结果发现有2个原因 有一篇博客了一行有上千字，导致generate的时生成对应的index.html耗时较长，生成这个页面用了6min44s 图片里有一张test.svg的矢量图，有1.48M，特别耗时，用了18s 生成index.html 和 page/2/index.html的时候，中间花了6min53s，正好那篇博客(一行有上千字)在这两个页面里 把上千行文字删除一部分，剩50 - 200字左右就可以了把test.svg删掉 优化前1212:29:12.409 INFO 481 files generated in 14 min12:29:12.750 DEBUG Database saved 优化后1214:46:15.546 INFO 483 files generated in 33 s14:46:15.741 DEBUG Database saved wordcount也比较费时，嫌时间长可以关掉 (7.2) 优化Mathjax 加载Mathjax的过程很费时间，根据next主题的默认写法，即使在网页中并没有生成公式时, 也会加载最基本 MathJax.js。为解决此问题，可根据 这篇文章 的做法，只有在用到公式的页面才加载 Mathjax。 References[1] 零基础免费搭建个人博客-hexo+github[2] Hexo + Github Pages 搭建个人博客[3] hexo多主题切换[4] 为Hexo的Next主题增加畅言评论的支持[5] Next主题官方文档[6] Hexo 4：【高阶】NexT 主题优化之加入网易云音乐、网易云跟帖、动态背景、自定义主题、统计功能[7] 在文章底部增加版权信息[8] 网易云跟帖[9] Hexo+Next主题集成Algolia搜索[10] Hexo博客添加站内搜索[11] 完美替代多说-gitment[12] hexo下新建页面下如何放多个文章[13] Hexo系列教程之三：next主题的配置和优化[14] 分类和标签不能显示(点击)[15] 无法生成categories和tags的页面[16] Hexo博客优化[17] 2017年最新基于hexo搭建个人免费博客——从零开始[18] Valine – 一款极简的评论系统[19] Hexo的NexT主题个性化：添加文章阅读量[20] github pages[21] 绑定自定义域名[22] HEXO+Github,搭建属于自己的博客[23] hexo官方中文资料[24] AlgoliaSearchError: Please provide an API key. Update index fails[25] hexo-algolia[26] 配置algolia，跟新algolia失败 #1293[27] 5.1.0使用algolia搜索问题 #1571[28] 2300-algolia-record-too-big [29] Algolia error when my index is updated #887[30] 2300-algolia-record-too-big[31] 为Github的Hexo博客启用SSL/TLS[32] 给hexo加上HTTPS[33] issues iissnan/hexo-theme-next[34] 如何让 NexT 不默认启用 Mathjax?[35] load mathjax only when mathjax is needed[36] Hexo博客优化加载速度[37] TypeError: Cannot set property ‘lastIndex’ of undefined[38] 加速访问基于 Github Pages 和 Hexo 的博客[39] Hexo不渲染.md或者.html]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot 笔记]]></title>
    <url>%2F2017%2F03%2F06%2Fspringboot-note%2F</url>
    <content type="text"><![CDATA[(1) SpringBoot以JSON格式返回对象1234567891011121314151617&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-core&lt;/artifactId&gt; &lt;version&gt;$&#123;jackson.version&#125;&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;$&#123;jackson.version&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;version&gt;$&#123;jackson.version&lt;/version&gt;&lt;/dependency&gt; (2) springboot hot deploy 添加对应jar包 添加插件 配置idea mvn dependencies 添加以下依赖 1234567&lt;!-- add hot deployment --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; build的时候添加以下plug 12345678&lt;plugin&gt; &lt;!-- hot deployment config --&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;fork&gt;true&lt;/fork&gt; &lt;/configuration&gt;&lt;/plugin&gt; 配置idea 3.1 File -&gt; Settings -&gt; Compiler 勾选 Build Project automatically 3.2 双击 Shift，输入 registry , 点击 Registry.. ，勾上 compiler.automake.allow.when.app.running Allow auto-make to start even if developed application is currently running. Note that automatically started make may eventually delete some classes that are required by the application. References[1] Spring Boot Reference Documentation[2] using-spring-boot[3] spring-boot[4] spring.io[5] Spring Boot 学习笔记：以JSON格式返回对象 [6] springboot+idea热部署(自动刷新)[7] spring-boot 速成(2) devtools之热部署及LiveReload]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 入门]]></title>
    <url>%2F2017%2F03%2F05%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment 测试$x^{y^z}=(1+{\rm e}^x)^{-2xy^w}$]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 spring 遇到的错误]]></title>
    <url>%2F2017%2F02%2F28%2Fspring-error%2F</url>
    <content type="text"><![CDATA[(1) NoSuchBeanDefinitionException NoSuchBeanDefinitionException: No qualifying bean of type found for dependency: Spring NoSuchBeanDefinitionException原因分析 spring容器无法注入service（No qualifying bean of type） 忘记写 @Service @Component 如果是用xml配置的，确定下bean的name是否正确，配置是否完整， 如果是用注解，确定下你xml配置里面的扫描包是否扫描到了类所在的包 (2) org.apache.ibatis.binding.BindingException org.apache.ibatis.binding.BindingException: Invalid bound statement (not found) xml文件里方法名写错了，对应不上 (3) org.apache.ibatis.builder.BuilderException1Caused by: org.apache.ibatis.builder.BuilderException: Error creating document instance. Cause: org.xml.sax.SAXParseException; lineNumber: 18; columnNumber: 4; 文件提前结束。 xml里有错误 (4) org.springframework.beans.factory.BeanCreationException123Caused by: org.springframework.beans.factory.BeanCreationException: Could not autowire field: private ; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No qualifying bean of type [] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: &#123;@org.springframework.beans.factory.annotation.Autowired(required=true)&#125;Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder &apos;jdbc_url&apos; in string value &quot;$&#123;jdbc_url&#125;&quot; (5) org.springframework.jdbc.BadSqlGrammarException12org.springframework.jdbc.BadSqlGrammarException: Error querying database. Cause: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &apos;.contact, (6) failure in bulk execution1&quot;failure in bulk execution:\n[270]: index [dev_20180802], type [docs], id [11456], message [ElasticsearchException[Elasticsearch exception [type=mapper_parsing_exception, reason=failed to parse [ucAddress.zipcode]]]; nested: ElasticsearchException[Elasticsearch exception [type=number_format_exception, reason=For input string: \&quot;３１００００\&quot;]];]&quot; (7) com.alibaba.otter.canal.server.exception.CanalServerException1exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination: should start first 名字配置错了 (8) Required request body is missing1org.springframework.web.servlet.mvc.support.DefaultHandlerExceptionResolver.handleHttpMessageNotReadable | Failed to read HTTP message: org.springframework.http.converter.HttpMessageNotReadableException: Required request body is missing 使用GET请求，没有传输body信息。但是加了@RequestBody 使用@RequestBody，POST请求没传body信息 @RequestBody 异常：Required request body is missing]]></content>
      <categories>
        <category>java</category>
      </categories>
  </entry>
</search>
